{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# packages\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# custom code\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from utils import load_word_to_ipa, load_pairs\n",
    "from atp import ATP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Development\n",
    "\n",
    "The following cells generate Figure 1 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_children = 100\n",
    "\n",
    "ing = [0] * 20\n",
    "s = [0] * 20\n",
    "d = [0] * 20\n",
    "\n",
    "word_to_ipa = load_word_to_ipa()\n",
    "\n",
    "for child in range(num_children):\n",
    "    if child % 10 == 0:\n",
    "        print(f'==== Child {child} ====')\n",
    "    vocabs = list(glob.glob(f'../data/english/growth/child-{child}/*.txt'))\n",
    "    vocabs = sorted(vocabs, key=lambda it: int(it.split('/')[-1].split('.txt')[0]))\n",
    "    for i, vocab in enumerate(vocabs):\n",
    "        pairs, feature_space = load_pairs(vocab, sep=' ', preprocessing=lambda s: word_to_ipa[s])\n",
    "        atp = ATP(feature_space=feature_space, apply_phonology=True)\n",
    "        atp.train(pairs)\n",
    "        \n",
    "        suffixes = set()\n",
    "        for leaf in atp.get_leaves():\n",
    "            if leaf.switch_statement.productive:\n",
    "                suffix = leaf.switch_statement.default_case.name.split('lemma')[-1].replace(' + ', '')\n",
    "                suffixes.add(suffix)\n",
    "        if 'z' in suffixes and 's' in suffixes:\n",
    "            suffixes.discard('z')\n",
    "        if 't' in suffixes and 'd' in suffixes:\n",
    "            suffixes.discard('t')\n",
    "        for suffix in suffixes:\n",
    "            if suffix == 'ɪŋ':\n",
    "                ing[i] += 1\n",
    "            if suffix in {'d', 't'}:\n",
    "                d[i] += 1\n",
    "            if suffix in {'s', 'z'}:\n",
    "                s[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grays = sns.color_palette('Greys')\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "fontsize=40\n",
    "x = np.arange(50, 1050, 50)\n",
    "Z = num_children\n",
    "\n",
    "plt.plot(x, np.asarray(ing) / Z, '-o',  markersize=24, linewidth=12, color=grays[-4], label='-ing')\n",
    "plt.plot(x, np.asarray(s) / Z, '-o',  markersize=24, linewidth=12, color=grays[-2], linestyle=':', label='-s')\n",
    "plt.plot(x, np.asarray(d) / Z, '-o',  markersize=24, linewidth=12, color='black', label='-ed')\n",
    "\n",
    "plt.legend(fontsize=fontsize + 6)\n",
    "plt.xticks(np.arange(50, 1050, 150), fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "plt.ylabel('Fraction Children with Rule', fontsize=fontsize)\n",
    "plt.xlabel('Child\\'s Vocabulary Size', fontsize=fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German Development\n",
    "\n",
    "The following cells generate Figure 1 (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seeds = 100\n",
    "train_sizes = [50, 100, 200, 300, 400]\n",
    "\n",
    "en = [0] * len(train_sizes)\n",
    "n = [0] * len(train_sizes)\n",
    "e = [0] * len(train_sizes)\n",
    "null = [0] * len(train_sizes)\n",
    "er = [0] * len(train_sizes)\n",
    "s = [0] * len(train_sizes)\n",
    "\n",
    "for i, size in enumerate(train_sizes):\n",
    "    for seed in range(num_seeds):\n",
    "        fname = f'../data/german/growth/train{size}_{seed}.txt'\n",
    "        pairs, feature_space = load_pairs(fname)\n",
    "        atp = ATP(feature_space=feature_space)\n",
    "        atp.train(pairs)\n",
    "\n",
    "        suffixes = set()\n",
    "        for leaf in atp.get_leaves():\n",
    "            if leaf.switch_statement.productive:\n",
    "                suffix = leaf.switch_statement.default_case.name.split('lemma')[-1].replace(' + ', '')\n",
    "                suffixes.add(suffix)\n",
    "        for suffix in suffixes:\n",
    "            if suffix == 'en':\n",
    "                en[i] += 1\n",
    "            if suffix == 'e':\n",
    "                e[i] += 1\n",
    "            if suffix == 'n':\n",
    "                n[i] += 1\n",
    "            if suffix == '':\n",
    "                null[i] += 1\n",
    "            if suffix == 'er':\n",
    "                er[i] += 1\n",
    "            if suffix == 's':\n",
    "                s[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "fontsize=40\n",
    "x = train_sizes\n",
    "Z = num_seeds\n",
    "lw = 14\n",
    "ms = 26\n",
    "\n",
    "ca = grays\n",
    "\n",
    "plt.plot(x, np.asarray(n) / Z, '-o', markersize=ms, linewidth=lw,    color=ca[1],  linestyle='-',  label='-n')\n",
    "plt.plot(x, np.asarray(null) / Z, '-o', markersize=ms, linewidth=lw, color=ca[1],  linestyle=':',  label='-∅')\n",
    "plt.plot(x, np.asarray(e) / Z, '-o',  markersize=ms, linewidth=lw,   color=ca[2],  linestyle='-',  label='-e')\n",
    "plt.plot(x, np.asarray(en) / Z, '-o', markersize=ms, linewidth=lw,   color=ca[3],  linestyle=':',  label='-en')\n",
    "plt.plot(x, np.asarray(er) / Z, '-o',  markersize=ms, linewidth=lw,  color=ca[4],  linestyle='-',  label='-er')\n",
    "plt.plot(x, np.asarray(s) / Z, '-o',  markersize=ms, linewidth=lw,   color=ca[5],  linestyle=':',  label='-s')\n",
    "\n",
    "plt.legend(fontsize=fontsize - 1, loc=(0.75, 0.21), facecolor='white', framealpha=1.0)\n",
    "\n",
    "plt.xticks([100, 200, 300, 400], fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "plt.ylabel('Fraction Children with Rule', fontsize=fontsize)\n",
    "plt.xlabel('Child\\'s Vocabulary Size', fontsize=fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Performance\n",
    "\n",
    "The following cells generate Figure 2 (a)\n",
    "\n",
    "While ATP only takes a moment to run on all the datasets, the ED model takes hours. Thus, we load the saved results for that model, rather than running it from scratch.\n",
    "\n",
    "See https://github.com/cbelth/ATP-morphology/blob/main/notebooks/ED-English.ipynb for the notebook used to train the ED model (courtesy of Deniz Beser). Note that there is substantially more setup to re-run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printable_acc(accs, r=2):\n",
    "    return list(np.round(np.mean(accs, axis=0), r))\n",
    "\n",
    "num_seeds = 10\n",
    "train_sizes = [100, 200, 400, 600, 800, 1000]\n",
    "\n",
    "# accs arrays\n",
    "test_accs = np.zeros((num_seeds, len(train_sizes)))\n",
    "    \n",
    "# irregulars stuff (for U-shape curve)\n",
    "seed_to_seen_irregulars = defaultdict(set)\n",
    "seed_to_irregulars_currently_covered = defaultdict(dict)\n",
    "train_irregular_accs = np.zeros((num_seeds, len(train_sizes)))\n",
    "\n",
    "for col, size in enumerate(train_sizes):\n",
    "    row = 0\n",
    "    for seed in range(num_seeds):\n",
    "        train_path = f'../data/english/quant/unimorph_celex0_train{size}_{seed}.txt'\n",
    "        test_path = train_path.replace(f\"{train_path.split('_')[-2]}\", 'test')\n",
    "        pairs, feature_space = load_pairs(train_path)\n",
    "        atp = ATP(feature_space=feature_space)\n",
    "        atp.train(pairs)\n",
    "\n",
    "        # compute accuracy\n",
    "        test_accs[row, col] = atp.accuracy(load_pairs(test_path)[0])\n",
    "                \n",
    "        ''' begin irrgular performance '''\n",
    "        # all the irregulars previously seen during training\n",
    "        for pair in pairs:\n",
    "            lemma, inflected, _ = pair\n",
    "            if inflected != f'{lemma}d' and inflected != f'{lemma}t' and inflected != f'{lemma}ɪd':\n",
    "                seed_to_seen_irregulars[seed].add(pair)\n",
    "\n",
    "        irr_c = 0\n",
    "        irr_t = 0\n",
    "        for word in seed_to_seen_irregulars[seed]:\n",
    "            lemma_ipa, _, feats = word\n",
    "            leaf = atp.probe(lemma_ipa, feats)\n",
    "            if leaf.switch_statement.productive and (word not in seed_to_irregulars_currently_covered[seed] or not seed_to_irregulars_currently_covered[seed][word]):\n",
    "                seed_to_irregulars_currently_covered[seed][word] = True\n",
    "            else:\n",
    "                irr_c += 1\n",
    "            irr_t += 1\n",
    "        train_irregular_accs[row, col] = irr_c / irr_t\n",
    "        ''' end irrgular performance '''\n",
    "            \n",
    "        row += 1\n",
    "\n",
    "    print(f'Test = {printable_acc(test_accs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "fontsize=40\n",
    "\n",
    "x = train_sizes\n",
    "\n",
    "atp_color = 'black'\n",
    "ed_color = 'gray'\n",
    "\n",
    "atp_accs = np.mean(test_accs, axis=0)\n",
    "atp_stddevs = np.std(test_accs, axis=0)\n",
    "\n",
    "u_shape_avg = np.mean(train_irregular_accs, axis=0)\n",
    "u_shape_std = np.std(train_irregular_accs, axis=0)\n",
    "\n",
    "# load ED results\n",
    "df = pd.read_csv('../data/english/ed-output.csv')\n",
    "nn_res = np.zeros((10, 6))\n",
    "for seed in range(10):\n",
    "    for size_idx, size in enumerate(train_sizes):\n",
    "        nn_res[seed, size_idx] = float(df[(df['split'] == seed) & (df['datasize'] == size)]['test'])\n",
    "nn_accs = np.mean(nn_res, axis=0)\n",
    "nn_stddevs = np.std(nn_res, axis=0)\n",
    "\n",
    "\n",
    "# ATP\n",
    "plt.plot(x, atp_accs, '-o', markersize=22, linewidth=10, color=atp_color, label='ATP')\n",
    "plt.plot(x, atp_accs, marker='X', markersize=30, color=atp_color)\n",
    "plt.fill_between(x, \n",
    "                 np.asarray(atp_accs) - np.asarray(atp_stddevs), \n",
    "                 np.asarray(atp_accs) + np.asarray(atp_stddevs), color='gray', alpha=0.2)\n",
    "\n",
    "plt.plot(x, nn_accs, '-o', markersize=24, linewidth=10, color=ed_color, label='ED')\n",
    "plt.fill_between(x, \n",
    "                 np.asarray(nn_accs) - np.asarray(nn_stddevs), \n",
    "                 np.asarray(nn_accs) + np.asarray(nn_stddevs), color='gray', alpha=0.2)\n",
    "\n",
    "plt.plot(x, u_shape_avg, '-o', markersize=22, linewidth=10, color=atp_color, linestyle=':', label='ATP Irr.')\n",
    "plt.fill_between(x, \n",
    "                 np.asarray(u_shape_avg) - np.asarray(u_shape_std), \n",
    "                 np.asarray(u_shape_avg) + np.asarray(u_shape_std), color='gray', alpha=0.2)\n",
    "\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.xticks([100, 250, 500, 750, 1000], fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "plt.ylabel('Accuracy', fontsize=fontsize)\n",
    "plt.xlabel('Training Size', fontsize=fontsize)\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German Performance\n",
    "\n",
    "The following cells generate Figure 2 (b)\n",
    "\n",
    "As for English, the ED model takes hours to run, so we load the saved results for that model.\n",
    "\n",
    "See https://github.com/cbelth/ATP-morphology/blob/main/notebooks/ED-German.ipynb for the notebook used to train the ED model (courtesy of Deniz Beser). Note that there is substantially more setup to re-run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seeds = 25\n",
    "train_sizes = [60, 120, 180, 240, 300, 360]\n",
    "\n",
    "# accs arrays\n",
    "test_accs = np.zeros((num_seeds, len(train_sizes)))\n",
    "test_no_gen_accs = np.zeros((num_seeds, len(train_sizes)))\n",
    "\n",
    "for col, size in enumerate(train_sizes):\n",
    "    row = 0\n",
    "    for seed in range(num_seeds):\n",
    "        train_path = f'../data/german/quant/train{size}_{seed}.txt'\n",
    "        test_path = train_path.replace(f'train{size}', 'test')\n",
    "        pairs, feature_space = load_pairs(train_path)\n",
    "        atp = ATP(feature_space=feature_space)\n",
    "        atp.train(pairs)\n",
    "\n",
    "        # compute accuracies\n",
    "        test_accs[row, col] = atp.accuracy(load_pairs(test_path)[0])\n",
    "        test_no_gen_accs[row, col] = atp.accuracy(load_pairs(test_path)[0], no_feats=True)\n",
    "                \n",
    "        row += 1\n",
    "\n",
    "    print(f'Test = {list(np.round(np.mean(test_accs, axis=0), 2))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "fontsize=40\n",
    "\n",
    "x = train_sizes\n",
    "\n",
    "grays = sns.color_palette('Greys')\n",
    "\n",
    "atp_accs = np.mean(test_accs, axis=0)\n",
    "atp_stddevs = np.std(test_accs, axis=0)\n",
    "\n",
    "atp_no_gen_accs = np.mean(test_no_gen_accs, axis=0)\n",
    "\n",
    "# load ED results\n",
    "df = pd.read_csv('../data/german/ed-output.csv')\n",
    "df = df[(df['test_subset'] == 'All') & (df['condition'] == 'regular_test')]\n",
    "nn_res = np.zeros((num_seeds, len(train_sizes)))\n",
    "for seed in range(num_seeds):\n",
    "    for size_idx, size in enumerate(train_sizes):\n",
    "        nn_res[seed, size_idx] = float(df[(df['split'] == seed) & (df['datasize'] == size)]['acc'])\n",
    "nn_accs = np.mean(nn_res, axis=0)\n",
    "nn_stddevs = np.std(nn_res, axis=0)\n",
    "\n",
    "df = pd.read_csv('../data/german/ed-output.csv')\n",
    "df = df[(df['test_subset'] == 'All') & (df['condition'] == 'genderless_test')]\n",
    "nn_res_no_gen = np.zeros((num_seeds, len(train_sizes)))\n",
    "for seed in range(num_seeds):\n",
    "    for size_idx, size in enumerate(train_sizes):\n",
    "        nn_res_no_gen[seed, size_idx] = float(df[(df['split'] == seed) & (df['datasize'] == size)]['acc'])\n",
    "nn_accs_no_gen = np.mean(nn_res_no_gen, axis=0)\n",
    "nn_stddevs_no_gen = np.std(nn_res_no_gen, axis=0)\n",
    "\n",
    "atp_color = 'black'\n",
    "ed_color = 'gray'\n",
    "\n",
    "# no gender\n",
    "plt.plot(x, atp_no_gen_accs, '-o', markersize=24, linewidth=10, color=grays[-1], linestyle='dashed', alpha=1.0)\n",
    "plt.plot(x, nn_accs_no_gen, '-o', markersize=24, linewidth=10, color=grays[-3], linestyle='dashed', alpha=1.0)\n",
    "\n",
    "# ATP\n",
    "plt.plot(x[4:], atp_accs[4:], '-o', markersize=24, linewidth=10, color=atp_color, label='ATP')\n",
    "plt.plot(x, atp_accs, '-', markersize=24, linewidth=10, color=atp_color)\n",
    "plt.plot(x[:4], atp_accs[:4], marker='X', markersize=26, color=atp_color)\n",
    "plt.fill_between(x, \n",
    "                 np.asarray(atp_accs) - np.asarray(atp_stddevs),\n",
    "                 np.asarray(atp_accs) + np.asarray(atp_stddevs), color='gray', alpha=0.2)\n",
    "\n",
    "# ED\n",
    "plt.plot(x, nn_accs, '-o', markersize=24, linewidth=10, color=ed_color, label='ED')\n",
    "plt.fill_between(x, \n",
    "                 np.asarray(nn_accs) - np.asarray(nn_stddevs), \n",
    "                 np.asarray(nn_accs) + np.asarray(nn_stddevs), color='gray', alpha=0.2)\n",
    "\n",
    "plt.plot(x, atp_accs, '-', markersize=24, linewidth=10, color=dt_color)\n",
    "plt.plot(x[4:], atp_accs[4:], '-o', markersize=24, linewidth=10, color=dt_color)\n",
    "\n",
    "plt.legend(fontsize=fontsize, loc='upper left')\n",
    "plt.xticks(train_sizes, fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "plt.ylabel('Test Accuracy', fontsize=fontsize)\n",
    "plt.xlabel('Training Size', fontsize=fontsize)\n",
    "plt.ylim([0.3, 1.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German Wug test\n",
    "\n",
    "The following cells generate Table 1. While ATP is deterministic, the numbers may differ some due to arbitrariness at runtime, such as hashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/german/german_wug.csv')\n",
    "df = df[df['Q_Task'] == 'production'].fillna('')\n",
    "\n",
    "wugs = set()\n",
    "wug_to_child_preds = defaultdict(list)\n",
    "wug_to_rhyme = dict()\n",
    "suffixes = set()\n",
    "for wug, child_pred, suffix, rhyme in zip(df['Q_Lemma'], df['Q_PL_Form'], df['Q_PL_Label'], df['Q_Lemma_HasRhymes']):\n",
    "    wug, child_pred = wug.capitalize(), child_pred.capitalize()\n",
    "    wug = wug.replace(u'ä', 'a').replace(u'ü', 'u').replace(u'ö', 'o').replace(u'Ä', 'A').replace(u'Ü', 'U').replace(u'Ö', 'O')\n",
    "    child_pred = child_pred.replace(u'ä', 'a').replace(u'ü', 'u').replace(u'ö', 'o').replace(u'Ä', 'A').replace(u'Ü', 'U').replace(u'Ö', 'O')\n",
    "    \n",
    "    wugs.add(wug)\n",
    "    suffix = suffix.replace('umlaut_', '')\n",
    "    if suffix == 'umlaut':\n",
    "        suffix = 'zero'\n",
    "        assert(child_pred == wug)\n",
    "    wug_to_child_preds[wug].append(suffix)\n",
    "    suffixes.add(suffix)\n",
    "    \n",
    "    if suffix == 'en':\n",
    "        assert(child_pred[len(wug):] == 'en')\n",
    "        \n",
    "    if child_pred == f'{wug}n':\n",
    "        print(child_pred, wug)\n",
    "        \n",
    "    wug_to_rhyme[wug] = rhyme\n",
    "    \n",
    "wugs = sorted(wugs)\n",
    "    \n",
    "wug_to_TP_preds_CHILDES_N = defaultdict(list)\n",
    "wug_to_TP_preds_CHILDES_ques = defaultdict(list)\n",
    "\n",
    "def update(result_dict, model_pred, wug):\n",
    "    suffix = model_pred[len(wug):]\n",
    "    if suffix == '':\n",
    "        result_dict[wug].append('zero')\n",
    "    elif suffix == 'n':\n",
    "        result_dict[wug].append('en')\n",
    "    elif suffix in suffixes:\n",
    "        result_dict[wug].append(suffix)\n",
    "    else:\n",
    "        result_dict[wug].append('other')\n",
    "\n",
    "for i in range(500):\n",
    "    _pairs, feature_space = load_pairs(f'../data/german/growth/train400_{i}.txt')\n",
    "    atp = ATP(feature_space=feature_space)\n",
    "    atp.train(_pairs)\n",
    "    for wug in wugs:\n",
    "        update(wug_to_TP_preds_CHILDES_N, atp.inflect(wug, ('N',)), wug)\n",
    "        update(wug_to_TP_preds_CHILDES_ques, atp.inflect_no_feat(wug, ()), wug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(preds):\n",
    "    dist = defaultdict(int)\n",
    "    for p in preds:\n",
    "        dist[p] += 1\n",
    "    return dict(dist)\n",
    "\n",
    "models = {'CHILDES_N': wug_to_TP_preds_CHILDES_N, \n",
    "          'CHILDES_?': wug_to_TP_preds_CHILDES_ques}\n",
    "\n",
    "model_res = dict()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    suffix_to_percentage_R = defaultdict(int)\n",
    "    suffix_to_percentage_NR = defaultdict(int)\n",
    "    pearsons = dict()\n",
    "    Z_R = 0\n",
    "    Z_NR = 0\n",
    "\n",
    "    for s in suffixes:\n",
    "        child_p = list()\n",
    "        model_p = list()\n",
    "        for wug in wugs:\n",
    "            child_dist = get_dist(wug_to_child_preds[wug])\n",
    "            model_dist = get_dist(model[wug])\n",
    "            child_p.append((child_dist[s] / sum(child_dist.values()) if s in child_dist else 0))\n",
    "            model_p.append((model_dist[s] / sum(model_dist.values()) if s in model_dist else 0))\n",
    "\n",
    "            if s in model_dist:\n",
    "                if wug_to_rhyme[wug]:\n",
    "                    suffix_to_percentage_R[s] += model_dist[s]\n",
    "                    Z_R += model_dist[s]\n",
    "                else:\n",
    "                    suffix_to_percentage_NR[s] += model_dist[s]\n",
    "                    Z_NR += model_dist[s]\n",
    "                    \n",
    "        rho = spearmanr(child_p, model_p)\n",
    "        if rho.pvalue < 0.05:\n",
    "            pearsons[s] = f'{round(rho.correlation, 2)}*'\n",
    "        else:\n",
    "            pearsons[s] = rho.correlation\n",
    "            \n",
    "    for s in suffix_to_percentage_R:\n",
    "        suffix_to_percentage_R[s] /= Z_R\n",
    "\n",
    "    for s in suffix_to_percentage_NR:\n",
    "        suffix_to_percentage_NR[s] /= Z_NR\n",
    "        \n",
    "    model_res[model_name] = {'R': dict(suffix_to_percentage_R), \n",
    "                             'NR': dict(suffix_to_percentage_NR), \n",
    "                             'rho': dict(pearsons)}\n",
    "\n",
    "metrics = ['R', 'NR', 'rho']\n",
    "\n",
    "def get_res_for_suffix(suf):\n",
    "    return list(model_res['CHILDES_N'][metric][suf] if suf in model_res['CHILDES_N'][metric] else '' for metric in metrics) + list(model_res['CHILDES_?'][metric][suf] if suf in model_res['CHILDES_?'][metric] else '' for metric in metrics)\n",
    "\n",
    "def pretty(vals):\n",
    "    return list(round(v, 2) if type(v) is not str else v for v in vals)\n",
    "\n",
    "en_row = pretty(get_res_for_suffix('en'))\n",
    "e_row = pretty(get_res_for_suffix('e'))\n",
    "zero_row = pretty(get_res_for_suffix('zero'))\n",
    "er_row = pretty(get_res_for_suffix('er'))\n",
    "s_row = pretty(get_res_for_suffix('s'))\n",
    "other_row = pretty(get_res_for_suffix('other'))\n",
    "\n",
    "pd.DataFrame.from_dict({'': ['%R', '%NR', 'rho', '%R', '%NR', 'rho'],\n",
    "                        '-(e)n': en_row, \n",
    "                        '-e': e_row,\n",
    "                        '-zero': zero_row,\n",
    "                        '-er': er_row,\n",
    "                        '-s': s_row,\n",
    "                        'other': other_row},\n",
    "                       columns=['', '', 'A', '', 'D', ''],\n",
    "                       orient='index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
