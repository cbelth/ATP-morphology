{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLHSxL9lDr9J"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uLQ18rZUDlS5",
    "outputId": "84ad5664-e86a-4abb-b818-61439be3d737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "fatal: destination path 'OpenNMT-py' already exists and is not an empty directory.\n",
      "Requirement already satisfied: OpenNMT-py in /usr/local/lib/python3.6/dist-packages (2.0.1)\n",
      "Requirement already satisfied: flask==1.1.2 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (2.4.1)\n",
      "Requirement already satisfied: pyyaml==5.3.1 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (5.3.1)\n",
      "Requirement already satisfied: pyonmttok<2,>=1.23; platform_system == \"Linux\" or platform_system == \"Darwin\" in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.23.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.51 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (4.56.0)\n",
      "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.6.0+cu101)\n",
      "Requirement already satisfied: torchtext==0.5.0 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (0.5.0)\n",
      "Requirement already satisfied: waitress==1.4.4 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.4.4)\n",
      "Requirement already satisfied: configargparse<2,>=1.2.3 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.2.3)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask==1.1.2->OpenNMT-py) (1.0.1)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask==1.1.2->OpenNMT-py) (7.1.2)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask==1.1.2->OpenNMT-py) (2.11.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask==1.1.2->OpenNMT-py) (1.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (0.4.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (0.10.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.32.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.19.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (53.0.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.24.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (0.36.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (3.12.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (3.3.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (2.23.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.8.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->OpenNMT-py) (0.16.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0->OpenNMT-py) (0.1.95)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask==1.1.2->OpenNMT-py) (1.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3->OpenNMT-py) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (4.7)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (4.2.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py) (3.4.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3->OpenNMT-py) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3->OpenNMT-py) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3->OpenNMT-py) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3->OpenNMT-py) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3->OpenNMT-py) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py) (3.7.4.3)\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.6.0+cu101 in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
      "Requirement already satisfied: torchvision==0.7.0+cu101 in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0+cu101) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0+cu101) (1.19.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.7.0+cu101) (7.0.0)\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!git clone -b legacy https://github.com/OpenNMT/OpenNMT-py\n",
    "!pip install OpenNMT-py\n",
    "import os\n",
    "outdir = 'drive/MyDrive/GermanToleranceBaselineCogSci/output'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EB3U9arVEhGX"
   },
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM5mepJlvmQp"
   },
   "outputs": [],
   "source": [
    "datasizes = [f'{s}_{i}' for s in ['60','120','180','240','300','360'] for i in range(25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfvT6eBRSdTj"
   },
   "outputs": [],
   "source": [
    "def remove_umlaut(string):\n",
    "    u = 'ü'.encode()\n",
    "    U = 'Ü'.encode()\n",
    "    a = 'ä'.encode()\n",
    "    A = 'Ä'.encode()\n",
    "    o = 'ö'.encode()\n",
    "    O = 'Ö'.encode()\n",
    "    ss = 'ß'.encode()\n",
    "\n",
    "    string = string.encode()\n",
    "    string = string.replace(u, b'u')\n",
    "    string = string.replace(U, b'U')\n",
    "    string = string.replace(a, b'a')\n",
    "    string = string.replace(A, b'A')\n",
    "    string = string.replace(o, b'o')\n",
    "    string = string.replace(O, b'O')\n",
    "\n",
    "    string = string.decode('utf-8')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jtghj7YcDtxt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import operator\n",
    "random.seed(1)\n",
    "\n",
    "def preprocess_line(line):\n",
    "  tokens = line.split()\n",
    "  source = remove_umlaut(tokens[0]).lower()\n",
    "  target = remove_umlaut(tokens[1]).lower()\n",
    "  gender = f'{tokens[2]}'\n",
    "  if gender == 'M':\n",
    "    gender = 'MAS'\n",
    "  elif gender == 'N':\n",
    "    gender = 'NTR'\n",
    "  elif gender == 'F':\n",
    "    gender = 'FEM'\n",
    "  else:\n",
    "    raise RuntimeError\n",
    "  return f'{gender} <s> {\" \".join(source)}',  f'{\" \".join(target)}'\n",
    "\n",
    "\n",
    "def preprocess_line_genderless(line):\n",
    "  tokens = line.split()\n",
    "  source = remove_umlaut(tokens[0]).lower()\n",
    "  target = remove_umlaut(tokens[1]).lower()\n",
    "  return f'{\" \".join(source)}',  f'{\" \".join(target)}'\n",
    "\n",
    "\n",
    "for datasize in datasizes:\n",
    "  new_data_path = f'drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/{datasize}/'\n",
    "  if not os.path.exists(new_data_path):\n",
    "    os.makedirs(new_data_path)\n",
    "\n",
    "\n",
    "  with open(f'drive/MyDrive/GermanToleranceBaselineCogSci/raw_data/train{datasize}.txt','r') as raw_file:\n",
    "    with open(f'{new_data_path}german-src-train.txt','w') as src_file:\n",
    "      with open(f'{new_data_path}german-tgt-train.txt','w') as tgt_file:\n",
    "        lines = raw_file.readlines()\n",
    "        random.shuffle(lines)\n",
    "        for line in lines:\n",
    "          src, tgt = preprocess_line(line)\n",
    "          print(src, file=src_file)\n",
    "          print(tgt, file=tgt_file)\n",
    "           \n",
    "  with open(f'drive/MyDrive/GermanToleranceBaselineCogSci/raw_data/dev_{datasize.split(\"_\")[1]}.txt','r') as raw_file:\n",
    "    with open(f'{new_data_path}german-src-val.txt','w') as src_file:\n",
    "      with open(f'{new_data_path}german-tgt-val.txt','w') as tgt_file:\n",
    "        lines = raw_file.readlines()\n",
    "        random.shuffle(lines)\n",
    "        for line in lines:\n",
    "          src, tgt = preprocess_line(line)\n",
    "          print(src, file=src_file)\n",
    "          print(tgt, file=tgt_file)\n",
    "  \n",
    "  with open(f'drive/MyDrive/GermanToleranceBaselineCogSci/raw_data/test_{datasize.split(\"_\")[1]}.txt','r') as raw_file:\n",
    "    with open(f'{new_data_path}german-src-test.txt','w') as src_file:\n",
    "      with open(f'{new_data_path}german-tgt-test.txt','w') as tgt_file:\n",
    "        lines = raw_file.readlines()\n",
    "        random.shuffle(lines)\n",
    "        for line in lines:\n",
    "          src, tgt = preprocess_line(line)\n",
    "          print(src, file=src_file)\n",
    "          print(tgt, file=tgt_file)\n",
    "\n",
    "  with open(f'drive/MyDrive/GermanToleranceBaselineCogSci/raw_data/test_{datasize.split(\"_\")[1]}.txt','r') as raw_file:\n",
    "    with open(f'{new_data_path}german-src-test-genderless.txt','w') as src_file:\n",
    "      with open(f'{new_data_path}german-tgt-test-genderless.txt','w') as tgt_file:\n",
    "        lines = raw_file.readlines()\n",
    "        random.shuffle(lines)\n",
    "        for line in lines:\n",
    "          src, tgt = preprocess_line_genderless(line)\n",
    "          print(src, file=src_file)\n",
    "          print(tgt, file=tgt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ManprMYhPQ3x"
   },
   "source": [
    "## Preprocess data for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qrvYjSvrPP4I",
    "outputId": "d94e218e-2b4b-45f4-b1a0-a4f81551125c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-02-03 00:12:12,146 INFO] Extracting features...\n",
      "[2021-02-03 00:12:12,148 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:12,148 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:12,148 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:12,148 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:12,159 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:12,163 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_0/processed.train.0.pt.\n",
      "[2021-02-03 00:12:12,259 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:12,259 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:12,264 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:12,277 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:12,278 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_0/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:13,287 INFO] Extracting features...\n",
      "[2021-02-03 00:12:13,289 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:13,289 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:13,289 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:13,289 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:13,299 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:13,303 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_1/processed.train.0.pt.\n",
      "[2021-02-03 00:12:13,400 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:13,400 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:13,406 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:13,419 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:13,421 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_1/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:14,387 INFO] Extracting features...\n",
      "[2021-02-03 00:12:14,389 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:14,389 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:14,389 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:14,390 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:14,400 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:14,403 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_2/processed.train.0.pt.\n",
      "[2021-02-03 00:12:14,500 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:14,500 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:14,506 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:14,518 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:14,520 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_2/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:15,531 INFO] Extracting features...\n",
      "[2021-02-03 00:12:15,533 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:15,533 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:15,533 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:15,533 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:15,543 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:15,547 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_3/processed.train.0.pt.\n",
      "[2021-02-03 00:12:15,644 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:15,644 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:15,652 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:15,665 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:15,666 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_3/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:16,603 INFO] Extracting features...\n",
      "[2021-02-03 00:12:16,605 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:16,605 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:16,605 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:16,605 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:16,615 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:16,619 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_4/processed.train.0.pt.\n",
      "[2021-02-03 00:12:16,717 INFO]  * tgt vocab size: 25.\n",
      "[2021-02-03 00:12:16,717 INFO]  * src vocab size: 27.\n",
      "[2021-02-03 00:12:16,722 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:16,735 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:16,736 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_4/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:17,712 INFO] Extracting features...\n",
      "[2021-02-03 00:12:17,714 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:17,714 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:17,714 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:17,714 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:17,725 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:17,729 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_5/processed.train.0.pt.\n",
      "[2021-02-03 00:12:17,825 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:17,825 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:17,830 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:17,843 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:17,844 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_5/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:18,844 INFO] Extracting features...\n",
      "[2021-02-03 00:12:18,846 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:18,846 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:18,846 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:18,846 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:18,857 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:18,861 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_6/processed.train.0.pt.\n",
      "[2021-02-03 00:12:18,957 INFO]  * tgt vocab size: 25.\n",
      "[2021-02-03 00:12:18,957 INFO]  * src vocab size: 27.\n",
      "[2021-02-03 00:12:18,963 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:18,976 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:18,977 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_6/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:19,958 INFO] Extracting features...\n",
      "[2021-02-03 00:12:19,960 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:19,961 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:19,961 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:19,961 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:19,971 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:19,975 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_7/processed.train.0.pt.\n",
      "[2021-02-03 00:12:20,071 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:20,072 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:20,077 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:20,090 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:20,091 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_7/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:21,094 INFO] Extracting features...\n",
      "[2021-02-03 00:12:21,097 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:21,097 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:21,097 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:21,097 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:21,108 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:21,111 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_8/processed.train.0.pt.\n",
      "[2021-02-03 00:12:21,208 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:21,208 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:21,213 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:21,226 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:21,228 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_8/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:22,176 INFO] Extracting features...\n",
      "[2021-02-03 00:12:22,178 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:22,178 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:22,178 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:22,178 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:22,189 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:22,192 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_9/processed.train.0.pt.\n",
      "[2021-02-03 00:12:22,289 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:22,289 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:22,294 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:22,308 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:22,310 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_9/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:23,308 INFO] Extracting features...\n",
      "[2021-02-03 00:12:23,310 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:23,310 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:23,310 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:23,310 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:23,320 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:23,324 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_10/processed.train.0.pt.\n",
      "[2021-02-03 00:12:23,420 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:23,421 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:23,425 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:23,439 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:23,440 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_10/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:24,401 INFO] Extracting features...\n",
      "[2021-02-03 00:12:24,403 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:24,403 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:24,403 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:24,403 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:24,414 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:24,418 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_11/processed.train.0.pt.\n",
      "[2021-02-03 00:12:24,514 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:24,514 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:24,520 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:24,532 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:24,533 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_11/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:25,513 INFO] Extracting features...\n",
      "[2021-02-03 00:12:25,515 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:25,515 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:25,515 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:25,515 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:25,525 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:25,529 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_12/processed.train.0.pt.\n",
      "[2021-02-03 00:12:25,626 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:25,626 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:25,631 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:25,645 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:25,646 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_12/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:26,647 INFO] Extracting features...\n",
      "[2021-02-03 00:12:26,649 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:26,649 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:26,649 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:26,650 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:26,662 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:26,667 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_13/processed.train.0.pt.\n",
      "[2021-02-03 00:12:26,761 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:26,761 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:26,767 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:26,780 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:26,781 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_13/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:27,761 INFO] Extracting features...\n",
      "[2021-02-03 00:12:27,763 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:27,763 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:27,763 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:27,763 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:27,773 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:27,777 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_14/processed.train.0.pt.\n",
      "[2021-02-03 00:12:27,873 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:27,874 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:27,879 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:27,892 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:27,893 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_14/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:28,886 INFO] Extracting features...\n",
      "[2021-02-03 00:12:28,888 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:28,888 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:28,888 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:28,888 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:28,899 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:28,902 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_15/processed.train.0.pt.\n",
      "[2021-02-03 00:12:28,999 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:28,999 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:29,005 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:29,018 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:29,019 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_15/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:29,977 INFO] Extracting features...\n",
      "[2021-02-03 00:12:29,980 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:29,980 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:29,980 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:29,980 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:29,990 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:29,994 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_16/processed.train.0.pt.\n",
      "[2021-02-03 00:12:30,090 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:30,091 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:30,096 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:30,108 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:30,110 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_16/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:31,098 INFO] Extracting features...\n",
      "[2021-02-03 00:12:31,100 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:31,101 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:31,101 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:31,101 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:31,111 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:31,115 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_17/processed.train.0.pt.\n",
      "[2021-02-03 00:12:31,211 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:31,211 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:31,216 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:31,230 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:31,231 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_17/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:32,217 INFO] Extracting features...\n",
      "[2021-02-03 00:12:32,219 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:32,220 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:32,220 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:32,220 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:32,230 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:32,234 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_18/processed.train.0.pt.\n",
      "[2021-02-03 00:12:32,330 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:32,331 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:32,336 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:32,350 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:32,351 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_18/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:33,327 INFO] Extracting features...\n",
      "[2021-02-03 00:12:33,329 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:33,329 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:33,329 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:33,330 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:33,340 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:33,343 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_19/processed.train.0.pt.\n",
      "[2021-02-03 00:12:33,440 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:33,440 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:33,449 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:33,461 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:33,462 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_19/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:34,431 INFO] Extracting features...\n",
      "[2021-02-03 00:12:34,433 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:34,433 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:34,433 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:34,433 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:34,444 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:34,448 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_20/processed.train.0.pt.\n",
      "[2021-02-03 00:12:34,543 INFO]  * tgt vocab size: 25.\n",
      "[2021-02-03 00:12:34,543 INFO]  * src vocab size: 27.\n",
      "[2021-02-03 00:12:34,549 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:34,562 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:34,563 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_20/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:35,566 INFO] Extracting features...\n",
      "[2021-02-03 00:12:35,568 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:35,568 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:35,569 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:35,569 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:35,580 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:35,583 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_21/processed.train.0.pt.\n",
      "[2021-02-03 00:12:35,680 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:35,680 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:35,686 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:35,699 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:35,701 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_21/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:36,673 INFO] Extracting features...\n",
      "[2021-02-03 00:12:36,675 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:36,675 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:36,675 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:36,676 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:36,686 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:36,690 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_22/processed.train.0.pt.\n",
      "[2021-02-03 00:12:36,786 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:36,786 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:36,792 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:36,806 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:36,807 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_22/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:37,776 INFO] Extracting features...\n",
      "[2021-02-03 00:12:37,777 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:37,778 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:37,778 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:37,778 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:37,788 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:37,792 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_23/processed.train.0.pt.\n",
      "[2021-02-03 00:12:37,888 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:37,888 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:37,894 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:37,907 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:37,909 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_23/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:38,919 INFO] Extracting features...\n",
      "[2021-02-03 00:12:38,921 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:38,921 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:38,921 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:38,921 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:38,932 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:38,936 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_24/processed.train.0.pt.\n",
      "[2021-02-03 00:12:39,032 INFO]  * tgt vocab size: 26.\n",
      "[2021-02-03 00:12:39,032 INFO]  * src vocab size: 28.\n",
      "[2021-02-03 00:12:39,038 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:39,059 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:39,060 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/60_24/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:40,015 INFO] Extracting features...\n",
      "[2021-02-03 00:12:40,017 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:40,017 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:40,017 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:40,018 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:40,038 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:40,044 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_0/processed.train.0.pt.\n",
      "[2021-02-03 00:12:40,128 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:12:40,129 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:12:40,134 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:40,147 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:40,148 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_0/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:41,163 INFO] Extracting features...\n",
      "[2021-02-03 00:12:41,165 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:41,165 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:41,165 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:41,165 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:41,176 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:41,181 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_1/processed.train.0.pt.\n",
      "[2021-02-03 00:12:41,275 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:12:41,275 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:12:41,281 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:41,293 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:41,295 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_1/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:42,246 INFO] Extracting features...\n",
      "[2021-02-03 00:12:42,248 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:42,248 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:42,248 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:42,248 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:42,260 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:42,265 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_2/processed.train.0.pt.\n",
      "[2021-02-03 00:12:42,360 INFO]  * tgt vocab size: 28.\n",
      "[2021-02-03 00:12:42,360 INFO]  * src vocab size: 30.\n",
      "[2021-02-03 00:12:42,366 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:42,379 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:42,380 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_2/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:43,353 INFO] Extracting features...\n",
      "[2021-02-03 00:12:43,355 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:43,355 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:43,355 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:43,355 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:43,366 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:43,371 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_3/processed.train.0.pt.\n",
      "[2021-02-03 00:12:43,465 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:12:43,465 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:12:43,471 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:43,484 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:43,485 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_3/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:44,469 INFO] Extracting features...\n",
      "[2021-02-03 00:12:44,473 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:44,473 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:44,473 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:44,473 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:44,484 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:44,490 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_4/processed.train.0.pt.\n",
      "[2021-02-03 00:12:44,584 INFO]  * tgt vocab size: 27.\n",
      "[2021-02-03 00:12:44,584 INFO]  * src vocab size: 29.\n",
      "[2021-02-03 00:12:44,589 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:44,602 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:44,604 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_4/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:45,588 INFO] Extracting features...\n",
      "[2021-02-03 00:12:45,591 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:45,591 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:45,591 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:45,591 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:45,602 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:45,607 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_5/processed.train.0.pt.\n",
      "[2021-02-03 00:12:45,701 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:12:45,702 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:12:45,731 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:45,744 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:45,746 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_5/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:46,702 INFO] Extracting features...\n",
      "[2021-02-03 00:12:46,704 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:46,704 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:46,704 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:46,705 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:46,716 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:46,721 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_6/processed.train.0.pt.\n",
      "[2021-02-03 00:12:46,815 INFO]  * tgt vocab size: 27.\n",
      "[2021-02-03 00:12:46,815 INFO]  * src vocab size: 29.\n",
      "[2021-02-03 00:12:46,821 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:46,834 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:46,835 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_6/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:47,833 INFO] Extracting features...\n",
      "[2021-02-03 00:12:47,835 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:47,836 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:47,836 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:47,836 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:47,847 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:47,852 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_7/processed.train.0.pt.\n",
      "[2021-02-03 00:12:47,946 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:12:47,947 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:12:47,952 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:47,965 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:47,966 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_7/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:48,930 INFO] Extracting features...\n",
      "[2021-02-03 00:12:48,932 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:48,932 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:48,932 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:48,932 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:48,944 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:48,949 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_8/processed.train.0.pt.\n",
      "[2021-02-03 00:12:49,043 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:12:49,043 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:12:49,049 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:49,062 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:49,063 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_8/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:50,063 INFO] Extracting features...\n",
      "[2021-02-03 00:12:50,088 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:50,088 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:50,088 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:50,089 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:50,102 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:50,110 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_9/processed.train.0.pt.\n",
      "[2021-02-03 00:12:50,202 INFO]  * tgt vocab size: 28.\n",
      "[2021-02-03 00:12:50,202 INFO]  * src vocab size: 30.\n",
      "[2021-02-03 00:12:50,207 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:50,221 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:50,222 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_9/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:51,154 INFO] Extracting features...\n",
      "[2021-02-03 00:12:51,156 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:51,156 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:51,156 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:51,156 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:51,167 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:51,173 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_10/processed.train.0.pt.\n",
      "[2021-02-03 00:12:51,267 INFO]  * tgt vocab size: 28.\n",
      "[2021-02-03 00:12:51,267 INFO]  * src vocab size: 30.\n",
      "[2021-02-03 00:12:51,272 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:51,285 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:51,286 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_10/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:52,260 INFO] Extracting features...\n",
      "[2021-02-03 00:12:52,261 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:52,262 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:52,262 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:52,262 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:52,273 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:52,279 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_11/processed.train.0.pt.\n",
      "[2021-02-03 00:12:52,373 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:12:52,373 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:12:52,379 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:52,392 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:52,393 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_11/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:53,378 INFO] Extracting features...\n",
      "[2021-02-03 00:12:53,379 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:53,380 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:53,380 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:53,380 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:53,392 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:53,399 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_12/processed.train.0.pt.\n",
      "[2021-02-03 00:12:53,492 INFO]  * tgt vocab size: 28.\n",
      "[2021-02-03 00:12:53,492 INFO]  * src vocab size: 30.\n",
      "[2021-02-03 00:12:53,497 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:53,511 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:53,512 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_12/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:54,501 INFO] Extracting features...\n",
      "[2021-02-03 00:12:54,503 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:54,503 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:54,503 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:54,503 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:54,515 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:54,520 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_13/processed.train.0.pt.\n",
      "[2021-02-03 00:12:54,614 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:12:54,615 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:12:54,620 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:54,633 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:54,634 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_13/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:55,596 INFO] Extracting features...\n",
      "[2021-02-03 00:12:55,598 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:55,598 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:55,598 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:55,599 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:55,609 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:55,614 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_14/processed.train.0.pt.\n",
      "[2021-02-03 00:12:55,709 INFO]  * tgt vocab size: 28.\n",
      "[2021-02-03 00:12:55,709 INFO]  * src vocab size: 30.\n",
      "[2021-02-03 00:12:55,714 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:55,727 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:55,729 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_14/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:56,738 INFO] Extracting features...\n",
      "[2021-02-03 00:12:56,740 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:56,740 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:56,740 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:56,740 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:56,751 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:56,756 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_15/processed.train.0.pt.\n",
      "[2021-02-03 00:12:56,850 INFO]  * tgt vocab size: 27.\n",
      "[2021-02-03 00:12:56,851 INFO]  * src vocab size: 29.\n",
      "[2021-02-03 00:12:56,856 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:56,869 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:56,871 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_15/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:57,832 INFO] Extracting features...\n",
      "[2021-02-03 00:12:57,834 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:57,834 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:57,834 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:57,834 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:57,845 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:57,850 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_16/processed.train.0.pt.\n",
      "[2021-02-03 00:12:57,944 INFO]  * tgt vocab size: 28.\n",
      "[2021-02-03 00:12:57,945 INFO]  * src vocab size: 30.\n",
      "[2021-02-03 00:12:57,950 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:57,963 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:57,964 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_16/processed.valid.0.pt.\n",
      "[2021-02-03 00:12:59,255 INFO] Extracting features...\n",
      "[2021-02-03 00:12:59,257 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:12:59,257 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:12:59,257 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:12:59,258 INFO] Building & saving training data...\n",
      "[2021-02-03 00:12:59,268 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:59,274 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_17/processed.train.0.pt.\n",
      "[2021-02-03 00:12:59,368 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:12:59,368 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:12:59,374 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:12:59,387 INFO] Building shard 0.\n",
      "[2021-02-03 00:12:59,388 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_17/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:00,397 INFO] Extracting features...\n",
      "[2021-02-03 00:13:00,399 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:00,399 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:00,399 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:00,399 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:00,410 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:00,415 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_18/processed.train.0.pt.\n",
      "[2021-02-03 00:13:00,510 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:13:00,511 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:13:00,516 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:00,530 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:00,531 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_18/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:01,520 INFO] Extracting features...\n",
      "[2021-02-03 00:13:01,522 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:01,522 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:01,522 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:01,522 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:01,532 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:01,538 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_19/processed.train.0.pt.\n",
      "[2021-02-03 00:13:01,632 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:13:01,632 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:13:01,638 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:01,651 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:01,652 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_19/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:02,611 INFO] Extracting features...\n",
      "[2021-02-03 00:13:02,613 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:02,613 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:02,613 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:02,613 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:02,624 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:02,629 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_20/processed.train.0.pt.\n",
      "[2021-02-03 00:13:02,724 INFO]  * tgt vocab size: 27.\n",
      "[2021-02-03 00:13:02,724 INFO]  * src vocab size: 29.\n",
      "[2021-02-03 00:13:02,753 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:02,767 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:02,768 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_20/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:03,741 INFO] Extracting features...\n",
      "[2021-02-03 00:13:03,743 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:03,743 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:03,743 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:03,743 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:03,754 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:03,759 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_21/processed.train.0.pt.\n",
      "[2021-02-03 00:13:03,853 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:13:03,854 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:13:03,859 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:03,872 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:03,873 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_21/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:04,863 INFO] Extracting features...\n",
      "[2021-02-03 00:13:04,865 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:04,865 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:04,865 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:04,865 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:04,876 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:04,881 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_22/processed.train.0.pt.\n",
      "[2021-02-03 00:13:04,976 INFO]  * tgt vocab size: 27.\n",
      "[2021-02-03 00:13:04,976 INFO]  * src vocab size: 29.\n",
      "[2021-02-03 00:13:04,981 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:04,994 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:04,995 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_22/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:05,969 INFO] Extracting features...\n",
      "[2021-02-03 00:13:05,971 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:05,971 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:05,971 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:05,971 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:05,982 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:05,987 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_23/processed.train.0.pt.\n",
      "[2021-02-03 00:13:06,081 INFO]  * tgt vocab size: 28.\n",
      "[2021-02-03 00:13:06,081 INFO]  * src vocab size: 30.\n",
      "[2021-02-03 00:13:06,086 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:06,100 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:06,101 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_23/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:07,086 INFO] Extracting features...\n",
      "[2021-02-03 00:13:07,088 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:07,088 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:07,088 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:07,088 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:07,100 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:07,105 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_24/processed.train.0.pt.\n",
      "[2021-02-03 00:13:07,199 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:13:07,199 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:13:07,227 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:07,241 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:07,242 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/120_24/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:08,194 INFO] Extracting features...\n",
      "[2021-02-03 00:13:08,197 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:08,197 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:08,197 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:08,197 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:08,208 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:08,215 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_0/processed.train.0.pt.\n",
      "[2021-02-03 00:13:08,308 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:08,308 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:08,313 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:08,327 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:08,329 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_0/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:09,310 INFO] Extracting features...\n",
      "[2021-02-03 00:13:09,312 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:09,312 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:09,312 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:09,312 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:09,323 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:09,330 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_1/processed.train.0.pt.\n",
      "[2021-02-03 00:13:09,423 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:09,423 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:09,428 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:09,441 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:09,442 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_1/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:10,415 INFO] Extracting features...\n",
      "[2021-02-03 00:13:10,417 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:10,417 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:10,417 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:10,417 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:10,428 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:10,435 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_2/processed.train.0.pt.\n",
      "[2021-02-03 00:13:10,528 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:10,528 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:10,534 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:10,547 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:10,548 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_2/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:11,529 INFO] Extracting features...\n",
      "[2021-02-03 00:13:11,531 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:11,531 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:11,531 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:11,531 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:11,542 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:11,548 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_3/processed.train.0.pt.\n",
      "[2021-02-03 00:13:11,641 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:13:11,642 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:13:11,647 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:11,660 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:11,661 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_3/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:12,659 INFO] Extracting features...\n",
      "[2021-02-03 00:13:12,661 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:12,661 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:12,661 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:12,661 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:12,672 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:12,678 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_4/processed.train.0.pt.\n",
      "[2021-02-03 00:13:12,771 INFO]  * tgt vocab size: 28.\n",
      "[2021-02-03 00:13:12,772 INFO]  * src vocab size: 30.\n",
      "[2021-02-03 00:13:12,777 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:12,790 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:12,791 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_4/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:13,771 INFO] Extracting features...\n",
      "[2021-02-03 00:13:13,773 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:13,773 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:13,773 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:13,773 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:13,785 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:13,792 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_5/processed.train.0.pt.\n",
      "[2021-02-03 00:13:13,884 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:13,885 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:13,890 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:13,912 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:13,913 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_5/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:14,902 INFO] Extracting features...\n",
      "[2021-02-03 00:13:14,904 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:14,904 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:14,904 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:14,904 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:14,915 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:14,922 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_6/processed.train.0.pt.\n",
      "[2021-02-03 00:13:15,015 INFO]  * tgt vocab size: 28.\n",
      "[2021-02-03 00:13:15,015 INFO]  * src vocab size: 30.\n",
      "[2021-02-03 00:13:15,020 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:15,035 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:15,036 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_6/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:15,980 INFO] Extracting features...\n",
      "[2021-02-03 00:13:15,982 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:15,982 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:15,982 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:15,982 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:15,993 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:16,000 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_7/processed.train.0.pt.\n",
      "[2021-02-03 00:13:16,093 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:16,093 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:16,099 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:16,112 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:16,113 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_7/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:17,115 INFO] Extracting features...\n",
      "[2021-02-03 00:13:17,117 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:17,117 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:17,117 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:17,117 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:17,128 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:17,135 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_8/processed.train.0.pt.\n",
      "[2021-02-03 00:13:17,228 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:17,228 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:17,233 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:17,246 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:17,247 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_8/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:18,224 INFO] Extracting features...\n",
      "[2021-02-03 00:13:18,226 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:18,226 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:18,226 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:18,226 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:18,237 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:18,244 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_9/processed.train.0.pt.\n",
      "[2021-02-03 00:13:18,337 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:18,337 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:18,342 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:18,356 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:18,357 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_9/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:19,340 INFO] Extracting features...\n",
      "[2021-02-03 00:13:19,342 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:19,342 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:19,342 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:19,342 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:19,354 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:19,361 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_10/processed.train.0.pt.\n",
      "[2021-02-03 00:13:19,453 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:13:19,453 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:13:19,459 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:19,472 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:19,473 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_10/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:20,459 INFO] Extracting features...\n",
      "[2021-02-03 00:13:20,461 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:20,461 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:20,461 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:20,461 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:20,472 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:20,479 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_11/processed.train.0.pt.\n",
      "[2021-02-03 00:13:20,572 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:20,572 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:20,599 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:20,613 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:20,614 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_11/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:21,577 INFO] Extracting features...\n",
      "[2021-02-03 00:13:21,579 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:21,579 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:21,579 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:21,579 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:21,590 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:21,596 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_12/processed.train.0.pt.\n",
      "[2021-02-03 00:13:21,689 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:21,690 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:21,695 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:21,708 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:21,709 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_12/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:22,684 INFO] Extracting features...\n",
      "[2021-02-03 00:13:22,686 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:22,686 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:22,686 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:22,686 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:22,697 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:22,703 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_13/processed.train.0.pt.\n",
      "[2021-02-03 00:13:22,796 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:22,796 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:22,802 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:22,814 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:22,816 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_13/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:23,812 INFO] Extracting features...\n",
      "[2021-02-03 00:13:23,814 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:23,814 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:23,814 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:23,814 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:23,825 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:23,832 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_14/processed.train.0.pt.\n",
      "[2021-02-03 00:13:23,925 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:23,925 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:23,931 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:23,953 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:23,955 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_14/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:24,909 INFO] Extracting features...\n",
      "[2021-02-03 00:13:24,911 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:24,911 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:24,911 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:24,911 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:24,921 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:24,928 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_15/processed.train.0.pt.\n",
      "[2021-02-03 00:13:25,021 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:13:25,022 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:13:25,027 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:25,040 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:25,041 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_15/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:26,033 INFO] Extracting features...\n",
      "[2021-02-03 00:13:26,035 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:26,035 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:26,035 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:26,036 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:26,046 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:26,053 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_16/processed.train.0.pt.\n",
      "[2021-02-03 00:13:26,146 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:13:26,146 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:13:26,152 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:26,185 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:26,186 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_16/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:27,158 INFO] Extracting features...\n",
      "[2021-02-03 00:13:27,160 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:27,160 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:27,160 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:27,160 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:27,171 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:27,178 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_17/processed.train.0.pt.\n",
      "[2021-02-03 00:13:27,270 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:27,271 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:27,276 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:27,290 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:27,291 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_17/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:28,270 INFO] Extracting features...\n",
      "[2021-02-03 00:13:28,272 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:28,272 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:28,272 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:28,272 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:28,283 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:28,290 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_18/processed.train.0.pt.\n",
      "[2021-02-03 00:13:28,383 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:28,383 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:28,389 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:28,402 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:28,403 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_18/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:29,378 INFO] Extracting features...\n",
      "[2021-02-03 00:13:29,381 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:29,381 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:29,381 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:29,381 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:29,392 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:29,399 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_19/processed.train.0.pt.\n",
      "[2021-02-03 00:13:29,491 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:29,492 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:29,497 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:29,510 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:29,512 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_19/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:30,502 INFO] Extracting features...\n",
      "[2021-02-03 00:13:30,504 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:30,504 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:30,504 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:30,504 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:30,515 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:30,522 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_20/processed.train.0.pt.\n",
      "[2021-02-03 00:13:30,614 INFO]  * tgt vocab size: 28.\n",
      "[2021-02-03 00:13:30,614 INFO]  * src vocab size: 30.\n",
      "[2021-02-03 00:13:30,620 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:30,633 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:30,635 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_20/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:31,608 INFO] Extracting features...\n",
      "[2021-02-03 00:13:31,610 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:31,610 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:31,610 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:31,611 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:31,621 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:31,628 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_21/processed.train.0.pt.\n",
      "[2021-02-03 00:13:31,721 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:31,721 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:31,727 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:31,740 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:31,741 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_21/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:32,722 INFO] Extracting features...\n",
      "[2021-02-03 00:13:32,724 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:32,724 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:32,725 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:32,725 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:32,736 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:32,743 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_22/processed.train.0.pt.\n",
      "[2021-02-03 00:13:32,836 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:13:32,836 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:13:32,841 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:32,855 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:32,856 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_22/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:33,839 INFO] Extracting features...\n",
      "[2021-02-03 00:13:33,841 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:33,841 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:33,841 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:33,841 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:33,853 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:33,860 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_23/processed.train.0.pt.\n",
      "[2021-02-03 00:13:33,953 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:33,953 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:33,968 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:33,981 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:33,982 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_23/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:34,946 INFO] Extracting features...\n",
      "[2021-02-03 00:13:34,947 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:34,948 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:34,948 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:34,948 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:34,959 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:34,966 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_24/processed.train.0.pt.\n",
      "[2021-02-03 00:13:35,058 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:35,058 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:35,064 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:35,077 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:35,078 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/180_24/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:36,063 INFO] Extracting features...\n",
      "[2021-02-03 00:13:36,065 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:36,065 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:36,066 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:36,066 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:36,077 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:36,085 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_0/processed.train.0.pt.\n",
      "[2021-02-03 00:13:36,176 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:36,177 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:36,182 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:36,195 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:36,197 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_0/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:37,177 INFO] Extracting features...\n",
      "[2021-02-03 00:13:37,179 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:37,179 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:37,179 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:37,179 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:37,191 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:37,199 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_1/processed.train.0.pt.\n",
      "[2021-02-03 00:13:37,290 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:37,290 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:37,296 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:37,308 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:37,309 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_1/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:38,303 INFO] Extracting features...\n",
      "[2021-02-03 00:13:38,306 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:38,306 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:38,306 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:38,306 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:38,318 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:38,326 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_2/processed.train.0.pt.\n",
      "[2021-02-03 00:13:38,418 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:38,418 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:38,423 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:38,437 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:38,439 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_2/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:39,408 INFO] Extracting features...\n",
      "[2021-02-03 00:13:39,410 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:39,410 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:39,410 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:39,411 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:39,421 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:39,429 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_3/processed.train.0.pt.\n",
      "[2021-02-03 00:13:39,521 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:39,521 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:39,527 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:39,540 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:39,541 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_3/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:40,520 INFO] Extracting features...\n",
      "[2021-02-03 00:13:40,522 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:40,522 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:40,522 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:40,522 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:40,533 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:40,541 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_4/processed.train.0.pt.\n",
      "[2021-02-03 00:13:40,633 INFO]  * tgt vocab size: 28.\n",
      "[2021-02-03 00:13:40,633 INFO]  * src vocab size: 30.\n",
      "[2021-02-03 00:13:40,638 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:40,651 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:40,652 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_4/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:41,626 INFO] Extracting features...\n",
      "[2021-02-03 00:13:41,628 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:41,628 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:41,628 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:41,628 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:41,639 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:41,647 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_5/processed.train.0.pt.\n",
      "[2021-02-03 00:13:41,738 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:41,739 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:41,744 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:41,758 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:41,759 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_5/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:42,763 INFO] Extracting features...\n",
      "[2021-02-03 00:13:42,765 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:42,765 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:42,765 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:42,766 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:42,777 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:42,785 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_6/processed.train.0.pt.\n",
      "[2021-02-03 00:13:42,876 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:13:42,877 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:13:42,882 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:42,894 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:42,895 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_6/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:43,856 INFO] Extracting features...\n",
      "[2021-02-03 00:13:43,859 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:43,859 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:43,859 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:43,859 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:43,870 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:43,878 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_7/processed.train.0.pt.\n",
      "[2021-02-03 00:13:43,969 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:43,970 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:43,995 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:44,008 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:44,010 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_7/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:44,977 INFO] Extracting features...\n",
      "[2021-02-03 00:13:44,979 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:44,980 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:44,980 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:44,980 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:44,991 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:44,999 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_8/processed.train.0.pt.\n",
      "[2021-02-03 00:13:45,090 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:45,091 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:45,095 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:45,108 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:45,110 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_8/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:46,105 INFO] Extracting features...\n",
      "[2021-02-03 00:13:46,108 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:46,108 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:46,108 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:46,108 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:46,119 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:46,128 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_9/processed.train.0.pt.\n",
      "[2021-02-03 00:13:46,219 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:46,219 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:46,224 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:46,238 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:46,239 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_9/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:47,233 INFO] Extracting features...\n",
      "[2021-02-03 00:13:47,234 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:47,234 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:47,234 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:47,235 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:47,246 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:47,255 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_10/processed.train.0.pt.\n",
      "[2021-02-03 00:13:47,345 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:13:47,346 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:13:47,351 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:47,363 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:47,364 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_10/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:48,330 INFO] Extracting features...\n",
      "[2021-02-03 00:13:48,333 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:48,333 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:48,333 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:48,333 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:48,344 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:48,352 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_11/processed.train.0.pt.\n",
      "[2021-02-03 00:13:48,443 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:48,444 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:48,449 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:48,462 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:48,464 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_11/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:49,472 INFO] Extracting features...\n",
      "[2021-02-03 00:13:49,473 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:49,474 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:49,474 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:49,474 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:49,485 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:49,493 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_12/processed.train.0.pt.\n",
      "[2021-02-03 00:13:49,584 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:49,585 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:49,590 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:49,603 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:49,604 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_12/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:50,561 INFO] Extracting features...\n",
      "[2021-02-03 00:13:50,563 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:50,563 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:50,563 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:50,563 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:50,573 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:50,582 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_13/processed.train.0.pt.\n",
      "[2021-02-03 00:13:50,673 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:50,674 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:50,678 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:50,691 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:50,692 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_13/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:51,671 INFO] Extracting features...\n",
      "[2021-02-03 00:13:51,673 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:51,673 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:51,673 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:51,673 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:51,684 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:51,692 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_14/processed.train.0.pt.\n",
      "[2021-02-03 00:13:51,784 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:51,784 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:51,789 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:51,804 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:51,805 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_14/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:52,813 INFO] Extracting features...\n",
      "[2021-02-03 00:13:52,815 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:52,815 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:52,815 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:52,815 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:52,827 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:52,835 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_15/processed.train.0.pt.\n",
      "[2021-02-03 00:13:52,926 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:13:52,926 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:13:52,932 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:52,945 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:52,946 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_15/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:53,917 INFO] Extracting features...\n",
      "[2021-02-03 00:13:53,919 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:53,919 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:53,919 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:53,919 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:53,930 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:53,939 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_16/processed.train.0.pt.\n",
      "[2021-02-03 00:13:54,030 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:13:54,031 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:13:54,036 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:54,049 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:54,050 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_16/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:55,024 INFO] Extracting features...\n",
      "[2021-02-03 00:13:55,025 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:55,025 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:55,025 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:55,026 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:55,036 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:55,045 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_17/processed.train.0.pt.\n",
      "[2021-02-03 00:13:55,136 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:55,136 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:55,141 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:55,154 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:55,155 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_17/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:56,143 INFO] Extracting features...\n",
      "[2021-02-03 00:13:56,145 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:56,145 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:56,145 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:56,145 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:56,156 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:56,164 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_18/processed.train.0.pt.\n",
      "[2021-02-03 00:13:56,256 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:56,256 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:56,261 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:56,274 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:56,275 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_18/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:57,258 INFO] Extracting features...\n",
      "[2021-02-03 00:13:57,261 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:57,261 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:57,261 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:57,261 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:57,271 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:57,279 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_19/processed.train.0.pt.\n",
      "[2021-02-03 00:13:57,371 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:57,372 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:57,377 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:57,390 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:57,391 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_19/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:58,367 INFO] Extracting features...\n",
      "[2021-02-03 00:13:58,369 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:58,369 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:58,369 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:58,369 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:58,381 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:58,389 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_20/processed.train.0.pt.\n",
      "[2021-02-03 00:13:58,480 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:13:58,481 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:13:58,486 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:58,499 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:58,501 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_20/processed.valid.0.pt.\n",
      "[2021-02-03 00:13:59,507 INFO] Extracting features...\n",
      "[2021-02-03 00:13:59,509 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:13:59,509 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:13:59,509 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:13:59,509 INFO] Building & saving training data...\n",
      "[2021-02-03 00:13:59,519 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:59,528 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_21/processed.train.0.pt.\n",
      "[2021-02-03 00:13:59,619 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:13:59,619 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:13:59,625 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:13:59,638 INFO] Building shard 0.\n",
      "[2021-02-03 00:13:59,639 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_21/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:00,595 INFO] Extracting features...\n",
      "[2021-02-03 00:14:00,597 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:00,597 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:00,598 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:00,598 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:00,609 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:00,617 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_22/processed.train.0.pt.\n",
      "[2021-02-03 00:14:00,708 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:14:00,708 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:14:00,714 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:00,726 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:00,728 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_22/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:01,709 INFO] Extracting features...\n",
      "[2021-02-03 00:14:01,711 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:01,711 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:01,711 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:01,711 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:01,723 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:01,731 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_23/processed.train.0.pt.\n",
      "[2021-02-03 00:14:01,822 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:01,822 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:01,828 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:01,841 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:01,842 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_23/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:02,833 INFO] Extracting features...\n",
      "[2021-02-03 00:14:02,835 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:02,836 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:02,836 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:02,836 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:02,847 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:02,855 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_24/processed.train.0.pt.\n",
      "[2021-02-03 00:14:02,946 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:02,947 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:02,952 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:02,966 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:02,967 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/240_24/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:03,953 INFO] Extracting features...\n",
      "[2021-02-03 00:14:03,955 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:03,955 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:03,955 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:03,955 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:03,966 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:03,977 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_0/processed.train.0.pt.\n",
      "[2021-02-03 00:14:04,067 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:04,067 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:04,073 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:04,086 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:04,087 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_0/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:05,063 INFO] Extracting features...\n",
      "[2021-02-03 00:14:05,065 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:05,065 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:05,065 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:05,066 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:05,077 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:05,087 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_1/processed.train.0.pt.\n",
      "[2021-02-03 00:14:05,177 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:05,177 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:05,182 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:05,195 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:05,196 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_1/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:06,174 INFO] Extracting features...\n",
      "[2021-02-03 00:14:06,176 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:06,176 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:06,176 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:06,176 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:06,188 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:06,200 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_2/processed.train.0.pt.\n",
      "[2021-02-03 00:14:06,287 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:06,287 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:06,293 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:06,308 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:06,309 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_2/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:07,309 INFO] Extracting features...\n",
      "[2021-02-03 00:14:07,311 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:07,311 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:07,312 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:07,312 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:07,324 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:07,334 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_3/processed.train.0.pt.\n",
      "[2021-02-03 00:14:07,423 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:07,424 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:07,429 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:07,443 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:07,444 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_3/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:08,425 INFO] Extracting features...\n",
      "[2021-02-03 00:14:08,427 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:08,427 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:08,428 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:08,428 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:08,440 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:08,450 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_4/processed.train.0.pt.\n",
      "[2021-02-03 00:14:08,539 INFO]  * tgt vocab size: 28.\n",
      "[2021-02-03 00:14:08,539 INFO]  * src vocab size: 30.\n",
      "[2021-02-03 00:14:08,545 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:08,559 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:08,560 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_4/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:09,544 INFO] Extracting features...\n",
      "[2021-02-03 00:14:09,546 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:09,546 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:09,546 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:09,546 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:09,558 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:09,568 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_5/processed.train.0.pt.\n",
      "[2021-02-03 00:14:09,658 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:09,658 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:09,664 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:09,677 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:09,678 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_5/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:10,636 INFO] Extracting features...\n",
      "[2021-02-03 00:14:10,638 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:10,638 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:10,638 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:10,638 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:10,650 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:10,661 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_6/processed.train.0.pt.\n",
      "[2021-02-03 00:14:10,749 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:10,750 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:10,755 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:10,769 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:10,770 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_6/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:11,758 INFO] Extracting features...\n",
      "[2021-02-03 00:14:11,760 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:11,760 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:11,760 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:11,760 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:11,771 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:11,780 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_7/processed.train.0.pt.\n",
      "[2021-02-03 00:14:11,871 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:11,871 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:11,876 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:11,888 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:11,890 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_7/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:12,862 INFO] Extracting features...\n",
      "[2021-02-03 00:14:12,864 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:12,864 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:12,864 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:12,864 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:12,876 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:12,886 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_8/processed.train.0.pt.\n",
      "[2021-02-03 00:14:12,975 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:12,975 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:12,981 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:12,994 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:12,995 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_8/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:13,992 INFO] Extracting features...\n",
      "[2021-02-03 00:14:13,994 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:13,994 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:13,994 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:13,994 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:14,005 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:14,015 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_9/processed.train.0.pt.\n",
      "[2021-02-03 00:14:14,105 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:14,105 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:14,111 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:14,125 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:14,126 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_9/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:15,113 INFO] Extracting features...\n",
      "[2021-02-03 00:14:15,115 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:15,116 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:15,116 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:15,116 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:15,126 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:15,136 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_10/processed.train.0.pt.\n",
      "[2021-02-03 00:14:15,226 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:14:15,227 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:14:15,232 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:15,244 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:15,246 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_10/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:16,271 INFO] Extracting features...\n",
      "[2021-02-03 00:14:16,356 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:16,356 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:16,356 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:16,357 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:16,370 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:16,386 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_11/processed.train.0.pt.\n",
      "[2021-02-03 00:14:16,470 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:16,471 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:16,477 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:16,492 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:16,493 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_11/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:17,542 INFO] Extracting features...\n",
      "[2021-02-03 00:14:17,544 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:17,544 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:17,544 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:17,544 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:17,555 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:17,565 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_12/processed.train.0.pt.\n",
      "[2021-02-03 00:14:17,654 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:17,655 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:17,660 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:17,672 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:17,674 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_12/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:18,651 INFO] Extracting features...\n",
      "[2021-02-03 00:14:18,653 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:18,653 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:18,653 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:18,653 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:18,664 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:18,674 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_13/processed.train.0.pt.\n",
      "[2021-02-03 00:14:18,764 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:18,764 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:18,769 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:18,782 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:18,783 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_13/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:19,759 INFO] Extracting features...\n",
      "[2021-02-03 00:14:19,760 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:19,760 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:19,761 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:19,761 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:19,771 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:19,781 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_14/processed.train.0.pt.\n",
      "[2021-02-03 00:14:19,871 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:19,871 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:19,876 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:19,890 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:19,891 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_14/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:20,871 INFO] Extracting features...\n",
      "[2021-02-03 00:14:20,873 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:20,873 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:20,873 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:20,873 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:20,884 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:20,894 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_15/processed.train.0.pt.\n",
      "[2021-02-03 00:14:20,983 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:14:20,983 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:14:20,989 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:21,002 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:21,003 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_15/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:21,993 INFO] Extracting features...\n",
      "[2021-02-03 00:14:21,995 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:21,995 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:21,995 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:21,995 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:22,007 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:22,018 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_16/processed.train.0.pt.\n",
      "[2021-02-03 00:14:22,106 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:14:22,106 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:14:22,111 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:22,125 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:22,126 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_16/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:23,097 INFO] Extracting features...\n",
      "[2021-02-03 00:14:23,100 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:23,100 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:23,100 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:23,100 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:23,112 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:23,123 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_17/processed.train.0.pt.\n",
      "[2021-02-03 00:14:23,212 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:23,212 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:23,217 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:23,230 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:23,232 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_17/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:24,232 INFO] Extracting features...\n",
      "[2021-02-03 00:14:24,234 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:24,234 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:24,234 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:24,235 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:24,245 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:24,255 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_18/processed.train.0.pt.\n",
      "[2021-02-03 00:14:24,345 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:24,345 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:24,351 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:24,364 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:24,365 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_18/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:25,324 INFO] Extracting features...\n",
      "[2021-02-03 00:14:25,325 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:25,326 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:25,326 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:25,326 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:25,336 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:25,346 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_19/processed.train.0.pt.\n",
      "[2021-02-03 00:14:25,437 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:25,437 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:25,445 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:25,458 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:25,459 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_19/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:26,441 INFO] Extracting features...\n",
      "[2021-02-03 00:14:26,443 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:26,443 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:26,443 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:26,443 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:26,454 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:26,464 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_20/processed.train.0.pt.\n",
      "[2021-02-03 00:14:26,554 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:14:26,554 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:14:26,559 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:26,573 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:26,574 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_20/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:27,562 INFO] Extracting features...\n",
      "[2021-02-03 00:14:27,565 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:27,565 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:27,565 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:27,565 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:27,576 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:27,586 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_21/processed.train.0.pt.\n",
      "[2021-02-03 00:14:27,676 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:27,676 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:27,681 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:27,694 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:27,696 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_21/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:28,687 INFO] Extracting features...\n",
      "[2021-02-03 00:14:28,689 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:28,689 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:28,689 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:28,690 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:28,700 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:28,710 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_22/processed.train.0.pt.\n",
      "[2021-02-03 00:14:28,800 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:14:28,800 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:14:28,806 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:28,818 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:28,819 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_22/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:29,787 INFO] Extracting features...\n",
      "[2021-02-03 00:14:29,789 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:29,789 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:29,789 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:29,789 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:29,800 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:29,810 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_23/processed.train.0.pt.\n",
      "[2021-02-03 00:14:29,900 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:29,900 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:29,906 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:29,919 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:29,920 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_23/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:30,902 INFO] Extracting features...\n",
      "[2021-02-03 00:14:30,904 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:30,904 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:30,904 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:30,904 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:30,915 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:30,925 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_24/processed.train.0.pt.\n",
      "[2021-02-03 00:14:31,015 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:31,015 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:31,021 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:31,034 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:31,035 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/300_24/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:32,027 INFO] Extracting features...\n",
      "[2021-02-03 00:14:32,029 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:32,029 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:32,029 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:32,029 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:32,040 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:32,051 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_0/processed.train.0.pt.\n",
      "[2021-02-03 00:14:32,140 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:32,140 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:32,146 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:32,159 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:32,160 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_0/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:33,140 INFO] Extracting features...\n",
      "[2021-02-03 00:14:33,142 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:33,142 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:33,142 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:33,142 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:33,154 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:33,165 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_1/processed.train.0.pt.\n",
      "[2021-02-03 00:14:33,253 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:33,253 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:33,259 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:33,272 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:33,273 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_1/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:34,256 INFO] Extracting features...\n",
      "[2021-02-03 00:14:34,258 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:34,258 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:34,258 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:34,258 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:34,270 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:34,285 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_2/processed.train.0.pt.\n",
      "[2021-02-03 00:14:34,369 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:34,369 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:34,374 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:34,387 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:34,388 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_2/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:35,373 INFO] Extracting features...\n",
      "[2021-02-03 00:14:35,376 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:35,376 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:35,376 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:35,376 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:35,387 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:35,398 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_3/processed.train.0.pt.\n",
      "[2021-02-03 00:14:35,486 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:35,487 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:35,493 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:35,505 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:35,507 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_3/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:36,488 INFO] Extracting features...\n",
      "[2021-02-03 00:14:36,490 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:36,490 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:36,491 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:36,491 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:36,502 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:36,514 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_4/processed.train.0.pt.\n",
      "[2021-02-03 00:14:36,601 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:14:36,602 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:14:36,607 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:36,620 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:36,621 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_4/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:37,614 INFO] Extracting features...\n",
      "[2021-02-03 00:14:37,616 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:37,617 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:37,617 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:37,617 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:37,628 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:37,639 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_5/processed.train.0.pt.\n",
      "[2021-02-03 00:14:37,727 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:37,727 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:37,762 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:37,775 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:37,777 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_5/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:38,725 INFO] Extracting features...\n",
      "[2021-02-03 00:14:38,727 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:38,727 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:38,727 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:38,727 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:38,739 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:38,750 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_6/processed.train.0.pt.\n",
      "[2021-02-03 00:14:38,838 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:38,838 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:38,843 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:38,855 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:38,857 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_6/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:39,838 INFO] Extracting features...\n",
      "[2021-02-03 00:14:39,840 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:39,840 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:39,840 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:39,840 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:39,851 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:39,863 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt.\n",
      "[2021-02-03 00:14:39,950 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:39,951 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:39,956 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:39,969 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:39,971 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:40,959 INFO] Extracting features...\n",
      "[2021-02-03 00:14:40,961 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:40,962 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:40,962 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:40,962 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:40,973 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:40,985 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt.\n",
      "[2021-02-03 00:14:41,072 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:41,072 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:41,078 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:41,090 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:41,092 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:42,067 INFO] Extracting features...\n",
      "[2021-02-03 00:14:42,069 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:42,069 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:42,069 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:42,069 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:42,080 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:42,091 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt.\n",
      "[2021-02-03 00:14:42,180 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:42,180 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:42,185 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:42,199 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:42,200 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:43,181 INFO] Extracting features...\n",
      "[2021-02-03 00:14:43,184 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:43,184 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:43,184 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:43,184 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:43,194 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:43,206 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt.\n",
      "[2021-02-03 00:14:43,294 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:14:43,294 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:14:43,302 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:43,317 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:43,319 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:44,288 INFO] Extracting features...\n",
      "[2021-02-03 00:14:44,290 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:44,290 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:44,290 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:44,291 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:44,302 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:44,313 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt.\n",
      "[2021-02-03 00:14:44,402 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:44,402 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:44,407 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:44,421 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:44,422 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:45,405 INFO] Extracting features...\n",
      "[2021-02-03 00:14:45,407 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:45,407 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:45,407 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:45,408 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:45,420 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:45,431 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt.\n",
      "[2021-02-03 00:14:45,519 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:45,519 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:45,525 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:45,538 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:45,539 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:46,528 INFO] Extracting features...\n",
      "[2021-02-03 00:14:46,531 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:46,531 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:46,531 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:46,531 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:46,542 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:46,554 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt.\n",
      "[2021-02-03 00:14:46,641 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:46,641 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:46,646 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:46,660 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:46,661 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:47,626 INFO] Extracting features...\n",
      "[2021-02-03 00:14:47,628 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:47,628 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:47,628 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:47,628 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:47,639 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:47,651 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt.\n",
      "[2021-02-03 00:14:47,739 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:47,739 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:47,745 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:47,757 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:47,758 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:48,755 INFO] Extracting features...\n",
      "[2021-02-03 00:14:48,757 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:48,757 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:48,757 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:48,757 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:48,768 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:48,780 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt.\n",
      "[2021-02-03 00:14:48,869 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:14:48,869 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:14:48,898 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:48,911 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:48,913 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:49,871 INFO] Extracting features...\n",
      "[2021-02-03 00:14:49,873 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:49,873 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:49,873 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:49,874 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:49,885 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:49,896 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt.\n",
      "[2021-02-03 00:14:49,985 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:14:49,985 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:14:49,990 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:50,003 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:50,004 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:50,983 INFO] Extracting features...\n",
      "[2021-02-03 00:14:50,985 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:50,985 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:50,985 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:50,985 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:50,996 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:51,008 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt.\n",
      "[2021-02-03 00:14:51,096 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:51,096 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:51,102 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:51,115 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:51,117 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:52,104 INFO] Extracting features...\n",
      "[2021-02-03 00:14:52,106 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:52,106 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:52,106 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:52,106 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:52,117 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:52,128 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt.\n",
      "[2021-02-03 00:14:52,217 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:52,217 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:52,222 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:52,235 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:52,236 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:53,211 INFO] Extracting features...\n",
      "[2021-02-03 00:14:53,212 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:53,213 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:53,213 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:53,213 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:53,224 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:53,235 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt.\n",
      "[2021-02-03 00:14:53,323 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:53,324 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:53,329 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:53,342 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:53,343 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:54,325 INFO] Extracting features...\n",
      "[2021-02-03 00:14:54,327 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:54,327 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:54,327 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:54,328 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:54,339 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:54,351 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt.\n",
      "[2021-02-03 00:14:54,438 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:14:54,438 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:14:54,444 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:54,457 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:54,458 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:55,431 INFO] Extracting features...\n",
      "[2021-02-03 00:14:55,434 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:55,434 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:55,434 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:55,434 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:55,445 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:55,456 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt.\n",
      "[2021-02-03 00:14:55,544 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:55,544 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:55,550 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:55,563 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:55,564 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:56,550 INFO] Extracting features...\n",
      "[2021-02-03 00:14:56,553 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:56,553 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:56,553 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:56,553 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:56,564 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:56,575 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt.\n",
      "[2021-02-03 00:14:56,663 INFO]  * tgt vocab size: 29.\n",
      "[2021-02-03 00:14:56,664 INFO]  * src vocab size: 31.\n",
      "[2021-02-03 00:14:56,681 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:56,695 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:56,696 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:57,654 INFO] Extracting features...\n",
      "[2021-02-03 00:14:57,655 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:57,656 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:57,656 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:57,656 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:57,667 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:57,678 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt.\n",
      "[2021-02-03 00:14:57,767 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:57,767 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:57,772 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:57,785 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:57,787 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.valid.0.pt.\n",
      "[2021-02-03 00:14:58,793 INFO] Extracting features...\n",
      "[2021-02-03 00:14:58,795 INFO]  * number of source features: 0.\n",
      "[2021-02-03 00:14:58,795 INFO]  * number of target features: 0.\n",
      "[2021-02-03 00:14:58,795 INFO] Building `Fields` object...\n",
      "[2021-02-03 00:14:58,796 INFO] Building & saving training data...\n",
      "[2021-02-03 00:14:58,807 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:58,819 INFO]  * saving 0th train data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt.\n",
      "[2021-02-03 00:14:58,906 INFO]  * tgt vocab size: 30.\n",
      "[2021-02-03 00:14:58,906 INFO]  * src vocab size: 32.\n",
      "[2021-02-03 00:14:58,911 INFO] Building & saving validation data...\n",
      "[2021-02-03 00:14:58,925 INFO] Building shard 0.\n",
      "[2021-02-03 00:14:58,926 INFO]  * saving 0th valid data shard to drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.valid.0.pt.\n"
     ]
    }
   ],
   "source": [
    "for datasize in datasizes:\n",
    "  datadir = f'drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/{datasize}'\n",
    "  !python OpenNMT-py/preprocess.py -train_src $datadir/german-src-train.txt -train_tgt $datadir/german-tgt-train.txt -valid_src $datadir/german-src-val.txt -valid_tgt $datadir/german-tgt-val.txt -save_data $datadir/processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XVK7SxcEf6G"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znoSyi2SD3dx",
    "outputId": "7440e603-2c6d-42d5-eb7a-aa65f2ac144e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "[2021-02-03 01:22:38,207 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:38,671 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:38,695 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:39,168 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:39,173 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:39,277 INFO] Step 400/ 1800; acc:  45.40; ppl:  5.99; xent: 1.79; lr: 1.00000; 6427/6342 tok/s;     11 sec\n",
      "[2021-02-03 01:22:39,637 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:39,641 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:40,105 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:40,110 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:40,571 INFO] Step 450/ 1800; acc:  49.38; ppl:  5.16; xent: 1.64; lr: 1.00000; 6699/6603 tok/s;     12 sec\n",
      "[2021-02-03 01:22:40,571 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:40,575 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:41,046 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:41,050 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:41,524 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:41,529 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:41,890 INFO] Step 500/ 1800; acc:  53.82; ppl:  4.45; xent: 1.49; lr: 1.00000; 6560/6459 tok/s;     13 sec\n",
      "[2021-02-03 01:22:42,004 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:42,009 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:42,471 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:42,476 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:42,928 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:42,932 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:43,189 INFO] Step 550/ 1800; acc:  63.07; ppl:  3.31; xent: 1.20; lr: 1.00000; 6734/6636 tok/s;     14 sec\n",
      "[2021-02-03 01:22:43,394 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:43,398 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:43,874 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:43,878 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:44,352 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:44,356 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:44,507 INFO] Step 600/ 1800; acc:  75.33; ppl:  2.21; xent: 0.80; lr: 1.00000; 6570/6460 tok/s;     16 sec\n",
      "[2021-02-03 01:22:44,838 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:44,843 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:45,327 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:45,332 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:45,807 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:45,811 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:45,864 INFO] Step 650/ 1800; acc:  85.07; ppl:  1.65; xent: 0.50; lr: 1.00000; 6396/6323 tok/s;     17 sec\n",
      "[2021-02-03 01:22:46,291 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:46,295 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:46,758 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:46,762 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:47,169 INFO] Step 700/ 1800; acc:  88.62; ppl:  1.46; xent: 0.38; lr: 1.00000; 6625/6519 tok/s;     18 sec\n",
      "[2021-02-03 01:22:47,227 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:47,231 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:47,704 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:47,709 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:48,168 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:48,172 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:48,470 INFO] Step 750/ 1800; acc:  91.88; ppl:  1.32; xent: 0.28; lr: 1.00000; 6652/6560 tok/s;     20 sec\n",
      "[2021-02-03 01:22:48,628 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:48,633 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:49,121 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:49,125 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:49,593 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:49,597 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:49,791 INFO] Step 800/ 1800; acc:  93.04; ppl:  1.27; xent: 0.24; lr: 1.00000; 6609/6497 tok/s;     21 sec\n",
      "[2021-02-03 01:22:50,062 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:50,067 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:50,528 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:50,532 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:51,012 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:51,016 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:51,118 INFO] Step 850/ 1800; acc:  94.59; ppl:  1.21; xent: 0.19; lr: 1.00000; 6570/6483 tok/s;     22 sec\n",
      "[2021-02-03 01:22:51,485 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:51,489 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:51,966 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:51,970 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:52,431 INFO] Step 900/ 1800; acc:  94.82; ppl:  1.20; xent: 0.18; lr: 1.00000; 6602/6508 tok/s;     24 sec\n",
      "[2021-02-03 01:22:52,432 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:52,436 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:52,909 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:52,913 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:53,375 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:53,379 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:53,739 INFO] Step 950/ 1800; acc:  95.69; ppl:  1.17; xent: 0.16; lr: 1.00000; 6616/6514 tok/s;     25 sec\n",
      "[2021-02-03 01:22:53,845 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:53,850 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:54,314 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:54,318 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:54,787 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:54,791 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:55,055 INFO] Step 1000/ 1800; acc:  97.03; ppl:  1.11; xent: 0.11; lr: 1.00000; 6647/6550 tok/s;     26 sec\n",
      "[2021-02-03 01:22:55,277 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:55,281 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:55,768 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:55,772 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:56,249 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:56,254 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:56,407 INFO] Step 1050/ 1800; acc:  96.43; ppl:  1.14; xent: 0.13; lr: 1.00000; 6406/6299 tok/s;     28 sec\n",
      "[2021-02-03 01:22:56,726 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:56,731 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:57,207 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:57,212 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:57,684 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:57,689 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:57,741 INFO] Step 1100/ 1800; acc:  96.90; ppl:  1.13; xent: 0.12; lr: 1.00000; 6507/6432 tok/s;     29 sec\n",
      "[2021-02-03 01:22:58,158 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:58,163 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:58,624 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:58,628 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:59,052 INFO] Step 1150/ 1800; acc:  97.51; ppl:  1.10; xent: 0.10; lr: 1.00000; 6596/6490 tok/s;     30 sec\n",
      "[2021-02-03 01:22:59,110 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:59,114 INFO] number of examples: 360\n",
      "[2021-02-03 01:22:59,578 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:22:59,603 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:00,070 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:00,075 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:00,375 INFO] Step 1200/ 1800; acc:  97.69; ppl:  1.10; xent: 0.09; lr: 1.00000; 6542/6451 tok/s;     32 sec\n",
      "[2021-02-03 01:23:00,548 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:00,552 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:01,037 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:01,043 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:01,513 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:01,518 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:01,709 INFO] Step 1250/ 1800; acc:  97.35; ppl:  1.11; xent: 0.11; lr: 1.00000; 6541/6431 tok/s;     33 sec\n",
      "[2021-02-03 01:23:01,967 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:01,972 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:02,430 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:02,435 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:02,928 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:02,933 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:03,040 INFO] Step 1300/ 1800; acc:  96.84; ppl:  1.13; xent: 0.13; lr: 1.00000; 6553/6466 tok/s;     34 sec\n",
      "[2021-02-03 01:23:03,399 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:03,404 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:03,864 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:03,869 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:04,324 INFO] Step 1350/ 1800; acc:  97.19; ppl:  1.11; xent: 0.11; lr: 1.00000; 6753/6657 tok/s;     36 sec\n",
      "[2021-02-03 01:23:04,324 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:04,328 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:04,779 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:04,783 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:05,245 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:05,249 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:05,599 INFO] Step 1400/ 1800; acc:  97.98; ppl:  1.08; xent: 0.08; lr: 1.00000; 6788/6683 tok/s;     37 sec\n",
      "[2021-02-03 01:23:05,704 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:05,708 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:06,158 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:06,162 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:06,630 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:06,634 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:06,886 INFO] Step 1450/ 1800; acc:  97.88; ppl:  1.09; xent: 0.09; lr: 1.00000; 6800/6701 tok/s;     38 sec\n",
      "[2021-02-03 01:23:07,085 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:07,089 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:07,547 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:07,551 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:08,026 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:08,030 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:08,179 INFO] Step 1500/ 1800; acc:  97.96; ppl:  1.09; xent: 0.08; lr: 1.00000; 6691/6579 tok/s;     39 sec\n",
      "[2021-02-03 01:23:08,483 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:08,487 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:08,955 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:08,960 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:09,419 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:09,424 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:09,478 INFO] Step 1550/ 1800; acc:  98.09; ppl:  1.08; xent: 0.08; lr: 1.00000; 6685/6608 tok/s;     41 sec\n",
      "[2021-02-03 01:23:09,938 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:09,943 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:10,425 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:10,430 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:10,838 INFO] Step 1600/ 1800; acc:  97.88; ppl:  1.09; xent: 0.09; lr: 1.00000; 6355/6253 tok/s;     42 sec\n",
      "[2021-02-03 01:23:10,894 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:10,899 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:11,363 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:11,368 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:11,849 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:11,853 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:12,171 INFO] Step 1650/ 1800; acc:  98.03; ppl:  1.09; xent: 0.08; lr: 1.00000; 6498/6408 tok/s;     43 sec\n",
      "[2021-02-03 01:23:12,337 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:12,341 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:12,843 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:12,848 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:13,329 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:13,334 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:13,546 INFO] Step 1700/ 1800; acc:  98.11; ppl:  1.09; xent: 0.09; lr: 1.00000; 6345/6239 tok/s;     45 sec\n",
      "[2021-02-03 01:23:13,829 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:13,835 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:14,324 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:14,329 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:14,820 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:14,825 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:14,931 INFO] Step 1750/ 1800; acc:  97.73; ppl:  1.09; xent: 0.09; lr: 1.00000; 6299/6216 tok/s;     46 sec\n",
      "[2021-02-03 01:23:15,305 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:15,310 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:15,780 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_7/processed.train.0.pt\n",
      "[2021-02-03 01:23:15,785 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:16,270 INFO] Step 1800/ 1800; acc:  97.95; ppl:  1.09; xent: 0.09; lr: 1.00000; 6473/6381 tok/s;     48 sec\n",
      "[2021-02-03 01:23:16,271 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_7_step_1800.pt\n",
      "[2021-02-03 01:23:17,400 INFO]  * src vocab size = 32\n",
      "[2021-02-03 01:23:17,400 INFO]  * tgt vocab size = 30\n",
      "[2021-02-03 01:23:17,400 INFO] Building model...\n",
      "[2021-02-03 01:23:21,954 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(32, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(30, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=30, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-02-03 01:23:21,955 INFO] encoder: 251200\n",
      "[2021-02-03 01:23:21,955 INFO] decoder: 323630\n",
      "[2021-02-03 01:23:21,955 INFO] * number of parameters: 574830\n",
      "[2021-02-03 01:23:21,958 INFO] Starting training on GPU: [0]\n",
      "[2021-02-03 01:23:21,958 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-02-03 01:23:21,958 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:21,962 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:22,429 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:22,434 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:22,907 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:22,911 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:23,256 INFO] Step 50/ 1800; acc:  13.65; ppl: 28.87; xent: 3.36; lr: 1.00000; 6703/6601 tok/s;      1 sec\n",
      "[2021-02-03 01:23:23,349 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:23,353 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:23,790 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:23,794 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:24,235 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:24,239 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:24,471 INFO] Step 100/ 1800; acc:  23.26; ppl: 15.43; xent: 2.74; lr: 1.00000; 7065/6955 tok/s;      3 sec\n",
      "[2021-02-03 01:23:24,690 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:24,695 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:25,161 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:25,165 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:25,631 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:25,636 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:25,786 INFO] Step 150/ 1800; acc:  31.18; ppl: 11.48; xent: 2.44; lr: 1.00000; 6647/6555 tok/s;      4 sec\n",
      "[2021-02-03 01:23:26,083 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:26,087 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:26,547 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:26,552 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:27,013 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:27,018 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:27,059 INFO] Step 200/ 1800; acc:  35.23; ppl:  9.12; xent: 2.21; lr: 1.00000; 6740/6633 tok/s;      5 sec\n",
      "[2021-02-03 01:23:27,469 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:27,474 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:27,933 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:27,937 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:28,345 INFO] Step 250/ 1800; acc:  36.88; ppl:  8.36; xent: 2.12; lr: 1.00000; 6828/6714 tok/s;      6 sec\n",
      "[2021-02-03 01:23:28,394 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:28,398 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:28,849 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:28,853 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:29,315 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:29,320 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:29,609 INFO] Step 300/ 1800; acc:  38.84; ppl:  7.75; xent: 2.05; lr: 1.00000; 6791/6695 tok/s;      8 sec\n",
      "[2021-02-03 01:23:29,767 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:29,771 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:30,250 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:30,255 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:30,698 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:30,703 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:30,893 INFO] Step 350/ 1800; acc:  40.28; ppl:  7.15; xent: 1.97; lr: 1.00000; 6757/6657 tok/s;      9 sec\n",
      "[2021-02-03 01:23:31,161 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:31,165 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:31,618 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:31,645 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:32,098 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:32,102 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:32,199 INFO] Step 400/ 1800; acc:  42.23; ppl:  6.53; xent: 1.88; lr: 1.00000; 6658/6558 tok/s;     10 sec\n",
      "[2021-02-03 01:23:32,568 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:32,572 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:33,031 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:33,036 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:33,496 INFO] Step 450/ 1800; acc:  45.67; ppl:  5.83; xent: 1.76; lr: 1.00000; 6725/6610 tok/s;     12 sec\n",
      "[2021-02-03 01:23:33,496 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:33,500 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:33,981 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:33,986 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:34,451 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:34,455 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:34,816 INFO] Step 500/ 1800; acc:  49.66; ppl:  5.11; xent: 1.63; lr: 1.00000; 6588/6488 tok/s;     13 sec\n",
      "[2021-02-03 01:23:34,921 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:34,925 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:35,396 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:35,400 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:35,861 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:35,865 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:36,109 INFO] Step 550/ 1800; acc:  57.81; ppl:  3.85; xent: 1.35; lr: 1.00000; 6640/6536 tok/s;     14 sec\n",
      "[2021-02-03 01:23:36,336 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:36,340 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:36,807 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:36,812 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:37,275 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:37,279 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:37,432 INFO] Step 600/ 1800; acc:  70.40; ppl:  2.62; xent: 0.96; lr: 1.00000; 6609/6517 tok/s;     15 sec\n",
      "[2021-02-03 01:23:37,757 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:37,762 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:38,233 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:38,237 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:38,680 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:38,684 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:38,725 INFO] Step 650/ 1800; acc:  81.23; ppl:  1.84; xent: 0.61; lr: 1.00000; 6634/6528 tok/s;     17 sec\n",
      "[2021-02-03 01:23:39,152 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:39,157 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:39,619 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:39,623 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:40,050 INFO] Step 700/ 1800; acc:  88.00; ppl:  1.49; xent: 0.40; lr: 1.00000; 6623/6513 tok/s;     18 sec\n",
      "[2021-02-03 01:23:40,105 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:40,109 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:40,570 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:40,575 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:41,035 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:41,040 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:41,348 INFO] Step 750/ 1800; acc:  89.98; ppl:  1.42; xent: 0.35; lr: 1.00000; 6615/6521 tok/s;     19 sec\n",
      "[2021-02-03 01:23:41,517 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:41,521 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:41,993 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:41,998 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:42,465 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:42,470 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:42,678 INFO] Step 800/ 1800; acc:  92.35; ppl:  1.30; xent: 0.27; lr: 1.00000; 6524/6428 tok/s;     21 sec\n",
      "[2021-02-03 01:23:42,949 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:42,954 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:43,436 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:43,440 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:43,899 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:43,903 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:44,002 INFO] Step 850/ 1800; acc:  93.94; ppl:  1.23; xent: 0.20; lr: 1.00000; 6571/6472 tok/s;     22 sec\n",
      "[2021-02-03 01:23:44,357 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:44,361 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:44,834 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:44,839 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:45,305 INFO] Step 900/ 1800; acc:  94.75; ppl:  1.20; xent: 0.18; lr: 1.00000; 6694/6579 tok/s;     23 sec\n",
      "[2021-02-03 01:23:45,305 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:45,309 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:45,770 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:45,774 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:46,249 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:46,253 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:46,632 INFO] Step 950/ 1800; acc:  95.75; ppl:  1.17; xent: 0.15; lr: 1.00000; 6553/6453 tok/s;     25 sec\n",
      "[2021-02-03 01:23:46,736 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:46,740 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:47,205 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:47,209 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:47,674 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:47,679 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:47,939 INFO] Step 1000/ 1800; acc:  95.30; ppl:  1.21; xent: 0.19; lr: 1.00000; 6568/6465 tok/s;     26 sec\n",
      "[2021-02-03 01:23:48,150 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:48,155 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:48,630 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:48,634 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:49,098 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:49,102 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:49,258 INFO] Step 1050/ 1800; acc:  95.66; ppl:  1.16; xent: 0.15; lr: 1.00000; 6626/6534 tok/s;     27 sec\n",
      "[2021-02-03 01:23:49,570 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:49,574 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:50,054 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:50,059 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:50,529 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:50,534 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:50,576 INFO] Step 1100/ 1800; acc:  96.38; ppl:  1.14; xent: 0.13; lr: 1.00000; 6513/6409 tok/s;     29 sec\n",
      "[2021-02-03 01:23:50,999 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:51,003 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:51,483 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:51,487 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:51,904 INFO] Step 1150/ 1800; acc:  97.38; ppl:  1.11; xent: 0.10; lr: 1.00000; 6608/6498 tok/s;     30 sec\n",
      "[2021-02-03 01:23:51,955 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:51,959 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:52,436 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:52,460 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:52,915 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:52,920 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:53,232 INFO] Step 1200/ 1800; acc:  97.00; ppl:  1.12; xent: 0.11; lr: 1.00000; 6465/6373 tok/s;     31 sec\n",
      "[2021-02-03 01:23:53,401 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:53,405 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:53,860 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:53,865 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:54,321 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:54,326 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:54,514 INFO] Step 1250/ 1800; acc:  97.20; ppl:  1.10; xent: 0.10; lr: 1.00000; 6771/6672 tok/s;     33 sec\n",
      "[2021-02-03 01:23:54,794 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:54,798 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:55,261 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:55,265 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:55,711 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:55,715 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:55,812 INFO] Step 1300/ 1800; acc:  96.95; ppl:  1.13; xent: 0.12; lr: 1.00000; 6697/6596 tok/s;     34 sec\n",
      "[2021-02-03 01:23:56,189 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:56,193 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:56,631 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:56,635 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:57,088 INFO] Step 1350/ 1800; acc:  97.49; ppl:  1.11; xent: 0.11; lr: 1.00000; 6835/6718 tok/s;     35 sec\n",
      "[2021-02-03 01:23:57,088 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:57,092 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:57,539 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:57,543 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:58,015 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:58,020 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:58,373 INFO] Step 1400/ 1800; acc:  97.78; ppl:  1.09; xent: 0.08; lr: 1.00000; 6770/6667 tok/s;     36 sec\n",
      "[2021-02-03 01:23:58,468 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:58,472 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:58,922 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:58,927 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:59,384 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:59,389 INFO] number of examples: 360\n",
      "[2021-02-03 01:23:59,624 INFO] Step 1450/ 1800; acc:  97.81; ppl:  1.09; xent: 0.09; lr: 1.00000; 6857/6750 tok/s;     38 sec\n",
      "[2021-02-03 01:23:59,836 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:23:59,841 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:00,318 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:00,323 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:00,786 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:00,790 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:00,944 INFO] Step 1500/ 1800; acc:  97.85; ppl:  1.09; xent: 0.08; lr: 1.00000; 6628/6537 tok/s;     39 sec\n",
      "[2021-02-03 01:24:01,254 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:01,258 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:01,717 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:01,721 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:02,167 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:02,171 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:02,215 INFO] Step 1550/ 1800; acc:  98.09; ppl:  1.08; xent: 0.08; lr: 1.00000; 6749/6642 tok/s;     40 sec\n",
      "[2021-02-03 01:24:02,643 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:02,648 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:03,117 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:03,122 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:03,523 INFO] Step 1600/ 1800; acc:  97.68; ppl:  1.11; xent: 0.11; lr: 1.00000; 6710/6598 tok/s;     42 sec\n",
      "[2021-02-03 01:24:03,571 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:03,576 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:04,025 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:04,029 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:04,489 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:04,494 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:04,789 INFO] Step 1650/ 1800; acc:  97.61; ppl:  1.10; xent: 0.09; lr: 1.00000; 6782/6685 tok/s;     43 sec\n",
      "[2021-02-03 01:24:04,945 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:04,949 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:05,441 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:05,446 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:05,905 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:05,909 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:06,105 INFO] Step 1700/ 1800; acc:  98.40; ppl:  1.07; xent: 0.07; lr: 1.00000; 6593/6496 tok/s;     44 sec\n",
      "[2021-02-03 01:24:06,362 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:06,366 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:06,847 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:06,851 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:07,312 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:07,317 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:07,413 INFO] Step 1750/ 1800; acc:  98.11; ppl:  1.08; xent: 0.08; lr: 1.00000; 6649/6548 tok/s;     45 sec\n",
      "[2021-02-03 01:24:07,763 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:07,767 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:08,228 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_8/processed.train.0.pt\n",
      "[2021-02-03 01:24:08,232 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:08,697 INFO] Step 1800/ 1800; acc:  98.27; ppl:  1.08; xent: 0.07; lr: 1.00000; 6791/6675 tok/s;     47 sec\n",
      "[2021-02-03 01:24:08,698 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_8_step_1800.pt\n",
      "[2021-02-03 01:24:09,747 INFO]  * src vocab size = 32\n",
      "[2021-02-03 01:24:09,747 INFO]  * tgt vocab size = 30\n",
      "[2021-02-03 01:24:09,747 INFO] Building model...\n",
      "[2021-02-03 01:24:14,264 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(32, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(30, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=30, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-02-03 01:24:14,265 INFO] encoder: 251200\n",
      "[2021-02-03 01:24:14,265 INFO] decoder: 323630\n",
      "[2021-02-03 01:24:14,265 INFO] * number of parameters: 574830\n",
      "[2021-02-03 01:24:14,267 INFO] Starting training on GPU: [0]\n",
      "[2021-02-03 01:24:14,268 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-02-03 01:24:14,268 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:14,272 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:14,763 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:14,767 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:15,232 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:15,236 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:15,588 INFO] Step 50/ 1800; acc:  14.28; ppl: 28.74; xent: 3.36; lr: 1.00000; 6551/6443 tok/s;      1 sec\n",
      "[2021-02-03 01:24:15,688 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:15,692 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:16,165 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:16,170 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:16,625 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:16,629 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:16,882 INFO] Step 100/ 1800; acc:  28.98; ppl: 13.07; xent: 2.57; lr: 1.00000; 6643/6550 tok/s;      3 sec\n",
      "[2021-02-03 01:24:17,086 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:17,091 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:17,543 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:17,547 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:18,015 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:18,020 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:18,165 INFO] Step 150/ 1800; acc:  33.87; ppl: 10.22; xent: 2.32; lr: 1.00000; 6712/6605 tok/s;      4 sec\n",
      "[2021-02-03 01:24:18,476 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:18,480 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:18,948 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:18,952 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:19,429 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:19,433 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:19,480 INFO] Step 200/ 1800; acc:  35.22; ppl:  9.00; xent: 2.20; lr: 1.00000; 6573/6474 tok/s;      5 sec\n",
      "[2021-02-03 01:24:19,878 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:19,883 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:20,342 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:20,347 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:20,738 INFO] Step 250/ 1800; acc:  36.60; ppl:  8.35; xent: 2.12; lr: 1.00000; 6828/6710 tok/s;      6 sec\n",
      "[2021-02-03 01:24:20,796 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:20,800 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:21,248 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:21,252 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:21,700 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:21,704 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:22,013 INFO] Step 300/ 1800; acc:  38.17; ppl:  7.69; xent: 2.04; lr: 1.00000; 6845/6739 tok/s;      8 sec\n",
      "[2021-02-03 01:24:22,172 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:22,177 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:22,631 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:22,636 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:23,088 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:23,093 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:23,283 INFO] Step 350/ 1800; acc:  41.16; ppl:  6.86; xent: 1.93; lr: 1.00000; 6698/6607 tok/s;      9 sec\n",
      "[2021-02-03 01:24:23,556 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:23,561 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:24,024 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:24,051 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:24,521 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:24,525 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:24,618 INFO] Step 400/ 1800; acc:  45.18; ppl:  5.89; xent: 1.77; lr: 1.00000; 6453/6355 tok/s;     10 sec\n",
      "[2021-02-03 01:24:25,002 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:25,006 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:25,460 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:25,464 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:25,916 INFO] Step 450/ 1800; acc:  49.77; ppl:  5.09; xent: 1.63; lr: 1.00000; 6723/6614 tok/s;     12 sec\n",
      "[2021-02-03 01:24:25,916 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:25,920 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:26,385 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:26,389 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:26,846 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:26,851 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:27,222 INFO] Step 500/ 1800; acc:  55.23; ppl:  4.28; xent: 1.45; lr: 1.00000; 6620/6511 tok/s;     13 sec\n",
      "[2021-02-03 01:24:27,337 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:27,341 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:27,804 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:27,809 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:28,295 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:28,299 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:28,562 INFO] Step 550/ 1800; acc:  66.07; ppl:  3.02; xent: 1.11; lr: 1.00000; 6419/6329 tok/s;     14 sec\n",
      "[2021-02-03 01:24:28,776 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:28,780 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:29,246 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:29,250 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:29,717 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:29,722 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:29,874 INFO] Step 600/ 1800; acc:  81.15; ppl:  1.90; xent: 0.64; lr: 1.00000; 6559/6455 tok/s;     16 sec\n",
      "[2021-02-03 01:24:30,205 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:30,209 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:30,681 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:30,685 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:31,168 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:31,172 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:31,228 INFO] Step 650/ 1800; acc:  88.19; ppl:  1.49; xent: 0.40; lr: 1.00000; 6387/6291 tok/s;     17 sec\n",
      "[2021-02-03 01:24:31,637 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:31,641 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:32,110 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:32,114 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:32,521 INFO] Step 700/ 1800; acc:  90.61; ppl:  1.40; xent: 0.34; lr: 1.00000; 6640/6526 tok/s;     18 sec\n",
      "[2021-02-03 01:24:32,578 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:32,582 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:33,045 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:33,050 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:33,523 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:33,528 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:33,850 INFO] Step 750/ 1800; acc:  93.23; ppl:  1.27; xent: 0.24; lr: 1.00000; 6570/6469 tok/s;     20 sec\n",
      "[2021-02-03 01:24:33,993 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:33,998 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:34,476 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:34,480 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:34,951 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:34,955 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:35,146 INFO] Step 800/ 1800; acc:  94.78; ppl:  1.20; xent: 0.18; lr: 1.00000; 6563/6475 tok/s;     21 sec\n",
      "[2021-02-03 01:24:35,414 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:35,420 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:35,902 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:35,906 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:36,372 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:36,376 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:36,471 INFO] Step 850/ 1800; acc:  96.22; ppl:  1.14; xent: 0.13; lr: 1.00000; 6501/6402 tok/s;     22 sec\n",
      "[2021-02-03 01:24:36,840 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:36,844 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:37,317 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:37,322 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:37,784 INFO] Step 900/ 1800; acc:  96.06; ppl:  1.16; xent: 0.15; lr: 1.00000; 6642/6534 tok/s;     24 sec\n",
      "[2021-02-03 01:24:37,784 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:37,789 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:38,249 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:38,253 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:38,729 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:38,734 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:39,101 INFO] Step 950/ 1800; acc:  96.98; ppl:  1.12; xent: 0.12; lr: 1.00000; 6569/6462 tok/s;     25 sec\n",
      "[2021-02-03 01:24:39,202 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:39,206 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:39,665 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:39,669 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:40,129 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:40,133 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:40,391 INFO] Step 1000/ 1800; acc:  96.78; ppl:  1.12; xent: 0.11; lr: 1.00000; 6664/6571 tok/s;     26 sec\n",
      "[2021-02-03 01:24:40,607 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:40,611 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:41,104 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:41,109 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:41,581 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:41,586 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:41,739 INFO] Step 1050/ 1800; acc:  96.93; ppl:  1.14; xent: 0.13; lr: 1.00000; 6384/6283 tok/s;     27 sec\n",
      "[2021-02-03 01:24:42,075 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:42,079 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:42,546 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:42,551 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:43,019 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:43,023 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:43,072 INFO] Step 1100/ 1800; acc:  97.19; ppl:  1.12; xent: 0.12; lr: 1.00000; 6487/6389 tok/s;     29 sec\n",
      "[2021-02-03 01:24:43,484 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:43,489 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:43,959 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:43,963 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:44,370 INFO] Step 1150/ 1800; acc:  97.07; ppl:  1.13; xent: 0.12; lr: 1.00000; 6617/6503 tok/s;     30 sec\n",
      "[2021-02-03 01:24:44,429 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:44,434 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:44,904 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:44,932 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:45,388 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:45,393 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:45,711 INFO] Step 1200/ 1800; acc:  97.29; ppl:  1.12; xent: 0.11; lr: 1.00000; 6507/6407 tok/s;     31 sec\n",
      "[2021-02-03 01:24:45,870 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:45,875 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:46,336 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:46,340 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:46,804 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:46,809 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:46,997 INFO] Step 1250/ 1800; acc:  97.63; ppl:  1.10; xent: 0.10; lr: 1.00000; 6614/6525 tok/s;     33 sec\n",
      "[2021-02-03 01:24:47,281 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:47,285 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:47,747 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:47,752 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:48,219 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:48,223 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:48,315 INFO] Step 1300/ 1800; acc:  97.83; ppl:  1.10; xent: 0.09; lr: 1.00000; 6538/6439 tok/s;     34 sec\n",
      "[2021-02-03 01:24:48,691 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:48,696 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:49,168 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:49,172 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:49,619 INFO] Step 1350/ 1800; acc:  97.91; ppl:  1.10; xent: 0.09; lr: 1.00000; 6690/6581 tok/s;     35 sec\n",
      "[2021-02-03 01:24:49,619 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:49,623 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:50,081 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:50,085 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:50,534 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:50,538 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:50,904 INFO] Step 1400/ 1800; acc:  97.72; ppl:  1.10; xent: 0.09; lr: 1.00000; 6727/6617 tok/s;     37 sec\n",
      "[2021-02-03 01:24:51,002 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:51,006 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:51,479 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:51,484 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:51,932 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:51,936 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:52,192 INFO] Step 1450/ 1800; acc:  98.03; ppl:  1.08; xent: 0.08; lr: 1.00000; 6674/6581 tok/s;     38 sec\n",
      "[2021-02-03 01:24:52,409 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:52,413 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:52,873 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:52,878 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:53,334 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:53,338 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:53,485 INFO] Step 1500/ 1800; acc:  98.10; ppl:  1.08; xent: 0.08; lr: 1.00000; 6659/6553 tok/s;     39 sec\n",
      "[2021-02-03 01:24:53,793 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:53,797 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:54,268 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:54,273 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:54,720 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:54,724 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:54,774 INFO] Step 1550/ 1800; acc:  98.38; ppl:  1.08; xent: 0.08; lr: 1.00000; 6709/6607 tok/s;     41 sec\n",
      "[2021-02-03 01:24:55,186 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:55,190 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:55,661 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:55,665 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:56,067 INFO] Step 1600/ 1800; acc:  97.84; ppl:  1.10; xent: 0.09; lr: 1.00000; 6640/6526 tok/s;     42 sec\n",
      "[2021-02-03 01:24:56,128 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:56,132 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:56,588 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:56,593 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:57,059 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:57,064 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:57,377 INFO] Step 1650/ 1800; acc:  98.38; ppl:  1.07; xent: 0.07; lr: 1.00000; 6664/6562 tok/s;     43 sec\n",
      "[2021-02-03 01:24:57,521 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:57,525 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:58,004 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:58,009 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:58,459 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:58,463 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:58,649 INFO] Step 1700/ 1800; acc:  97.64; ppl:  1.10; xent: 0.10; lr: 1.00000; 6689/6598 tok/s;     44 sec\n",
      "[2021-02-03 01:24:58,917 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:58,922 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:59,405 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:59,409 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:59,862 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:24:59,866 INFO] number of examples: 360\n",
      "[2021-02-03 01:24:59,956 INFO] Step 1750/ 1800; acc:  98.13; ppl:  1.08; xent: 0.07; lr: 1.00000; 6589/6489 tok/s;     46 sec\n",
      "[2021-02-03 01:25:00,323 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:25:00,327 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:00,794 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_9/processed.train.0.pt\n",
      "[2021-02-03 01:25:00,799 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:01,264 INFO] Step 1800/ 1800; acc:  98.01; ppl:  1.11; xent: 0.10; lr: 1.00000; 6671/6562 tok/s;     47 sec\n",
      "[2021-02-03 01:25:01,265 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_9_step_1800.pt\n",
      "[2021-02-03 01:25:02,334 INFO]  * src vocab size = 31\n",
      "[2021-02-03 01:25:02,334 INFO]  * tgt vocab size = 29\n",
      "[2021-02-03 01:25:02,334 INFO] Building model...\n",
      "[2021-02-03 01:25:06,853 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(31, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(29, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=29, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-02-03 01:25:06,853 INFO] encoder: 250900\n",
      "[2021-02-03 01:25:06,853 INFO] decoder: 323229\n",
      "[2021-02-03 01:25:06,853 INFO] * number of parameters: 574129\n",
      "[2021-02-03 01:25:06,856 INFO] Starting training on GPU: [0]\n",
      "[2021-02-03 01:25:06,856 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-02-03 01:25:06,856 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:06,860 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:07,359 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:07,363 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:07,832 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:07,836 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:08,207 INFO] Step 50/ 1800; acc:  14.85; ppl: 27.24; xent: 3.30; lr: 1.00000; 6452/6372 tok/s;      1 sec\n",
      "[2021-02-03 01:25:08,311 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:08,316 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:08,795 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:08,800 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:09,273 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:09,277 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:09,522 INFO] Step 100/ 1800; acc:  27.60; ppl: 13.11; xent: 2.57; lr: 1.00000; 6601/6505 tok/s;      3 sec\n",
      "[2021-02-03 01:25:09,747 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:09,752 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:10,235 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:10,239 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:10,701 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:10,705 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:10,853 INFO] Step 150/ 1800; acc:  33.91; ppl: 10.12; xent: 2.31; lr: 1.00000; 6579/6484 tok/s;      4 sec\n",
      "[2021-02-03 01:25:11,183 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:11,188 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:11,702 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:11,707 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:12,181 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:12,185 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:12,233 INFO] Step 200/ 1800; acc:  35.11; ppl:  9.20; xent: 2.22; lr: 1.00000; 6379/6283 tok/s;      5 sec\n",
      "[2021-02-03 01:25:12,651 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:12,656 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:13,122 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:13,127 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:13,544 INFO] Step 250/ 1800; acc:  36.38; ppl:  8.51; xent: 2.14; lr: 1.00000; 6682/6596 tok/s;      7 sec\n",
      "[2021-02-03 01:25:13,595 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:13,600 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:14,062 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:14,067 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:14,524 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:14,529 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:14,830 INFO] Step 300/ 1800; acc:  37.05; ppl:  7.87; xent: 2.06; lr: 1.00000; 6725/6639 tok/s;      8 sec\n",
      "[2021-02-03 01:25:15,006 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:15,010 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:15,501 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:15,505 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:15,976 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:15,980 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:16,183 INFO] Step 350/ 1800; acc:  40.86; ppl:  7.07; xent: 1.96; lr: 1.00000; 6523/6428 tok/s;      9 sec\n",
      "[2021-02-03 01:25:16,438 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:16,443 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:16,927 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:16,955 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:17,425 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:17,429 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:17,533 INFO] Step 400/ 1800; acc:  43.71; ppl:  6.20; xent: 1.83; lr: 1.00000; 6521/6424 tok/s;     11 sec\n",
      "[2021-02-03 01:25:17,895 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:17,899 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:18,365 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:18,370 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:18,829 INFO] Step 450/ 1800; acc:  48.32; ppl:  5.29; xent: 1.67; lr: 1.00000; 6750/6652 tok/s;     12 sec\n",
      "[2021-02-03 01:25:18,829 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:18,833 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:19,298 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:19,303 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:19,765 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:19,770 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:20,151 INFO] Step 500/ 1800; acc:  51.44; ppl:  4.70; xent: 1.55; lr: 1.00000; 6590/6508 tok/s;     13 sec\n",
      "[2021-02-03 01:25:20,263 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:20,267 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:20,730 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:20,735 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:21,226 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:21,231 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:21,497 INFO] Step 550/ 1800; acc:  60.96; ppl:  3.47; xent: 1.24; lr: 1.00000; 6451/6357 tok/s;     15 sec\n",
      "[2021-02-03 01:25:21,710 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:21,715 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:22,195 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:22,200 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:22,677 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:22,682 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:22,827 INFO] Step 600/ 1800; acc:  73.02; ppl:  2.39; xent: 0.87; lr: 1.00000; 6582/6487 tok/s;     16 sec\n",
      "[2021-02-03 01:25:23,180 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:23,185 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:23,687 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:23,691 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:24,156 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:24,161 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:24,210 INFO] Step 650/ 1800; acc:  84.27; ppl:  1.69; xent: 0.53; lr: 1.00000; 6368/6273 tok/s;     17 sec\n",
      "[2021-02-03 01:25:24,636 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:24,641 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:25,124 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:25,129 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:25,541 INFO] Step 700/ 1800; acc:  88.02; ppl:  1.50; xent: 0.41; lr: 1.00000; 6579/6494 tok/s;     19 sec\n",
      "[2021-02-03 01:25:25,592 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:25,596 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:26,077 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:26,082 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:26,556 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:26,560 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:26,858 INFO] Step 750/ 1800; acc:  90.23; ppl:  1.40; xent: 0.33; lr: 1.00000; 6566/6483 tok/s;     20 sec\n",
      "[2021-02-03 01:25:27,023 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:27,028 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:27,479 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:27,483 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:27,940 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:27,944 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:28,147 INFO] Step 800/ 1800; acc:  92.29; ppl:  1.29; xent: 0.26; lr: 1.00000; 6846/6746 tok/s;     21 sec\n",
      "[2021-02-03 01:25:28,397 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:28,402 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:28,875 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:28,879 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:29,335 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:29,339 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:29,440 INFO] Step 850/ 1800; acc:  93.50; ppl:  1.24; xent: 0.22; lr: 1.00000; 6806/6705 tok/s;     23 sec\n",
      "[2021-02-03 01:25:29,814 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:29,818 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:30,278 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:30,283 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:30,752 INFO] Step 900/ 1800; acc:  94.52; ppl:  1.20; xent: 0.18; lr: 1.00000; 6668/6572 tok/s;     24 sec\n",
      "[2021-02-03 01:25:30,752 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:30,757 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:31,220 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:31,224 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:31,678 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:31,682 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:32,038 INFO] Step 950/ 1800; acc:  95.37; ppl:  1.19; xent: 0.18; lr: 1.00000; 6775/6691 tok/s;     25 sec\n",
      "[2021-02-03 01:25:32,141 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:32,146 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:32,613 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:32,617 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:33,080 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:33,084 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:33,337 INFO] Step 1000/ 1800; acc:  95.80; ppl:  1.15; xent: 0.14; lr: 1.00000; 6685/6588 tok/s;     26 sec\n",
      "[2021-02-03 01:25:33,564 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:33,569 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:34,053 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:34,057 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:34,519 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:34,523 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:34,668 INFO] Step 1050/ 1800; acc:  95.76; ppl:  1.16; xent: 0.15; lr: 1.00000; 6576/6481 tok/s;     28 sec\n",
      "[2021-02-03 01:25:34,996 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:35,000 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:35,465 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:35,470 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:35,937 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:35,942 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:35,991 INFO] Step 1100/ 1800; acc:  95.79; ppl:  1.18; xent: 0.16; lr: 1.00000; 6655/6555 tok/s;     29 sec\n",
      "[2021-02-03 01:25:36,412 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:36,416 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:36,884 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:36,888 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:37,303 INFO] Step 1150/ 1800; acc:  96.74; ppl:  1.13; xent: 0.12; lr: 1.00000; 6675/6589 tok/s;     30 sec\n",
      "[2021-02-03 01:25:37,354 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:37,358 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:37,818 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:37,844 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:38,320 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:38,325 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:38,620 INFO] Step 1200/ 1800; acc:  97.06; ppl:  1.12; xent: 0.11; lr: 1.00000; 6566/6483 tok/s;     32 sec\n",
      "[2021-02-03 01:25:38,782 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:38,786 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:39,255 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:39,259 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:39,705 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:39,709 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:39,913 INFO] Step 1250/ 1800; acc:  97.16; ppl:  1.11; xent: 0.10; lr: 1.00000; 6832/6733 tok/s;     33 sec\n",
      "[2021-02-03 01:25:40,182 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:40,186 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:40,661 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:40,665 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:41,130 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:41,135 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:41,240 INFO] Step 1300/ 1800; acc:  97.82; ppl:  1.09; xent: 0.08; lr: 1.00000; 6630/6532 tok/s;     34 sec\n",
      "[2021-02-03 01:25:41,601 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:41,605 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:42,071 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:42,075 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:42,525 INFO] Step 1350/ 1800; acc:  98.26; ppl:  1.06; xent: 0.06; lr: 1.00000; 6804/6706 tok/s;     36 sec\n",
      "[2021-02-03 01:25:42,526 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:42,530 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:42,992 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:42,997 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:43,467 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:43,472 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:43,834 INFO] Step 1400/ 1800; acc:  98.33; ppl:  1.07; xent: 0.07; lr: 1.00000; 6660/6578 tok/s;     37 sec\n",
      "[2021-02-03 01:25:43,941 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:43,946 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:44,407 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:44,412 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:44,856 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:44,860 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:45,103 INFO] Step 1450/ 1800; acc:  98.01; ppl:  1.09; xent: 0.09; lr: 1.00000; 6839/6740 tok/s;     38 sec\n",
      "[2021-02-03 01:25:45,344 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:45,348 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:45,801 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:45,805 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:46,297 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:46,301 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:46,448 INFO] Step 1500/ 1800; acc:  97.18; ppl:  1.12; xent: 0.12; lr: 1.00000; 6511/6417 tok/s;     40 sec\n",
      "[2021-02-03 01:25:46,791 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:46,795 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:47,295 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:47,299 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:47,779 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:47,784 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:47,832 INFO] Step 1550/ 1800; acc:  98.34; ppl:  1.07; xent: 0.07; lr: 1.00000; 6360/6265 tok/s;     41 sec\n",
      "[2021-02-03 01:25:48,250 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:48,256 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:48,713 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:48,717 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:49,139 INFO] Step 1600/ 1800; acc:  98.00; ppl:  1.09; xent: 0.08; lr: 1.00000; 6701/6615 tok/s;     42 sec\n",
      "[2021-02-03 01:25:49,192 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:49,196 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:49,653 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:49,657 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:50,122 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:50,127 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:50,434 INFO] Step 1650/ 1800; acc:  97.93; ppl:  1.10; xent: 0.09; lr: 1.00000; 6678/6593 tok/s;     44 sec\n",
      "[2021-02-03 01:25:50,613 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:50,618 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:51,088 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:51,092 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:51,585 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:51,590 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:51,798 INFO] Step 1700/ 1800; acc:  97.93; ppl:  1.08; xent: 0.08; lr: 1.00000; 6474/6380 tok/s;     45 sec\n",
      "[2021-02-03 01:25:52,060 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:52,065 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:52,540 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:52,544 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:53,023 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:53,028 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:53,130 INFO] Step 1750/ 1800; acc:  98.21; ppl:  1.08; xent: 0.08; lr: 1.00000; 6606/6508 tok/s;     46 sec\n",
      "[2021-02-03 01:25:53,491 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:53,495 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:53,969 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_10/processed.train.0.pt\n",
      "[2021-02-03 01:25:53,973 INFO] number of examples: 360\n",
      "[2021-02-03 01:25:54,453 INFO] Step 1800/ 1800; acc:  97.62; ppl:  1.11; xent: 0.10; lr: 1.00000; 6612/6517 tok/s;     48 sec\n",
      "[2021-02-03 01:25:54,454 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_10_step_1800.pt\n",
      "[2021-02-03 01:25:55,522 INFO]  * src vocab size = 32\n",
      "[2021-02-03 01:25:55,522 INFO]  * tgt vocab size = 30\n",
      "[2021-02-03 01:25:55,522 INFO] Building model...\n",
      "[2021-02-03 01:26:00,043 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(32, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(30, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=30, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-02-03 01:26:00,043 INFO] encoder: 251200\n",
      "[2021-02-03 01:26:00,044 INFO] decoder: 323630\n",
      "[2021-02-03 01:26:00,044 INFO] * number of parameters: 574830\n",
      "[2021-02-03 01:26:00,046 INFO] Starting training on GPU: [0]\n",
      "[2021-02-03 01:26:00,047 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-02-03 01:26:00,047 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:00,051 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:00,529 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:00,534 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:00,977 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:00,981 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:01,344 INFO] Step 50/ 1800; acc:  13.95; ppl: 29.72; xent: 3.39; lr: 1.00000; 6705/6582 tok/s;      1 sec\n",
      "[2021-02-03 01:26:01,434 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:01,438 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:01,903 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:01,908 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:02,367 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:02,372 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:02,629 INFO] Step 100/ 1800; acc:  29.51; ppl: 12.53; xent: 2.53; lr: 1.00000; 6720/6592 tok/s;      3 sec\n",
      "[2021-02-03 01:26:02,820 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:02,825 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:03,281 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:03,285 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:03,745 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:03,749 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:03,884 INFO] Step 150/ 1800; acc:  35.30; ppl:  9.96; xent: 2.30; lr: 1.00000; 6770/6649 tok/s;      4 sec\n",
      "[2021-02-03 01:26:04,190 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:04,195 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:04,645 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:04,649 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:05,105 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:05,109 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:05,149 INFO] Step 200/ 1800; acc:  36.50; ppl:  8.88; xent: 2.18; lr: 1.00000; 6838/6726 tok/s;      5 sec\n",
      "[2021-02-03 01:26:05,562 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:05,567 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:06,018 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:06,022 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:06,423 INFO] Step 250/ 1800; acc:  37.83; ppl:  8.19; xent: 2.10; lr: 1.00000; 6879/6746 tok/s;      6 sec\n",
      "[2021-02-03 01:26:06,467 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:06,470 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:06,931 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:06,935 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:07,400 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:07,405 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:07,709 INFO] Step 300/ 1800; acc:  38.82; ppl:  7.76; xent: 2.05; lr: 1.00000; 6708/6593 tok/s;      8 sec\n",
      "[2021-02-03 01:26:07,853 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:07,857 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:08,322 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:08,327 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:08,770 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:08,774 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:08,959 INFO] Step 350/ 1800; acc:  41.97; ppl:  6.87; xent: 1.93; lr: 1.00000; 6836/6700 tok/s;      9 sec\n",
      "[2021-02-03 01:26:09,233 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:09,238 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:09,689 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:09,715 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:10,167 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:10,171 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:10,262 INFO] Step 400/ 1800; acc:  44.35; ppl:  6.26; xent: 1.83; lr: 1.00000; 6641/6529 tok/s;     10 sec\n",
      "[2021-02-03 01:26:10,624 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:10,628 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:11,083 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:11,087 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:11,540 INFO] Step 450/ 1800; acc:  47.52; ppl:  5.61; xent: 1.72; lr: 1.00000; 6823/6704 tok/s;     11 sec\n",
      "[2021-02-03 01:26:11,540 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:11,544 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:12,004 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:12,009 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:12,462 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:12,466 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:12,837 INFO] Step 500/ 1800; acc:  52.31; ppl:  4.74; xent: 1.55; lr: 1.00000; 6708/6585 tok/s;     13 sec\n",
      "[2021-02-03 01:26:12,930 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:12,934 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:13,383 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:13,387 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:13,851 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:13,855 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:14,110 INFO] Step 550/ 1800; acc:  58.39; ppl:  3.78; xent: 1.33; lr: 1.00000; 6785/6656 tok/s;     14 sec\n",
      "[2021-02-03 01:26:14,298 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:14,303 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:14,754 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:14,758 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:15,218 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:15,223 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:15,359 INFO] Step 600/ 1800; acc:  67.40; ppl:  2.87; xent: 1.05; lr: 1.00000; 6797/6676 tok/s;     15 sec\n",
      "[2021-02-03 01:26:15,678 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:15,682 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:16,135 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:16,139 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:16,599 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:16,603 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:16,642 INFO] Step 650/ 1800; acc:  79.27; ppl:  1.96; xent: 0.67; lr: 1.00000; 6740/6630 tok/s;     17 sec\n",
      "[2021-02-03 01:26:17,048 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:17,053 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:17,520 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:17,525 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:17,930 INFO] Step 700/ 1800; acc:  84.97; ppl:  1.64; xent: 0.50; lr: 1.00000; 6808/6677 tok/s;     18 sec\n",
      "[2021-02-03 01:26:17,974 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:17,978 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:18,436 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:18,441 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:18,889 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:18,893 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:19,201 INFO] Step 750/ 1800; acc:  89.88; ppl:  1.41; xent: 0.35; lr: 1.00000; 6783/6667 tok/s;     19 sec\n",
      "[2021-02-03 01:26:19,361 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:19,365 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:19,818 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:19,823 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:20,290 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:20,294 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:20,481 INFO] Step 800/ 1800; acc:  92.22; ppl:  1.30; xent: 0.26; lr: 1.00000; 6681/6548 tok/s;     20 sec\n",
      "[2021-02-03 01:26:20,740 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:20,745 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:21,193 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:21,197 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:21,652 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:21,656 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:21,748 INFO] Step 850/ 1800; acc:  93.78; ppl:  1.24; xent: 0.22; lr: 1.00000; 6828/6713 tok/s;     22 sec\n",
      "[2021-02-03 01:26:22,109 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:22,114 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:22,570 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:22,575 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:23,032 INFO] Step 900/ 1800; acc:  94.29; ppl:  1.23; xent: 0.21; lr: 1.00000; 6791/6673 tok/s;     23 sec\n",
      "[2021-02-03 01:26:23,032 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:23,037 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:23,479 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:23,483 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:23,952 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:23,956 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:24,316 INFO] Step 950/ 1800; acc:  94.98; ppl:  1.19; xent: 0.18; lr: 1.00000; 6775/6651 tok/s;     24 sec\n",
      "[2021-02-03 01:26:24,404 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:24,408 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:24,861 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:24,866 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:25,327 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:25,331 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:25,579 INFO] Step 1000/ 1800; acc:  95.31; ppl:  1.17; xent: 0.16; lr: 1.00000; 6835/6705 tok/s;     26 sec\n",
      "[2021-02-03 01:26:25,779 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:25,783 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:26,238 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:26,242 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:26,704 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:26,708 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:26,842 INFO] Step 1050/ 1800; acc:  95.97; ppl:  1.16; xent: 0.15; lr: 1.00000; 6725/6606 tok/s;     27 sec\n",
      "[2021-02-03 01:26:27,169 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:27,174 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:27,629 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:27,633 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:28,078 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:28,082 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:28,120 INFO] Step 1100/ 1800; acc:  96.44; ppl:  1.14; xent: 0.13; lr: 1.00000; 6767/6656 tok/s;     28 sec\n",
      "[2021-02-03 01:26:28,544 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:28,548 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:29,023 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:29,027 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:29,433 INFO] Step 1150/ 1800; acc:  96.03; ppl:  1.17; xent: 0.15; lr: 1.00000; 6677/6548 tok/s;     29 sec\n",
      "[2021-02-03 01:26:29,476 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:29,480 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:29,928 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:29,953 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:30,432 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:30,436 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:30,750 INFO] Step 1200/ 1800; acc:  96.19; ppl:  1.15; xent: 0.14; lr: 1.00000; 6550/6437 tok/s;     31 sec\n",
      "[2021-02-03 01:26:30,898 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:30,903 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:31,363 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:31,368 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:31,836 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:31,841 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:32,032 INFO] Step 1250/ 1800; acc:  96.54; ppl:  1.14; xent: 0.13; lr: 1.00000; 6670/6537 tok/s;     32 sec\n",
      "[2021-02-03 01:26:32,315 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:32,319 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:32,768 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:32,773 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:33,247 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:33,252 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:33,347 INFO] Step 1300/ 1800; acc:  97.11; ppl:  1.12; xent: 0.11; lr: 1.00000; 6582/6471 tok/s;     33 sec\n",
      "[2021-02-03 01:26:33,716 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:33,721 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:34,188 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:34,192 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:34,660 INFO] Step 1350/ 1800; acc:  97.35; ppl:  1.12; xent: 0.11; lr: 1.00000; 6636/6520 tok/s;     35 sec\n",
      "[2021-02-03 01:26:34,661 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:34,665 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:35,128 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:35,132 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:35,589 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:35,593 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:35,954 INFO] Step 1400/ 1800; acc:  96.83; ppl:  1.13; xent: 0.12; lr: 1.00000; 6723/6601 tok/s;     36 sec\n",
      "[2021-02-03 01:26:36,045 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:36,049 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:36,510 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:36,514 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:36,984 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:36,989 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:37,242 INFO] Step 1450/ 1800; acc:  97.27; ppl:  1.12; xent: 0.11; lr: 1.00000; 6707/6579 tok/s;     37 sec\n",
      "[2021-02-03 01:26:37,433 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:37,437 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:37,916 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:37,921 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:38,381 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:38,386 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:38,523 INFO] Step 1500/ 1800; acc:  97.72; ppl:  1.09; xent: 0.09; lr: 1.00000; 6629/6511 tok/s;     38 sec\n",
      "[2021-02-03 01:26:38,845 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:38,850 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:39,317 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:39,321 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:39,779 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:39,783 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:39,825 INFO] Step 1550/ 1800; acc:  98.02; ppl:  1.08; xent: 0.08; lr: 1.00000; 6641/6532 tok/s;     40 sec\n",
      "[2021-02-03 01:26:40,256 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:40,260 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:40,704 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:40,708 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:41,114 INFO] Step 1600/ 1800; acc:  97.65; ppl:  1.10; xent: 0.10; lr: 1.00000; 6799/6668 tok/s;     41 sec\n",
      "[2021-02-03 01:26:41,160 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:41,164 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:41,637 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:41,642 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:42,104 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:42,109 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:42,426 INFO] Step 1650/ 1800; acc:  97.57; ppl:  1.10; xent: 0.10; lr: 1.00000; 6577/6464 tok/s;     42 sec\n",
      "[2021-02-03 01:26:42,590 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:42,594 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:43,061 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:43,066 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:43,522 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:43,526 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:43,712 INFO] Step 1700/ 1800; acc:  97.64; ppl:  1.11; xent: 0.10; lr: 1.00000; 6648/6516 tok/s;     44 sec\n",
      "[2021-02-03 01:26:43,981 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:43,985 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:44,437 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:44,441 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:44,896 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:44,901 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:44,995 INFO] Step 1750/ 1800; acc:  97.67; ppl:  1.10; xent: 0.09; lr: 1.00000; 6742/6628 tok/s;     45 sec\n",
      "[2021-02-03 01:26:45,365 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:45,369 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:45,827 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_11/processed.train.0.pt\n",
      "[2021-02-03 01:26:45,831 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:46,302 INFO] Step 1800/ 1800; acc:  97.56; ppl:  1.11; xent: 0.10; lr: 1.00000; 6671/6555 tok/s;     46 sec\n",
      "[2021-02-03 01:26:46,303 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_11_step_1800.pt\n",
      "[2021-02-03 01:26:47,389 INFO]  * src vocab size = 32\n",
      "[2021-02-03 01:26:47,389 INFO]  * tgt vocab size = 30\n",
      "[2021-02-03 01:26:47,389 INFO] Building model...\n",
      "[2021-02-03 01:26:51,888 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(32, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(30, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=30, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-02-03 01:26:51,888 INFO] encoder: 251200\n",
      "[2021-02-03 01:26:51,888 INFO] decoder: 323630\n",
      "[2021-02-03 01:26:51,888 INFO] * number of parameters: 574830\n",
      "[2021-02-03 01:26:51,891 INFO] Starting training on GPU: [0]\n",
      "[2021-02-03 01:26:51,891 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-02-03 01:26:51,891 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:51,895 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:52,359 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:52,363 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:52,812 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:52,816 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:53,190 INFO] Step 50/ 1800; acc:  15.47; ppl: 36.69; xent: 3.60; lr: 1.00000; 6745/6663 tok/s;      1 sec\n",
      "[2021-02-03 01:26:53,286 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:53,290 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:53,740 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:53,745 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:54,203 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:54,207 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:54,433 INFO] Step 100/ 1800; acc:  28.79; ppl: 12.95; xent: 2.56; lr: 1.00000; 6847/6749 tok/s;      3 sec\n",
      "[2021-02-03 01:26:54,646 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:54,650 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:55,106 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:55,110 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:55,566 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:55,571 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:55,728 INFO] Step 150/ 1800; acc:  33.87; ppl: 10.30; xent: 2.33; lr: 1.00000; 6791/6695 tok/s;      4 sec\n",
      "[2021-02-03 01:26:56,045 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:56,050 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:56,519 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:56,523 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:56,953 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:56,957 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:57,007 INFO] Step 200/ 1800; acc:  36.25; ppl:  8.83; xent: 2.18; lr: 1.00000; 6830/6738 tok/s;      5 sec\n",
      "[2021-02-03 01:26:57,392 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:57,396 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:57,856 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:57,860 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:58,265 INFO] Step 250/ 1800; acc:  37.36; ppl:  8.22; xent: 2.11; lr: 1.00000; 6930/6843 tok/s;      6 sec\n",
      "[2021-02-03 01:26:58,310 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:58,315 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:58,762 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:58,766 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:59,238 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:59,243 INFO] number of examples: 360\n",
      "[2021-02-03 01:26:59,537 INFO] Step 300/ 1800; acc:  40.14; ppl:  7.38; xent: 2.00; lr: 1.00000; 6750/6663 tok/s;      8 sec\n",
      "[2021-02-03 01:26:59,697 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:26:59,701 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:00,155 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:00,159 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:00,614 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:00,618 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:00,806 INFO] Step 350/ 1800; acc:  40.80; ppl:  7.18; xent: 1.97; lr: 1.00000; 6859/6756 tok/s;      9 sec\n",
      "[2021-02-03 01:27:01,070 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:01,075 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:01,522 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:01,549 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:02,007 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:02,012 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:02,106 INFO] Step 400/ 1800; acc:  42.95; ppl:  6.35; xent: 1.85; lr: 1.00000; 6717/6631 tok/s;     10 sec\n",
      "[2021-02-03 01:27:02,462 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:02,466 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:02,926 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:02,931 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:03,378 INFO] Step 450/ 1800; acc:  46.21; ppl:  5.54; xent: 1.71; lr: 1.00000; 6871/6780 tok/s;     11 sec\n",
      "[2021-02-03 01:27:03,379 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:03,383 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:03,830 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:03,834 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:04,290 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:04,295 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:04,655 INFO] Step 500/ 1800; acc:  49.38; ppl:  5.00; xent: 1.61; lr: 1.00000; 6864/6780 tok/s;     13 sec\n",
      "[2021-02-03 01:27:04,749 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:04,753 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:05,199 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:05,203 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:05,643 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:05,647 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:05,879 INFO] Step 550/ 1800; acc:  54.46; ppl:  4.18; xent: 1.43; lr: 1.00000; 6953/6854 tok/s;     14 sec\n",
      "[2021-02-03 01:27:06,096 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:06,100 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:06,558 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:06,562 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:07,019 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:07,024 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:07,170 INFO] Step 600/ 1800; acc:  60.08; ppl:  3.53; xent: 1.26; lr: 1.00000; 6812/6715 tok/s;     15 sec\n",
      "[2021-02-03 01:27:07,483 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:07,487 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:07,949 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:07,953 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:08,412 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:08,416 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:08,468 INFO] Step 650/ 1800; acc:  72.61; ppl:  2.40; xent: 0.88; lr: 1.00000; 6731/6640 tok/s;     17 sec\n",
      "[2021-02-03 01:27:08,875 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:08,879 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:09,333 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:09,337 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:09,740 INFO] Step 700/ 1800; acc:  82.72; ppl:  1.76; xent: 0.57; lr: 1.00000; 6851/6765 tok/s;     18 sec\n",
      "[2021-02-03 01:27:09,787 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:09,792 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:10,241 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:10,245 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:10,706 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:10,710 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:11,001 INFO] Step 750/ 1800; acc:  88.78; ppl:  1.46; xent: 0.38; lr: 1.00000; 6809/6722 tok/s;     19 sec\n",
      "[2021-02-03 01:27:11,156 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:11,160 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:11,624 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:11,628 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:12,074 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:12,078 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:12,270 INFO] Step 800/ 1800; acc:  91.00; ppl:  1.35; xent: 0.30; lr: 1.00000; 6865/6761 tok/s;     20 sec\n",
      "[2021-02-03 01:27:12,537 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:12,541 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:13,028 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:13,033 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:13,486 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:13,490 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:13,586 INFO] Step 850/ 1800; acc:  92.40; ppl:  1.31; xent: 0.27; lr: 1.00000; 6632/6547 tok/s;     22 sec\n",
      "[2021-02-03 01:27:13,936 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:13,940 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:14,411 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:14,415 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:14,862 INFO] Step 900/ 1800; acc:  94.17; ppl:  1.22; xent: 0.20; lr: 1.00000; 6850/6759 tok/s;     23 sec\n",
      "[2021-02-03 01:27:14,863 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:14,868 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:15,336 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:15,340 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:15,793 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:15,797 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:16,160 INFO] Step 950/ 1800; acc:  94.42; ppl:  1.22; xent: 0.20; lr: 1.00000; 6752/6670 tok/s;     24 sec\n",
      "[2021-02-03 01:27:16,263 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:16,268 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:16,729 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:16,733 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:17,200 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:17,205 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:17,457 INFO] Step 1000/ 1800; acc:  96.16; ppl:  1.15; xent: 0.14; lr: 1.00000; 6564/6470 tok/s;     26 sec\n",
      "[2021-02-03 01:27:17,682 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:17,687 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:18,150 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:18,154 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:18,623 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:18,627 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:18,773 INFO] Step 1050/ 1800; acc:  95.25; ppl:  1.19; xent: 0.17; lr: 1.00000; 6679/6584 tok/s;     27 sec\n",
      "[2021-02-03 01:27:19,080 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:19,085 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:19,554 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:19,558 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:20,002 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:20,007 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:20,060 INFO] Step 1100/ 1800; acc:  95.90; ppl:  1.16; xent: 0.15; lr: 1.00000; 6788/6696 tok/s;     28 sec\n",
      "[2021-02-03 01:27:20,456 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:20,460 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:20,926 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:20,930 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:21,355 INFO] Step 1150/ 1800; acc:  96.12; ppl:  1.15; xent: 0.14; lr: 1.00000; 6734/6650 tok/s;     29 sec\n",
      "[2021-02-03 01:27:21,403 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:21,407 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:21,883 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:21,911 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:22,377 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:22,381 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:22,680 INFO] Step 1200/ 1800; acc:  95.95; ppl:  1.16; xent: 0.14; lr: 1.00000; 6478/6395 tok/s;     31 sec\n",
      "[2021-02-03 01:27:22,844 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:22,849 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:23,326 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:23,331 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:23,785 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:23,789 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:23,982 INFO] Step 1250/ 1800; acc:  96.51; ppl:  1.14; xent: 0.13; lr: 1.00000; 6686/6586 tok/s;     32 sec\n",
      "[2021-02-03 01:27:24,261 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:24,266 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:24,742 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:24,747 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:25,215 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:25,220 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:25,318 INFO] Step 1300/ 1800; acc:  97.42; ppl:  1.09; xent: 0.09; lr: 1.00000; 6538/6454 tok/s;     33 sec\n",
      "[2021-02-03 01:27:25,682 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:25,686 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:26,155 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:26,159 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:26,622 INFO] Step 1350/ 1800; acc:  97.46; ppl:  1.09; xent: 0.09; lr: 1.00000; 6701/6612 tok/s;     35 sec\n",
      "[2021-02-03 01:27:26,623 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:26,627 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:27,112 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:27,117 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:27,579 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:27,583 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:27,956 INFO] Step 1400/ 1800; acc:  97.11; ppl:  1.12; xent: 0.11; lr: 1.00000; 6569/6489 tok/s;     36 sec\n",
      "[2021-02-03 01:27:28,051 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:28,055 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:28,529 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:28,533 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:28,991 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:28,995 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:29,234 INFO] Step 1450/ 1800; acc:  97.82; ppl:  1.09; xent: 0.08; lr: 1.00000; 6663/6568 tok/s;     37 sec\n",
      "[2021-02-03 01:27:29,460 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:29,465 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:29,947 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:29,952 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:30,431 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:30,436 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:30,584 INFO] Step 1500/ 1800; acc:  96.93; ppl:  1.12; xent: 0.12; lr: 1.00000; 6513/6420 tok/s;     39 sec\n",
      "[2021-02-03 01:27:30,897 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:30,902 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:31,398 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:31,403 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:31,872 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:31,877 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:31,931 INFO] Step 1550/ 1800; acc:  97.55; ppl:  1.10; xent: 0.09; lr: 1.00000; 6484/6396 tok/s;     40 sec\n",
      "[2021-02-03 01:27:32,338 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:32,343 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:32,816 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:32,821 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:33,243 INFO] Step 1600/ 1800; acc:  98.02; ppl:  1.10; xent: 0.09; lr: 1.00000; 6644/6561 tok/s;     41 sec\n",
      "[2021-02-03 01:27:33,291 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:33,296 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:33,756 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:33,761 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:34,237 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:34,241 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:34,545 INFO] Step 1650/ 1800; acc:  98.16; ppl:  1.08; xent: 0.07; lr: 1.00000; 6598/6513 tok/s;     43 sec\n",
      "[2021-02-03 01:27:34,715 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:34,719 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:35,174 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:35,179 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:35,638 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:35,643 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:35,835 INFO] Step 1700/ 1800; acc:  98.01; ppl:  1.08; xent: 0.08; lr: 1.00000; 6749/6647 tok/s;     44 sec\n",
      "[2021-02-03 01:27:36,114 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:36,118 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:36,583 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:36,588 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:37,038 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:37,043 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:37,136 INFO] Step 1750/ 1800; acc:  97.96; ppl:  1.08; xent: 0.08; lr: 1.00000; 6707/6621 tok/s;     45 sec\n",
      "[2021-02-03 01:27:37,522 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:37,527 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:37,974 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_12/processed.train.0.pt\n",
      "[2021-02-03 01:27:37,978 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:38,449 INFO] Step 1800/ 1800; acc:  97.82; ppl:  1.09; xent: 0.09; lr: 1.00000; 6660/6572 tok/s;     47 sec\n",
      "[2021-02-03 01:27:38,450 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_12_step_1800.pt\n",
      "[2021-02-03 01:27:39,553 INFO]  * src vocab size = 32\n",
      "[2021-02-03 01:27:39,553 INFO]  * tgt vocab size = 30\n",
      "[2021-02-03 01:27:39,553 INFO] Building model...\n",
      "[2021-02-03 01:27:44,148 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(32, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(30, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=30, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-02-03 01:27:44,149 INFO] encoder: 251200\n",
      "[2021-02-03 01:27:44,149 INFO] decoder: 323630\n",
      "[2021-02-03 01:27:44,149 INFO] * number of parameters: 574830\n",
      "[2021-02-03 01:27:44,151 INFO] Starting training on GPU: [0]\n",
      "[2021-02-03 01:27:44,151 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-02-03 01:27:44,152 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:44,156 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:44,645 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:44,650 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:45,109 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:45,113 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:45,487 INFO] Step 50/ 1800; acc:  14.01; ppl: 29.25; xent: 3.38; lr: 1.00000; 6588/6496 tok/s;      1 sec\n",
      "[2021-02-03 01:27:45,569 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:45,573 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:46,024 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:46,028 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:46,482 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:46,486 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:46,752 INFO] Step 100/ 1800; acc:  28.18; ppl: 13.04; xent: 2.57; lr: 1.00000; 6893/6782 tok/s;      3 sec\n",
      "[2021-02-03 01:27:46,935 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:46,940 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:47,410 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:47,415 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:47,871 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:47,876 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:48,046 INFO] Step 150/ 1800; acc:  33.95; ppl: 10.12; xent: 2.31; lr: 1.00000; 6716/6609 tok/s;      4 sec\n",
      "[2021-02-03 01:27:48,348 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:48,352 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:48,808 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:48,812 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:49,269 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:49,274 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:49,323 INFO] Step 200/ 1800; acc:  35.97; ppl:  9.19; xent: 2.22; lr: 1.00000; 6708/6617 tok/s;      5 sec\n",
      "[2021-02-03 01:27:49,732 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:49,737 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:50,206 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:50,211 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:50,616 INFO] Step 250/ 1800; acc:  36.68; ppl:  8.42; xent: 2.13; lr: 1.00000; 6767/6667 tok/s;      6 sec\n",
      "[2021-02-03 01:27:50,659 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:50,663 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:51,115 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:51,120 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:51,565 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:51,569 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:51,904 INFO] Step 300/ 1800; acc:  37.39; ppl:  7.90; xent: 2.07; lr: 1.00000; 6827/6723 tok/s;      8 sec\n",
      "[2021-02-03 01:27:52,035 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:52,039 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:52,495 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:52,499 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:52,952 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:52,957 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:53,168 INFO] Step 350/ 1800; acc:  40.45; ppl:  7.23; xent: 1.98; lr: 1.00000; 6834/6725 tok/s;      9 sec\n",
      "[2021-02-03 01:27:53,416 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:53,420 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:53,891 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:53,918 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:54,371 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:54,375 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:54,473 INFO] Step 400/ 1800; acc:  43.01; ppl:  6.28; xent: 1.84; lr: 1.00000; 6604/6496 tok/s;     10 sec\n",
      "[2021-02-03 01:27:54,823 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:54,827 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:55,279 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:55,284 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:55,750 INFO] Step 450/ 1800; acc:  46.36; ppl:  5.66; xent: 1.73; lr: 1.00000; 6825/6735 tok/s;     12 sec\n",
      "[2021-02-03 01:27:55,750 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:55,754 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:56,210 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:56,214 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:56,663 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:56,667 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:57,038 INFO] Step 500/ 1800; acc:  50.87; ppl:  4.89; xent: 1.59; lr: 1.00000; 6829/6733 tok/s;     13 sec\n",
      "[2021-02-03 01:27:57,124 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:57,129 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:57,579 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:57,583 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:58,055 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:58,059 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:58,331 INFO] Step 550/ 1800; acc:  59.81; ppl:  3.62; xent: 1.29; lr: 1.00000; 6745/6636 tok/s;     14 sec\n",
      "[2021-02-03 01:27:58,515 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:58,519 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:59,015 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:59,020 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:59,479 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:59,484 INFO] number of examples: 360\n",
      "[2021-02-03 01:27:59,653 INFO] Step 600/ 1800; acc:  72.48; ppl:  2.50; xent: 0.92; lr: 1.00000; 6576/6472 tok/s;     16 sec\n",
      "[2021-02-03 01:27:59,948 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:27:59,953 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:00,424 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:00,428 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:00,879 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:00,883 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:00,934 INFO] Step 650/ 1800; acc:  82.87; ppl:  1.75; xent: 0.56; lr: 1.00000; 6681/6590 tok/s;     17 sec\n",
      "[2021-02-03 01:28:01,355 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:01,359 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:01,831 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:01,836 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:02,242 INFO] Step 700/ 1800; acc:  87.19; ppl:  1.53; xent: 0.43; lr: 1.00000; 6690/6592 tok/s;     18 sec\n",
      "[2021-02-03 01:28:02,285 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:02,290 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:02,749 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:02,753 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:03,221 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:03,226 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:03,550 INFO] Step 750/ 1800; acc:  91.21; ppl:  1.34; xent: 0.29; lr: 1.00000; 6722/6619 tok/s;     19 sec\n",
      "[2021-02-03 01:28:03,681 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:03,685 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:04,147 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:04,152 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:04,612 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:04,616 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:04,832 INFO] Step 800/ 1800; acc:  92.51; ppl:  1.28; xent: 0.25; lr: 1.00000; 6740/6633 tok/s;     21 sec\n",
      "[2021-02-03 01:28:05,094 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:05,099 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:05,563 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:05,567 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:06,016 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:06,020 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:06,120 INFO] Step 850/ 1800; acc:  93.16; ppl:  1.28; xent: 0.25; lr: 1.00000; 6691/6582 tok/s;     22 sec\n",
      "[2021-02-03 01:28:06,480 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:06,484 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:06,943 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:06,948 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:07,418 INFO] Step 900/ 1800; acc:  95.21; ppl:  1.18; xent: 0.17; lr: 1.00000; 6715/6626 tok/s;     23 sec\n",
      "[2021-02-03 01:28:07,418 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:07,422 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:07,902 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:07,906 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:08,368 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:08,372 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:08,755 INFO] Step 950/ 1800; acc:  94.94; ppl:  1.20; xent: 0.18; lr: 1.00000; 6581/6489 tok/s;     25 sec\n",
      "[2021-02-03 01:28:08,844 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:08,849 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:09,323 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:09,327 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:09,785 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:09,790 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:10,066 INFO] Step 1000/ 1800; acc:  96.45; ppl:  1.13; xent: 0.12; lr: 1.00000; 6645/6538 tok/s;     26 sec\n",
      "[2021-02-03 01:28:10,277 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:10,282 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:10,769 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:10,774 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:11,246 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:11,251 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:11,425 INFO] Step 1050/ 1800; acc:  96.22; ppl:  1.15; xent: 0.14; lr: 1.00000; 6398/6296 tok/s;     27 sec\n",
      "[2021-02-03 01:28:11,740 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:11,744 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:12,226 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:12,231 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:12,694 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:12,699 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:12,750 INFO] Step 1100/ 1800; acc:  96.63; ppl:  1.14; xent: 0.13; lr: 1.00000; 6465/6378 tok/s;     29 sec\n",
      "[2021-02-03 01:28:13,174 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:13,179 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:13,636 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:13,640 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:14,059 INFO] Step 1150/ 1800; acc:  96.75; ppl:  1.13; xent: 0.12; lr: 1.00000; 6684/6585 tok/s;     30 sec\n",
      "[2021-02-03 01:28:14,105 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:14,110 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:14,575 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:14,603 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:15,083 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:15,087 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:15,423 INFO] Step 1200/ 1800; acc:  97.34; ppl:  1.10; xent: 0.10; lr: 1.00000; 6448/6349 tok/s;     31 sec\n",
      "[2021-02-03 01:28:15,566 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:15,570 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:16,030 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:16,034 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:16,507 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:16,512 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:16,721 INFO] Step 1250/ 1800; acc:  97.67; ppl:  1.10; xent: 0.09; lr: 1.00000; 6654/6547 tok/s;     33 sec\n",
      "[2021-02-03 01:28:16,981 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:16,985 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:17,445 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:17,449 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:17,933 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:17,937 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:18,041 INFO] Step 1300/ 1800; acc:  97.56; ppl:  1.10; xent: 0.10; lr: 1.00000; 6528/6422 tok/s;     34 sec\n",
      "[2021-02-03 01:28:18,413 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:18,417 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:18,875 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:18,880 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:19,358 INFO] Step 1350/ 1800; acc:  97.98; ppl:  1.08; xent: 0.08; lr: 1.00000; 6616/6529 tok/s;     35 sec\n",
      "[2021-02-03 01:28:19,358 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:19,362 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:19,825 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:19,829 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:20,290 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:20,295 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:20,694 INFO] Step 1400/ 1800; acc:  97.28; ppl:  1.12; xent: 0.12; lr: 1.00000; 6587/6495 tok/s;     37 sec\n",
      "[2021-02-03 01:28:20,784 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:20,789 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:21,275 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:21,280 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:21,742 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:21,748 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:22,023 INFO] Step 1450/ 1800; acc:  97.39; ppl:  1.11; xent: 0.11; lr: 1.00000; 6558/6452 tok/s;     38 sec\n",
      "[2021-02-03 01:28:22,245 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:22,251 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:22,721 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:22,725 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:23,206 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:23,211 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:23,377 INFO] Step 1500/ 1800; acc:  97.44; ppl:  1.11; xent: 0.10; lr: 1.00000; 6422/6320 tok/s;     39 sec\n",
      "[2021-02-03 01:28:23,704 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:23,709 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:24,218 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:24,224 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:24,704 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:24,709 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:24,762 INFO] Step 1550/ 1800; acc:  97.90; ppl:  1.08; xent: 0.08; lr: 1.00000; 6184/6101 tok/s;     41 sec\n",
      "[2021-02-03 01:28:25,180 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:25,185 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:25,667 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:25,672 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:26,105 INFO] Step 1600/ 1800; acc:  97.77; ppl:  1.10; xent: 0.10; lr: 1.00000; 6512/6416 tok/s;     42 sec\n",
      "[2021-02-03 01:28:26,150 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:26,155 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:26,653 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:26,658 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:27,124 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:27,129 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:27,463 INFO] Step 1650/ 1800; acc:  97.69; ppl:  1.10; xent: 0.09; lr: 1.00000; 6478/6380 tok/s;     43 sec\n",
      "[2021-02-03 01:28:27,596 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:27,600 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:28,118 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:28,124 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:28,625 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:28,629 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:28,849 INFO] Step 1700/ 1800; acc:  98.06; ppl:  1.08; xent: 0.07; lr: 1.00000; 6234/6134 tok/s;     45 sec\n",
      "[2021-02-03 01:28:29,101 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:29,106 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:29,566 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:29,571 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:30,071 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:30,076 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:30,185 INFO] Step 1750/ 1800; acc:  98.08; ppl:  1.09; xent: 0.08; lr: 1.00000; 6449/6343 tok/s;     46 sec\n",
      "[2021-02-03 01:28:30,556 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:30,561 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:31,058 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_13/processed.train.0.pt\n",
      "[2021-02-03 01:28:31,063 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:31,522 INFO] Step 1800/ 1800; acc:  98.05; ppl:  1.09; xent: 0.09; lr: 1.00000; 6515/6429 tok/s;     47 sec\n",
      "[2021-02-03 01:28:31,523 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_13_step_1800.pt\n",
      "[2021-02-03 01:28:32,602 INFO]  * src vocab size = 32\n",
      "[2021-02-03 01:28:32,602 INFO]  * tgt vocab size = 30\n",
      "[2021-02-03 01:28:32,602 INFO] Building model...\n",
      "[2021-02-03 01:28:37,093 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(32, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(30, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=30, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-02-03 01:28:37,093 INFO] encoder: 251200\n",
      "[2021-02-03 01:28:37,093 INFO] decoder: 323630\n",
      "[2021-02-03 01:28:37,093 INFO] * number of parameters: 574830\n",
      "[2021-02-03 01:28:37,096 INFO] Starting training on GPU: [0]\n",
      "[2021-02-03 01:28:37,096 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-02-03 01:28:37,096 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:37,100 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:37,584 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:37,589 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:38,049 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:38,053 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:38,411 INFO] Step 50/ 1800; acc:  13.41; ppl: 28.19; xent: 3.34; lr: 1.00000; 6523/6431 tok/s;      1 sec\n",
      "[2021-02-03 01:28:38,516 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:38,520 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:38,986 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:38,990 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:39,463 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:39,467 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:39,711 INFO] Step 100/ 1800; acc:  25.30; ppl: 14.14; xent: 2.65; lr: 1.00000; 6602/6500 tok/s;      3 sec\n",
      "[2021-02-03 01:28:39,929 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:39,934 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:40,402 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:40,407 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:40,868 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:40,872 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:41,028 INFO] Step 150/ 1800; acc:  31.21; ppl: 11.43; xent: 2.44; lr: 1.00000; 6564/6465 tok/s;      4 sec\n",
      "[2021-02-03 01:28:41,345 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:41,349 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:41,815 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:41,819 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:42,281 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:42,285 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:42,326 INFO] Step 200/ 1800; acc:  35.79; ppl:  9.23; xent: 2.22; lr: 1.00000; 6572/6479 tok/s;      5 sec\n",
      "[2021-02-03 01:28:42,745 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:42,749 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:43,211 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:43,216 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:43,619 INFO] Step 250/ 1800; acc:  37.61; ppl:  8.26; xent: 2.11; lr: 1.00000; 6730/6635 tok/s;      7 sec\n",
      "[2021-02-03 01:28:43,669 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:43,674 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:44,134 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:44,139 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:44,617 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:44,622 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:44,925 INFO] Step 300/ 1800; acc:  38.98; ppl:  7.72; xent: 2.04; lr: 1.00000; 6625/6521 tok/s;      8 sec\n",
      "[2021-02-03 01:28:45,073 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:45,078 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:45,540 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:45,544 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:46,010 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:46,014 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:46,208 INFO] Step 350/ 1800; acc:  40.06; ppl:  7.15; xent: 1.97; lr: 1.00000; 6671/6572 tok/s;      9 sec\n",
      "[2021-02-03 01:28:46,475 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:46,480 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:46,941 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:46,969 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:47,437 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:47,441 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:47,532 INFO] Step 400/ 1800; acc:  42.24; ppl:  6.44; xent: 1.86; lr: 1.00000; 6475/6375 tok/s;     10 sec\n",
      "[2021-02-03 01:28:47,897 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:47,901 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:48,353 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:48,357 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:48,801 INFO] Step 450/ 1800; acc:  46.40; ppl:  5.59; xent: 1.72; lr: 1.00000; 6849/6747 tok/s;     12 sec\n",
      "[2021-02-03 01:28:48,801 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:48,805 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:49,277 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:49,281 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:49,741 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:49,745 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:50,097 INFO] Step 500/ 1800; acc:  51.42; ppl:  4.81; xent: 1.57; lr: 1.00000; 6615/6523 tok/s;     13 sec\n",
      "[2021-02-03 01:28:50,199 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:50,203 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:50,701 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:50,706 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:51,176 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:51,180 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:51,425 INFO] Step 550/ 1800; acc:  59.11; ppl:  3.74; xent: 1.32; lr: 1.00000; 6464/6365 tok/s;     14 sec\n",
      "[2021-02-03 01:28:51,640 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:51,645 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:52,113 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:52,118 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:52,563 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:52,567 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:52,714 INFO] Step 600/ 1800; acc:  71.69; ppl:  2.56; xent: 0.94; lr: 1.00000; 6708/6607 tok/s;     16 sec\n",
      "[2021-02-03 01:28:53,033 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:53,037 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:53,523 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:53,528 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:53,984 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:53,988 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:54,029 INFO] Step 650/ 1800; acc:  81.10; ppl:  1.87; xent: 0.63; lr: 1.00000; 6485/6393 tok/s;     17 sec\n",
      "[2021-02-03 01:28:54,456 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:54,461 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:54,939 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:54,944 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:55,359 INFO] Step 700/ 1800; acc:  88.82; ppl:  1.46; xent: 0.38; lr: 1.00000; 6545/6452 tok/s;     18 sec\n",
      "[2021-02-03 01:28:55,412 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:55,416 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:55,878 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:55,882 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:56,358 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:56,362 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:56,675 INFO] Step 750/ 1800; acc:  90.60; ppl:  1.36; xent: 0.31; lr: 1.00000; 6571/6468 tok/s;     20 sec\n",
      "[2021-02-03 01:28:56,833 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:56,837 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:57,312 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:57,316 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:57,784 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:57,789 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:57,985 INFO] Step 800/ 1800; acc:  93.40; ppl:  1.25; xent: 0.22; lr: 1.00000; 6536/6439 tok/s;     21 sec\n",
      "[2021-02-03 01:28:58,248 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:58,252 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:58,732 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:58,736 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:59,208 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:59,212 INFO] number of examples: 360\n",
      "[2021-02-03 01:28:59,309 INFO] Step 850/ 1800; acc:  95.66; ppl:  1.15; xent: 0.14; lr: 1.00000; 6478/6377 tok/s;     22 sec\n",
      "[2021-02-03 01:28:59,668 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:28:59,672 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:00,134 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:00,138 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:00,592 INFO] Step 900/ 1800; acc:  95.83; ppl:  1.17; xent: 0.16; lr: 1.00000; 6770/6670 tok/s;     23 sec\n",
      "[2021-02-03 01:29:00,592 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:00,596 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:01,080 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:01,085 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:01,547 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:01,551 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:01,909 INFO] Step 950/ 1800; acc:  95.95; ppl:  1.15; xent: 0.14; lr: 1.00000; 6512/6421 tok/s;     25 sec\n",
      "[2021-02-03 01:29:02,017 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:02,021 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:02,502 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:02,506 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:02,990 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:02,994 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:03,241 INFO] Step 1000/ 1800; acc:  96.51; ppl:  1.14; xent: 0.13; lr: 1.00000; 6441/6342 tok/s;     26 sec\n",
      "[2021-02-03 01:29:03,461 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:03,466 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:03,942 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:03,947 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:04,408 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:04,412 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:04,564 INFO] Step 1050/ 1800; acc:  96.23; ppl:  1.15; xent: 0.14; lr: 1.00000; 6541/6443 tok/s;     27 sec\n",
      "[2021-02-03 01:29:04,878 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:04,882 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:05,350 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:05,354 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:05,811 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:05,815 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:05,858 INFO] Step 1100/ 1800; acc:  96.78; ppl:  1.12; xent: 0.11; lr: 1.00000; 6588/6494 tok/s;     29 sec\n",
      "[2021-02-03 01:29:06,298 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:06,302 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:06,755 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:06,759 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:07,166 INFO] Step 1150/ 1800; acc:  96.95; ppl:  1.12; xent: 0.11; lr: 1.00000; 6655/6561 tok/s;     30 sec\n",
      "[2021-02-03 01:29:07,217 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:07,222 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:07,682 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:07,708 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:08,174 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:08,178 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:08,487 INFO] Step 1200/ 1800; acc:  97.45; ppl:  1.10; xent: 0.09; lr: 1.00000; 6550/6448 tok/s;     31 sec\n",
      "[2021-02-03 01:29:08,646 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:08,650 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:09,104 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:09,108 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:09,561 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:09,566 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:09,762 INFO] Step 1250/ 1800; acc:  97.38; ppl:  1.11; xent: 0.10; lr: 1.00000; 6711/6612 tok/s;     33 sec\n",
      "[2021-02-03 01:29:10,023 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:10,028 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:10,491 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:10,495 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:10,953 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:10,957 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:11,052 INFO] Step 1300/ 1800; acc:  96.84; ppl:  1.14; xent: 0.13; lr: 1.00000; 6647/6544 tok/s;     34 sec\n",
      "[2021-02-03 01:29:11,425 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:11,430 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:11,889 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:11,893 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:12,356 INFO] Step 1350/ 1800; acc:  97.28; ppl:  1.12; xent: 0.11; lr: 1.00000; 6663/6564 tok/s;     35 sec\n",
      "[2021-02-03 01:29:12,356 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:12,360 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:12,821 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:12,825 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:13,291 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:13,295 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:13,652 INFO] Step 1400/ 1800; acc:  97.98; ppl:  1.08; xent: 0.08; lr: 1.00000; 6616/6524 tok/s;     37 sec\n",
      "[2021-02-03 01:29:13,762 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:13,766 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:14,250 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:14,255 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:14,707 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:14,711 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:14,951 INFO] Step 1450/ 1800; acc:  97.68; ppl:  1.10; xent: 0.10; lr: 1.00000; 6606/6504 tok/s;     38 sec\n",
      "[2021-02-03 01:29:15,175 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:15,179 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:15,654 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:15,658 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:16,118 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:16,122 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:16,273 INFO] Step 1500/ 1800; acc:  97.84; ppl:  1.10; xent: 0.09; lr: 1.00000; 6543/6445 tok/s;     39 sec\n",
      "[2021-02-03 01:29:16,573 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:16,577 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:17,047 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:17,052 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:17,534 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:17,538 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:17,579 INFO] Step 1550/ 1800; acc:  97.67; ppl:  1.10; xent: 0.10; lr: 1.00000; 6528/6436 tok/s;     40 sec\n",
      "[2021-02-03 01:29:18,012 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:18,017 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:18,468 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:18,472 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:18,878 INFO] Step 1600/ 1800; acc:  98.31; ppl:  1.08; xent: 0.08; lr: 1.00000; 6706/6611 tok/s;     42 sec\n",
      "[2021-02-03 01:29:18,927 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:18,932 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:19,379 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:19,384 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:19,831 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:19,835 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:20,138 INFO] Step 1650/ 1800; acc:  98.14; ppl:  1.07; xent: 0.07; lr: 1.00000; 6863/6755 tok/s;     43 sec\n",
      "[2021-02-03 01:29:20,296 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:20,300 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:20,761 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:20,766 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:21,240 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:21,244 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:21,446 INFO] Step 1700/ 1800; acc:  97.87; ppl:  1.10; xent: 0.10; lr: 1.00000; 6546/6448 tok/s;     44 sec\n",
      "[2021-02-03 01:29:21,723 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:21,727 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:22,201 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:22,206 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:22,652 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:22,656 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:22,754 INFO] Step 1750/ 1800; acc:  98.31; ppl:  1.08; xent: 0.08; lr: 1.00000; 6553/6451 tok/s;     46 sec\n",
      "[2021-02-03 01:29:23,136 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:23,140 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:23,593 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_14/processed.train.0.pt\n",
      "[2021-02-03 01:29:23,598 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:24,073 INFO] Step 1800/ 1800; acc:  98.17; ppl:  1.08; xent: 0.08; lr: 1.00000; 6586/6488 tok/s;     47 sec\n",
      "[2021-02-03 01:29:24,075 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_14_step_1800.pt\n",
      "[2021-02-03 01:29:25,183 INFO]  * src vocab size = 31\n",
      "[2021-02-03 01:29:25,183 INFO]  * tgt vocab size = 29\n",
      "[2021-02-03 01:29:25,183 INFO] Building model...\n",
      "[2021-02-03 01:29:29,705 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(31, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(29, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=29, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-02-03 01:29:29,706 INFO] encoder: 250900\n",
      "[2021-02-03 01:29:29,706 INFO] decoder: 323229\n",
      "[2021-02-03 01:29:29,706 INFO] * number of parameters: 574129\n",
      "[2021-02-03 01:29:29,709 INFO] Starting training on GPU: [0]\n",
      "[2021-02-03 01:29:29,709 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-02-03 01:29:29,709 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:29,713 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:30,186 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:30,190 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:30,631 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:30,635 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:30,974 INFO] Step 50/ 1800; acc:  16.01; ppl: 29.44; xent: 3.38; lr: 1.00000; 6675/6565 tok/s;      1 sec\n",
      "[2021-02-03 01:29:31,098 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:31,103 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:31,576 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:31,580 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:32,025 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:32,029 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:32,269 INFO] Step 100/ 1800; acc:  28.22; ppl: 13.07; xent: 2.57; lr: 1.00000; 6710/6596 tok/s;      3 sec\n",
      "[2021-02-03 01:29:32,486 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:32,493 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:32,957 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:32,961 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:33,406 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:33,411 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:33,560 INFO] Step 150/ 1800; acc:  33.67; ppl: 10.37; xent: 2.34; lr: 1.00000; 6714/6591 tok/s;      4 sec\n",
      "[2021-02-03 01:29:33,856 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:33,861 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:34,316 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:34,320 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:34,777 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:34,781 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:34,843 INFO] Step 200/ 1800; acc:  35.94; ppl:  8.85; xent: 2.18; lr: 1.00000; 6724/6602 tok/s;      5 sec\n",
      "[2021-02-03 01:29:35,248 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:35,261 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:35,696 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:35,700 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:36,100 INFO] Step 250/ 1800; acc:  37.49; ppl:  8.12; xent: 2.09; lr: 1.00000; 6679/6557 tok/s;      6 sec\n",
      "[2021-02-03 01:29:36,174 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:36,179 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:36,627 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:36,631 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:37,090 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:37,095 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:37,385 INFO] Step 300/ 1800; acc:  38.72; ppl:  7.63; xent: 2.03; lr: 1.00000; 6697/6589 tok/s;      8 sec\n",
      "[2021-02-03 01:29:37,560 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:37,565 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:38,045 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:38,049 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:38,515 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:38,519 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:38,721 INFO] Step 350/ 1800; acc:  40.60; ppl:  6.95; xent: 1.94; lr: 1.00000; 6521/6406 tok/s;      9 sec\n",
      "[2021-02-03 01:29:38,980 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:38,985 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:39,471 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:39,495 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:39,956 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:39,960 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:40,063 INFO] Step 400/ 1800; acc:  44.78; ppl:  5.95; xent: 1.78; lr: 1.00000; 6425/6295 tok/s;     10 sec\n",
      "[2021-02-03 01:29:40,414 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:40,418 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:40,887 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:40,892 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:41,359 INFO] Step 450/ 1800; acc:  48.92; ppl:  5.12; xent: 1.63; lr: 1.00000; 6632/6513 tok/s;     12 sec\n",
      "[2021-02-03 01:29:41,360 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:41,364 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:41,831 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:41,835 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:42,290 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:42,294 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:42,638 INFO] Step 500/ 1800; acc:  53.37; ppl:  4.47; xent: 1.50; lr: 1.00000; 6605/6496 tok/s;     13 sec\n",
      "[2021-02-03 01:29:42,763 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:42,767 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:43,246 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:43,251 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:43,701 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:43,705 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:43,955 INFO] Step 550/ 1800; acc:  64.05; ppl:  3.14; xent: 1.15; lr: 1.00000; 6594/6482 tok/s;     14 sec\n",
      "[2021-02-03 01:29:44,166 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:44,171 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:44,651 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:44,656 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:45,121 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:45,125 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:45,285 INFO] Step 600/ 1800; acc:  76.93; ppl:  2.15; xent: 0.77; lr: 1.00000; 6517/6398 tok/s;     16 sec\n",
      "[2021-02-03 01:29:45,599 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:45,603 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:46,075 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:46,080 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:46,537 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:46,541 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:46,598 INFO] Step 650/ 1800; acc:  85.34; ppl:  1.63; xent: 0.49; lr: 1.00000; 6571/6451 tok/s;     17 sec\n",
      "[2021-02-03 01:29:47,003 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:47,007 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:47,487 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:47,492 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:47,881 INFO] Step 700/ 1800; acc:  89.24; ppl:  1.44; xent: 0.36; lr: 1.00000; 6545/6426 tok/s;     18 sec\n",
      "[2021-02-03 01:29:47,957 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:47,964 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:48,439 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:48,444 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:48,904 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:48,908 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:49,205 INFO] Step 750/ 1800; acc:  89.39; ppl:  1.44; xent: 0.36; lr: 1.00000; 6499/6394 tok/s;     19 sec\n",
      "[2021-02-03 01:29:49,374 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:49,378 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:49,861 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:49,866 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:50,348 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:50,352 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:50,554 INFO] Step 800/ 1800; acc:  92.46; ppl:  1.29; xent: 0.26; lr: 1.00000; 6459/6346 tok/s;     21 sec\n",
      "[2021-02-03 01:29:50,806 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:50,811 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:51,284 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:51,288 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:51,749 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:51,753 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:51,857 INFO] Step 850/ 1800; acc:  93.47; ppl:  1.26; xent: 0.23; lr: 1.00000; 6620/6486 tok/s;     22 sec\n",
      "[2021-02-03 01:29:52,212 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:52,216 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:52,675 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:52,679 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:53,150 INFO] Step 900/ 1800; acc:  94.30; ppl:  1.21; xent: 0.19; lr: 1.00000; 6646/6527 tok/s;     23 sec\n",
      "[2021-02-03 01:29:53,150 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:53,154 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:53,605 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:53,610 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:54,078 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:54,083 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:54,429 INFO] Step 950/ 1800; acc:  95.36; ppl:  1.18; xent: 0.16; lr: 1.00000; 6603/6494 tok/s;     25 sec\n",
      "[2021-02-03 01:29:54,556 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:54,560 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:55,010 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:55,014 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:55,473 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:55,477 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:55,730 INFO] Step 1000/ 1800; acc:  95.76; ppl:  1.15; xent: 0.14; lr: 1.00000; 6680/6566 tok/s;     26 sec\n",
      "[2021-02-03 01:29:55,949 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:55,953 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:56,438 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:56,442 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:56,895 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:56,899 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:57,060 INFO] Step 1050/ 1800; acc:  96.22; ppl:  1.15; xent: 0.14; lr: 1.00000; 6518/6399 tok/s;     27 sec\n",
      "[2021-02-03 01:29:57,382 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:57,386 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:57,862 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:57,866 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:58,314 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:58,319 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:58,377 INFO] Step 1100/ 1800; acc:  96.49; ppl:  1.14; xent: 0.13; lr: 1.00000; 6547/6428 tok/s;     29 sec\n",
      "[2021-02-03 01:29:58,762 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:58,767 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:59,229 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:59,233 INFO] number of examples: 360\n",
      "[2021-02-03 01:29:59,612 INFO] Step 1150/ 1800; acc:  97.15; ppl:  1.11; xent: 0.11; lr: 1.00000; 6801/6677 tok/s;     30 sec\n",
      "[2021-02-03 01:29:59,680 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:29:59,685 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:00,156 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:00,182 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:00,642 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:00,646 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:00,933 INFO] Step 1200/ 1800; acc:  96.60; ppl:  1.13; xent: 0.12; lr: 1.00000; 6515/6410 tok/s;     31 sec\n",
      "[2021-02-03 01:30:01,104 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:01,108 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:01,577 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:01,582 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:02,028 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:02,032 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:02,241 INFO] Step 1250/ 1800; acc:  96.53; ppl:  1.14; xent: 0.13; lr: 1.00000; 6657/6540 tok/s;     33 sec\n",
      "[2021-02-03 01:30:02,497 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:02,502 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:02,982 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:02,986 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:03,446 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:03,450 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:03,551 INFO] Step 1300/ 1800; acc:  97.66; ppl:  1.09; xent: 0.09; lr: 1.00000; 6587/6454 tok/s;     34 sec\n",
      "[2021-02-03 01:30:03,912 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:03,916 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:04,384 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:04,388 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:04,846 INFO] Step 1350/ 1800; acc:  97.88; ppl:  1.07; xent: 0.07; lr: 1.00000; 6636/6517 tok/s;     35 sec\n",
      "[2021-02-03 01:30:04,846 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:04,851 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:05,314 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:05,319 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:05,777 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:05,781 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:06,115 INFO] Step 1400/ 1800; acc:  97.53; ppl:  1.10; xent: 0.09; lr: 1.00000; 6657/6548 tok/s;     36 sec\n",
      "[2021-02-03 01:30:06,237 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:06,241 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:06,702 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:06,707 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:07,152 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:07,156 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:07,407 INFO] Step 1450/ 1800; acc:  97.15; ppl:  1.12; xent: 0.11; lr: 1.00000; 6724/6609 tok/s;     38 sec\n",
      "[2021-02-03 01:30:07,628 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:07,633 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:08,099 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:08,104 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:08,570 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:08,574 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:08,727 INFO] Step 1500/ 1800; acc:  97.53; ppl:  1.10; xent: 0.10; lr: 1.00000; 6564/6444 tok/s;     39 sec\n",
      "[2021-02-03 01:30:09,035 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:09,039 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:09,518 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:09,523 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:09,977 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:09,981 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:10,040 INFO] Step 1550/ 1800; acc:  97.80; ppl:  1.09; xent: 0.08; lr: 1.00000; 6572/6452 tok/s;     40 sec\n",
      "[2021-02-03 01:30:10,462 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:10,467 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:10,928 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:10,932 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:11,331 INFO] Step 1600/ 1800; acc:  98.16; ppl:  1.07; xent: 0.07; lr: 1.00000; 6502/6384 tok/s;     42 sec\n",
      "[2021-02-03 01:30:11,409 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:11,414 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:11,884 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:11,888 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:12,348 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:12,352 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:12,637 INFO] Step 1650/ 1800; acc:  98.07; ppl:  1.08; xent: 0.08; lr: 1.00000; 6592/6485 tok/s;     43 sec\n",
      "[2021-02-03 01:30:12,802 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:12,807 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:13,280 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:13,284 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:13,744 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:13,748 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:13,949 INFO] Step 1700/ 1800; acc:  98.24; ppl:  1.07; xent: 0.07; lr: 1.00000; 6640/6523 tok/s;     44 sec\n",
      "[2021-02-03 01:30:14,225 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:14,229 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:14,695 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:14,699 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:15,153 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:15,158 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:15,259 INFO] Step 1750/ 1800; acc:  98.11; ppl:  1.08; xent: 0.08; lr: 1.00000; 6581/6449 tok/s;     46 sec\n",
      "[2021-02-03 01:30:15,617 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:15,622 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:16,075 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_15/processed.train.0.pt\n",
      "[2021-02-03 01:30:16,080 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:16,563 INFO] Step 1800/ 1800; acc:  98.39; ppl:  1.07; xent: 0.06; lr: 1.00000; 6594/6476 tok/s;     47 sec\n",
      "[2021-02-03 01:30:16,564 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_15_step_1800.pt\n",
      "[2021-02-03 01:30:17,669 INFO]  * src vocab size = 31\n",
      "[2021-02-03 01:30:17,669 INFO]  * tgt vocab size = 29\n",
      "[2021-02-03 01:30:17,669 INFO] Building model...\n",
      "[2021-02-03 01:30:22,257 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(31, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(29, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=29, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-02-03 01:30:22,257 INFO] encoder: 250900\n",
      "[2021-02-03 01:30:22,257 INFO] decoder: 323229\n",
      "[2021-02-03 01:30:22,257 INFO] * number of parameters: 574129\n",
      "[2021-02-03 01:30:22,260 INFO] Starting training on GPU: [0]\n",
      "[2021-02-03 01:30:22,260 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-02-03 01:30:22,260 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:22,264 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:22,734 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:22,738 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:23,197 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:23,201 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:23,579 INFO] Step 50/ 1800; acc:  13.93; ppl: 35.04; xent: 3.56; lr: 1.00000; 6676/6573 tok/s;      1 sec\n",
      "[2021-02-03 01:30:23,668 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:23,672 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:24,167 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:24,172 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:24,631 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:24,635 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:24,889 INFO] Step 100/ 1800; acc:  26.39; ppl: 13.26; xent: 2.58; lr: 1.00000; 6628/6505 tok/s;      3 sec\n",
      "[2021-02-03 01:30:25,091 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:25,095 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:25,557 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:25,561 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:26,034 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:26,039 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:26,181 INFO] Step 150/ 1800; acc:  34.35; ppl:  9.71; xent: 2.27; lr: 1.00000; 6699/6567 tok/s;      4 sec\n",
      "[2021-02-03 01:30:26,506 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:26,511 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:26,998 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:27,003 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:27,488 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:27,493 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:27,546 INFO] Step 200/ 1800; acc:  36.43; ppl:  8.61; xent: 2.15; lr: 1.00000; 6454/6330 tok/s;      5 sec\n",
      "[2021-02-03 01:30:27,988 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:27,993 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:28,456 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:28,461 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:28,886 INFO] Step 250/ 1800; acc:  37.30; ppl:  8.12; xent: 2.09; lr: 1.00000; 6535/6420 tok/s;      7 sec\n",
      "[2021-02-03 01:30:28,932 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:28,937 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:29,397 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:29,402 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:29,884 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:29,889 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:30,203 INFO] Step 300/ 1800; acc:  38.85; ppl:  7.70; xent: 2.04; lr: 1.00000; 6655/6552 tok/s;      8 sec\n",
      "[2021-02-03 01:30:30,347 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:30,352 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:30,826 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:30,832 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:31,288 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:31,293 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:31,507 INFO] Step 350/ 1800; acc:  39.91; ppl:  7.09; xent: 1.96; lr: 1.00000; 6706/6583 tok/s;      9 sec\n",
      "[2021-02-03 01:30:31,752 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:31,757 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:32,234 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:32,262 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:32,712 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:32,717 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:32,806 INFO] Step 400/ 1800; acc:  43.29; ppl:  6.21; xent: 1.83; lr: 1.00000; 6640/6503 tok/s;     11 sec\n",
      "[2021-02-03 01:30:33,184 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:33,188 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:33,648 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:33,652 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:34,111 INFO] Step 450/ 1800; acc:  45.50; ppl:  5.79; xent: 1.76; lr: 1.00000; 6756/6633 tok/s;     12 sec\n",
      "[2021-02-03 01:30:34,111 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:34,116 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:34,570 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:34,574 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:35,047 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:35,052 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:35,425 INFO] Step 500/ 1800; acc:  51.37; ppl:  4.65; xent: 1.54; lr: 1.00000; 6701/6597 tok/s;     13 sec\n",
      "[2021-02-03 01:30:35,519 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:35,523 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:35,975 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:35,979 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:36,431 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:36,435 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:36,690 INFO] Step 550/ 1800; acc:  58.89; ppl:  3.69; xent: 1.31; lr: 1.00000; 6864/6736 tok/s;     14 sec\n",
      "[2021-02-03 01:30:36,902 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:36,907 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:37,390 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:37,394 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:37,875 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:37,880 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:38,025 INFO] Step 600/ 1800; acc:  73.57; ppl:  2.32; xent: 0.84; lr: 1.00000; 6486/6358 tok/s;     16 sec\n",
      "[2021-02-03 01:30:38,354 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:38,358 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:38,829 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:38,833 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:39,315 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:39,319 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:39,370 INFO] Step 650/ 1800; acc:  81.98; ppl:  1.86; xent: 0.62; lr: 1.00000; 6548/6421 tok/s;     17 sec\n",
      "[2021-02-03 01:30:39,785 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:39,789 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:40,243 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:40,248 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:40,658 INFO] Step 700/ 1800; acc:  87.35; ppl:  1.54; xent: 0.43; lr: 1.00000; 6800/6680 tok/s;     18 sec\n",
      "[2021-02-03 01:30:40,704 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:40,708 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:41,166 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:41,170 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:41,626 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:41,631 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:41,956 INFO] Step 750/ 1800; acc:  90.28; ppl:  1.38; xent: 0.32; lr: 1.00000; 6756/6652 tok/s;     20 sec\n",
      "[2021-02-03 01:30:42,110 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:42,114 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:42,587 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:42,592 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:43,050 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:43,054 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:43,266 INFO] Step 800/ 1800; acc:  93.12; ppl:  1.26; xent: 0.23; lr: 1.00000; 6670/6548 tok/s;     21 sec\n",
      "[2021-02-03 01:30:43,508 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:43,513 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:44,009 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:44,014 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:44,473 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:44,478 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:44,567 INFO] Step 850/ 1800; acc:  95.05; ppl:  1.18; xent: 0.17; lr: 1.00000; 6629/6493 tok/s;     22 sec\n",
      "[2021-02-03 01:30:44,948 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:44,953 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:45,430 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:45,434 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:45,893 INFO] Step 900/ 1800; acc:  94.10; ppl:  1.24; xent: 0.21; lr: 1.00000; 6651/6530 tok/s;     24 sec\n",
      "[2021-02-03 01:30:45,893 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:45,898 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:46,354 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:46,358 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:46,823 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:46,827 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:47,204 INFO] Step 950/ 1800; acc:  95.89; ppl:  1.14; xent: 0.13; lr: 1.00000; 6715/6612 tok/s;     25 sec\n",
      "[2021-02-03 01:30:47,305 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:47,309 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:47,774 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:47,779 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:48,241 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:48,246 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:48,506 INFO] Step 1000/ 1800; acc:  95.48; ppl:  1.17; xent: 0.15; lr: 1.00000; 6671/6547 tok/s;     26 sec\n",
      "[2021-02-03 01:30:48,712 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:48,716 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:49,194 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:49,199 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:49,654 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:49,660 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:49,798 INFO] Step 1050/ 1800; acc:  96.70; ppl:  1.13; xent: 0.12; lr: 1.00000; 6700/6567 tok/s;     28 sec\n",
      "[2021-02-03 01:30:50,114 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:50,119 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:50,617 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:50,621 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:51,082 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:51,087 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:51,135 INFO] Step 1100/ 1800; acc:  97.05; ppl:  1.10; xent: 0.10; lr: 1.00000; 6588/6461 tok/s;     29 sec\n",
      "[2021-02-03 01:30:51,538 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:51,543 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:52,006 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:52,010 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:52,428 INFO] Step 1150/ 1800; acc:  96.84; ppl:  1.12; xent: 0.11; lr: 1.00000; 6773/6654 tok/s;     30 sec\n",
      "[2021-02-03 01:30:52,475 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:52,479 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:52,956 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:52,980 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:53,438 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:53,442 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:53,753 INFO] Step 1200/ 1800; acc:  97.09; ppl:  1.11; xent: 0.11; lr: 1.00000; 6614/6512 tok/s;     31 sec\n",
      "[2021-02-03 01:30:53,894 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:53,899 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:54,364 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:54,367 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:54,841 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:54,845 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:55,055 INFO] Step 1250/ 1800; acc:  96.40; ppl:  1.14; xent: 0.13; lr: 1.00000; 6717/6594 tok/s;     33 sec\n",
      "[2021-02-03 01:30:55,302 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:55,306 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:55,783 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:55,787 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:56,245 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:56,249 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:56,337 INFO] Step 1300/ 1800; acc:  97.32; ppl:  1.11; xent: 0.10; lr: 1.00000; 6726/6587 tok/s;     34 sec\n",
      "[2021-02-03 01:30:56,710 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:56,714 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:57,170 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:57,175 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:57,634 INFO] Step 1350/ 1800; acc:  97.25; ppl:  1.11; xent: 0.11; lr: 1.00000; 6801/6678 tok/s;     35 sec\n",
      "[2021-02-03 01:30:57,634 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:57,638 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:58,100 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:58,104 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:58,571 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:58,575 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:58,954 INFO] Step 1400/ 1800; acc:  97.72; ppl:  1.09; xent: 0.09; lr: 1.00000; 6670/6567 tok/s;     37 sec\n",
      "[2021-02-03 01:30:59,046 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:59,050 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:59,503 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:59,507 INFO] number of examples: 360\n",
      "[2021-02-03 01:30:59,975 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:30:59,979 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:00,237 INFO] Step 1450/ 1800; acc:  97.75; ppl:  1.10; xent: 0.10; lr: 1.00000; 6764/6639 tok/s;     38 sec\n",
      "[2021-02-03 01:31:00,444 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:00,450 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:00,923 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:00,927 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:01,401 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:01,405 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:01,547 INFO] Step 1500/ 1800; acc:  98.22; ppl:  1.07; xent: 0.07; lr: 1.00000; 6612/6482 tok/s;     39 sec\n",
      "[2021-02-03 01:31:01,866 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:01,870 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:02,347 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:02,352 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:02,811 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:02,817 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:02,867 INFO] Step 1550/ 1800; acc:  97.92; ppl:  1.11; xent: 0.10; lr: 1.00000; 6671/6543 tok/s;     41 sec\n",
      "[2021-02-03 01:31:03,305 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:03,309 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:03,775 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:03,779 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:04,204 INFO] Step 1600/ 1800; acc:  97.77; ppl:  1.10; xent: 0.09; lr: 1.00000; 6550/6435 tok/s;     42 sec\n",
      "[2021-02-03 01:31:04,253 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:04,258 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:04,721 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:04,725 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:05,183 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:05,188 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:05,507 INFO] Step 1650/ 1800; acc:  98.40; ppl:  1.07; xent: 0.07; lr: 1.00000; 6730/6626 tok/s;     43 sec\n",
      "[2021-02-03 01:31:05,651 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:05,655 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:06,148 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:06,153 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:06,614 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:06,619 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:06,829 INFO] Step 1700/ 1800; acc:  98.33; ppl:  1.08; xent: 0.07; lr: 1.00000; 6610/6489 tok/s;     45 sec\n",
      "[2021-02-03 01:31:07,097 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:07,102 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:07,561 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:07,565 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:08,033 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:08,038 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:08,129 INFO] Step 1750/ 1800; acc:  98.18; ppl:  1.08; xent: 0.08; lr: 1.00000; 6633/6496 tok/s;     46 sec\n",
      "[2021-02-03 01:31:08,515 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:08,519 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:08,991 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_16/processed.train.0.pt\n",
      "[2021-02-03 01:31:08,995 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:09,454 INFO] Step 1800/ 1800; acc:  97.08; ppl:  1.15; xent: 0.14; lr: 1.00000; 6655/6534 tok/s;     47 sec\n",
      "[2021-02-03 01:31:09,456 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_16_step_1800.pt\n",
      "[2021-02-03 01:31:10,539 INFO]  * src vocab size = 32\n",
      "[2021-02-03 01:31:10,539 INFO]  * tgt vocab size = 30\n",
      "[2021-02-03 01:31:10,539 INFO] Building model...\n",
      "[2021-02-03 01:31:15,068 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(32, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(30, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=30, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-02-03 01:31:15,068 INFO] encoder: 251200\n",
      "[2021-02-03 01:31:15,068 INFO] decoder: 323630\n",
      "[2021-02-03 01:31:15,068 INFO] * number of parameters: 574830\n",
      "[2021-02-03 01:31:15,071 INFO] Starting training on GPU: [0]\n",
      "[2021-02-03 01:31:15,071 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-02-03 01:31:15,072 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:15,076 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:15,546 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:15,550 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:16,031 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:16,035 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:16,385 INFO] Step 50/ 1800; acc:  14.85; ppl: 28.77; xent: 3.36; lr: 1.00000; 6642/6548 tok/s;      1 sec\n",
      "[2021-02-03 01:31:16,482 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:16,486 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:16,948 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:16,953 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:17,394 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:17,399 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:17,658 INFO] Step 100/ 1800; acc:  26.30; ppl: 14.03; xent: 2.64; lr: 1.00000; 6905/6791 tok/s;      3 sec\n",
      "[2021-02-03 01:31:17,844 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:17,848 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:18,330 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:18,334 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:18,792 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:18,796 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:18,953 INFO] Step 150/ 1800; acc:  33.90; ppl: 10.26; xent: 2.33; lr: 1.00000; 6699/6600 tok/s;      4 sec\n",
      "[2021-02-03 01:31:19,251 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:19,258 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:19,716 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:19,720 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:20,175 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:20,180 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:20,228 INFO] Step 200/ 1800; acc:  36.75; ppl:  8.80; xent: 2.17; lr: 1.00000; 6767/6651 tok/s;      5 sec\n",
      "[2021-02-03 01:31:20,639 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:20,644 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:21,098 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:21,103 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:21,499 INFO] Step 250/ 1800; acc:  36.81; ppl:  8.52; xent: 2.14; lr: 1.00000; 6836/6750 tok/s;      6 sec\n",
      "[2021-02-03 01:31:21,552 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:21,557 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:22,009 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:22,014 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:22,470 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:22,474 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:22,786 INFO] Step 300/ 1800; acc:  38.74; ppl:  7.84; xent: 2.06; lr: 1.00000; 6879/6774 tok/s;      8 sec\n",
      "[2021-02-03 01:31:22,927 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:22,931 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:23,407 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:23,411 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:23,858 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:23,862 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:24,076 INFO] Step 350/ 1800; acc:  40.35; ppl:  7.19; xent: 1.97; lr: 1.00000; 6734/6628 tok/s;      9 sec\n",
      "[2021-02-03 01:31:24,313 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:24,318 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:24,781 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:24,805 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:25,264 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:25,268 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:25,357 INFO] Step 400/ 1800; acc:  41.59; ppl:  6.73; xent: 1.91; lr: 1.00000; 6675/6562 tok/s;     10 sec\n",
      "[2021-02-03 01:31:25,714 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:25,718 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:26,184 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:26,188 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:26,651 INFO] Step 450/ 1800; acc:  44.85; ppl:  6.05; xent: 1.80; lr: 1.00000; 6809/6710 tok/s;     12 sec\n",
      "[2021-02-03 01:31:26,651 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:26,656 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:27,113 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:27,118 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:27,578 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:27,582 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:27,944 INFO] Step 500/ 1800; acc:  47.16; ppl:  5.55; xent: 1.71; lr: 1.00000; 6745/6649 tok/s;     13 sec\n",
      "[2021-02-03 01:31:28,046 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:28,051 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:28,511 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:28,515 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:28,961 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:28,965 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:29,234 INFO] Step 550/ 1800; acc:  50.88; ppl:  4.92; xent: 1.59; lr: 1.00000; 6820/6708 tok/s;     14 sec\n",
      "[2021-02-03 01:31:29,420 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:29,424 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:29,897 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:29,902 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:30,355 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:30,360 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:30,516 INFO] Step 600/ 1800; acc:  58.63; ppl:  3.74; xent: 1.32; lr: 1.00000; 6765/6665 tok/s;     15 sec\n",
      "[2021-02-03 01:31:30,803 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:30,807 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:31,266 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:31,271 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:31,725 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:31,729 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:31,775 INFO] Step 650/ 1800; acc:  67.82; ppl:  2.79; xent: 1.03; lr: 1.00000; 6851/6733 tok/s;     17 sec\n",
      "[2021-02-03 01:31:32,184 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:32,188 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:32,648 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:32,652 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:33,068 INFO] Step 700/ 1800; acc:  78.71; ppl:  2.00; xent: 0.69; lr: 1.00000; 6716/6632 tok/s;     18 sec\n",
      "[2021-02-03 01:31:33,131 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:33,135 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:33,603 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:33,607 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:34,088 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:34,093 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:34,418 INFO] Step 750/ 1800; acc:  84.73; ppl:  1.66; xent: 0.51; lr: 1.00000; 6562/6461 tok/s;     19 sec\n",
      "[2021-02-03 01:31:34,557 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:34,562 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:35,038 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:35,042 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:35,504 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:35,508 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:35,721 INFO] Step 800/ 1800; acc:  89.63; ppl:  1.42; xent: 0.35; lr: 1.00000; 6670/6565 tok/s;     21 sec\n",
      "[2021-02-03 01:31:35,979 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:35,983 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:36,465 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:36,469 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:36,929 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:36,933 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:37,026 INFO] Step 850/ 1800; acc:  92.99; ppl:  1.27; xent: 0.24; lr: 1.00000; 6550/6439 tok/s;     22 sec\n",
      "[2021-02-03 01:31:37,398 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:37,402 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:37,859 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:37,864 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:38,338 INFO] Step 900/ 1800; acc:  93.31; ppl:  1.26; xent: 0.23; lr: 1.00000; 6718/6619 tok/s;     23 sec\n",
      "[2021-02-03 01:31:38,338 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:38,342 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:38,793 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:38,797 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:39,272 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:39,276 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:39,634 INFO] Step 950/ 1800; acc:  95.41; ppl:  1.16; xent: 0.15; lr: 1.00000; 6731/6636 tok/s;     25 sec\n",
      "[2021-02-03 01:31:39,733 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:39,737 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:40,209 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:40,213 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:40,677 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:40,681 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:40,947 INFO] Step 1000/ 1800; acc:  94.72; ppl:  1.22; xent: 0.20; lr: 1.00000; 6700/6589 tok/s;     26 sec\n",
      "[2021-02-03 01:31:41,140 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:41,144 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:41,620 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:41,625 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:42,094 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:42,098 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:42,262 INFO] Step 1050/ 1800; acc:  95.17; ppl:  1.20; xent: 0.18; lr: 1.00000; 6594/6497 tok/s;     27 sec\n",
      "[2021-02-03 01:31:42,556 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:42,560 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:43,010 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:43,015 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:43,478 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:43,482 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:43,528 INFO] Step 1100/ 1800; acc:  95.53; ppl:  1.17; xent: 0.16; lr: 1.00000; 6812/6695 tok/s;     28 sec\n",
      "[2021-02-03 01:31:43,945 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:43,950 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:44,410 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:44,414 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:44,820 INFO] Step 1150/ 1800; acc:  96.14; ppl:  1.15; xent: 0.14; lr: 1.00000; 6722/6638 tok/s;     30 sec\n",
      "[2021-02-03 01:31:44,879 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:44,883 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:45,334 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:45,362 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:45,817 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:45,821 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:46,146 INFO] Step 1200/ 1800; acc:  96.80; ppl:  1.12; xent: 0.11; lr: 1.00000; 6679/6576 tok/s;     31 sec\n",
      "[2021-02-03 01:31:46,292 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:46,296 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:46,752 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:46,757 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:47,204 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:47,209 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:47,431 INFO] Step 1250/ 1800; acc:  96.84; ppl:  1.12; xent: 0.11; lr: 1.00000; 6765/6659 tok/s;     32 sec\n",
      "[2021-02-03 01:31:47,674 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:47,679 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:48,154 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:48,158 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:48,648 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:48,653 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:48,743 INFO] Step 1300/ 1800; acc:  97.26; ppl:  1.12; xent: 0.11; lr: 1.00000; 6513/6402 tok/s;     34 sec\n",
      "[2021-02-03 01:31:49,106 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:49,110 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:49,560 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:49,565 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:50,046 INFO] Step 1350/ 1800; acc:  97.16; ppl:  1.12; xent: 0.11; lr: 1.00000; 6764/6665 tok/s;     35 sec\n",
      "[2021-02-03 01:31:50,047 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:50,051 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:50,521 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:50,525 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:50,991 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:50,996 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:51,371 INFO] Step 1400/ 1800; acc:  97.36; ppl:  1.11; xent: 0.10; lr: 1.00000; 6587/6493 tok/s;     36 sec\n",
      "[2021-02-03 01:31:51,465 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:51,469 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:51,939 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:51,944 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:52,412 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:52,416 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:52,681 INFO] Step 1450/ 1800; acc:  97.17; ppl:  1.12; xent: 0.11; lr: 1.00000; 6711/6600 tok/s;     38 sec\n",
      "[2021-02-03 01:31:52,881 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:52,886 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:53,372 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:53,377 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:53,850 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:53,855 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:54,019 INFO] Step 1500/ 1800; acc:  97.28; ppl:  1.10; xent: 0.10; lr: 1.00000; 6485/6389 tok/s;     39 sec\n",
      "[2021-02-03 01:31:54,315 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:54,319 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:54,786 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:54,790 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:55,260 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:55,265 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:55,314 INFO] Step 1550/ 1800; acc:  97.77; ppl:  1.10; xent: 0.09; lr: 1.00000; 6660/6546 tok/s;     40 sec\n",
      "[2021-02-03 01:31:55,736 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:55,741 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:56,201 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:56,206 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:56,601 INFO] Step 1600/ 1800; acc:  98.05; ppl:  1.08; xent: 0.08; lr: 1.00000; 6751/6666 tok/s;     42 sec\n",
      "[2021-02-03 01:31:56,655 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:56,660 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:57,121 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:57,126 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:57,585 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:57,589 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:57,913 INFO] Step 1650/ 1800; acc:  97.49; ppl:  1.11; xent: 0.10; lr: 1.00000; 6750/6647 tok/s;     43 sec\n",
      "[2021-02-03 01:31:58,052 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:58,056 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:58,526 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:58,530 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:58,998 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:59,003 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:59,216 INFO] Step 1700/ 1800; acc:  97.39; ppl:  1.13; xent: 0.12; lr: 1.00000; 6671/6567 tok/s;     44 sec\n",
      "[2021-02-03 01:31:59,467 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:59,472 INFO] number of examples: 360\n",
      "[2021-02-03 01:31:59,913 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:31:59,917 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:00,363 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:32:00,368 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:00,458 INFO] Step 1750/ 1800; acc:  97.87; ppl:  1.09; xent: 0.09; lr: 1.00000; 6880/6763 tok/s;     45 sec\n",
      "[2021-02-03 01:32:00,826 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:32:00,831 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:01,280 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_17/processed.train.0.pt\n",
      "[2021-02-03 01:32:01,285 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:01,733 INFO] Step 1800/ 1800; acc:  97.98; ppl:  1.09; xent: 0.09; lr: 1.00000; 6913/6811 tok/s;     47 sec\n",
      "[2021-02-03 01:32:01,734 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_17_step_1800.pt\n",
      "[2021-02-03 01:32:02,793 INFO]  * src vocab size = 32\n",
      "[2021-02-03 01:32:02,793 INFO]  * tgt vocab size = 30\n",
      "[2021-02-03 01:32:02,793 INFO] Building model...\n",
      "[2021-02-03 01:32:07,283 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(32, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(30, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=30, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-02-03 01:32:07,283 INFO] encoder: 251200\n",
      "[2021-02-03 01:32:07,283 INFO] decoder: 323630\n",
      "[2021-02-03 01:32:07,283 INFO] * number of parameters: 574830\n",
      "[2021-02-03 01:32:07,286 INFO] Starting training on GPU: [0]\n",
      "[2021-02-03 01:32:07,286 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-02-03 01:32:07,287 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:07,291 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:07,757 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:07,761 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:08,210 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:08,215 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:08,568 INFO] Step 50/ 1800; acc:  12.35; ppl: 38.68; xent: 3.66; lr: 1.00000; 6781/6673 tok/s;      1 sec\n",
      "[2021-02-03 01:32:08,672 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:08,676 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:09,124 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:09,129 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:09,571 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:09,575 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:09,832 INFO] Step 100/ 1800; acc:  26.46; ppl: 14.08; xent: 2.64; lr: 1.00000; 6926/6826 tok/s;      3 sec\n",
      "[2021-02-03 01:32:10,034 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:10,038 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:10,505 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:10,510 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:10,967 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:10,971 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:11,124 INFO] Step 150/ 1800; acc:  32.22; ppl: 10.93; xent: 2.39; lr: 1.00000; 6694/6582 tok/s;      4 sec\n",
      "[2021-02-03 01:32:11,416 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:11,420 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:11,892 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:11,896 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:12,349 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:12,353 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:12,416 INFO] Step 200/ 1800; acc:  34.82; ppl:  9.69; xent: 2.27; lr: 1.00000; 6792/6684 tok/s;      5 sec\n",
      "[2021-02-03 01:32:12,794 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:12,799 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:13,266 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:13,270 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:13,673 INFO] Step 250/ 1800; acc:  35.81; ppl:  8.67; xent: 2.16; lr: 1.00000; 6844/6730 tok/s;      6 sec\n",
      "[2021-02-03 01:32:13,723 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:13,727 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:14,175 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:14,179 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:14,626 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:14,631 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:14,939 INFO] Step 300/ 1800; acc:  37.80; ppl:  7.91; xent: 2.07; lr: 1.00000; 6903/6793 tok/s;      8 sec\n",
      "[2021-02-03 01:32:15,078 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:15,082 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:15,544 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:15,548 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:15,998 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:16,002 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:16,203 INFO] Step 350/ 1800; acc:  38.93; ppl:  7.46; xent: 2.01; lr: 1.00000; 6872/6769 tok/s;      9 sec\n",
      "[2021-02-03 01:32:16,446 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:16,451 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:16,920 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:16,948 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:17,404 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:17,408 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:17,513 INFO] Step 400/ 1800; acc:  40.55; ppl:  6.85; xent: 1.92; lr: 1.00000; 6638/6538 tok/s;     10 sec\n",
      "[2021-02-03 01:32:17,844 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:17,849 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:18,313 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:18,317 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:18,784 INFO] Step 450/ 1800; acc:  42.56; ppl:  6.33; xent: 1.85; lr: 1.00000; 6800/6675 tok/s;     11 sec\n",
      "[2021-02-03 01:32:18,784 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:18,789 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:19,251 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:19,255 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:19,712 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:19,716 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:20,080 INFO] Step 500/ 1800; acc:  46.47; ppl:  5.51; xent: 1.71; lr: 1.00000; 6706/6600 tok/s;     13 sec\n",
      "[2021-02-03 01:32:20,178 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:20,183 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:20,650 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:20,654 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:21,111 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:21,115 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:21,386 INFO] Step 550/ 1800; acc:  48.72; ppl:  5.05; xent: 1.62; lr: 1.00000; 6702/6605 tok/s;     14 sec\n",
      "[2021-02-03 01:32:21,592 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:21,596 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:22,073 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:22,077 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:22,548 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:22,552 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:22,708 INFO] Step 600/ 1800; acc:  54.47; ppl:  4.25; xent: 1.45; lr: 1.00000; 6543/6434 tok/s;     15 sec\n",
      "[2021-02-03 01:32:23,015 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:23,020 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:23,490 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:23,494 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:23,956 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:23,960 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:24,024 INFO] Step 650/ 1800; acc:  63.34; ppl:  3.28; xent: 1.19; lr: 1.00000; 6668/6561 tok/s;     17 sec\n",
      "[2021-02-03 01:32:24,426 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:24,430 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:24,887 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:24,891 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:25,315 INFO] Step 700/ 1800; acc:  74.10; ppl:  2.29; xent: 0.83; lr: 1.00000; 6664/6554 tok/s;     18 sec\n",
      "[2021-02-03 01:32:25,367 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:25,371 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:25,837 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:25,841 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:26,303 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:26,307 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:26,627 INFO] Step 750/ 1800; acc:  82.83; ppl:  1.81; xent: 0.59; lr: 1.00000; 6660/6554 tok/s;     19 sec\n",
      "[2021-02-03 01:32:26,770 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:26,775 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:27,241 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:27,245 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:27,721 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:27,725 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:27,936 INFO] Step 800/ 1800; acc:  87.76; ppl:  1.52; xent: 0.42; lr: 1.00000; 6638/6538 tok/s;     21 sec\n",
      "[2021-02-03 01:32:28,180 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:28,185 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:28,646 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:28,650 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:29,113 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:29,117 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:29,227 INFO] Step 850/ 1800; acc:  90.75; ppl:  1.35; xent: 0.30; lr: 1.00000; 6735/6634 tok/s;     22 sec\n",
      "[2021-02-03 01:32:29,581 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:29,586 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:30,059 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:30,064 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:30,526 INFO] Step 900/ 1800; acc:  93.66; ppl:  1.23; xent: 0.21; lr: 1.00000; 6654/6532 tok/s;     23 sec\n",
      "[2021-02-03 01:32:30,526 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:30,530 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:30,982 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:30,986 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:31,466 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:31,470 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:31,832 INFO] Step 950/ 1800; acc:  94.01; ppl:  1.24; xent: 0.21; lr: 1.00000; 6655/6549 tok/s;     25 sec\n",
      "[2021-02-03 01:32:31,929 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:31,933 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:32,402 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:32,407 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:32,862 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:32,866 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:33,134 INFO] Step 1000/ 1800; acc:  94.19; ppl:  1.21; xent: 0.19; lr: 1.00000; 6722/6626 tok/s;     26 sec\n",
      "[2021-02-03 01:32:33,339 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:33,344 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:33,789 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:33,794 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:34,255 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:34,259 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:34,411 INFO] Step 1050/ 1800; acc:  95.88; ppl:  1.16; xent: 0.15; lr: 1.00000; 6771/6658 tok/s;     27 sec\n",
      "[2021-02-03 01:32:34,719 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:34,723 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:35,195 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:35,200 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:35,643 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:35,647 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:35,710 INFO] Step 1100/ 1800; acc:  96.24; ppl:  1.14; xent: 0.13; lr: 1.00000; 6756/6649 tok/s;     28 sec\n",
      "[2021-02-03 01:32:36,100 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:36,105 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:36,548 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:36,552 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:36,966 INFO] Step 1150/ 1800; acc:  96.41; ppl:  1.14; xent: 0.13; lr: 1.00000; 6853/6740 tok/s;     30 sec\n",
      "[2021-02-03 01:32:37,013 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:37,017 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:37,473 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:37,499 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:37,955 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:37,959 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:38,274 INFO] Step 1200/ 1800; acc:  96.96; ppl:  1.12; xent: 0.11; lr: 1.00000; 6679/6573 tok/s;     31 sec\n",
      "[2021-02-03 01:32:38,414 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:38,418 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:38,862 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:38,866 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:39,322 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:39,326 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:39,531 INFO] Step 1250/ 1800; acc:  96.96; ppl:  1.12; xent: 0.11; lr: 1.00000; 6912/6808 tok/s;     32 sec\n",
      "[2021-02-03 01:32:39,777 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:39,780 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:40,241 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:40,246 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:40,699 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:40,703 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:40,810 INFO] Step 1300/ 1800; acc:  96.49; ppl:  1.14; xent: 0.13; lr: 1.00000; 6797/6694 tok/s;     34 sec\n",
      "[2021-02-03 01:32:41,170 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:41,174 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:41,626 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:41,630 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:42,077 INFO] Step 1350/ 1800; acc:  97.22; ppl:  1.12; xent: 0.11; lr: 1.00000; 6822/6696 tok/s;     35 sec\n",
      "[2021-02-03 01:32:42,078 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:42,081 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:42,524 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:42,528 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:42,975 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:42,980 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:43,336 INFO] Step 1400/ 1800; acc:  97.23; ppl:  1.12; xent: 0.11; lr: 1.00000; 6902/6792 tok/s;     36 sec\n",
      "[2021-02-03 01:32:43,441 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:43,446 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:43,904 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:43,909 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:44,362 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:44,366 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:44,624 INFO] Step 1450/ 1800; acc:  97.51; ppl:  1.10; xent: 0.09; lr: 1.00000; 6799/6701 tok/s;     37 sec\n",
      "[2021-02-03 01:32:44,811 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:44,816 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:45,283 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:45,287 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:45,735 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:45,739 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:45,892 INFO] Step 1500/ 1800; acc:  97.65; ppl:  1.10; xent: 0.10; lr: 1.00000; 6820/6707 tok/s;     39 sec\n",
      "[2021-02-03 01:32:46,212 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:46,216 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:46,658 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:46,662 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:47,117 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:47,121 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:47,185 INFO] Step 1550/ 1800; acc:  97.57; ppl:  1.10; xent: 0.09; lr: 1.00000; 6785/6677 tok/s;     40 sec\n",
      "[2021-02-03 01:32:47,591 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:47,596 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:48,065 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:48,069 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:48,467 INFO] Step 1600/ 1800; acc:  97.70; ppl:  1.09; xent: 0.09; lr: 1.00000; 6715/6604 tok/s;     41 sec\n",
      "[2021-02-03 01:32:48,517 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:48,522 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:48,973 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:48,978 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:49,441 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:49,445 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:49,754 INFO] Step 1650/ 1800; acc:  97.74; ppl:  1.09; xent: 0.09; lr: 1.00000; 6786/6678 tok/s;     42 sec\n",
      "[2021-02-03 01:32:49,900 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:49,904 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:50,382 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:50,387 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:50,837 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:50,841 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:51,053 INFO] Step 1700/ 1800; acc:  97.59; ppl:  1.11; xent: 0.10; lr: 1.00000; 6688/6588 tok/s;     44 sec\n",
      "[2021-02-03 01:32:51,301 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:51,305 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:51,759 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:51,763 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:52,222 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:52,226 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:52,335 INFO] Step 1750/ 1800; acc:  97.98; ppl:  1.09; xent: 0.09; lr: 1.00000; 6783/6680 tok/s;     45 sec\n",
      "[2021-02-03 01:32:52,678 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:52,682 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:53,147 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_18/processed.train.0.pt\n",
      "[2021-02-03 01:32:53,151 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:53,608 INFO] Step 1800/ 1800; acc:  97.82; ppl:  1.09; xent: 0.09; lr: 1.00000; 6788/6664 tok/s;     46 sec\n",
      "[2021-02-03 01:32:53,610 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_18_step_1800.pt\n",
      "[2021-02-03 01:32:54,660 INFO]  * src vocab size = 32\n",
      "[2021-02-03 01:32:54,660 INFO]  * tgt vocab size = 30\n",
      "[2021-02-03 01:32:54,660 INFO] Building model...\n",
      "[2021-02-03 01:32:59,113 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(32, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(30, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=30, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-02-03 01:32:59,114 INFO] encoder: 251200\n",
      "[2021-02-03 01:32:59,114 INFO] decoder: 323630\n",
      "[2021-02-03 01:32:59,114 INFO] * number of parameters: 574830\n",
      "[2021-02-03 01:32:59,117 INFO] Starting training on GPU: [0]\n",
      "[2021-02-03 01:32:59,117 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-02-03 01:32:59,117 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:32:59,121 INFO] number of examples: 360\n",
      "[2021-02-03 01:32:59,583 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:32:59,587 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:00,039 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:00,044 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:00,401 INFO] Step 50/ 1800; acc:  13.72; ppl: 30.53; xent: 3.42; lr: 1.00000; 6808/6699 tok/s;      1 sec\n",
      "[2021-02-03 01:33:00,488 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:00,492 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:00,937 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:00,942 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:01,402 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:01,406 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:01,667 INFO] Step 100/ 1800; acc:  24.70; ppl: 14.79; xent: 2.69; lr: 1.00000; 6830/6722 tok/s;      3 sec\n",
      "[2021-02-03 01:33:01,866 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:01,871 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:02,325 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:02,329 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:02,771 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:02,776 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:02,933 INFO] Step 150/ 1800; acc:  32.27; ppl: 11.21; xent: 2.42; lr: 1.00000; 6846/6739 tok/s;      4 sec\n",
      "[2021-02-03 01:33:03,225 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:03,230 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:03,698 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:03,702 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:04,159 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:04,163 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:04,209 INFO] Step 200/ 1800; acc:  34.98; ppl:  9.38; xent: 2.24; lr: 1.00000; 6649/6539 tok/s;      5 sec\n",
      "[2021-02-03 01:33:04,614 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:04,619 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:05,072 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:05,077 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:05,497 INFO] Step 250/ 1800; acc:  36.70; ppl:  8.34; xent: 2.12; lr: 1.00000; 6804/6691 tok/s;      6 sec\n",
      "[2021-02-03 01:33:05,538 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:05,543 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:06,011 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:06,015 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:06,472 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:06,476 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:06,784 INFO] Step 300/ 1800; acc:  38.99; ppl:  7.51; xent: 2.02; lr: 1.00000; 6694/6587 tok/s;      8 sec\n",
      "[2021-02-03 01:33:06,926 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:06,931 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:07,403 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:07,407 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:07,863 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:07,867 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:08,085 INFO] Step 350/ 1800; acc:  39.99; ppl:  6.87; xent: 1.93; lr: 1.00000; 6693/6589 tok/s;      9 sec\n",
      "[2021-02-03 01:33:08,333 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:08,338 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:08,806 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:08,833 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:09,301 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:09,305 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:09,416 INFO] Step 400/ 1800; acc:  42.73; ppl:  6.31; xent: 1.84; lr: 1.00000; 6484/6377 tok/s;     10 sec\n",
      "[2021-02-03 01:33:09,781 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:09,785 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:10,273 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:10,277 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:10,730 INFO] Step 450/ 1800; acc:  45.29; ppl:  5.75; xent: 1.75; lr: 1.00000; 6514/6409 tok/s;     12 sec\n",
      "[2021-02-03 01:33:10,730 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:10,734 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:11,192 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:11,196 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:11,662 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:11,666 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:12,037 INFO] Step 500/ 1800; acc:  49.61; ppl:  5.12; xent: 1.63; lr: 1.00000; 6686/6579 tok/s;     13 sec\n",
      "[2021-02-03 01:33:12,126 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:12,131 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:12,580 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:12,584 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:13,046 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:13,050 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:13,325 INFO] Step 550/ 1800; acc:  53.52; ppl:  4.52; xent: 1.51; lr: 1.00000; 6716/6610 tok/s;     14 sec\n",
      "[2021-02-03 01:33:13,514 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:13,518 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:13,984 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:13,988 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:14,432 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:14,437 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:14,603 INFO] Step 600/ 1800; acc:  61.51; ppl:  3.50; xent: 1.25; lr: 1.00000; 6779/6673 tok/s;     15 sec\n",
      "[2021-02-03 01:33:14,897 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:14,901 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:15,388 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:15,392 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:15,843 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:15,847 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:15,892 INFO] Step 650/ 1800; acc:  73.42; ppl:  2.45; xent: 0.90; lr: 1.00000; 6580/6471 tok/s;     17 sec\n",
      "[2021-02-03 01:33:16,304 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:16,309 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:16,777 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:16,782 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:17,204 INFO] Step 700/ 1800; acc:  82.96; ppl:  1.78; xent: 0.57; lr: 1.00000; 6681/6570 tok/s;     18 sec\n",
      "[2021-02-03 01:33:17,246 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:17,250 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:17,722 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:17,726 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:18,179 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:18,183 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:18,503 INFO] Step 750/ 1800; acc:  88.55; ppl:  1.49; xent: 0.40; lr: 1.00000; 6629/6524 tok/s;     19 sec\n",
      "[2021-02-03 01:33:18,645 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:18,650 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:19,135 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:19,139 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:19,594 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:19,599 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:19,813 INFO] Step 800/ 1800; acc:  90.26; ppl:  1.40; xent: 0.33; lr: 1.00000; 6650/6547 tok/s;     21 sec\n",
      "[2021-02-03 01:33:20,064 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:20,070 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:20,536 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:20,540 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:21,005 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:21,009 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:21,125 INFO] Step 850/ 1800; acc:  93.64; ppl:  1.24; xent: 0.22; lr: 1.00000; 6575/6467 tok/s;     22 sec\n",
      "[2021-02-03 01:33:21,463 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:21,467 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:21,907 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:21,911 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:22,352 INFO] Step 900/ 1800; acc:  94.23; ppl:  1.22; xent: 0.20; lr: 1.00000; 6976/6864 tok/s;     23 sec\n",
      "[2021-02-03 01:33:22,352 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:22,356 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:22,806 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:22,810 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:23,247 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:23,251 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:23,623 INFO] Step 950/ 1800; acc:  94.64; ppl:  1.22; xent: 0.20; lr: 1.00000; 6878/6768 tok/s;     25 sec\n",
      "[2021-02-03 01:33:23,707 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:23,711 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:24,162 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:24,166 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:24,604 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:24,609 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:24,852 INFO] Step 1000/ 1800; acc:  95.84; ppl:  1.17; xent: 0.15; lr: 1.00000; 7036/6925 tok/s;     26 sec\n",
      "[2021-02-03 01:33:25,045 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:25,049 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:25,519 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:25,523 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:25,971 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:25,975 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:26,138 INFO] Step 1050/ 1800; acc:  97.08; ppl:  1.10; xent: 0.10; lr: 1.00000; 6740/6635 tok/s;     27 sec\n",
      "[2021-02-03 01:33:26,426 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:26,430 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:26,895 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:26,899 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:27,361 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:27,365 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:27,407 INFO] Step 1100/ 1800; acc:  97.09; ppl:  1.12; xent: 0.11; lr: 1.00000; 6681/6570 tok/s;     28 sec\n",
      "[2021-02-03 01:33:27,792 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:27,796 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:28,251 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:28,256 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:28,652 INFO] Step 1150/ 1800; acc:  96.31; ppl:  1.15; xent: 0.14; lr: 1.00000; 7046/6928 tok/s;     30 sec\n",
      "[2021-02-03 01:33:28,689 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:28,693 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:29,142 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:29,168 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:29,602 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:29,606 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:29,902 INFO] Step 1200/ 1800; acc:  97.56; ppl:  1.10; xent: 0.10; lr: 1.00000; 6886/6776 tok/s;     31 sec\n",
      "[2021-02-03 01:33:30,041 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:30,046 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:30,511 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:30,515 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:30,961 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:30,966 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:31,181 INFO] Step 1250/ 1800; acc:  97.66; ppl:  1.11; xent: 0.10; lr: 1.00000; 6809/6703 tok/s;     32 sec\n",
      "[2021-02-03 01:33:31,421 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:31,428 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:31,920 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:31,925 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:32,428 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:32,434 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:32,550 INFO] Step 1300/ 1800; acc:  97.60; ppl:  1.10; xent: 0.09; lr: 1.00000; 6304/6200 tok/s;     33 sec\n",
      "[2021-02-03 01:33:32,899 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:32,905 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:33,376 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:33,381 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:33,859 INFO] Step 1350/ 1800; acc:  97.24; ppl:  1.11; xent: 0.11; lr: 1.00000; 6538/6432 tok/s;     35 sec\n",
      "[2021-02-03 01:33:33,859 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:33,864 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:34,349 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:34,354 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:34,836 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:34,841 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:35,226 INFO] Step 1400/ 1800; acc:  97.50; ppl:  1.09; xent: 0.09; lr: 1.00000; 6398/6296 tok/s;     36 sec\n",
      "[2021-02-03 01:33:35,312 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:35,318 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:35,781 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:35,786 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:36,251 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:36,256 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:36,521 INFO] Step 1450/ 1800; acc:  97.45; ppl:  1.11; xent: 0.11; lr: 1.00000; 6677/6571 tok/s;     37 sec\n",
      "[2021-02-03 01:33:36,727 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:36,732 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:37,191 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:37,196 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:37,661 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:37,666 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:37,831 INFO] Step 1500/ 1800; acc:  97.75; ppl:  1.10; xent: 0.09; lr: 1.00000; 6617/6514 tok/s;     39 sec\n",
      "[2021-02-03 01:33:38,133 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:38,138 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:38,600 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:38,605 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:39,066 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:39,071 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:39,115 INFO] Step 1550/ 1800; acc:  98.03; ppl:  1.08; xent: 0.07; lr: 1.00000; 6602/6492 tok/s;     40 sec\n",
      "[2021-02-03 01:33:39,554 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:39,559 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:40,040 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:40,045 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:40,492 INFO] Step 1600/ 1800; acc:  98.22; ppl:  1.08; xent: 0.07; lr: 1.00000; 6368/6262 tok/s;     41 sec\n",
      "[2021-02-03 01:33:40,538 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:40,544 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:41,002 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:41,007 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:41,466 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:41,471 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:41,806 INFO] Step 1650/ 1800; acc:  98.16; ppl:  1.08; xent: 0.07; lr: 1.00000; 6553/6449 tok/s;     43 sec\n",
      "[2021-02-03 01:33:41,972 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:41,977 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:42,430 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:42,435 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:42,880 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:42,884 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:43,095 INFO] Step 1700/ 1800; acc:  98.22; ppl:  1.08; xent: 0.08; lr: 1.00000; 6756/6651 tok/s;     44 sec\n",
      "[2021-02-03 01:33:43,327 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:43,331 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:43,783 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:43,787 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:44,252 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:44,257 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:44,364 INFO] Step 1750/ 1800; acc:  98.22; ppl:  1.09; xent: 0.08; lr: 1.00000; 6801/6689 tok/s;     45 sec\n",
      "[2021-02-03 01:33:44,696 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:44,700 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:45,146 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_19/processed.train.0.pt\n",
      "[2021-02-03 01:33:45,150 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:45,598 INFO] Step 1800/ 1800; acc:  97.62; ppl:  1.11; xent: 0.10; lr: 1.00000; 6937/6825 tok/s;     46 sec\n",
      "[2021-02-03 01:33:45,599 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_19_step_1800.pt\n",
      "[2021-02-03 01:33:46,616 INFO]  * src vocab size = 31\n",
      "[2021-02-03 01:33:46,616 INFO]  * tgt vocab size = 29\n",
      "[2021-02-03 01:33:46,616 INFO] Building model...\n",
      "[2021-02-03 01:33:51,090 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(31, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(29, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=29, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-02-03 01:33:51,091 INFO] encoder: 250900\n",
      "[2021-02-03 01:33:51,091 INFO] decoder: 323229\n",
      "[2021-02-03 01:33:51,091 INFO] * number of parameters: 574129\n",
      "[2021-02-03 01:33:51,093 INFO] Starting training on GPU: [0]\n",
      "[2021-02-03 01:33:51,093 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-02-03 01:33:51,093 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:51,097 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:51,564 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:51,568 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:52,020 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:52,024 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:52,374 INFO] Step 50/ 1800; acc:  14.25; ppl: 30.65; xent: 3.42; lr: 1.00000; 6727/6596 tok/s;      1 sec\n",
      "[2021-02-03 01:33:52,476 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:52,480 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:52,918 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:52,922 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:53,350 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:53,354 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:53,581 INFO] Step 100/ 1800; acc:  26.55; ppl: 13.29; xent: 2.59; lr: 1.00000; 7064/6934 tok/s;      2 sec\n",
      "[2021-02-03 01:33:53,790 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:53,794 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:54,239 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:54,244 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:54,708 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:54,713 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:54,852 INFO] Step 150/ 1800; acc:  34.25; ppl: 10.14; xent: 2.32; lr: 1.00000; 6727/6617 tok/s;      4 sec\n",
      "[2021-02-03 01:33:55,179 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:55,183 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:55,639 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:55,643 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:56,107 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:56,111 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:56,155 INFO] Step 200/ 1800; acc:  36.44; ppl:  8.82; xent: 2.18; lr: 1.00000; 6691/6587 tok/s;      5 sec\n",
      "[2021-02-03 01:33:56,563 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:56,567 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:57,010 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:57,014 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:57,408 INFO] Step 250/ 1800; acc:  37.92; ppl:  7.95; xent: 2.07; lr: 1.00000; 6890/6753 tok/s;      6 sec\n",
      "[2021-02-03 01:33:57,466 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:57,471 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:57,944 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:57,948 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:58,417 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:58,422 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:58,727 INFO] Step 300/ 1800; acc:  38.84; ppl:  7.75; xent: 2.05; lr: 1.00000; 6569/6445 tok/s;      8 sec\n",
      "[2021-02-03 01:33:58,894 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:58,898 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:59,349 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:59,353 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:59,809 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:33:59,813 INFO] number of examples: 360\n",
      "[2021-02-03 01:33:59,996 INFO] Step 350/ 1800; acc:  40.81; ppl:  6.96; xent: 1.94; lr: 1.00000; 6695/6580 tok/s;      9 sec\n",
      "[2021-02-03 01:34:00,270 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:00,275 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:00,740 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:00,762 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:01,209 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:01,213 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:01,301 INFO] Step 400/ 1800; acc:  44.33; ppl:  6.33; xent: 1.85; lr: 1.00000; 6654/6546 tok/s;     10 sec\n",
      "[2021-02-03 01:34:01,664 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:01,668 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:02,120 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:02,124 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:02,575 INFO] Step 450/ 1800; acc:  48.82; ppl:  5.50; xent: 1.70; lr: 1.00000; 6846/6728 tok/s;     11 sec\n",
      "[2021-02-03 01:34:02,575 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:02,580 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:03,032 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:03,036 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:03,496 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:03,500 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:03,857 INFO] Step 500/ 1800; acc:  53.66; ppl:  4.52; xent: 1.51; lr: 1.00000; 6720/6588 tok/s;     13 sec\n",
      "[2021-02-03 01:34:03,957 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:03,962 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:04,435 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:04,439 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:04,906 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:04,910 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:05,148 INFO] Step 550/ 1800; acc:  61.81; ppl:  3.49; xent: 1.25; lr: 1.00000; 6611/6489 tok/s;     14 sec\n",
      "[2021-02-03 01:34:05,372 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:05,377 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:05,844 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:05,850 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:06,307 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:06,312 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:06,444 INFO] Step 600/ 1800; acc:  72.71; ppl:  2.50; xent: 0.92; lr: 1.00000; 6595/6487 tok/s;     15 sec\n",
      "[2021-02-03 01:34:06,772 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:06,777 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:07,229 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:07,233 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:07,671 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:07,675 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:07,716 INFO] Step 650/ 1800; acc:  82.97; ppl:  1.78; xent: 0.58; lr: 1.00000; 6852/6745 tok/s;     17 sec\n",
      "[2021-02-03 01:34:08,132 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:08,136 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:08,585 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:08,589 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:08,974 INFO] Step 700/ 1800; acc:  87.80; ppl:  1.52; xent: 0.42; lr: 1.00000; 6860/6724 tok/s;     18 sec\n",
      "[2021-02-03 01:34:09,031 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:09,035 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:09,482 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:09,485 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:09,937 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:09,941 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:10,238 INFO] Step 750/ 1800; acc:  91.05; ppl:  1.37; xent: 0.31; lr: 1.00000; 6860/6731 tok/s;     19 sec\n",
      "[2021-02-03 01:34:10,383 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:10,388 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:10,815 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:10,819 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:11,276 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:11,280 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:11,453 INFO] Step 800/ 1800; acc:  93.85; ppl:  1.23; xent: 0.21; lr: 1.00000; 6986/6865 tok/s;     20 sec\n",
      "[2021-02-03 01:34:11,724 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:11,728 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:12,170 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:12,174 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:12,623 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:12,627 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:12,711 INFO] Step 850/ 1800; acc:  94.43; ppl:  1.23; xent: 0.20; lr: 1.00000; 6903/6791 tok/s;     22 sec\n",
      "[2021-02-03 01:34:13,069 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:13,074 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:13,522 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:13,526 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:13,959 INFO] Step 900/ 1800; acc:  94.58; ppl:  1.20; xent: 0.19; lr: 1.00000; 6993/6872 tok/s;     23 sec\n",
      "[2021-02-03 01:34:13,959 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:13,963 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:14,414 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:14,418 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:14,859 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:14,864 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:15,221 INFO] Step 950/ 1800; acc:  95.13; ppl:  1.18; xent: 0.17; lr: 1.00000; 6823/6690 tok/s;     24 sec\n",
      "[2021-02-03 01:34:15,314 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:15,318 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:15,756 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:15,761 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:16,229 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:16,233 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:16,467 INFO] Step 1000/ 1800; acc:  96.27; ppl:  1.14; xent: 0.13; lr: 1.00000; 6849/6723 tok/s;     25 sec\n",
      "[2021-02-03 01:34:16,680 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:16,684 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:17,127 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:17,132 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:17,575 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:17,579 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:17,709 INFO] Step 1050/ 1800; acc:  96.13; ppl:  1.16; xent: 0.14; lr: 1.00000; 6885/6773 tok/s;     27 sec\n",
      "[2021-02-03 01:34:18,037 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:18,042 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:18,491 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:18,495 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:18,931 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:18,935 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:18,974 INFO] Step 1100/ 1800; acc:  96.47; ppl:  1.14; xent: 0.13; lr: 1.00000; 6884/6777 tok/s;     28 sec\n",
      "[2021-02-03 01:34:19,386 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:19,390 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:19,833 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:19,837 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:20,224 INFO] Step 1150/ 1800; acc:  97.17; ppl:  1.11; xent: 0.10; lr: 1.00000; 6910/6772 tok/s;     29 sec\n",
      "[2021-02-03 01:34:20,279 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:20,284 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:20,737 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:20,761 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:21,211 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:21,215 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:21,505 INFO] Step 1200/ 1800; acc:  95.98; ppl:  1.17; xent: 0.16; lr: 1.00000; 6769/6642 tok/s;     30 sec\n",
      "[2021-02-03 01:34:21,649 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:21,653 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:22,103 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:22,107 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:22,559 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:22,564 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:22,741 INFO] Step 1250/ 1800; acc:  97.04; ppl:  1.12; xent: 0.11; lr: 1.00000; 6867/6748 tok/s;     32 sec\n",
      "[2021-02-03 01:34:23,021 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:23,025 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:23,479 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:23,484 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:23,933 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:23,938 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:24,024 INFO] Step 1300/ 1800; acc:  96.67; ppl:  1.14; xent: 0.13; lr: 1.00000; 6768/6658 tok/s;     33 sec\n",
      "[2021-02-03 01:34:24,379 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:24,383 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:24,814 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:24,818 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:25,255 INFO] Step 1350/ 1800; acc:  97.95; ppl:  1.09; xent: 0.08; lr: 1.00000; 7090/6967 tok/s;     34 sec\n",
      "[2021-02-03 01:34:25,255 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:25,259 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:25,704 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:25,708 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:26,167 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:26,172 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:26,526 INFO] Step 1400/ 1800; acc:  97.79; ppl:  1.09; xent: 0.09; lr: 1.00000; 6778/6646 tok/s;     35 sec\n",
      "[2021-02-03 01:34:26,621 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:26,626 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:27,084 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:27,089 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:27,542 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:27,546 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:27,781 INFO] Step 1450/ 1800; acc:  98.23; ppl:  1.08; xent: 0.08; lr: 1.00000; 6797/6672 tok/s;     37 sec\n",
      "[2021-02-03 01:34:28,007 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:28,012 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:28,468 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:28,472 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:28,942 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:28,946 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:29,081 INFO] Step 1500/ 1800; acc:  97.97; ppl:  1.09; xent: 0.09; lr: 1.00000; 6573/6466 tok/s;     38 sec\n",
      "[2021-02-03 01:34:29,400 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:29,404 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:29,850 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:29,855 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:30,313 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:30,317 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:30,360 INFO] Step 1550/ 1800; acc:  97.70; ppl:  1.11; xent: 0.10; lr: 1.00000; 6814/6708 tok/s;     39 sec\n",
      "[2021-02-03 01:34:30,772 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:30,776 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:31,231 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:31,235 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:31,643 INFO] Step 1600/ 1800; acc:  97.78; ppl:  1.09; xent: 0.09; lr: 1.00000; 6731/6597 tok/s;     41 sec\n",
      "[2021-02-03 01:34:31,699 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:31,703 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:32,145 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:32,149 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:32,596 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:32,600 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:32,891 INFO] Step 1650/ 1800; acc:  97.57; ppl:  1.11; xent: 0.10; lr: 1.00000; 6946/6815 tok/s;     42 sec\n",
      "[2021-02-03 01:34:33,048 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:33,053 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:33,536 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:33,541 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:33,988 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:33,992 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:34,169 INFO] Step 1700/ 1800; acc:  97.63; ppl:  1.10; xent: 0.10; lr: 1.00000; 6642/6527 tok/s;     43 sec\n",
      "[2021-02-03 01:34:34,441 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:34,445 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:34,898 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:34,902 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:35,368 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:35,372 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:35,460 INFO] Step 1750/ 1800; acc:  97.81; ppl:  1.10; xent: 0.09; lr: 1.00000; 6728/6618 tok/s;     44 sec\n",
      "[2021-02-03 01:34:35,815 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:35,819 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:36,283 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_20/processed.train.0.pt\n",
      "[2021-02-03 01:34:36,288 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:36,744 INFO] Step 1800/ 1800; acc:  97.63; ppl:  1.12; xent: 0.11; lr: 1.00000; 6798/6681 tok/s;     46 sec\n",
      "[2021-02-03 01:34:36,745 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_20_step_1800.pt\n",
      "[2021-02-03 01:34:37,824 INFO]  * src vocab size = 32\n",
      "[2021-02-03 01:34:37,824 INFO]  * tgt vocab size = 30\n",
      "[2021-02-03 01:34:37,824 INFO] Building model...\n",
      "[2021-02-03 01:34:42,351 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(32, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(30, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=30, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-02-03 01:34:42,351 INFO] encoder: 251200\n",
      "[2021-02-03 01:34:42,351 INFO] decoder: 323630\n",
      "[2021-02-03 01:34:42,351 INFO] * number of parameters: 574830\n",
      "[2021-02-03 01:34:42,354 INFO] Starting training on GPU: [0]\n",
      "[2021-02-03 01:34:42,354 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-02-03 01:34:42,354 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:42,358 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:42,806 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:42,810 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:43,268 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:43,272 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:43,613 INFO] Step 50/ 1800; acc:  13.52; ppl: 36.37; xent: 3.59; lr: 1.00000; 6761/6644 tok/s;      1 sec\n",
      "[2021-02-03 01:34:43,732 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:43,736 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:44,176 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:44,180 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:44,628 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:44,632 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:44,876 INFO] Step 100/ 1800; acc:  25.57; ppl: 13.71; xent: 2.62; lr: 1.00000; 6915/6797 tok/s;      3 sec\n",
      "[2021-02-03 01:34:45,092 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:45,096 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:45,568 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:45,572 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:46,011 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:46,015 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:46,174 INFO] Step 150/ 1800; acc:  32.24; ppl: 10.83; xent: 2.38; lr: 1.00000; 6765/6646 tok/s;      4 sec\n",
      "[2021-02-03 01:34:46,465 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:46,469 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:46,930 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:46,934 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:47,378 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:47,382 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:47,422 INFO] Step 200/ 1800; acc:  34.95; ppl:  9.24; xent: 2.22; lr: 1.00000; 6800/6686 tok/s;      5 sec\n",
      "[2021-02-03 01:34:47,821 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:47,826 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:48,299 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:48,303 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:48,679 INFO] Step 250/ 1800; acc:  36.97; ppl:  8.35; xent: 2.12; lr: 1.00000; 6846/6734 tok/s;      6 sec\n",
      "[2021-02-03 01:34:48,741 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:48,745 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:49,186 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:49,190 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:49,643 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:49,647 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:49,924 INFO] Step 300/ 1800; acc:  38.80; ppl:  7.80; xent: 2.05; lr: 1.00000; 6971/6847 tok/s;      8 sec\n",
      "[2021-02-03 01:34:50,088 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:50,091 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:50,550 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:50,554 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:50,995 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:50,999 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:51,198 INFO] Step 350/ 1800; acc:  40.07; ppl:  7.16; xent: 1.97; lr: 1.00000; 6864/6750 tok/s;      9 sec\n",
      "[2021-02-03 01:34:51,458 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:51,463 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:51,905 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:51,927 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:52,383 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:52,387 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:52,478 INFO] Step 400/ 1800; acc:  43.13; ppl:  6.28; xent: 1.84; lr: 1.00000; 6717/6598 tok/s;     10 sec\n",
      "[2021-02-03 01:34:52,839 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:52,843 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:53,287 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:53,291 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:53,716 INFO] Step 450/ 1800; acc:  46.11; ppl:  5.76; xent: 1.75; lr: 1.00000; 7005/6893 tok/s;     11 sec\n",
      "[2021-02-03 01:34:53,716 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:53,720 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:54,179 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:54,183 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:54,630 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:54,635 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:54,967 INFO] Step 500/ 1800; acc:  49.64; ppl:  5.04; xent: 1.62; lr: 1.00000; 6805/6687 tok/s;     13 sec\n",
      "[2021-02-03 01:34:55,081 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:55,085 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:55,538 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:55,542 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:55,973 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:55,977 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:56,214 INFO] Step 550/ 1800; acc:  55.48; ppl:  4.20; xent: 1.43; lr: 1.00000; 7006/6887 tok/s;     14 sec\n",
      "[2021-02-03 01:34:56,424 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:56,428 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:56,872 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:56,877 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:57,333 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:57,337 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:57,497 INFO] Step 600/ 1800; acc:  64.61; ppl:  3.15; xent: 1.15; lr: 1.00000; 6840/6719 tok/s;     15 sec\n",
      "[2021-02-03 01:34:57,799 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:57,803 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:58,252 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:58,256 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:58,700 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:58,704 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:58,745 INFO] Step 650/ 1800; acc:  78.00; ppl:  2.03; xent: 0.71; lr: 1.00000; 6803/6689 tok/s;     16 sec\n",
      "[2021-02-03 01:34:59,146 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:59,150 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:59,606 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:34:59,610 INFO] number of examples: 360\n",
      "[2021-02-03 01:34:59,985 INFO] Step 700/ 1800; acc:  85.04; ppl:  1.64; xent: 0.50; lr: 1.00000; 6939/6826 tok/s;     18 sec\n",
      "[2021-02-03 01:35:00,047 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:00,052 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:00,495 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:00,499 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:00,938 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:00,943 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:01,221 INFO] Step 750/ 1800; acc:  89.58; ppl:  1.45; xent: 0.37; lr: 1.00000; 7020/6895 tok/s;     19 sec\n",
      "[2021-02-03 01:35:01,373 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:01,377 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:01,812 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:01,816 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:02,257 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:02,261 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:02,454 INFO] Step 800/ 1800; acc:  92.10; ppl:  1.31; xent: 0.27; lr: 1.00000; 7094/6975 tok/s;     20 sec\n",
      "[2021-02-03 01:35:02,708 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:02,715 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:03,172 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:03,177 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:03,635 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:03,639 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:03,743 INFO] Step 850/ 1800; acc:  94.14; ppl:  1.23; xent: 0.21; lr: 1.00000; 6670/6551 tok/s;     21 sec\n",
      "[2021-02-03 01:35:04,092 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:04,096 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:04,534 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:04,539 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:04,988 INFO] Step 900/ 1800; acc:  94.51; ppl:  1.20; xent: 0.18; lr: 1.00000; 6965/6854 tok/s;     23 sec\n",
      "[2021-02-03 01:35:04,988 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:04,992 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:05,443 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:05,448 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:05,923 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:05,928 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:06,263 INFO] Step 950/ 1800; acc:  94.60; ppl:  1.20; xent: 0.19; lr: 1.00000; 6680/6565 tok/s;     24 sec\n",
      "[2021-02-03 01:35:06,379 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:06,384 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:06,843 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:06,847 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:07,294 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:07,298 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:07,552 INFO] Step 1000/ 1800; acc:  94.71; ppl:  1.22; xent: 0.20; lr: 1.00000; 6774/6659 tok/s;     25 sec\n",
      "[2021-02-03 01:35:07,755 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:07,760 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:08,238 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:08,242 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:08,692 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:08,696 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:08,856 INFO] Step 1050/ 1800; acc:  95.84; ppl:  1.16; xent: 0.15; lr: 1.00000; 6732/6613 tok/s;     27 sec\n",
      "[2021-02-03 01:35:09,160 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:09,164 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:09,640 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:09,644 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:10,112 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:10,117 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:10,164 INFO] Step 1100/ 1800; acc:  96.93; ppl:  1.11; xent: 0.10; lr: 1.00000; 6488/6380 tok/s;     28 sec\n",
      "[2021-02-03 01:35:10,596 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:10,600 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:11,048 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:11,053 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:11,435 INFO] Step 1150/ 1800; acc:  97.59; ppl:  1.09; xent: 0.09; lr: 1.00000; 6770/6659 tok/s;     29 sec\n",
      "[2021-02-03 01:35:11,501 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:11,505 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:11,983 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:12,006 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:12,473 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:12,477 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:12,771 INFO] Step 1200/ 1800; acc:  97.01; ppl:  1.13; xent: 0.12; lr: 1.00000; 6494/6379 tok/s;     30 sec\n",
      "[2021-02-03 01:35:12,930 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:12,934 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:13,383 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:13,387 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:13,838 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:13,842 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:14,041 INFO] Step 1250/ 1800; acc:  97.43; ppl:  1.10; xent: 0.10; lr: 1.00000; 6885/6770 tok/s;     32 sec\n",
      "[2021-02-03 01:35:14,316 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:14,321 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:14,789 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:14,794 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:15,257 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:15,261 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:15,357 INFO] Step 1300/ 1800; acc:  97.84; ppl:  1.09; xent: 0.08; lr: 1.00000; 6538/6422 tok/s;     33 sec\n",
      "[2021-02-03 01:35:15,716 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:15,720 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:16,175 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:16,179 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:16,640 INFO] Step 1350/ 1800; acc:  97.78; ppl:  1.09; xent: 0.08; lr: 1.00000; 6759/6651 tok/s;     34 sec\n",
      "[2021-02-03 01:35:16,640 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:16,644 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:17,100 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:17,104 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:17,556 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:17,560 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:17,894 INFO] Step 1400/ 1800; acc:  97.47; ppl:  1.11; xent: 0.10; lr: 1.00000; 6787/6670 tok/s;     36 sec\n",
      "[2021-02-03 01:35:18,010 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:18,014 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:18,462 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:18,467 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:18,906 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:18,910 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:19,154 INFO] Step 1450/ 1800; acc:  97.35; ppl:  1.13; xent: 0.12; lr: 1.00000; 6931/6813 tok/s;     37 sec\n",
      "[2021-02-03 01:35:19,375 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:19,380 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:19,825 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:19,829 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:20,282 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:20,285 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:20,448 INFO] Step 1500/ 1800; acc:  97.58; ppl:  1.10; xent: 0.10; lr: 1.00000; 6782/6663 tok/s;     38 sec\n",
      "[2021-02-03 01:35:20,741 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:20,745 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:21,204 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:21,209 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:21,643 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:21,648 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:21,696 INFO] Step 1550/ 1800; acc:  98.07; ppl:  1.07; xent: 0.06; lr: 1.00000; 6801/6687 tok/s;     39 sec\n",
      "[2021-02-03 01:35:22,094 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:22,098 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:22,546 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:22,550 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:22,935 INFO] Step 1600/ 1800; acc:  98.50; ppl:  1.06; xent: 0.06; lr: 1.00000; 6947/6833 tok/s;     41 sec\n",
      "[2021-02-03 01:35:23,007 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:23,011 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:23,466 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:23,470 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:23,916 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:23,921 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:24,202 INFO] Step 1650/ 1800; acc:  98.22; ppl:  1.09; xent: 0.08; lr: 1.00000; 6848/6727 tok/s;     42 sec\n",
      "[2021-02-03 01:35:24,367 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:24,371 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:24,809 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:24,814 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:25,265 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:25,270 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:25,466 INFO] Step 1700/ 1800; acc:  97.70; ppl:  1.11; xent: 0.10; lr: 1.00000; 6919/6804 tok/s;     43 sec\n",
      "[2021-02-03 01:35:25,721 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:25,727 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:26,192 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:26,197 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:26,656 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:26,660 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:26,754 INFO] Step 1750/ 1800; acc:  97.80; ppl:  1.11; xent: 0.10; lr: 1.00000; 6673/6555 tok/s;     44 sec\n",
      "[2021-02-03 01:35:27,107 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:27,112 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:27,563 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_21/processed.train.0.pt\n",
      "[2021-02-03 01:35:27,567 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:28,027 INFO] Step 1800/ 1800; acc:  97.63; ppl:  1.11; xent: 0.10; lr: 1.00000; 6816/6707 tok/s;     46 sec\n",
      "[2021-02-03 01:35:28,028 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_21_step_1800.pt\n",
      "[2021-02-03 01:35:29,159 INFO]  * src vocab size = 31\n",
      "[2021-02-03 01:35:29,160 INFO]  * tgt vocab size = 29\n",
      "[2021-02-03 01:35:29,160 INFO] Building model...\n",
      "[2021-02-03 01:35:33,617 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(31, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(29, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=29, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-02-03 01:35:33,617 INFO] encoder: 250900\n",
      "[2021-02-03 01:35:33,618 INFO] decoder: 323229\n",
      "[2021-02-03 01:35:33,618 INFO] * number of parameters: 574129\n",
      "[2021-02-03 01:35:33,620 INFO] Starting training on GPU: [0]\n",
      "[2021-02-03 01:35:33,620 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-02-03 01:35:33,620 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:33,625 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:34,119 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:34,124 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:34,582 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:34,587 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:34,938 INFO] Step 50/ 1800; acc:  12.78; ppl: 31.86; xent: 3.46; lr: 1.00000; 6508/6406 tok/s;      1 sec\n",
      "[2021-02-03 01:35:35,049 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:35,053 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:35,530 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:35,534 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:36,004 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:36,008 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:36,254 INFO] Step 100/ 1800; acc:  23.54; ppl: 14.73; xent: 2.69; lr: 1.00000; 6592/6509 tok/s;      3 sec\n",
      "[2021-02-03 01:35:36,456 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:36,460 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:36,940 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:36,944 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:37,411 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:37,416 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:37,550 INFO] Step 150/ 1800; acc:  32.67; ppl: 10.66; xent: 2.37; lr: 1.00000; 6581/6492 tok/s;      4 sec\n",
      "[2021-02-03 01:35:37,871 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:37,876 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:38,353 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:38,357 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:38,817 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:38,821 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:38,870 INFO] Step 200/ 1800; acc:  33.95; ppl:  9.28; xent: 2.23; lr: 1.00000; 6601/6512 tok/s;      5 sec\n",
      "[2021-02-03 01:35:39,298 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:39,302 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:39,777 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:39,781 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:40,168 INFO] Step 250/ 1800; acc:  36.35; ppl:  8.45; xent: 2.13; lr: 1.00000; 6661/6567 tok/s;      7 sec\n",
      "[2021-02-03 01:35:40,223 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:40,227 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:40,670 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:40,674 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:41,134 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:41,138 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:41,424 INFO] Step 300/ 1800; acc:  36.85; ppl:  7.98; xent: 2.08; lr: 1.00000; 6823/6733 tok/s;      8 sec\n",
      "[2021-02-03 01:35:41,576 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:41,581 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:42,028 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:42,032 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:42,473 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:42,477 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:42,677 INFO] Step 350/ 1800; acc:  38.60; ppl:  7.28; xent: 1.98; lr: 1.00000; 6965/6866 tok/s;      9 sec\n",
      "[2021-02-03 01:35:42,953 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:42,958 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:43,412 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:43,437 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:43,889 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:43,893 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:43,981 INFO] Step 400/ 1800; acc:  41.15; ppl:  6.64; xent: 1.89; lr: 1.00000; 6517/6433 tok/s;     10 sec\n",
      "[2021-02-03 01:35:44,342 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:44,346 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:44,797 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:44,802 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:45,276 INFO] Step 450/ 1800; acc:  43.81; ppl:  6.11; xent: 1.81; lr: 1.00000; 6737/6655 tok/s;     12 sec\n",
      "[2021-02-03 01:35:45,277 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:45,281 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:45,736 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:45,740 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:46,196 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:46,201 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:46,546 INFO] Step 500/ 1800; acc:  48.35; ppl:  5.18; xent: 1.64; lr: 1.00000; 6754/6648 tok/s;     13 sec\n",
      "[2021-02-03 01:35:46,658 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:46,662 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:47,119 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:47,124 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:47,564 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:47,568 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:47,807 INFO] Step 550/ 1800; acc:  53.18; ppl:  4.37; xent: 1.47; lr: 1.00000; 6878/6791 tok/s;     14 sec\n",
      "[2021-02-03 01:35:48,016 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:48,020 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:48,494 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:48,499 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:48,960 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:48,965 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:49,108 INFO] Step 600/ 1800; acc:  61.55; ppl:  3.43; xent: 1.23; lr: 1.00000; 6559/6470 tok/s;     15 sec\n",
      "[2021-02-03 01:35:49,432 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:49,436 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:49,905 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:49,909 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:50,373 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:50,377 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:50,426 INFO] Step 650/ 1800; acc:  73.06; ppl:  2.41; xent: 0.88; lr: 1.00000; 6611/6522 tok/s;     17 sec\n",
      "[2021-02-03 01:35:50,844 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:50,848 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:51,314 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:51,318 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:51,735 INFO] Step 700/ 1800; acc:  82.59; ppl:  1.77; xent: 0.57; lr: 1.00000; 6608/6515 tok/s;     18 sec\n",
      "[2021-02-03 01:35:51,795 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:51,799 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:52,265 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:52,270 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:52,736 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:52,741 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:53,050 INFO] Step 750/ 1800; acc:  88.77; ppl:  1.45; xent: 0.37; lr: 1.00000; 6512/6426 tok/s;     19 sec\n",
      "[2021-02-03 01:35:53,211 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:53,215 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:53,692 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:53,697 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:54,145 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:54,150 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:54,359 INFO] Step 800/ 1800; acc:  90.48; ppl:  1.38; xent: 0.32; lr: 1.00000; 6669/6574 tok/s;     21 sec\n",
      "[2021-02-03 01:35:54,624 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:54,629 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:55,105 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:55,109 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:55,568 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:55,573 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:55,662 INFO] Step 850/ 1800; acc:  92.37; ppl:  1.29; xent: 0.25; lr: 1.00000; 6525/6441 tok/s;     22 sec\n",
      "[2021-02-03 01:35:56,034 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:56,039 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:56,522 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:56,527 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:56,984 INFO] Step 900/ 1800; acc:  93.72; ppl:  1.24; xent: 0.22; lr: 1.00000; 6599/6519 tok/s;     23 sec\n",
      "[2021-02-03 01:35:56,984 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:56,988 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:57,458 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:57,462 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:57,929 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:57,933 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:58,288 INFO] Step 950/ 1800; acc:  94.93; ppl:  1.20; xent: 0.18; lr: 1.00000; 6575/6472 tok/s;     25 sec\n",
      "[2021-02-03 01:35:58,402 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:58,407 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:58,873 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:58,877 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:59,366 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:59,370 INFO] number of examples: 360\n",
      "[2021-02-03 01:35:59,629 INFO] Step 1000/ 1800; acc:  95.88; ppl:  1.16; xent: 0.15; lr: 1.00000; 6468/6386 tok/s;     26 sec\n",
      "[2021-02-03 01:35:59,843 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:35:59,847 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:00,304 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:00,309 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:00,773 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:00,777 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:00,926 INFO] Step 1050/ 1800; acc:  96.38; ppl:  1.14; xent: 0.13; lr: 1.00000; 6582/6493 tok/s;     27 sec\n",
      "[2021-02-03 01:36:01,246 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:01,250 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:01,722 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:01,727 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:02,193 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:02,197 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:02,246 INFO] Step 1100/ 1800; acc:  96.47; ppl:  1.15; xent: 0.14; lr: 1.00000; 6595/6506 tok/s;     29 sec\n",
      "[2021-02-03 01:36:02,664 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:02,669 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:03,149 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:03,153 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:03,564 INFO] Step 1150/ 1800; acc:  97.15; ppl:  1.12; xent: 0.11; lr: 1.00000; 6566/6473 tok/s;     30 sec\n",
      "[2021-02-03 01:36:03,617 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:03,621 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:04,086 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:04,109 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:04,556 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:04,560 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:04,853 INFO] Step 1200/ 1800; acc:  96.95; ppl:  1.13; xent: 0.12; lr: 1.00000; 6645/6557 tok/s;     31 sec\n",
      "[2021-02-03 01:36:05,014 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:05,019 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:05,479 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:05,483 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:05,918 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:05,922 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:06,117 INFO] Step 1250/ 1800; acc:  97.63; ppl:  1.09; xent: 0.09; lr: 1.00000; 6904/6806 tok/s;     32 sec\n",
      "[2021-02-03 01:36:06,369 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:06,373 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:06,845 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:06,849 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:07,310 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:07,314 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:07,397 INFO] Step 1300/ 1800; acc:  97.79; ppl:  1.08; xent: 0.08; lr: 1.00000; 6638/6553 tok/s;     34 sec\n",
      "[2021-02-03 01:36:07,771 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:07,775 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:08,228 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:08,232 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:08,673 INFO] Step 1350/ 1800; acc:  97.60; ppl:  1.10; xent: 0.10; lr: 1.00000; 6838/6755 tok/s;     35 sec\n",
      "[2021-02-03 01:36:08,674 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:08,678 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:09,115 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:09,119 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:09,584 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:09,588 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:09,937 INFO] Step 1400/ 1800; acc:  98.20; ppl:  1.07; xent: 0.07; lr: 1.00000; 6787/6681 tok/s;     36 sec\n",
      "[2021-02-03 01:36:10,055 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:10,059 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:10,508 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:10,512 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:10,976 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:10,980 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:11,226 INFO] Step 1450/ 1800; acc:  98.15; ppl:  1.08; xent: 0.08; lr: 1.00000; 6727/6642 tok/s;     38 sec\n",
      "[2021-02-03 01:36:11,446 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:11,452 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:11,904 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:11,908 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:12,351 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:12,356 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:12,487 INFO] Step 1500/ 1800; acc:  97.70; ppl:  1.09; xent: 0.09; lr: 1.00000; 6768/6677 tok/s;     39 sec\n",
      "[2021-02-03 01:36:12,817 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:12,821 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:13,282 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:13,287 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:13,744 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:13,748 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:13,793 INFO] Step 1550/ 1800; acc:  97.33; ppl:  1.11; xent: 0.10; lr: 1.00000; 6668/6579 tok/s;     40 sec\n",
      "[2021-02-03 01:36:14,202 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:14,207 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:14,650 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:14,654 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:15,050 INFO] Step 1600/ 1800; acc:  97.94; ppl:  1.08; xent: 0.08; lr: 1.00000; 6882/6785 tok/s;     41 sec\n",
      "[2021-02-03 01:36:15,102 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:15,106 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:15,554 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:15,559 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:16,030 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:16,034 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:16,326 INFO] Step 1650/ 1800; acc:  97.80; ppl:  1.09; xent: 0.08; lr: 1.00000; 6712/6624 tok/s;     43 sec\n",
      "[2021-02-03 01:36:16,484 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:16,489 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:16,943 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:16,948 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:17,420 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:17,424 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:17,623 INFO] Step 1700/ 1800; acc:  97.78; ppl:  1.10; xent: 0.10; lr: 1.00000; 6731/6635 tok/s;     44 sec\n",
      "[2021-02-03 01:36:17,874 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:17,879 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:18,344 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:18,348 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:18,808 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:18,813 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:18,900 INFO] Step 1750/ 1800; acc:  98.43; ppl:  1.07; xent: 0.07; lr: 1.00000; 6656/6571 tok/s;     45 sec\n",
      "[2021-02-03 01:36:19,262 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:19,266 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:19,716 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_22/processed.train.0.pt\n",
      "[2021-02-03 01:36:19,721 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:20,176 INFO] Step 1800/ 1800; acc:  98.07; ppl:  1.08; xent: 0.07; lr: 1.00000; 6836/6753 tok/s;     47 sec\n",
      "[2021-02-03 01:36:20,178 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_22_step_1800.pt\n",
      "[2021-02-03 01:36:21,201 INFO]  * src vocab size = 32\n",
      "[2021-02-03 01:36:21,202 INFO]  * tgt vocab size = 30\n",
      "[2021-02-03 01:36:21,202 INFO] Building model...\n",
      "[2021-02-03 01:36:25,552 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(32, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(30, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=30, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-02-03 01:36:25,552 INFO] encoder: 251200\n",
      "[2021-02-03 01:36:25,552 INFO] decoder: 323630\n",
      "[2021-02-03 01:36:25,552 INFO] * number of parameters: 574830\n",
      "[2021-02-03 01:36:25,555 INFO] Starting training on GPU: [0]\n",
      "[2021-02-03 01:36:25,555 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-02-03 01:36:25,555 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:25,559 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:26,019 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:26,024 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:26,472 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:26,476 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:26,822 INFO] Step 50/ 1800; acc:  13.09; ppl: 31.08; xent: 3.44; lr: 1.00000; 6811/6717 tok/s;      1 sec\n",
      "[2021-02-03 01:36:26,937 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:26,942 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:27,395 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:27,399 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:27,855 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:27,860 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:28,112 INFO] Step 100/ 1800; acc:  25.18; ppl: 14.45; xent: 2.67; lr: 1.00000; 6782/6680 tok/s;      3 sec\n",
      "[2021-02-03 01:36:28,323 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:28,327 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:28,774 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:28,778 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:29,243 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:29,247 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:29,387 INFO] Step 150/ 1800; acc:  31.94; ppl: 11.19; xent: 2.41; lr: 1.00000; 6772/6680 tok/s;      4 sec\n",
      "[2021-02-03 01:36:29,699 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:29,704 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:30,166 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:30,170 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:30,631 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:30,635 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:30,677 INFO] Step 200/ 1800; acc:  34.86; ppl:  9.62; xent: 2.26; lr: 1.00000; 6748/6643 tok/s;      5 sec\n",
      "[2021-02-03 01:36:31,083 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:31,088 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:31,549 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:31,553 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:31,954 INFO] Step 250/ 1800; acc:  36.66; ppl:  8.74; xent: 2.17; lr: 1.00000; 6822/6722 tok/s;      6 sec\n",
      "[2021-02-03 01:36:32,013 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:32,017 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:32,461 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:32,465 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:32,919 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:32,923 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:33,231 INFO] Step 300/ 1800; acc:  37.99; ppl:  8.14; xent: 2.10; lr: 1.00000; 6873/6771 tok/s;      8 sec\n",
      "[2021-02-03 01:36:33,376 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:33,380 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:33,842 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:33,846 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:34,307 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:34,311 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:34,509 INFO] Step 350/ 1800; acc:  39.20; ppl:  7.43; xent: 2.00; lr: 1.00000; 6808/6712 tok/s;      9 sec\n",
      "[2021-02-03 01:36:34,770 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:34,775 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:35,239 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:35,261 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:35,699 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:35,703 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:35,798 INFO] Step 400/ 1800; acc:  42.34; ppl:  6.63; xent: 1.89; lr: 1.00000; 6712/6620 tok/s;     10 sec\n",
      "[2021-02-03 01:36:36,157 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:36,161 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:36,613 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:36,617 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:37,076 INFO] Step 450/ 1800; acc:  45.35; ppl:  5.81; xent: 1.76; lr: 1.00000; 6847/6733 tok/s;     12 sec\n",
      "[2021-02-03 01:36:37,077 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:37,081 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:37,519 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:37,523 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:37,986 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:37,990 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:38,330 INFO] Step 500/ 1800; acc:  49.25; ppl:  4.94; xent: 1.60; lr: 1.00000; 6886/6791 tok/s;     13 sec\n",
      "[2021-02-03 01:36:38,443 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:38,448 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:38,920 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:38,924 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:39,382 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:39,387 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:39,650 INFO] Step 550/ 1800; acc:  54.51; ppl:  4.17; xent: 1.43; lr: 1.00000; 6623/6523 tok/s;     14 sec\n",
      "[2021-02-03 01:36:39,861 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:39,865 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:40,318 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:40,322 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:40,762 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:40,768 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:40,909 INFO] Step 600/ 1800; acc:  60.09; ppl:  3.42; xent: 1.23; lr: 1.00000; 6862/6768 tok/s;     15 sec\n",
      "[2021-02-03 01:36:41,201 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:41,207 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:41,675 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:41,679 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:42,145 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:42,149 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:42,190 INFO] Step 650/ 1800; acc:  72.38; ppl:  2.40; xent: 0.88; lr: 1.00000; 6796/6690 tok/s;     17 sec\n",
      "[2021-02-03 01:36:42,592 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:42,596 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:43,035 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:43,039 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:43,430 INFO] Step 700/ 1800; acc:  82.10; ppl:  1.79; xent: 0.58; lr: 1.00000; 7023/6920 tok/s;     18 sec\n",
      "[2021-02-03 01:36:43,489 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:43,493 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:43,940 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:43,944 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:44,388 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:44,392 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:44,696 INFO] Step 750/ 1800; acc:  86.98; ppl:  1.53; xent: 0.43; lr: 1.00000; 6930/6826 tok/s;     19 sec\n",
      "[2021-02-03 01:36:44,837 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:44,841 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:45,291 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:45,295 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:45,751 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:45,755 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:45,962 INFO] Step 800/ 1800; acc:  91.18; ppl:  1.35; xent: 0.30; lr: 1.00000; 6876/6780 tok/s;     20 sec\n",
      "[2021-02-03 01:36:46,221 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:46,225 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:46,664 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:46,668 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:47,100 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:47,104 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:47,196 INFO] Step 850/ 1800; acc:  93.14; ppl:  1.27; xent: 0.24; lr: 1.00000; 7010/6913 tok/s;     22 sec\n",
      "[2021-02-03 01:36:47,545 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:47,549 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:48,001 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:48,005 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:48,458 INFO] Step 900/ 1800; acc:  92.83; ppl:  1.26; xent: 0.23; lr: 1.00000; 6935/6820 tok/s;     23 sec\n",
      "[2021-02-03 01:36:48,459 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:48,463 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:48,909 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:48,913 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:49,371 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:49,375 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:49,724 INFO] Step 950/ 1800; acc:  94.24; ppl:  1.22; xent: 0.20; lr: 1.00000; 6819/6724 tok/s;     24 sec\n",
      "[2021-02-03 01:36:49,824 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:49,828 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:50,273 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:50,277 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:50,710 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:50,714 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:50,965 INFO] Step 1000/ 1800; acc:  94.23; ppl:  1.22; xent: 0.20; lr: 1.00000; 7051/6945 tok/s;     25 sec\n",
      "[2021-02-03 01:36:51,154 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:51,158 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:51,598 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:51,602 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:52,064 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:52,068 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:52,210 INFO] Step 1050/ 1800; acc:  95.41; ppl:  1.17; xent: 0.16; lr: 1.00000; 6932/6838 tok/s;     27 sec\n",
      "[2021-02-03 01:36:52,520 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:52,524 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:52,968 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:52,972 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:53,413 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:53,418 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:53,457 INFO] Step 1100/ 1800; acc:  95.80; ppl:  1.16; xent: 0.15; lr: 1.00000; 6980/6871 tok/s;     28 sec\n",
      "[2021-02-03 01:36:53,874 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:53,879 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:54,317 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:54,321 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:54,698 INFO] Step 1150/ 1800; acc:  96.19; ppl:  1.16; xent: 0.14; lr: 1.00000; 7018/6915 tok/s;     29 sec\n",
      "[2021-02-03 01:36:54,759 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:54,763 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:55,217 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:55,242 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:55,684 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:55,688 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:55,992 INFO] Step 1200/ 1800; acc:  95.82; ppl:  1.17; xent: 0.16; lr: 1.00000; 6787/6685 tok/s;     30 sec\n",
      "[2021-02-03 01:36:56,136 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:56,140 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:56,574 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:56,578 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:57,023 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:57,027 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:57,230 INFO] Step 1250/ 1800; acc:  96.93; ppl:  1.11; xent: 0.10; lr: 1.00000; 7025/6927 tok/s;     32 sec\n",
      "[2021-02-03 01:36:57,491 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:57,496 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:57,946 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:57,950 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:58,398 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:58,402 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:58,496 INFO] Step 1300/ 1800; acc:  96.88; ppl:  1.12; xent: 0.11; lr: 1.00000; 6836/6742 tok/s;     33 sec\n",
      "[2021-02-03 01:36:58,834 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:58,838 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:59,306 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:59,311 INFO] number of examples: 360\n",
      "[2021-02-03 01:36:59,761 INFO] Step 1350/ 1800; acc:  97.47; ppl:  1.10; xent: 0.10; lr: 1.00000; 6920/6805 tok/s;     34 sec\n",
      "[2021-02-03 01:36:59,761 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:36:59,765 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:00,223 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:00,227 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:00,673 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:00,677 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:01,026 INFO] Step 1400/ 1800; acc:  97.47; ppl:  1.10; xent: 0.10; lr: 1.00000; 6822/6727 tok/s;     35 sec\n",
      "[2021-02-03 01:37:01,142 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:01,146 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:01,603 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:01,607 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:02,052 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:02,056 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:02,301 INFO] Step 1450/ 1800; acc:  98.13; ppl:  1.08; xent: 0.07; lr: 1.00000; 6861/6757 tok/s;     37 sec\n",
      "[2021-02-03 01:37:02,504 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:02,508 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:02,983 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:02,987 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:03,434 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:03,439 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:03,575 INFO] Step 1500/ 1800; acc:  97.42; ppl:  1.11; xent: 0.10; lr: 1.00000; 6777/6685 tok/s;     38 sec\n",
      "[2021-02-03 01:37:03,886 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:03,890 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:04,356 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:04,361 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:04,812 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:04,816 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:04,860 INFO] Step 1550/ 1800; acc:  97.91; ppl:  1.09; xent: 0.08; lr: 1.00000; 6778/6672 tok/s;     39 sec\n",
      "[2021-02-03 01:37:05,268 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:05,272 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:05,725 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:05,729 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:06,140 INFO] Step 1600/ 1800; acc:  98.03; ppl:  1.08; xent: 0.08; lr: 1.00000; 6802/6702 tok/s;     41 sec\n",
      "[2021-02-03 01:37:06,203 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:06,207 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:06,654 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:06,658 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:07,128 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:07,132 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:07,434 INFO] Step 1650/ 1800; acc:  97.74; ppl:  1.10; xent: 0.09; lr: 1.00000; 6784/6682 tok/s;     42 sec\n",
      "[2021-02-03 01:37:07,577 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:07,581 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:08,048 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:08,052 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:08,514 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:08,518 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:08,716 INFO] Step 1700/ 1800; acc:  97.68; ppl:  1.11; xent: 0.10; lr: 1.00000; 6786/6691 tok/s;     43 sec\n",
      "[2021-02-03 01:37:08,984 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:08,989 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:09,450 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:09,454 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:09,900 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:09,906 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:10,005 INFO] Step 1750/ 1800; acc:  97.85; ppl:  1.09; xent: 0.09; lr: 1.00000; 6709/6617 tok/s;     44 sec\n",
      "[2021-02-03 01:37:10,369 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:10,373 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:10,830 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_23/processed.train.0.pt\n",
      "[2021-02-03 01:37:10,834 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:11,303 INFO] Step 1800/ 1800; acc:  97.92; ppl:  1.09; xent: 0.09; lr: 1.00000; 6745/6633 tok/s;     46 sec\n",
      "[2021-02-03 01:37:11,305 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_23_step_1800.pt\n",
      "[2021-02-03 01:37:12,402 INFO]  * src vocab size = 32\n",
      "[2021-02-03 01:37:12,402 INFO]  * tgt vocab size = 30\n",
      "[2021-02-03 01:37:12,402 INFO] Building model...\n",
      "[2021-02-03 01:37:16,889 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(32, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(30, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=30, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-02-03 01:37:16,889 INFO] encoder: 251200\n",
      "[2021-02-03 01:37:16,889 INFO] decoder: 323630\n",
      "[2021-02-03 01:37:16,889 INFO] * number of parameters: 574830\n",
      "[2021-02-03 01:37:16,891 INFO] Starting training on GPU: [0]\n",
      "[2021-02-03 01:37:16,892 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-02-03 01:37:16,892 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:16,896 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:17,376 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:17,380 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:17,823 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:17,827 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:18,193 INFO] Step 50/ 1800; acc:  14.10; ppl: 29.98; xent: 3.40; lr: 1.00000; 6720/6577 tok/s;      1 sec\n",
      "[2021-02-03 01:37:18,284 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:18,288 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:18,728 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:18,732 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:19,189 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:19,193 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:19,459 INFO] Step 100/ 1800; acc:  24.86; ppl: 14.08; xent: 2.65; lr: 1.00000; 6816/6690 tok/s;      3 sec\n",
      "[2021-02-03 01:37:19,650 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:19,654 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:20,121 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:20,125 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:20,553 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:20,557 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:20,713 INFO] Step 150/ 1800; acc:  34.24; ppl: 10.20; xent: 2.32; lr: 1.00000; 6854/6727 tok/s;      4 sec\n",
      "[2021-02-03 01:37:20,997 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:21,001 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:21,445 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:21,449 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:21,887 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:21,891 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:21,939 INFO] Step 200/ 1800; acc:  35.75; ppl:  9.08; xent: 2.21; lr: 1.00000; 6878/6761 tok/s;      5 sec\n",
      "[2021-02-03 01:37:22,352 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:22,356 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:22,811 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:22,815 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:23,218 INFO] Step 250/ 1800; acc:  37.32; ppl:  8.33; xent: 2.12; lr: 1.00000; 6770/6628 tok/s;      6 sec\n",
      "[2021-02-03 01:37:23,264 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:23,268 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:23,704 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:23,708 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:24,161 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:24,165 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:24,473 INFO] Step 300/ 1800; acc:  39.20; ppl:  7.63; xent: 2.03; lr: 1.00000; 6933/6794 tok/s;      8 sec\n",
      "[2021-02-03 01:37:24,605 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:24,609 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:25,042 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:25,046 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:25,492 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:25,497 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:25,732 INFO] Step 350/ 1800; acc:  40.08; ppl:  7.18; xent: 1.97; lr: 1.00000; 6927/6801 tok/s;      9 sec\n",
      "[2021-02-03 01:37:25,948 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:25,953 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:26,406 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:26,429 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:26,860 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:26,864 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:26,965 INFO] Step 400/ 1800; acc:  41.32; ppl:  6.54; xent: 1.88; lr: 1.00000; 6816/6692 tok/s;     10 sec\n",
      "[2021-02-03 01:37:27,307 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:27,311 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:27,740 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:27,744 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:28,193 INFO] Step 450/ 1800; acc:  43.09; ppl:  6.27; xent: 1.84; lr: 1.00000; 6985/6856 tok/s;     11 sec\n",
      "[2021-02-03 01:37:28,193 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:28,197 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:28,636 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:28,640 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:29,086 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:29,090 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:29,453 INFO] Step 500/ 1800; acc:  49.06; ppl:  5.19; xent: 1.65; lr: 1.00000; 6938/6789 tok/s;     13 sec\n",
      "[2021-02-03 01:37:29,542 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:29,547 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:29,985 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:29,990 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:30,445 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:30,449 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:30,713 INFO] Step 550/ 1800; acc:  55.16; ppl:  4.25; xent: 1.45; lr: 1.00000; 6853/6726 tok/s;     14 sec\n",
      "[2021-02-03 01:37:30,886 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:30,890 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:31,357 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:31,361 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:31,805 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:31,809 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:31,978 INFO] Step 600/ 1800; acc:  64.63; ppl:  3.15; xent: 1.15; lr: 1.00000; 6791/6665 tok/s;     15 sec\n",
      "[2021-02-03 01:37:32,272 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:32,277 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:32,735 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:32,740 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:33,188 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:33,192 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:33,239 INFO] Step 650/ 1800; acc:  77.36; ppl:  2.12; xent: 0.75; lr: 1.00000; 6687/6574 tok/s;     16 sec\n",
      "[2021-02-03 01:37:33,652 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:33,656 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:34,108 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:34,112 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:34,502 INFO] Step 700/ 1800; acc:  85.82; ppl:  1.60; xent: 0.47; lr: 1.00000; 6856/6712 tok/s;     18 sec\n",
      "[2021-02-03 01:37:34,546 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:34,550 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:34,986 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:34,990 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:35,441 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:35,445 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:35,748 INFO] Step 750/ 1800; acc:  90.02; ppl:  1.40; xent: 0.34; lr: 1.00000; 6981/6841 tok/s;     19 sec\n",
      "[2021-02-03 01:37:35,883 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:35,887 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:36,330 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:36,334 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:36,789 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:36,794 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:37,021 INFO] Step 800/ 1800; acc:  92.02; ppl:  1.32; xent: 0.28; lr: 1.00000; 6852/6727 tok/s;     20 sec\n",
      "[2021-02-03 01:37:37,244 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:37,248 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:37,704 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:37,708 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:38,155 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:38,159 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:38,261 INFO] Step 850/ 1800; acc:  92.73; ppl:  1.29; xent: 0.25; lr: 1.00000; 6782/6659 tok/s;     21 sec\n",
      "[2021-02-03 01:37:38,597 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:38,601 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:39,042 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:39,046 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:39,487 INFO] Step 900/ 1800; acc:  95.31; ppl:  1.18; xent: 0.16; lr: 1.00000; 6992/6863 tok/s;     23 sec\n",
      "[2021-02-03 01:37:39,487 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:39,491 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:39,936 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:39,941 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:40,390 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:40,394 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:40,749 INFO] Step 950/ 1800; acc:  95.77; ppl:  1.16; xent: 0.15; lr: 1.00000; 6930/6781 tok/s;     24 sec\n",
      "[2021-02-03 01:37:40,829 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:40,833 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:41,283 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:41,287 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:41,716 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:41,720 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:41,982 INFO] Step 1000/ 1800; acc:  95.81; ppl:  1.16; xent: 0.15; lr: 1.00000; 7001/6871 tok/s;     25 sec\n",
      "[2021-02-03 01:37:42,156 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:42,160 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:42,619 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:42,623 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:43,062 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:43,066 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:43,227 INFO] Step 1050/ 1800; acc:  95.53; ppl:  1.18; xent: 0.17; lr: 1.00000; 6900/6772 tok/s;     26 sec\n",
      "[2021-02-03 01:37:43,518 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:43,522 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:43,962 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:43,966 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:44,419 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:44,423 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:44,467 INFO] Step 1100/ 1800; acc:  96.66; ppl:  1.13; xent: 0.12; lr: 1.00000; 6800/6684 tok/s;     28 sec\n",
      "[2021-02-03 01:37:44,864 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:44,868 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:45,312 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:45,316 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:45,712 INFO] Step 1150/ 1800; acc:  96.58; ppl:  1.14; xent: 0.13; lr: 1.00000; 6955/6808 tok/s;     29 sec\n",
      "[2021-02-03 01:37:45,757 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:45,761 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:46,220 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:46,248 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:46,709 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:46,713 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:47,029 INFO] Step 1200/ 1800; acc:  97.14; ppl:  1.11; xent: 0.11; lr: 1.00000; 6606/6473 tok/s;     30 sec\n",
      "[2021-02-03 01:37:47,162 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:47,166 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:47,616 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:47,620 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:48,072 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:48,076 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:48,300 INFO] Step 1250/ 1800; acc:  97.52; ppl:  1.11; xent: 0.10; lr: 1.00000; 6864/6739 tok/s;     31 sec\n",
      "[2021-02-03 01:37:48,532 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:48,536 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:48,995 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:48,999 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:49,443 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:49,448 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:49,550 INFO] Step 1300/ 1800; acc:  97.44; ppl:  1.11; xent: 0.10; lr: 1.00000; 6726/6604 tok/s;     33 sec\n",
      "[2021-02-03 01:37:49,916 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:49,921 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:50,380 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:50,384 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:50,842 INFO] Step 1350/ 1800; acc:  97.55; ppl:  1.10; xent: 0.10; lr: 1.00000; 6635/6513 tok/s;     34 sec\n",
      "[2021-02-03 01:37:50,842 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:50,846 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:51,288 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:51,292 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:51,742 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:51,746 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:52,123 INFO] Step 1400/ 1800; acc:  97.78; ppl:  1.09; xent: 0.09; lr: 1.00000; 6828/6682 tok/s;     35 sec\n",
      "[2021-02-03 01:37:52,206 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:52,210 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:52,675 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:52,679 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:53,137 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:53,141 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:53,421 INFO] Step 1450/ 1800; acc:  98.18; ppl:  1.07; xent: 0.06; lr: 1.00000; 6650/6527 tok/s;     37 sec\n",
      "[2021-02-03 01:37:53,602 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:53,606 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:54,065 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:54,069 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:54,530 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:54,535 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:54,702 INFO] Step 1500/ 1800; acc:  98.14; ppl:  1.09; xent: 0.09; lr: 1.00000; 6706/6582 tok/s;     38 sec\n",
      "[2021-02-03 01:37:54,984 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:54,988 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:55,438 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:55,443 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:55,892 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:55,896 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:55,943 INFO] Step 1550/ 1800; acc:  97.89; ppl:  1.09; xent: 0.09; lr: 1.00000; 6798/6683 tok/s;     39 sec\n",
      "[2021-02-03 01:37:56,342 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:56,346 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:56,802 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:56,807 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:57,212 INFO] Step 1600/ 1800; acc:  97.98; ppl:  1.09; xent: 0.08; lr: 1.00000; 6816/6673 tok/s;     40 sec\n",
      "[2021-02-03 01:37:57,258 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:57,262 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:57,711 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:57,715 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:58,154 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:58,158 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:58,469 INFO] Step 1650/ 1800; acc:  97.75; ppl:  1.10; xent: 0.09; lr: 1.00000; 6921/6782 tok/s;     42 sec\n",
      "[2021-02-03 01:37:58,604 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:58,608 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:59,068 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:59,072 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:59,528 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:59,532 INFO] number of examples: 360\n",
      "[2021-02-03 01:37:59,760 INFO] Step 1700/ 1800; acc:  98.32; ppl:  1.07; xent: 0.07; lr: 1.00000; 6757/6634 tok/s;     43 sec\n",
      "[2021-02-03 01:37:59,980 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:37:59,984 INFO] number of examples: 360\n",
      "[2021-02-03 01:38:00,426 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:38:00,431 INFO] number of examples: 360\n",
      "[2021-02-03 01:38:00,882 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:38:00,886 INFO] number of examples: 360\n",
      "[2021-02-03 01:38:00,995 INFO] Step 1750/ 1800; acc:  98.75; ppl:  1.04; xent: 0.04; lr: 1.00000; 6810/6687 tok/s;     44 sec\n",
      "[2021-02-03 01:38:01,341 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:38:01,345 INFO] number of examples: 360\n",
      "[2021-02-03 01:38:01,780 INFO] Loading dataset from drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/360_24/processed.train.0.pt\n",
      "[2021-02-03 01:38:01,784 INFO] number of examples: 360\n",
      "[2021-02-03 01:38:02,217 INFO] Step 1800/ 1800; acc:  98.36; ppl:  1.07; xent: 0.07; lr: 1.00000; 7014/6884 tok/s;     45 sec\n",
      "[2021-02-03 01:38:02,218 INFO] Saving checkpoint drive/MyDrive/GermanToleranceBaselineCogSci/output/german_rnn_model_360_24_step_1800.pt\n"
     ]
    }
   ],
   "source": [
    "for datasize in datasizes:\n",
    "  epochs, n_examples, batchsize,  = 100, int(datasize.split('_')[0]), 20\n",
    "  steps = str(int(epochs * n_examples / batchsize))\n",
    "\n",
    "  datadir = f'drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/{datasize}'\n",
    "  rnn_modelpath = f'{outdir}/german_rnn_model_{datasize}'\n",
    "  rnn_train_args = ' '.join([\n",
    "    f'-data {datadir}/processed',\n",
    "    '-save_model '+rnn_modelpath,\n",
    "    '-enc_layers 2',\n",
    "    '-dec_layers 2',\n",
    "    '-rnn_size 100',\n",
    "    '-batch_size 20',\n",
    "    '-word_vec_size 300',\n",
    "    '-gpu_ranks 0',\n",
    "    '-train_steps '+steps,\n",
    "    '-save_checkpoint_steps '+steps\n",
    "    ])\n",
    "\n",
    "  !python OpenNMT-py/train.py $rnn_train_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM11J8k9hmmB"
   },
   "source": [
    "## Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDyD7XU3hmI-",
    "outputId": "0b373524-1749-488e-9f75-dc8cb4b96e38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "[2021-02-03 01:44:01,503 INFO] \n",
      "SENT 46: ['p', 'r', 'e', 's', 's', 'e']\n",
      "PRED 46: s p e n p e n\n",
      "PRED SCORE: -1.5027\n",
      "\n",
      "[2021-02-03 01:44:01,504 INFO] \n",
      "SENT 47: ['p', 'r', 'i', 'n', 'z']\n",
      "PRED 47: p p i n z i n z e\n",
      "PRED SCORE: -0.1054\n",
      "\n",
      "[2021-02-03 01:44:01,504 INFO] \n",
      "SENT 48: ['s', 't', 'r', 'u', 'd', 'e', 'l']\n",
      "PRED 48: t r u d s t e l u d e n\n",
      "PRED SCORE: -1.1731\n",
      "\n",
      "[2021-02-03 01:44:01,504 INFO] \n",
      "SENT 49: ['k', 'a', 'm', 'i', 'n']\n",
      "PRED 49: k m a m i n a m i n e\n",
      "PRED SCORE: -0.8108\n",
      "\n",
      "[2021-02-03 01:44:01,504 INFO] \n",
      "SENT 50: ['i', 'n', 's', 't', 'a', 'n', 'z']\n",
      "PRED 50: t a n s a n z a n z e\n",
      "PRED SCORE: -0.0809\n",
      "\n",
      "[2021-02-03 01:44:01,504 INFO] \n",
      "SENT 51: ['f', 'e', 'u', 'e', 'r', 'p', 'r', 'o', 'b', 'e']\n",
      "PRED 51: p f o f o b e u b e u b e n\n",
      "PRED SCORE: -2.6643\n",
      "\n",
      "[2021-02-03 01:44:01,505 INFO] \n",
      "SENT 52: ['b', 'e', 'i', 's', 'p', 'i', 'e', 'l']\n",
      "PRED 52: p i e b e n l i e n\n",
      "PRED SCORE: -1.8917\n",
      "\n",
      "[2021-02-03 01:44:01,505 INFO] \n",
      "SENT 53: ['a', 'b', 's', 'c', 'h', 'r', 'e', 'i', 'b', 'u', 'n', 'g']\n",
      "PRED 53: c a b s c h r e i b h r e i e r\n",
      "PRED SCORE: -1.4299\n",
      "\n",
      "[2021-02-03 01:44:01,505 INFO] \n",
      "SENT 54: ['k', 'a', 't', 'a', 'l', 'o', 'g']\n",
      "PRED 54: k a t k a t a t a g e n\n",
      "PRED SCORE: -1.3181\n",
      "\n",
      "[2021-02-03 01:44:01,505 INFO] \n",
      "SENT 55: ['p', 'a', 'r', 't', 'e', 'i']\n",
      "PRED 55: p a r t a r t e n\n",
      "PRED SCORE: -2.0212\n",
      "\n",
      "[2021-02-03 01:44:01,505 INFO] \n",
      "SENT 56: ['a', 'b', 'z', 'u', 'g']\n",
      "PRED 56: z a b z a g z e n\n",
      "PRED SCORE: -1.1287\n",
      "\n",
      "[2021-02-03 01:44:01,506 INFO] \n",
      "SENT 57: ['g', 'e', 's', 'e', 'l', 'l', 's', 'c', 'h', 'a', 'f', 't']\n",
      "PRED 57: c u f s c h a f t a f l e n\n",
      "PRED SCORE: -3.0698\n",
      "\n",
      "[2021-02-03 01:44:01,506 INFO] \n",
      "SENT 58: ['z', 'i', 'e', 'l']\n",
      "PRED 58: z i e l i e l z e\n",
      "PRED SCORE: -0.0375\n",
      "\n",
      "[2021-02-03 01:44:01,506 INFO] \n",
      "SENT 59: ['v', 'e', 'r', 'p', 'f', 'l', 'i', 'c', 'h', 't', 'u', 'n', 'g']\n",
      "PRED 59: t u c h t t u n z u n g e r\n",
      "PRED SCORE: -1.4338\n",
      "\n",
      "[2021-02-03 01:44:01,506 INFO] \n",
      "SENT 60: ['b', 'e', 'z', 'i', 'e', 'h', 'u', 'n', 'g']\n",
      "PRED 60: b i e b e n z i e n g e\n",
      "PRED SCORE: -2.0873\n",
      "\n",
      "[2021-02-03 01:44:01,758 INFO] \n",
      "SENT 61: ['m', 'i', 't', 'g', 'l', 'i', 'e', 'd']\n",
      "PRED 61: m m m i t i t l i e g e n\n",
      "PRED SCORE: -1.9116\n",
      "\n",
      "[2021-02-03 01:44:01,758 INFO] \n",
      "SENT 62: ['e', 'n', 't', 's', 'c', 'h', 'e', 'i', 'd', 'u', 'n', 'g']\n",
      "PRED 62: c e n t u n t u n g u n g e n\n",
      "PRED SCORE: -0.9792\n",
      "\n",
      "[2021-02-03 01:44:01,758 INFO] \n",
      "SENT 63: ['b', 'e', 'd', 'i', 'n', 'g', 'u', 'n', 'g']\n",
      "PRED 63: b i n b e n g u n g e n\n",
      "PRED SCORE: -0.5513\n",
      "\n",
      "[2021-02-03 01:44:01,759 INFO] \n",
      "SENT 64: ['d', 'e', 'm', 'o', 'k', 'r', 'a', 't']\n",
      "PRED 64: a m d e m o t a t e n\n",
      "PRED SCORE: -2.2320\n",
      "\n",
      "[2021-02-03 01:44:01,759 INFO] \n",
      "SENT 65: ['s', 'c', 'h', 'u', 'b', 's']\n",
      "PRED 65: c h u b s c h u b e n\n",
      "PRED SCORE: -0.9703\n",
      "\n",
      "[2021-02-03 01:44:01,759 INFO] \n",
      "SENT 66: ['g', 'e', 's', 'e', 't', 'z']\n",
      "PRED 66: t b e g e t z e t e n\n",
      "PRED SCORE: -1.6026\n",
      "\n",
      "[2021-02-03 01:44:01,759 INFO] \n",
      "SENT 67: ['b', 'a', 'u', 'c', 'h']\n",
      "PRED 67: h u u b a u c h e n\n",
      "PRED SCORE: -0.5022\n",
      "\n",
      "[2021-02-03 01:44:01,760 INFO] \n",
      "SENT 68: ['h', 'i', 'l', 'f', 'e']\n",
      "PRED 68: h i l f e n\n",
      "PRED SCORE: -0.5629\n",
      "\n",
      "[2021-02-03 01:44:01,760 INFO] \n",
      "SENT 69: ['u', 'm', 's', 'a', 't', 'z']\n",
      "PRED 69: a m s a t z a t z e\n",
      "PRED SCORE: -1.7963\n",
      "\n",
      "[2021-02-03 01:44:01,760 INFO] \n",
      "SENT 70: ['w', 'i', 'n', 'k', 'e', 'l']\n",
      "PRED 70: w w i n k e l\n",
      "PRED SCORE: -0.1551\n",
      "\n",
      "[2021-02-03 01:44:01,760 INFO] \n",
      "SENT 71: ['f', 'o', 't', 'o']\n",
      "PRED 71: f f o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t o t\n",
      "PRED SCORE: -0.5167\n",
      "\n",
      "[2021-02-03 01:44:01,760 INFO] \n",
      "SENT 72: ['s', 't', 'a', 'g', 'n', 'a', 't', 'i', 'o', 'n']\n",
      "PRED 72: t i o s a t i o n a t e n\n",
      "PRED SCORE: -0.7404\n",
      "\n",
      "[2021-02-03 01:44:01,760 INFO] \n",
      "SENT 73: ['a', 'g', 'n', 'o', 's', 't', 'i', 'k', 'e', 'r']\n",
      "PRED 73: t a g n o g t o g e r\n",
      "PRED SCORE: -2.2940\n",
      "\n",
      "[2021-02-03 01:44:01,761 INFO] \n",
      "SENT 74: ['m', 'e', 'i', 'n', 'u', 'n', 'g']\n",
      "PRED 74: m m m m e i n u n g e n\n",
      "PRED SCORE: -0.7738\n",
      "\n",
      "[2021-02-03 01:44:01,761 INFO] \n",
      "SENT 75: ['m', 'o', 'n', 'a', 'r', 'c', 'h']\n",
      "PRED 75: c m o r o n a r c h e n\n",
      "PRED SCORE: -0.9657\n",
      "\n",
      "[2021-02-03 01:44:01,761 INFO] \n",
      "SENT 76: ['m', 'i', 's', 'c', 'h', 'u', 'n', 'g']\n",
      "PRED 76: c h u n c h u n g e n\n",
      "PRED SCORE: -1.4249\n",
      "\n",
      "[2021-02-03 01:44:01,761 INFO] \n",
      "SENT 77: ['a', 'n', 'g', 'e', 'b', 'o', 't']\n",
      "PRED 77: b a n g a n g e b e n\n",
      "PRED SCORE: -0.5779\n",
      "\n",
      "[2021-02-03 01:44:01,761 INFO] \n",
      "SENT 78: ['f', 'o', 'h', 'l', 'e', 'n']\n",
      "PRED 78: n f o h l e n\n",
      "PRED SCORE: -0.0215\n",
      "\n",
      "[2021-02-03 01:44:01,761 INFO] \n",
      "SENT 79: ['h', 'e', 'r', 'r', 's', 'c', 'h', 'a', 'f', 't']\n",
      "PRED 79: c h a f s c h a f t a f t e r\n",
      "PRED SCORE: -1.5982\n",
      "\n",
      "[2021-02-03 01:44:01,762 INFO] \n",
      "SENT 80: ['s', 'c', 'h', 'i', 'c', 'k', 's', 'a', 'l']\n",
      "PRED 80: c i c h i c h k a l c h e n\n",
      "PRED SCORE: -1.4368\n",
      "\n",
      "[2021-02-03 01:44:01,762 INFO] PRED AVG SCORE: -0.0935, PRED PPL: 1.0980\n",
      "[2021-02-03 01:44:02,676 INFO] Translating shard 0.\n",
      "[2021-02-03 01:44:02,800 INFO] \n",
      "SENT 1: ['MAS', '<s>', 'p', 'f', 'a', 'n', 'd', 'b', 'r', 'i', 'e', 'f']\n",
      "PRED 1: p f a n d i e f e r\n",
      "PRED SCORE: -1.3321\n",
      "\n",
      "[2021-02-03 01:44:02,801 INFO] \n",
      "SENT 2: ['MAS', '<s>', 's', 'c', 'h', 'l', 'i', 't', 't', 's', 'c', 'h', 'u', 'h']\n",
      "PRED 2: s c h l i t t c h e n\n",
      "PRED SCORE: -0.2669\n",
      "\n",
      "[2021-02-03 01:44:02,801 INFO] \n",
      "SENT 3: ['MAS', '<s>', 'a', 'n', 't', 'e', 'i', 'l']\n",
      "PRED 3: a n t e i l e\n",
      "PRED SCORE: -0.0012\n",
      "\n",
      "[2021-02-03 01:44:02,801 INFO] \n",
      "SENT 4: ['MAS', '<s>', 'p', 'r', 'e', 'i', 's']\n",
      "PRED 4: p r e i s e\n",
      "PRED SCORE: -0.0130\n",
      "\n",
      "[2021-02-03 01:44:02,801 INFO] \n",
      "SENT 5: ['MAS', '<s>', 'a', 'r', 'b', 'e', 'i', 't', 'g', 'e', 'b', 'e', 'r']\n",
      "PRED 5: a r b e i t e i t e r\n",
      "PRED SCORE: -1.0515\n",
      "\n",
      "[2021-02-03 01:44:02,802 INFO] \n",
      "SENT 6: ['FEM', '<s>', 's', 'e', 'e', 'l', 'e']\n",
      "PRED 6: s e e l e n\n",
      "PRED SCORE: -0.0023\n",
      "\n",
      "[2021-02-03 01:44:02,802 INFO] \n",
      "SENT 7: ['MAS', '<s>', 'a', 'b', 's', 'a', 't', 'z']\n",
      "PRED 7: a b s a t z e\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-02-03 01:44:02,802 INFO] \n",
      "SENT 8: ['MAS', '<s>', 'p', 'l', 'a', 'n']\n",
      "PRED 8: p l a n e\n",
      "PRED SCORE: -0.0012\n",
      "\n",
      "[2021-02-03 01:44:02,802 INFO] \n",
      "SENT 9: ['MAS', '<s>', 'b', 'e', 'z', 'i', 'r', 'k']\n",
      "PRED 9: b e z i r k e\n",
      "PRED SCORE: -0.1845\n",
      "\n",
      "[2021-02-03 01:44:02,802 INFO] \n",
      "SENT 10: ['FEM', '<s>', 'a', 'b', 't', 'e', 'i', 'l', 'u', 'n', 'g']\n",
      "PRED 10: a b t e i l u n g e n\n",
      "PRED SCORE: -0.1866\n",
      "\n",
      "[2021-02-03 01:44:02,803 INFO] \n",
      "SENT 11: ['FEM', '<s>', 'i', 'n', 'v', 'e', 's', 't', 'i', 't', 'i', 'o', 'n']\n",
      "PRED 11: i n z e s t i o n e n\n",
      "PRED SCORE: -1.5919\n",
      "\n",
      "[2021-02-03 01:44:02,803 INFO] \n",
      "SENT 12: ['FEM', '<s>', 'b', 'e', 'z', 'i', 'e', 'h', 'u', 'n', 'g']\n",
      "PRED 12: b e z i e n u n g e n\n",
      "PRED SCORE: -0.4685\n",
      "\n",
      "[2021-02-03 01:44:02,803 INFO] \n",
      "SENT 13: ['MAS', '<s>', 'w', 'u', 'n', 's', 'c', 'h']\n",
      "PRED 13: w u n s c h e\n",
      "PRED SCORE: -0.5467\n",
      "\n",
      "[2021-02-03 01:44:02,803 INFO] \n",
      "SENT 14: ['FEM', '<s>', 'p', 'r', 'e', 's', 's', 'e']\n",
      "PRED 14: p r e s s e n\n",
      "PRED SCORE: -0.0317\n",
      "\n",
      "[2021-02-03 01:44:02,803 INFO] \n",
      "SENT 15: ['MAS', '<s>', 'v', 'e', 'r', 'l', 'a', 'g']\n",
      "PRED 15: v e r l a g e\n",
      "PRED SCORE: -0.0033\n",
      "\n",
      "[2021-02-03 01:44:02,804 INFO] \n",
      "SENT 16: ['NTR', '<s>', 'g', 'e', 'b', 'i', 's', 's']\n",
      "PRED 16: g e b i s e\n",
      "PRED SCORE: -0.1097\n",
      "\n",
      "[2021-02-03 01:44:02,804 INFO] \n",
      "SENT 17: ['MAS', '<s>', 'k', 'a', 't', 'a', 'l', 'o', 'g']\n",
      "PRED 17: k a t a l o g e\n",
      "PRED SCORE: -0.1198\n",
      "\n",
      "[2021-02-03 01:44:02,804 INFO] \n",
      "SENT 18: ['MAS', '<s>', 'd', 'i', 'c', 'h', 't', 'e', 'r']\n",
      "PRED 18: d i c h t e r\n",
      "PRED SCORE: -0.0091\n",
      "\n",
      "[2021-02-03 01:44:02,804 INFO] \n",
      "SENT 19: ['MAS', '<s>', 'a', 'n', 't', 'r', 'a', 'g']\n",
      "PRED 19: a n t r a g e\n",
      "PRED SCORE: -0.0025\n",
      "\n",
      "[2021-02-03 01:44:02,804 INFO] \n",
      "SENT 20: ['MAS', '<s>', 'g', 'o', 'l', 'd', 'f', 'i', 's', 'c', 'h']\n",
      "PRED 20: g o l d i g c h i g e\n",
      "PRED SCORE: -1.3515\n",
      "\n",
      "[2021-02-03 01:44:02,805 INFO] \n",
      "SENT 21: ['MAS', '<s>', 'v', 'o', 'r', 't', 'r', 'a', 'g']\n",
      "PRED 21: v o r t r a g e\n",
      "PRED SCORE: -0.0091\n",
      "\n",
      "[2021-02-03 01:44:02,805 INFO] \n",
      "SENT 22: ['MAS', '<s>', 'g', 'u', 'r', 't']\n",
      "PRED 22: g u r t e\n",
      "PRED SCORE: -0.0024\n",
      "\n",
      "[2021-02-03 01:44:02,805 INFO] \n",
      "SENT 23: ['MAS', '<s>', 'p', 'r', 'e', 'm', 'i', 'e', 'r', 'm', 'i', 'n', 'i', 's', 't', 'e', 'r']\n",
      "PRED 23: p r e m i e r m i e r m e r m i n e r\n",
      "PRED SCORE: -0.4944\n",
      "\n",
      "[2021-02-03 01:44:02,805 INFO] \n",
      "SENT 24: ['NTR', '<s>', 'a', 'n', 'g', 'e', 'b', 'o', 't']\n",
      "PRED 24: a n g e b o t e\n",
      "PRED SCORE: -0.9402\n",
      "\n",
      "[2021-02-03 01:44:02,805 INFO] \n",
      "SENT 25: ['FEM', '<s>', 'f', 'r', 'e', 'i', 'h', 'e', 'i', 't']\n",
      "PRED 25: f r e i h e i t e n\n",
      "PRED SCORE: -0.1345\n",
      "\n",
      "[2021-02-03 01:44:02,806 INFO] \n",
      "SENT 26: ['FEM', '<s>', 'r', 'a', 't', 'e']\n",
      "PRED 26: r a t e n\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-02-03 01:44:02,806 INFO] \n",
      "SENT 27: ['NTR', '<s>', 't', 'h', 'e', 'a', 't', 'e', 'r']\n",
      "PRED 27: t h e a t e r\n",
      "PRED SCORE: -0.0084\n",
      "\n",
      "[2021-02-03 01:44:02,806 INFO] \n",
      "SENT 28: ['MAS', '<s>', 'a', 'k', 't']\n",
      "PRED 28: a k t e\n",
      "PRED SCORE: -0.0068\n",
      "\n",
      "[2021-02-03 01:44:02,806 INFO] \n",
      "SENT 29: ['FEM', '<s>', 'h', 'o', 'c', 'h', 's', 'c', 'h', 'u', 'l', 'e']\n",
      "PRED 29: h o c h s c h e n\n",
      "PRED SCORE: -1.4627\n",
      "\n",
      "[2021-02-03 01:44:02,806 INFO] \n",
      "SENT 30: ['MAS', '<s>', 'e', 'i', 'n', 'b', 'r', 'u', 'c', 'h']\n",
      "PRED 30: e i n b r u c h e\n",
      "PRED SCORE: -0.0203\n",
      "\n",
      "[2021-02-03 01:44:02,936 INFO] \n",
      "SENT 31: ['FEM', '<s>', 'f', 'r', 'e', 'u', 'n', 'd', 'i', 'n']\n",
      "PRED 31: f r e u n i n e n\n",
      "PRED SCORE: -0.4728\n",
      "\n",
      "[2021-02-03 01:44:02,936 INFO] \n",
      "SENT 32: ['FEM', '<s>', 'e', 'h', 'e']\n",
      "PRED 32: e h e n\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-02-03 01:44:02,936 INFO] \n",
      "SENT 33: ['MAS', '<s>', 'b', 'e', 'w', 'e', 'i', 's']\n",
      "PRED 33: b e w e i s e\n",
      "PRED SCORE: -0.3419\n",
      "\n",
      "[2021-02-03 01:44:02,936 INFO] \n",
      "SENT 34: ['MAS', '<s>', 'v', 'e', 'r', 's', 'u', 'c', 'h']\n",
      "PRED 34: v e r s u c h e\n",
      "PRED SCORE: -0.0290\n",
      "\n",
      "[2021-02-03 01:44:02,937 INFO] \n",
      "SENT 35: ['FEM', '<s>', 'v', 'e', 'r', 'w', 'a', 'l', 't', 'u', 'n', 'g']\n",
      "PRED 35: v e r w a l t u n g e n\n",
      "PRED SCORE: -1.0731\n",
      "\n",
      "[2021-02-03 01:44:02,937 INFO] \n",
      "SENT 36: ['NTR', '<s>', 's', 'p', 'i', 'e', 'l', 'f', 'e', 'l', 'd']\n",
      "PRED 36: s p i e l d e l d e r\n",
      "PRED SCORE: -1.2354\n",
      "\n",
      "[2021-02-03 01:44:02,937 INFO] \n",
      "SENT 37: ['NTR', '<s>', 'z', 'i', 'e', 'l']\n",
      "PRED 37: z i e l e\n",
      "PRED SCORE: -0.0042\n",
      "\n",
      "[2021-02-03 01:44:02,937 INFO] \n",
      "SENT 38: ['FEM', '<s>', 'a', 'b', 'g', 'a', 'b', 'e']\n",
      "PRED 38: a b g a b e n\n",
      "PRED SCORE: -0.0011\n",
      "\n",
      "[2021-02-03 01:44:02,937 INFO] \n",
      "SENT 39: ['MAS', '<s>', 'k', 'a', 'm', 'p', 'f']\n",
      "PRED 39: k a m p f e\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-02-03 01:44:02,937 INFO] \n",
      "SENT 40: ['FEM', '<s>', 'g', 'r', 'u', 'n', 'd', 'l', 'a', 'g', 'e']\n",
      "PRED 40: g r u n d e n a g e n\n",
      "PRED SCORE: -0.7451\n",
      "\n",
      "[2021-02-03 01:44:02,937 INFO] \n",
      "SENT 41: ['MAS', '<s>', 'b', 'a', 'y', 'e', 'r']\n",
      "PRED 41: b a m e r\n",
      "PRED SCORE: -0.0520\n",
      "\n",
      "[2021-02-03 01:44:02,938 INFO] \n",
      "SENT 42: ['FEM', '<s>', 't', 'r', 'u', 'p', 'p', 'e']\n",
      "PRED 42: t r u p p e n\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-02-03 01:44:02,938 INFO] \n",
      "SENT 43: ['NTR', '<s>', 'p', 'r', 'o', 'd', 'u', 'k', 't']\n",
      "PRED 43: p r o d u k e r\n",
      "PRED SCORE: -0.7873\n",
      "\n",
      "[2021-02-03 01:44:02,938 INFO] \n",
      "SENT 44: ['FEM', '<s>', 'a', 'n', 'e', 'r', 'k', 'e', 'n', 'n', 'u', 'n', 'g']\n",
      "PRED 44: a n e r k e n u n g e n\n",
      "PRED SCORE: -0.2798\n",
      "\n",
      "[2021-02-03 01:44:02,938 INFO] \n",
      "SENT 45: ['FEM', '<s>', 'a', 'u', 's', 'w', 'a', 'h', 'l']\n",
      "PRED 45: a u s w a h l e n\n",
      "PRED SCORE: -0.0059\n",
      "\n",
      "[2021-02-03 01:44:02,938 INFO] \n",
      "SENT 46: ['NTR', '<s>', 'h', 'a', 'b', 'e', 'n']\n",
      "PRED 46: h a b e n\n",
      "PRED SCORE: -0.0317\n",
      "\n",
      "[2021-02-03 01:44:02,938 INFO] \n",
      "SENT 47: ['FEM', '<s>', 's', 'p', 'a', 'n', 'n', 'u', 'n', 'g']\n",
      "PRED 47: s p a n g u n g e n\n",
      "PRED SCORE: -0.6982\n",
      "\n",
      "[2021-02-03 01:44:02,938 INFO] \n",
      "SENT 48: ['NTR', '<s>', 'e', 'i', 'g', 'e', 'n', 't', 'u', 'm']\n",
      "PRED 48: e i g e n t u m e\n",
      "PRED SCORE: -0.0780\n",
      "\n",
      "[2021-02-03 01:44:02,938 INFO] \n",
      "SENT 49: ['MAS', '<s>', 'a', 'u', 's', 't', 'r', 'a', 'l', 'i', 'e', 'r']\n",
      "PRED 49: a u s t r a l i e r e\n",
      "PRED SCORE: -0.0965\n",
      "\n",
      "[2021-02-03 01:44:02,939 INFO] \n",
      "SENT 50: ['MAS', '<s>', 'b', 'o', 'd', 'e', 'n']\n",
      "PRED 50: b o d e n\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-02-03 01:44:02,939 INFO] \n",
      "SENT 51: ['MAS', '<s>', 's', 't', 'a', 'a', 't']\n",
      "PRED 51: s t a a t e\n",
      "PRED SCORE: -0.0090\n",
      "\n",
      "[2021-02-03 01:44:02,939 INFO] \n",
      "SENT 52: ['NTR', '<s>', 'p', 'r', 'o', 'z', 'e', 'n', 't']\n",
      "PRED 52: p r o z e n t e\n",
      "PRED SCORE: -0.2804\n",
      "\n",
      "[2021-02-03 01:44:02,939 INFO] \n",
      "SENT 53: ['NTR', '<s>', 'a', 'l', 'p', 'e', 'n', 'v', 'e', 'i', 'l', 'c', 'h', 'e', 'n']\n",
      "PRED 53: a l p e n e i l c h e n\n",
      "PRED SCORE: -2.7390\n",
      "\n",
      "[2021-02-03 01:44:02,939 INFO] \n",
      "SENT 54: ['MAS', '<s>', 'o', 'r', 'g', 'a', 'n', 'i', 's', 'm', 'u', 's']\n",
      "PRED 54: o r g a n i s m u s\n",
      "PRED SCORE: -0.4018\n",
      "\n",
      "[2021-02-03 01:44:02,939 INFO] \n",
      "SENT 55: ['FEM', '<s>', 'a', 'n', 'g', 'a', 'b', 'e']\n",
      "PRED 55: a n g a b e n\n",
      "PRED SCORE: -0.0018\n",
      "\n",
      "[2021-02-03 01:44:02,939 INFO] \n",
      "SENT 56: ['NTR', '<s>', 'g', 'r', 'u', 'n', 'd', 'g', 'e', 's', 'e', 't', 'z']\n",
      "PRED 56: g r u n d e s t e n\n",
      "PRED SCORE: -1.7911\n",
      "\n",
      "[2021-02-03 01:44:02,939 INFO] \n",
      "SENT 57: ['MAS', '<s>', 's', 'c', 'h', 'l', 'a', 'g', 'b', 'a', 'u', 'm']\n",
      "PRED 57: s c h l a u m a u m e m\n",
      "PRED SCORE: -1.1810\n",
      "\n",
      "[2021-02-03 01:44:02,940 INFO] \n",
      "SENT 58: ['FEM', '<s>', 'z', 'u', 's', 'a', 'm', 'm', 'e', 'n', 'z', 'i', 'e', 'h', 'u', 'n', 'g']\n",
      "PRED 58: z u s a m m e n z i e m m e n\n",
      "PRED SCORE: -0.7090\n",
      "\n",
      "[2021-02-03 01:44:02,940 INFO] \n",
      "SENT 59: ['MAS', '<s>', 'o', 'f', 'f', 'i', 'z', 'i', 'e', 'r']\n",
      "PRED 59: o f f i e r i e r e\n",
      "PRED SCORE: -0.0180\n",
      "\n",
      "[2021-02-03 01:44:02,940 INFO] \n",
      "SENT 60: ['FEM', '<s>', 's', 'p', 'r', 'a', 'c', 'h', 'w', 'i', 's', 's', 'e', 'n', 's', 'c', 'h', 'a', 'f', 't']\n",
      "PRED 60: s p r a c h e n s c h a f s c h a f t e n\n",
      "PRED SCORE: -0.9902\n",
      "\n",
      "[2021-02-03 01:44:03,032 INFO] \n",
      "SENT 61: ['NTR', '<s>', 'm', 'u', 's', 't', 'e', 'r']\n",
      "PRED 61: m u s t e r\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-02-03 01:44:03,033 INFO] \n",
      "SENT 62: ['NTR', '<s>', 'v', 'e', 'r', 'b', 'r', 'e', 'c', 'h', 'e', 'n']\n",
      "PRED 62: v e r b r e c h e n\n",
      "PRED SCORE: -0.0082\n",
      "\n",
      "[2021-02-03 01:44:03,033 INFO] \n",
      "SENT 63: ['MAS', '<s>', 's', 'o', 'z', 'i', 'a', 'l', 'i', 's', 'm', 'u', 's']\n",
      "PRED 63: s o z i a l i g u s\n",
      "PRED SCORE: -0.8525\n",
      "\n",
      "[2021-02-03 01:44:03,033 INFO] \n",
      "SENT 64: ['MAS', '<s>', 'm', 'i', 'n', 'i', 's', 't', 'e', 'r']\n",
      "PRED 64: m i n i s t e r\n",
      "PRED SCORE: -0.0182\n",
      "\n",
      "[2021-02-03 01:44:03,033 INFO] \n",
      "SENT 65: ['MAS', '<s>', 'b', 'e', 't', 'r', 'i', 'e', 'b']\n",
      "PRED 65: b e t r i e b e\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-02-03 01:44:03,034 INFO] \n",
      "SENT 66: ['FEM', '<s>', 'h', 'e', 'r', 'r', 's', 'c', 'h', 'a', 'f', 't']\n",
      "PRED 66: h e r r s c h a f t e n\n",
      "PRED SCORE: -0.0025\n",
      "\n",
      "[2021-02-03 01:44:03,034 INFO] \n",
      "SENT 67: ['MAS', '<s>', 'v', 'e', 'r', 't', 'r', 'a', 'g']\n",
      "PRED 67: v e r t r a g e\n",
      "PRED SCORE: -0.0049\n",
      "\n",
      "[2021-02-03 01:44:03,034 INFO] \n",
      "SENT 68: ['FEM', '<s>', 'g', 'e', 's', 'e', 'l', 'l', 's', 'c', 'h', 'a', 'f', 't']\n",
      "PRED 68: g e s e l l s c h a f t e n\n",
      "PRED SCORE: -0.2221\n",
      "\n",
      "[2021-02-03 01:44:03,034 INFO] \n",
      "SENT 69: ['FEM', '<s>', 'g', 'e', 'f', 'a', 'h', 'r']\n",
      "PRED 69: g e f a h r e n\n",
      "PRED SCORE: -0.0025\n",
      "\n",
      "[2021-02-03 01:44:03,034 INFO] \n",
      "SENT 70: ['MAS', '<s>', 'w', 'a', 'f', 'f', 'e', 'n', 's', 't', 'i', 'l', 'l', 's', 't', 'a', 'n', 'd']\n",
      "PRED 70: w a f f i l a n d e n s t a n d e r\n",
      "PRED SCORE: -2.2846\n",
      "\n",
      "[2021-02-03 01:44:03,035 INFO] \n",
      "SENT 71: ['MAS', '<s>', 'j', 'e', 's', 'u', 'i', 't']\n",
      "PRED 71: j e s u i t e\n",
      "PRED SCORE: -0.0958\n",
      "\n",
      "[2021-02-03 01:44:03,035 INFO] \n",
      "SENT 72: ['MAS', '<s>', 'b', 'a', 'u', 's', 't', 'e', 'i', 'n']\n",
      "PRED 72: b a u s t e i n e\n",
      "PRED SCORE: -0.0033\n",
      "\n",
      "[2021-02-03 01:44:03,035 INFO] \n",
      "SENT 73: ['MAS', '<s>', 'r', 'u', 'f']\n",
      "PRED 73: r u f f e\n",
      "PRED SCORE: -0.0092\n",
      "\n",
      "[2021-02-03 01:44:03,035 INFO] \n",
      "SENT 74: ['MAS', '<s>', 'b', 'e', 'l', 'e', 'g']\n",
      "PRED 74: b e l e g e\n",
      "PRED SCORE: -0.1305\n",
      "\n",
      "[2021-02-03 01:44:03,036 INFO] \n",
      "SENT 75: ['MAS', '<s>', 'p', 'f', 'a', 'r', 'r', 'e', 'r']\n",
      "PRED 75: p f a r r e r\n",
      "PRED SCORE: -0.0022\n",
      "\n",
      "[2021-02-03 01:44:03,036 INFO] \n",
      "SENT 76: ['MAS', '<s>', 'g', 'e', 'g', 'e', 'n', 's', 't', 'a', 'n', 'd']\n",
      "PRED 76: g e g e n d a n d e r\n",
      "PRED SCORE: -0.8467\n",
      "\n",
      "[2021-02-03 01:44:03,036 INFO] \n",
      "SENT 77: ['NTR', '<s>', 's', 'y', 'm', 'p', 't', 'o', 'm']\n",
      "PRED 77: s y m p o m p o m e n\n",
      "PRED SCORE: -1.0064\n",
      "\n",
      "[2021-02-03 01:44:03,036 INFO] \n",
      "SENT 78: ['NTR', '<s>', 'm', 'i', 't', 'g', 'l', 'i', 'e', 'd']\n",
      "PRED 78: m i t g l i e r e\n",
      "PRED SCORE: -0.0716\n",
      "\n",
      "[2021-02-03 01:44:03,037 INFO] \n",
      "SENT 79: ['FEM', '<s>', 's', 't', 'i', 'r', 'n']\n",
      "PRED 79: s t i r n e n\n",
      "PRED SCORE: -0.0014\n",
      "\n",
      "[2021-02-03 01:44:03,037 INFO] \n",
      "SENT 80: ['MAS', '<s>', 'r', 'a', 'h', 'm', 'e', 'n']\n",
      "PRED 80: r a h m e n\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-02-03 01:44:03,037 INFO] PRED AVG SCORE: -0.0429, PRED PPL: 1.0438\n",
      "[2021-02-03 01:44:03,907 INFO] Translating shard 0.\n",
      "[2021-02-03 01:44:04,048 INFO] \n",
      "SENT 1: ['h', 'e', 'r', 'r', 's', 'c', 'h', 'a', 'f', 't']\n",
      "PRED 1: s c h a f t c h e r s c h e r\n",
      "PRED SCORE: -1.5215\n",
      "\n",
      "[2021-02-03 01:44:04,049 INFO] \n",
      "SENT 2: ['g', 'u', 'r', 't']\n",
      "PRED 2: r g g u r t u r\n",
      "PRED SCORE: -0.4970\n",
      "\n",
      "[2021-02-03 01:44:04,049 INFO] \n",
      "SENT 3: ['w', 'a', 'f', 'f', 'e', 'n', 's', 't', 'i', 'l', 'l', 's', 't', 'a', 'n', 'd']\n",
      "PRED 3: l a f f a n d e n s t a f d e n\n",
      "PRED SCORE: -2.6928\n",
      "\n",
      "[2021-02-03 01:44:04,049 INFO] \n",
      "SENT 4: ['e', 'h', 'e']\n",
      "PRED 4: h e h e n\n",
      "PRED SCORE: -0.0169\n",
      "\n",
      "[2021-02-03 01:44:04,050 INFO] \n",
      "SENT 5: ['a', 'r', 'b', 'e', 'i', 't', 'g', 'e', 'b', 'e', 'r']\n",
      "PRED 5: e b r a r b e r b e r b e r\n",
      "PRED SCORE: -0.5700\n",
      "\n",
      "[2021-02-03 01:44:04,050 INFO] \n",
      "SENT 6: ['s', 't', 'a', 'a', 't']\n",
      "PRED 6: a a t t a a t e\n",
      "PRED SCORE: -1.1534\n",
      "\n",
      "[2021-02-03 01:44:04,050 INFO] \n",
      "SENT 7: ['v', 'e', 'r', 's', 'u', 'c', 'h']\n",
      "PRED 7: s c h e r s u c h e r\n",
      "PRED SCORE: -0.5877\n",
      "\n",
      "[2021-02-03 01:44:04,051 INFO] \n",
      "SENT 8: ['s', 'c', 'h', 'l', 'i', 't', 't', 's', 'c', 'h', 'u', 'h']\n",
      "PRED 8: t u h c h l i t c h l c h l i t e\n",
      "PRED SCORE: -2.3256\n",
      "\n",
      "[2021-02-03 01:44:04,051 INFO] \n",
      "SENT 9: ['z', 'u', 's', 'a', 'm', 'm', 'e', 'n', 'z', 'i', 'e', 'h', 'u', 'n', 'g']\n",
      "PRED 9: m q z u s m u s m e n s a m m e n\n",
      "PRED SCORE: -3.6828\n",
      "\n",
      "[2021-02-03 01:44:04,051 INFO] \n",
      "SENT 10: ['g', 'e', 's', 'e', 'l', 'l', 's', 'c', 'h', 'a', 'f', 't']\n",
      "PRED 10: s c h a f t c h a f t c h a f t e\n",
      "PRED SCORE: -0.3916\n",
      "\n",
      "[2021-02-03 01:44:04,051 INFO] \n",
      "SENT 11: ['g', 'r', 'u', 'n', 'd', 'l', 'a', 'g', 'e']\n",
      "PRED 11: g g r a g d u g e r\n",
      "PRED SCORE: -0.8079\n",
      "\n",
      "[2021-02-03 01:44:04,052 INFO] \n",
      "SENT 12: ['p', 'r', 'e', 's', 's', 'e']\n",
      "PRED 12: s p r e s p e r\n",
      "PRED SCORE: -0.4354\n",
      "\n",
      "[2021-02-03 01:44:04,052 INFO] \n",
      "SENT 13: ['a', 'u', 's', 't', 'r', 'a', 'l', 'i', 'e', 'r']\n",
      "PRED 13: a u s t a u s t a u s t e\n",
      "PRED SCORE: -1.0838\n",
      "\n",
      "[2021-02-03 01:44:04,052 INFO] \n",
      "SENT 14: ['a', 'k', 't']\n",
      "PRED 14: k a k a k t a k t e\n",
      "PRED SCORE: -0.2369\n",
      "\n",
      "[2021-02-03 01:44:04,052 INFO] \n",
      "SENT 15: ['b', 'e', 'l', 'e', 'g']\n",
      "PRED 15: g b b e l e g e g\n",
      "PRED SCORE: -0.6466\n",
      "\n",
      "[2021-02-03 01:44:04,052 INFO] \n",
      "SENT 16: ['g', 'o', 'l', 'd', 'f', 'i', 's', 'c', 'h']\n",
      "PRED 16: s c h o l d c h i g c h i g e\n",
      "PRED SCORE: -1.7003\n",
      "\n",
      "[2021-02-03 01:44:04,053 INFO] \n",
      "SENT 17: ['a', 'b', 'g', 'a', 'b', 'e']\n",
      "PRED 17: b b a b g a b e n\n",
      "PRED SCORE: -0.4361\n",
      "\n",
      "[2021-02-03 01:44:04,053 INFO] \n",
      "SENT 18: ['p', 'r', 'e', 'm', 'i', 'e', 'r', 'm', 'i', 'n', 'i', 's', 't', 'e', 'r']\n",
      "PRED 18: m p r e s p r i n i n i n i n i n e r\n",
      "PRED SCORE: -1.8036\n",
      "\n",
      "[2021-02-03 01:44:04,053 INFO] \n",
      "SENT 19: ['p', 'r', 'e', 'i', 's']\n",
      "PRED 19: s p r e i s p e\n",
      "PRED SCORE: -1.2955\n",
      "\n",
      "[2021-02-03 01:44:04,053 INFO] \n",
      "SENT 20: ['g', 'e', 'g', 'e', 'n', 's', 't', 'a', 'n', 'd']\n",
      "PRED 20: g n a n d e r g a n d e r\n",
      "PRED SCORE: -2.5094\n",
      "\n",
      "[2021-02-03 01:44:04,054 INFO] \n",
      "SENT 21: ['r', 'a', 'h', 'm', 'e', 'n']\n",
      "PRED 21: h n a h m e n m a h m e\n",
      "PRED SCORE: -1.1292\n",
      "\n",
      "[2021-02-03 01:44:04,054 INFO] \n",
      "SENT 22: ['b', 'a', 'y', 'e', 'r']\n",
      "PRED 22: a b b a m e r m a m e r\n",
      "PRED SCORE: -1.3440\n",
      "\n",
      "[2021-02-03 01:44:04,054 INFO] \n",
      "SENT 23: ['a', 'n', 'e', 'r', 'k', 'e', 'n', 'n', 'u', 'n', 'g']\n",
      "PRED 23: n u n a n g a n g e r k e r\n",
      "PRED SCORE: -1.8711\n",
      "\n",
      "[2021-02-03 01:44:04,054 INFO] \n",
      "SENT 24: ['d', 'i', 'c', 'h', 't', 'e', 'r']\n",
      "PRED 24: d c h e d i c h t e r\n",
      "PRED SCORE: -2.4863\n",
      "\n",
      "[2021-02-03 01:44:04,055 INFO] \n",
      "SENT 25: ['p', 'r', 'o', 'd', 'u', 'k', 't']\n",
      "PRED 25: d p r o d r o d u d e r\n",
      "PRED SCORE: -0.5845\n",
      "\n",
      "[2021-02-03 01:44:04,055 INFO] \n",
      "SENT 26: ['i', 'n', 'v', 'e', 's', 't', 'i', 't', 'i', 'o', 'n']\n",
      "PRED 26: s t i o n i n i n e r i o n e\n",
      "PRED SCORE: -2.4582\n",
      "\n",
      "[2021-02-03 01:44:04,055 INFO] \n",
      "SENT 27: ['s', 'c', 'h', 'l', 'a', 'g', 'b', 'a', 'u', 'm']\n",
      "PRED 27: m c h l a u m c h l a u m e m\n",
      "PRED SCORE: -0.3476\n",
      "\n",
      "[2021-02-03 01:44:04,055 INFO] \n",
      "SENT 28: ['a', 'b', 't', 'e', 'i', 'l', 'u', 'n', 'g']\n",
      "PRED 28: n b a b t a b u b e i n e\n",
      "PRED SCORE: -2.1050\n",
      "\n",
      "[2021-02-03 01:44:04,055 INFO] \n",
      "SENT 29: ['f', 'r', 'e', 'u', 'n', 'd', 'i', 'n']\n",
      "PRED 29: u f f r e u n i n e r\n",
      "PRED SCORE: -1.4494\n",
      "\n",
      "[2021-02-03 01:44:04,056 INFO] \n",
      "SENT 30: ['m', 'i', 't', 'g', 'l', 'i', 'e', 'd']\n",
      "PRED 30: t i t r i e r i e m e\n",
      "PRED SCORE: -2.3999\n",
      "\n",
      "[2021-02-03 01:44:04,196 INFO] \n",
      "SENT 31: ['v', 'e', 'r', 'w', 'a', 'l', 't', 'u', 'n', 'g']\n",
      "PRED 31: w y l e r w a l t u n g e r\n",
      "PRED SCORE: -3.3277\n",
      "\n",
      "[2021-02-03 01:44:04,197 INFO] \n",
      "SENT 32: ['a', 'n', 'g', 'e', 'b', 'o', 't']\n",
      "PRED 32: t n a n g a n g e r\n",
      "PRED SCORE: -0.2898\n",
      "\n",
      "[2021-02-03 01:44:04,197 INFO] \n",
      "SENT 33: ['g', 'e', 'b', 'i', 's', 's']\n",
      "PRED 33: s b g e s s a s i e b e\n",
      "PRED SCORE: -1.0611\n",
      "\n",
      "[2021-02-03 01:44:04,197 INFO] \n",
      "SENT 34: ['v', 'e', 'r', 't', 'r', 'a', 'g']\n",
      "PRED 34: r y r e r t r a g e r\n",
      "PRED SCORE: -2.7836\n",
      "\n",
      "[2021-02-03 01:44:04,198 INFO] \n",
      "SENT 35: ['b', 'a', 'u', 's', 't', 'e', 'i', 'n']\n",
      "PRED 35: a b b a u s t e u s t e\n",
      "PRED SCORE: -1.3990\n",
      "\n",
      "[2021-02-03 01:44:04,198 INFO] \n",
      "SENT 36: ['j', 'e', 's', 'u', 'i', 't']\n",
      "PRED 36: t e i t u i t u i t e\n",
      "PRED SCORE: -0.2641\n",
      "\n",
      "[2021-02-03 01:44:04,198 INFO] \n",
      "SENT 37: ['o', 'r', 'g', 'a', 'n', 'i', 's', 'm', 'u', 's']\n",
      "PRED 37: a n i s m u s m u n i g e r\n",
      "PRED SCORE: -1.8049\n",
      "\n",
      "[2021-02-03 01:44:04,198 INFO] \n",
      "SENT 38: ['s', 'p', 'r', 'a', 'c', 'h', 'w', 'i', 's', 's', 'e', 'n', 's', 'c', 'h', 'a', 'f', 't']\n",
      "PRED 38: s p a f s c h a f s c h a f s c h a f t e n\n",
      "PRED SCORE: -3.9546\n",
      "\n",
      "[2021-02-03 01:44:04,198 INFO] \n",
      "SENT 39: ['a', 'l', 'p', 'e', 'n', 'v', 'e', 'i', 'l', 'c', 'h', 'e', 'n']\n",
      "PRED 39: e n a l p e n t a l p e n e n\n",
      "PRED SCORE: -3.6767\n",
      "\n",
      "[2021-02-03 01:44:04,199 INFO] \n",
      "SENT 40: ['m', 'u', 's', 't', 'e', 'r']\n",
      "PRED 40: m u s t u s t e r\n",
      "PRED SCORE: -0.5775\n",
      "\n",
      "[2021-02-03 01:44:04,199 INFO] \n",
      "SENT 41: ['p', 'f', 'a', 'n', 'd', 'b', 'r', 'i', 'e', 'f']\n",
      "PRED 41: r a n d a n d e r i e r i e r\n",
      "PRED SCORE: -1.9573\n",
      "\n",
      "[2021-02-03 01:44:04,199 INFO] \n",
      "SENT 42: ['h', 'o', 'c', 'h', 's', 'c', 'h', 'u', 'l', 'e']\n",
      "PRED 42: c h o c h o c h o c h o c h e n\n",
      "PRED SCORE: -0.6850\n",
      "\n",
      "[2021-02-03 01:44:04,199 INFO] \n",
      "SENT 43: ['z', 'i', 'e', 'l']\n",
      "PRED 43: e z i e l i e l e\n",
      "PRED SCORE: -0.1495\n",
      "\n",
      "[2021-02-03 01:44:04,199 INFO] \n",
      "SENT 44: ['s', 'o', 'z', 'i', 'a', 'l', 'i', 's', 'm', 'u', 's']\n",
      "PRED 44: a z s i a z i a z m u s m e\n",
      "PRED SCORE: -2.2325\n",
      "\n",
      "[2021-02-03 01:44:04,200 INFO] \n",
      "SENT 45: ['v', 'e', 'r', 'l', 'a', 'g']\n",
      "PRED 45: g g e r l a g e r\n",
      "PRED SCORE: -1.7696\n",
      "\n",
      "[2021-02-03 01:44:04,200 INFO] \n",
      "SENT 46: ['s', 'e', 'e', 'l', 'e']\n",
      "PRED 46: e l s e e e e r\n",
      "PRED SCORE: -0.7636\n",
      "\n",
      "[2021-02-03 01:44:04,200 INFO] \n",
      "SENT 47: ['b', 'e', 'z', 'i', 'r', 'k']\n",
      "PRED 47: m b b e b e r k e r\n",
      "PRED SCORE: -1.9792\n",
      "\n",
      "[2021-02-03 01:44:04,201 INFO] \n",
      "SENT 48: ['a', 'n', 't', 'r', 'a', 'g']\n",
      "PRED 48: a n t a n t a n t e r\n",
      "PRED SCORE: -1.3775\n",
      "\n",
      "[2021-02-03 01:44:04,201 INFO] \n",
      "SENT 49: ['s', 't', 'i', 'r', 'n']\n",
      "PRED 49: r s t i r n e r\n",
      "PRED SCORE: -1.0007\n",
      "\n",
      "[2021-02-03 01:44:04,201 INFO] \n",
      "SENT 50: ['s', 'p', 'a', 'n', 'n', 'u', 'n', 'g']\n",
      "PRED 50: n p s p a n g a n g e\n",
      "PRED SCORE: -1.3507\n",
      "\n",
      "[2021-02-03 01:44:04,201 INFO] \n",
      "SENT 51: ['h', 'a', 'b', 'e', 'n']\n",
      "PRED 51: b b h a b h a b e n\n",
      "PRED SCORE: -0.3066\n",
      "\n",
      "[2021-02-03 01:44:04,201 INFO] \n",
      "SENT 52: ['g', 'r', 'u', 'n', 'd', 'g', 'e', 's', 'e', 't', 'z']\n",
      "PRED 52: s b g r u s e\n",
      "PRED SCORE: -3.0177\n",
      "\n",
      "[2021-02-03 01:44:04,202 INFO] \n",
      "SENT 53: ['g', 'e', 'f', 'a', 'h', 'r']\n",
      "PRED 53: f a h r e a h r e\n",
      "PRED SCORE: -1.8352\n",
      "\n",
      "[2021-02-03 01:44:04,202 INFO] \n",
      "SENT 54: ['a', 'n', 'g', 'a', 'b', 'e']\n",
      "PRED 54: b b a b e a b e n\n",
      "PRED SCORE: -1.4761\n",
      "\n",
      "[2021-02-03 01:44:04,202 INFO] \n",
      "SENT 55: ['k', 'a', 't', 'a', 'l', 'o', 'g']\n",
      "PRED 55: a l k a l o g a l o g e n\n",
      "PRED SCORE: -1.2078\n",
      "\n",
      "[2021-02-03 01:44:04,202 INFO] \n",
      "SENT 56: ['p', 'l', 'a', 'n']\n",
      "PRED 56: a n p l a n e\n",
      "PRED SCORE: -0.1326\n",
      "\n",
      "[2021-02-03 01:44:04,203 INFO] \n",
      "SENT 57: ['p', 'f', 'a', 'r', 'r', 'e', 'r']\n",
      "PRED 57: r f a r r e r r e r\n",
      "PRED SCORE: -1.0496\n",
      "\n",
      "[2021-02-03 01:44:04,203 INFO] \n",
      "SENT 58: ['b', 'e', 'z', 'i', 'e', 'h', 'u', 'n', 'g']\n",
      "PRED 58: e b b u b e n i e b e n\n",
      "PRED SCORE: -2.4307\n",
      "\n",
      "[2021-02-03 01:44:04,203 INFO] \n",
      "SENT 59: ['b', 'e', 't', 'r', 'i', 'e', 'b']\n",
      "PRED 59: e b b r i e b e t e\n",
      "PRED SCORE: -0.8032\n",
      "\n",
      "[2021-02-03 01:44:04,203 INFO] \n",
      "SENT 60: ['e', 'i', 'g', 'e', 'n', 't', 'u', 'm']\n",
      "PRED 60: m i g u m e i g u m e\n",
      "PRED SCORE: -1.3375\n",
      "\n",
      "[2021-02-03 01:44:04,285 INFO] \n",
      "SENT 61: ['s', 'y', 'm', 'p', 't', 'o', 'm']\n",
      "PRED 61: m p o m p o m p o m e n\n",
      "PRED SCORE: -1.5103\n",
      "\n",
      "[2021-02-03 01:44:04,285 INFO] \n",
      "SENT 62: ['t', 'h', 'e', 'a', 't', 'e', 'r']\n",
      "PRED 62: t a t t a t e r n\n",
      "PRED SCORE: -1.6900\n",
      "\n",
      "[2021-02-03 01:44:04,286 INFO] \n",
      "SENT 63: ['p', 'r', 'o', 'z', 'e', 'n', 't']\n",
      "PRED 63: z p r o z o z o z e r n\n",
      "PRED SCORE: -0.6512\n",
      "\n",
      "[2021-02-03 01:44:04,286 INFO] \n",
      "SENT 64: ['a', 'u', 's', 'w', 'a', 'h', 'l']\n",
      "PRED 64: h u s w a h l a h l e\n",
      "PRED SCORE: -0.6113\n",
      "\n",
      "[2021-02-03 01:44:04,286 INFO] \n",
      "SENT 65: ['e', 'i', 'n', 'b', 'r', 'u', 'c', 'h']\n",
      "PRED 65: n i n b r u c h e r n\n",
      "PRED SCORE: -1.3104\n",
      "\n",
      "[2021-02-03 01:44:04,286 INFO] \n",
      "SENT 66: ['a', 'b', 's', 'a', 't', 'z']\n",
      "PRED 66: a b s a t z a t z e\n",
      "PRED SCORE: -0.8048\n",
      "\n",
      "[2021-02-03 01:44:04,287 INFO] \n",
      "SENT 67: ['v', 'o', 'r', 't', 'r', 'a', 'g']\n",
      "PRED 67: r o g r o g r o g r a g e r\n",
      "PRED SCORE: -3.4650\n",
      "\n",
      "[2021-02-03 01:44:04,287 INFO] \n",
      "SENT 68: ['k', 'a', 'm', 'p', 'f']\n",
      "PRED 68: p a m p a m p f e\n",
      "PRED SCORE: -0.4597\n",
      "\n",
      "[2021-02-03 01:44:04,287 INFO] \n",
      "SENT 69: ['v', 'e', 'r', 'b', 'r', 'e', 'c', 'h', 'e', 'n']\n",
      "PRED 69: r z e r b e r b r e c h e r\n",
      "PRED SCORE: -2.5347\n",
      "\n",
      "[2021-02-03 01:44:04,287 INFO] \n",
      "SENT 70: ['o', 'f', 'f', 'i', 'z', 'i', 'e', 'r']\n",
      "PRED 70: z o f f i e r i e r i e r\n",
      "PRED SCORE: -0.9134\n",
      "\n",
      "[2021-02-03 01:44:04,287 INFO] \n",
      "SENT 71: ['w', 'u', 'n', 's', 'c', 'h']\n",
      "PRED 71: n w u s c h c h e n\n",
      "PRED SCORE: -1.0789\n",
      "\n",
      "[2021-02-03 01:44:04,288 INFO] \n",
      "SENT 72: ['b', 'o', 'd', 'e', 'n']\n",
      "PRED 72: d b b o b o d e n\n",
      "PRED SCORE: -0.5116\n",
      "\n",
      "[2021-02-03 01:44:04,288 INFO] \n",
      "SENT 73: ['a', 'n', 't', 'e', 'i', 'l']\n",
      "PRED 73: i n t a n t e n t e n\n",
      "PRED SCORE: -1.5235\n",
      "\n",
      "[2021-02-03 01:44:04,288 INFO] \n",
      "SENT 74: ['t', 'r', 'u', 'p', 'p', 'e']\n",
      "PRED 74: p r u p p u p p e\n",
      "PRED SCORE: -0.7106\n",
      "\n",
      "[2021-02-03 01:44:04,288 INFO] \n",
      "SENT 75: ['b', 'e', 'w', 'e', 'i', 's']\n",
      "PRED 75: e b b e b e w e i e r\n",
      "PRED SCORE: -1.5982\n",
      "\n",
      "[2021-02-03 01:44:04,288 INFO] \n",
      "SENT 76: ['s', 'p', 'i', 'e', 'l', 'f', 'e', 'l', 'd']\n",
      "PRED 76: l p i e l e l d i e l e\n",
      "PRED SCORE: -2.8186\n",
      "\n",
      "[2021-02-03 01:44:04,289 INFO] \n",
      "SENT 77: ['r', 'a', 't', 'e']\n",
      "PRED 77: t r a t e r\n",
      "PRED SCORE: -1.0921\n",
      "\n",
      "[2021-02-03 01:44:04,289 INFO] \n",
      "SENT 78: ['r', 'u', 'f']\n",
      "PRED 78: f f r u f r e\n",
      "PRED SCORE: -0.8508\n",
      "\n",
      "[2021-02-03 01:44:04,289 INFO] \n",
      "SENT 79: ['m', 'i', 'n', 'i', 's', 't', 'e', 'r']\n",
      "PRED 79: m i s t i n i n i n e r\n",
      "PRED SCORE: -1.2931\n",
      "\n",
      "[2021-02-03 01:44:04,289 INFO] \n",
      "SENT 80: ['f', 'r', 'e', 'i', 'h', 'e', 'i', 't']\n",
      "PRED 80: t f r e i t r e i t e\n",
      "PRED SCORE: -0.2269\n",
      "\n",
      "[2021-02-03 01:44:04,289 INFO] PRED AVG SCORE: -0.1220, PRED PPL: 1.1297\n",
      "[2021-02-03 01:44:05,203 INFO] Translating shard 0.\n",
      "[2021-02-03 01:44:05,304 INFO] \n",
      "SENT 1: ['NTR', '<s>', 't', 'h', 'e', 'a', 't', 'e', 'r']\n",
      "PRED 1: t h e a t e r\n",
      "PRED SCORE: -0.0036\n",
      "\n",
      "[2021-02-03 01:44:05,304 INFO] \n",
      "SENT 2: ['MAS', '<s>', 'j', 'o', 'u', 'r', 'n', 'a', 'l', 'i', 's', 't']\n",
      "PRED 2: j o u r n a l n t e\n",
      "PRED SCORE: -0.6117\n",
      "\n",
      "[2021-02-03 01:44:05,305 INFO] \n",
      "SENT 3: ['FEM', '<s>', 'l', 'a', 'g', 'e']\n",
      "PRED 3: l a g e n\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-02-03 01:44:05,305 INFO] \n",
      "SENT 4: ['FEM', '<s>', 'e', 'm', 'p', 'f', 'e', 'h', 'l', 'u', 'n', 'g']\n",
      "PRED 4: e m p f l u n g e n\n",
      "PRED SCORE: -0.6122\n",
      "\n",
      "[2021-02-03 01:44:05,305 INFO] \n",
      "SENT 5: ['MAS', '<s>', 'b', 'a', 'y', 'e', 'r']\n",
      "PRED 5: b a a e r\n",
      "PRED SCORE: -0.0278\n",
      "\n",
      "[2021-02-03 01:44:05,305 INFO] \n",
      "SENT 6: ['MAS', '<s>', 'j', 'u', 'w', 'e', 'l', 'i', 'e', 'r']\n",
      "PRED 6: j u w e l i e r e\n",
      "PRED SCORE: -0.4614\n",
      "\n",
      "[2021-02-03 01:44:05,305 INFO] \n",
      "SENT 7: ['MAS', '<s>', 'b', 'r', 'u', 'd', 'e', 'r']\n",
      "PRED 7: b r u d e r\n",
      "PRED SCORE: -0.0036\n",
      "\n",
      "[2021-02-03 01:44:05,306 INFO] \n",
      "SENT 8: ['MAS', '<s>', 'g', 'e', 'b', 'r', 'a', 'u', 'c', 'h']\n",
      "PRED 8: g e b r a u c h e\n",
      "PRED SCORE: -0.0551\n",
      "\n",
      "[2021-02-03 01:44:05,306 INFO] \n",
      "SENT 9: ['FEM', '<s>', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n']\n",
      "PRED 9: r e d o l u n e n\n",
      "PRED SCORE: -1.4102\n",
      "\n",
      "[2021-02-03 01:44:05,306 INFO] \n",
      "SENT 10: ['FEM', '<s>', 'b', 'o', 'h', 'r', 'u', 'n', 'g']\n",
      "PRED 10: b o h r u n g e n\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-02-03 01:44:05,306 INFO] \n",
      "SENT 11: ['FEM', '<s>', 'k', 'l', 'a', 's', 's', 'e']\n",
      "PRED 11: k l a s s e n\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-02-03 01:44:05,306 INFO] \n",
      "SENT 12: ['FEM', '<s>', 'p', 'o', 'l', 'i', 't', 'i', 'k']\n",
      "PRED 12: p o l i k e n\n",
      "PRED SCORE: -0.2191\n",
      "\n",
      "[2021-02-03 01:44:05,307 INFO] \n",
      "SENT 13: ['FEM', '<s>', 'r', 'e', 'p', 'u', 'b', 'l', 'i', 'k']\n",
      "PRED 13: r e p u k i k e n\n",
      "PRED SCORE: -0.0115\n",
      "\n",
      "[2021-02-03 01:44:05,307 INFO] \n",
      "SENT 14: ['FEM', '<s>', 'f', 'e', 's', 't', 'u', 'n', 'g']\n",
      "PRED 14: f e s t u n g e n\n",
      "PRED SCORE: -0.0023\n",
      "\n",
      "[2021-02-03 01:44:05,307 INFO] \n",
      "SENT 15: ['NTR', '<s>', 'j', 'o', 'c', 'h']\n",
      "PRED 15: j o c h e r\n",
      "PRED SCORE: -0.1914\n",
      "\n",
      "[2021-02-03 01:44:05,307 INFO] \n",
      "SENT 16: ['FEM', '<s>', 'm', 'e', 'i', 'n', 'u', 'n', 'g']\n",
      "PRED 16: m e i n u n g e n\n",
      "PRED SCORE: -0.0160\n",
      "\n",
      "[2021-02-03 01:44:05,308 INFO] \n",
      "SENT 17: ['NTR', '<s>', 'o', 'p', 'f', 'e', 'r']\n",
      "PRED 17: o p f e r\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-02-03 01:44:05,308 INFO] \n",
      "SENT 18: ['FEM', '<s>', 'r', 'e', 'g', 'i', 'e', 'r', 'u', 'n', 'g']\n",
      "PRED 18: r e g i e r u n g e n\n",
      "PRED SCORE: -0.0106\n",
      "\n",
      "[2021-02-03 01:44:05,308 INFO] \n",
      "SENT 19: ['FEM', '<s>', 's', 'e', 'n', 'd', 'u', 'n', 'g']\n",
      "PRED 19: s e n d u n g e n\n",
      "PRED SCORE: -0.0025\n",
      "\n",
      "[2021-02-03 01:44:05,308 INFO] \n",
      "SENT 20: ['FEM', '<s>', 'p', 'o', 'l', 'i', 'z', 'e', 'i']\n",
      "PRED 20: p o l i z e i e n\n",
      "PRED SCORE: -0.3067\n",
      "\n",
      "[2021-02-03 01:44:05,308 INFO] \n",
      "SENT 21: ['MAS', '<s>', 'v', 'e', 'r', 'l', 'a', 'g']\n",
      "PRED 21: v e r l a g e\n",
      "PRED SCORE: -0.0194\n",
      "\n",
      "[2021-02-03 01:44:05,309 INFO] \n",
      "SENT 22: ['MAS', '<s>', 'j', 'u', 'n', 'i', 'o', 'r']\n",
      "PRED 22: v u n i o r e n\n",
      "PRED SCORE: -0.6758\n",
      "\n",
      "[2021-02-03 01:44:05,309 INFO] \n",
      "SENT 23: ['MAS', '<s>', 'h', 'a', 'n', 'd', 'e', 'l']\n",
      "PRED 23: h a n d e l\n",
      "PRED SCORE: -0.0140\n",
      "\n",
      "[2021-02-03 01:44:05,309 INFO] \n",
      "SENT 24: ['FEM', '<s>', 'h', 'o', 'c', 'h', 's', 'c', 'h', 'u', 'l', 'e']\n",
      "PRED 24: h o c h s c h u l e n\n",
      "PRED SCORE: -0.0031\n",
      "\n",
      "[2021-02-03 01:44:05,309 INFO] \n",
      "SENT 25: ['NTR', '<s>', 'm', 'i', 't', 'g', 'l', 'i', 'e', 'd']\n",
      "PRED 25: m i t g i e l e\n",
      "PRED SCORE: -0.5349\n",
      "\n",
      "[2021-02-03 01:44:05,309 INFO] \n",
      "SENT 26: ['MAS', '<s>', 'm', 'a', 'r', 'k', 't', 'a', 'n', 't', 'e', 'i', 'l']\n",
      "PRED 26: m a r k a n t e i l e n\n",
      "PRED SCORE: -0.5281\n",
      "\n",
      "[2021-02-03 01:44:05,310 INFO] \n",
      "SENT 27: ['FEM', '<s>', 'w', 'i', 'r', 'k', 'l', 'i', 'c', 'h', 'k', 'e', 'i', 't']\n",
      "PRED 27: w i r k i c h k e i t e n\n",
      "PRED SCORE: -0.0037\n",
      "\n",
      "[2021-02-03 01:44:05,310 INFO] \n",
      "SENT 28: ['MAS', '<s>', 'v', 'o', 'r', 't', 'e', 'i', 'l']\n",
      "PRED 28: v o r t e i l e n\n",
      "PRED SCORE: -0.2446\n",
      "\n",
      "[2021-02-03 01:44:05,310 INFO] \n",
      "SENT 29: ['FEM', '<s>', 't', 'e', 'n', 'd', 'e', 'n', 'z']\n",
      "PRED 29: t e n d e n z e n\n",
      "PRED SCORE: -0.0272\n",
      "\n",
      "[2021-02-03 01:44:05,310 INFO] \n",
      "SENT 30: ['NTR', '<s>', 'p', 'a', 'p', 'i', 'e', 'r']\n",
      "PRED 30: p a p i e r e\n",
      "PRED SCORE: -0.0141\n",
      "\n",
      "[2021-02-03 01:44:05,569 INFO] \n",
      "SENT 31: ['MAS', '<s>', 'k', 'o', 'n', 'g', 'r', 'e', 's', 's']\n",
      "PRED 31: k o n g e s s s e\n",
      "PRED SCORE: -0.0040\n",
      "\n",
      "[2021-02-03 01:44:05,569 INFO] \n",
      "SENT 32: ['NTR', '<s>', 'g', 'e', 's', 'e', 't', 'z']\n",
      "PRED 32: g e s e t z e\n",
      "PRED SCORE: -0.0657\n",
      "\n",
      "[2021-02-03 01:44:05,569 INFO] \n",
      "SENT 33: ['MAS', '<s>', 's', 't', 'r', 'a', 'n', 'd']\n",
      "PRED 33: s t r a n d e\n",
      "PRED SCORE: -0.0015\n",
      "\n",
      "[2021-02-03 01:44:05,570 INFO] \n",
      "SENT 34: ['FEM', '<s>', 'v', 'e', 'r', 'p', 'f', 'l', 'i', 'c', 'h', 't', 'u', 'n', 'g']\n",
      "PRED 34: v e r p f i c h t u n g e n\n",
      "PRED SCORE: -0.7411\n",
      "\n",
      "[2021-02-03 01:44:05,570 INFO] \n",
      "SENT 35: ['MAS', '<s>', 'm', 'i', 'n', 'i', 's', 't', 'e', 'r']\n",
      "PRED 35: m i n i s t e r\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-02-03 01:44:05,570 INFO] \n",
      "SENT 36: ['MAS', '<s>', 'k', 'a', 'i', 's', 'e', 'r']\n",
      "PRED 36: k a i s e r\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-02-03 01:44:05,570 INFO] \n",
      "SENT 37: ['FEM', '<s>', 's', 'i', 'c', 'h', 'e', 'r', 'h', 'e', 'i', 't']\n",
      "PRED 37: s i c h e r h e i t e n\n",
      "PRED SCORE: -0.0092\n",
      "\n",
      "[2021-02-03 01:44:05,571 INFO] \n",
      "SENT 38: ['MAS', '<s>', 'm', 'a', 'g', 'e', 'n']\n",
      "PRED 38: m a g e n\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-02-03 01:44:05,571 INFO] \n",
      "SENT 39: ['MAS', '<s>', 's', 'o', 'z', 'i', 'a', 'l', 'i', 's', 'm', 'u', 's']\n",
      "PRED 39: s o z i a l i s m e\n",
      "PRED SCORE: -0.5754\n",
      "\n",
      "[2021-02-03 01:44:05,571 INFO] \n",
      "SENT 40: ['MAS', '<s>', 'v', 'o', 'r', 's', 'c', 'h', 'l', 'a', 'g']\n",
      "PRED 40: v o r s c h l a g l e\n",
      "PRED SCORE: -0.0657\n",
      "\n",
      "[2021-02-03 01:44:05,571 INFO] \n",
      "SENT 41: ['FEM', '<s>', 'r', 'e', 'i', 'h', 'e', 'n', 'f', 'o', 'l', 'g', 'e']\n",
      "PRED 41: r e i h e n f o l e n\n",
      "PRED SCORE: -0.6242\n",
      "\n",
      "[2021-02-03 01:44:05,571 INFO] \n",
      "SENT 42: ['FEM', '<s>', 'r', 'e', 't', 't', 'u', 'n', 'g']\n",
      "PRED 42: r e t t u n g e n\n",
      "PRED SCORE: -0.0019\n",
      "\n",
      "[2021-02-03 01:44:05,572 INFO] \n",
      "SENT 43: ['FEM', '<s>', 'g', 'r', 'u', 'b', 'e']\n",
      "PRED 43: g r u b e n\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-02-03 01:44:05,572 INFO] \n",
      "SENT 44: ['NTR', '<s>', 'a', 'n', 'g', 'e', 'b', 'o', 't']\n",
      "PRED 44: a n g e b e r\n",
      "PRED SCORE: -0.9693\n",
      "\n",
      "[2021-02-03 01:44:05,572 INFO] \n",
      "SENT 45: ['MAS', '<s>', 'r', 'i', 's', 's']\n",
      "PRED 45: r i s s e\n",
      "PRED SCORE: -0.0027\n",
      "\n",
      "[2021-02-03 01:44:05,572 INFO] \n",
      "SENT 46: ['MAS', '<s>', 'r', 'o', 'h', 's', 't', 'o', 'f', 'f']\n",
      "PRED 46: r o h s t o f e\n",
      "PRED SCORE: -0.0200\n",
      "\n",
      "[2021-02-03 01:44:05,572 INFO] \n",
      "SENT 47: ['MAS', '<s>', 'a', 'n', 't', 'e', 'i', 'l']\n",
      "PRED 47: a n t e i l e n\n",
      "PRED SCORE: -0.0290\n",
      "\n",
      "[2021-02-03 01:44:05,573 INFO] \n",
      "SENT 48: ['FEM', '<s>', 's', 'y', 'm', 'm', 'e', 't', 'r', 'i', 'e']\n",
      "PRED 48: s y m m e t i e n\n",
      "PRED SCORE: -0.9677\n",
      "\n",
      "[2021-02-03 01:44:05,573 INFO] \n",
      "SENT 49: ['FEM', '<s>', 'b', 'e', 'd', 'e', 'u', 't', 'u', 'n', 'g']\n",
      "PRED 49: b e d e u t n u n g e n\n",
      "PRED SCORE: -0.8437\n",
      "\n",
      "[2021-02-03 01:44:05,573 INFO] \n",
      "SENT 50: ['FEM', '<s>', 'u', 'n', 'w', 'a', 'h', 'r', 'h', 'e', 'i', 't']\n",
      "PRED 50: u n w a h r h e i t e n\n",
      "PRED SCORE: -0.0681\n",
      "\n",
      "[2021-02-03 01:44:05,573 INFO] \n",
      "SENT 51: ['FEM', '<s>', 't', 'e', 'm', 'p', 'e', 'r', 'a', 't', 'u', 'r']\n",
      "PRED 51: t e m p e r a t u r e n\n",
      "PRED SCORE: -1.0753\n",
      "\n",
      "[2021-02-03 01:44:05,574 INFO] \n",
      "SENT 52: ['MAS', '<s>', 'z', 'e', 'n', 's', 'u', 's']\n",
      "PRED 52: z e n s u s e\n",
      "PRED SCORE: -0.0472\n",
      "\n",
      "[2021-02-03 01:44:05,574 INFO] \n",
      "SENT 53: ['MAS', '<s>', 'i', 'n', 'h', 'a', 'l', 't']\n",
      "PRED 53: i n h a l t e n\n",
      "PRED SCORE: -0.0231\n",
      "\n",
      "[2021-02-03 01:44:05,574 INFO] \n",
      "SENT 54: ['NTR', '<s>', 'g', 'l', 'e', 'i', 'c', 'h', 'n', 'i', 's']\n",
      "PRED 54: g l e i c h n i s e\n",
      "PRED SCORE: -0.0014\n",
      "\n",
      "[2021-02-03 01:44:05,574 INFO] \n",
      "SENT 55: ['NTR', '<s>', 'p', 'r', 'o', 'z', 'e', 'n', 't']\n",
      "PRED 55: p r o z e n t e\n",
      "PRED SCORE: -0.0612\n",
      "\n",
      "[2021-02-03 01:44:05,574 INFO] \n",
      "SENT 56: ['MAS', '<s>', 'b', 'e', 's', 'c', 'h', 'e', 'i', 'd']\n",
      "PRED 56: b e s c h e i d e i d e i d e n\n",
      "PRED SCORE: -2.7080\n",
      "\n",
      "[2021-02-03 01:44:05,575 INFO] \n",
      "SENT 57: ['FEM', '<s>', 'e', 'r', 'z', 'e', 'u', 'g', 'u', 'n', 'g']\n",
      "PRED 57: e r z e u n g e n\n",
      "PRED SCORE: -0.1395\n",
      "\n",
      "[2021-02-03 01:44:05,575 INFO] \n",
      "SENT 58: ['MAS', '<s>', 'a', 'u', 'f', 's', 'e', 'h', 'e', 'r']\n",
      "PRED 58: a u f s e h e r\n",
      "PRED SCORE: -0.2372\n",
      "\n",
      "[2021-02-03 01:44:05,575 INFO] \n",
      "SENT 59: ['FEM', '<s>', 'k', 'o', 'n', 'f', 'e', 'r', 'e', 'n', 'z']\n",
      "PRED 59: k o n f e r e n\n",
      "PRED SCORE: -0.0044\n",
      "\n",
      "[2021-02-03 01:44:05,575 INFO] \n",
      "SENT 60: ['MAS', '<s>', 'w', 'e', 'r', 't']\n",
      "PRED 60: w e r t e n\n",
      "PRED SCORE: -0.3216\n",
      "\n",
      "[2021-02-03 01:44:05,656 INFO] \n",
      "SENT 61: ['MAS', '<s>', 'm', 'i', 't', 't', 'w', 'o', 'c', 'h']\n",
      "PRED 61: m i t w o c h e\n",
      "PRED SCORE: -0.0820\n",
      "\n",
      "[2021-02-03 01:44:05,656 INFO] \n",
      "SENT 62: ['MAS', '<s>', 'p', 'l', 'a', 'n']\n",
      "PRED 62: p l a n e\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-02-03 01:44:05,656 INFO] \n",
      "SENT 63: ['FEM', '<s>', 'e', 'i', 'n', 'z', 'e', 'l', 'h', 'e', 'i', 't']\n",
      "PRED 63: e i n z e i t e l h e i t e n\n",
      "PRED SCORE: -0.1822\n",
      "\n",
      "[2021-02-03 01:44:05,656 INFO] \n",
      "SENT 64: ['MAS', '<s>', 'e', 'm', 'p', 'f', 'a', 'n', 'g']\n",
      "PRED 64: e m p f a n g e\n",
      "PRED SCORE: -0.0021\n",
      "\n",
      "[2021-02-03 01:44:05,656 INFO] \n",
      "SENT 65: ['MAS', '<s>', 'h', 'o', 'r', 'i', 'z', 'o', 'n', 't']\n",
      "PRED 65: h o r i z o n t e n\n",
      "PRED SCORE: -0.7852\n",
      "\n",
      "[2021-02-03 01:44:05,657 INFO] \n",
      "SENT 66: ['MAS', '<s>', 'f', 'e', 'r', 'n', 's', 'e', 'h', 'a', 'p', 'p', 'a', 'r', 'a', 't']\n",
      "PRED 66: f e r n s a t p h a t p h e n\n",
      "PRED SCORE: -1.0948\n",
      "\n",
      "[2021-02-03 01:44:05,657 INFO] \n",
      "SENT 67: ['MAS', '<s>', 'u', 'n', 't', 'e', 'r', 'g', 'a', 'n', 'g']\n",
      "PRED 67: u n t e r g a n g e\n",
      "PRED SCORE: -0.2420\n",
      "\n",
      "[2021-02-03 01:44:05,657 INFO] \n",
      "SENT 68: ['MAS', '<s>', 'o', 'p', 't', 'i', 'm', 'i', 's', 't']\n",
      "PRED 68: o p t i s t e n\n",
      "PRED SCORE: -0.1290\n",
      "\n",
      "[2021-02-03 01:44:05,657 INFO] \n",
      "SENT 69: ['FEM', '<s>', 'k', 'o', 'n', 'z', 'e', 'n', 't', 'r', 'a', 't', 'i', 'o', 'n']\n",
      "PRED 69: k o n z e n t i o n e n\n",
      "PRED SCORE: -0.1491\n",
      "\n",
      "[2021-02-03 01:44:05,658 INFO] \n",
      "SENT 70: ['MAS', '<s>', 'v', 'e', 'r', 'k', 'a', 'u', 'f']\n",
      "PRED 70: v e r k a u f e\n",
      "PRED SCORE: -0.0035\n",
      "\n",
      "[2021-02-03 01:44:05,658 INFO] \n",
      "SENT 71: ['MAS', '<s>', 'd', 'u', 'r', 'c', 'h', 'g', 'a', 'n', 'g']\n",
      "PRED 71: d u r c h u n g e n\n",
      "PRED SCORE: -0.3678\n",
      "\n",
      "[2021-02-03 01:44:05,658 INFO] \n",
      "SENT 72: ['MAS', '<s>', 'k', 'u', 'r', 's']\n",
      "PRED 72: k u r s e\n",
      "PRED SCORE: -0.0020\n",
      "\n",
      "[2021-02-03 01:44:05,658 INFO] \n",
      "SENT 73: ['MAS', '<s>', 'r', 'e', 't', 't', 'i', 'c', 'h']\n",
      "PRED 73: r e t t i c h e\n",
      "PRED SCORE: -0.1156\n",
      "\n",
      "[2021-02-03 01:44:05,658 INFO] \n",
      "SENT 74: ['FEM', '<s>', 'k', 'r', 'a', 'n', 'k', 'h', 'e', 'i', 't']\n",
      "PRED 74: k r a n k e i t e n\n",
      "PRED SCORE: -0.0037\n",
      "\n",
      "[2021-02-03 01:44:05,659 INFO] \n",
      "SENT 75: ['MAS', '<s>', 's', 'c', 'h', 'l', 'u', 's', 's']\n",
      "PRED 75: s c h l u s s e\n",
      "PRED SCORE: -0.0011\n",
      "\n",
      "[2021-02-03 01:44:05,659 INFO] \n",
      "SENT 76: ['MAS', '<s>', 'r', 'a', 'b', 'a', 't', 't']\n",
      "PRED 76: r a b a t t e n\n",
      "PRED SCORE: -0.0255\n",
      "\n",
      "[2021-02-03 01:44:05,659 INFO] \n",
      "SENT 77: ['FEM', '<s>', 'r', 'e', 'd', 'e']\n",
      "PRED 77: r e d e n\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-02-03 01:44:05,659 INFO] \n",
      "SENT 78: ['NTR', '<s>', 'v', 'o', 'r', 's', 'p', 'i', 'e', 'l']\n",
      "PRED 78: v o r s p i e l e\n",
      "PRED SCORE: -0.0114\n",
      "\n",
      "[2021-02-03 01:44:05,659 INFO] \n",
      "SENT 79: ['NTR', '<s>', 'm', 'i', 'n', 'i', 's', 't', 'e', 'r', 'i', 'u', 'm']\n",
      "PRED 79: m i n i s t e r i u m e\n",
      "PRED SCORE: -0.2125\n",
      "\n",
      "[2021-02-03 01:44:05,660 INFO] \n",
      "SENT 80: ['MAS', '<s>', 'v', 'e', 'r', 'l', 'u', 's', 't']\n",
      "PRED 80: v e r l u s t e\n",
      "PRED SCORE: -0.0030\n",
      "\n",
      "[2021-02-03 01:44:05,660 INFO] PRED AVG SCORE: -0.0272, PRED PPL: 1.0276\n",
      "[2021-02-03 01:44:06,484 INFO] Translating shard 0.\n",
      "[2021-02-03 01:44:06,605 INFO] \n",
      "SENT 1: ['s', 'o', 'z', 'i', 'a', 'l', 'i', 's', 'm', 'u', 's']\n",
      "PRED 1: s m a l i s m a l i s m a l i s m e\n",
      "PRED SCORE: -1.2430\n",
      "\n",
      "[2021-02-03 01:44:06,605 INFO] \n",
      "SENT 2: ['k', 'o', 'n', 'g', 'r', 'e', 's', 's']\n",
      "PRED 2: s s o s s s s e s s e\n",
      "PRED SCORE: -1.0174\n",
      "\n",
      "[2021-02-03 01:44:06,605 INFO] \n",
      "SENT 3: ['r', 'e', 't', 't', 'u', 'n', 'g']\n",
      "PRED 3: t t r e t u n g e r\n",
      "PRED SCORE: -1.1673\n",
      "\n",
      "[2021-02-03 01:44:06,605 INFO] \n",
      "SENT 4: ['m', 'i', 'n', 'i', 's', 't', 'e', 'r', 'i', 'u', 'm']\n",
      "PRED 4: s t i n i n u s t e\n",
      "PRED SCORE: -1.7634\n",
      "\n",
      "[2021-02-03 01:44:06,605 INFO] \n",
      "SENT 5: ['s', 'i', 'c', 'h', 'e', 'r', 'h', 'e', 'i', 't']\n",
      "PRED 5: s e r s i c h e r h i c h e r\n",
      "PRED SCORE: -1.8783\n",
      "\n",
      "[2021-02-03 01:44:06,606 INFO] \n",
      "SENT 6: ['k', 'u', 'r', 's']\n",
      "PRED 6: s u r k u r s k u r s k e\n",
      "PRED SCORE: -1.4139\n",
      "\n",
      "[2021-02-03 01:44:06,606 INFO] \n",
      "SENT 7: ['t', 'h', 'e', 'a', 't', 'e', 'r']\n",
      "PRED 7: t e a t h e r t e\n",
      "PRED SCORE: -1.3157\n",
      "\n",
      "[2021-02-03 01:44:06,606 INFO] \n",
      "SENT 8: ['e', 'r', 'z', 'e', 'u', 'g', 'u', 'n', 'g']\n",
      "PRED 8: u r z e u n g e u n g e n\n",
      "PRED SCORE: -0.5334\n",
      "\n",
      "[2021-02-03 01:44:06,606 INFO] \n",
      "SENT 9: ['o', 'p', 't', 'i', 'm', 'i', 's', 't']\n",
      "PRED 9: s t o p t i s t i s t e n\n",
      "PRED SCORE: -0.6014\n",
      "\n",
      "[2021-02-03 01:44:06,606 INFO] \n",
      "SENT 10: ['p', 'o', 'l', 'i', 't', 'i', 'k']\n",
      "PRED 10: k l i k o k i k e i t e l\n",
      "PRED SCORE: -0.7554\n",
      "\n",
      "[2021-02-03 01:44:06,606 INFO] \n",
      "SENT 11: ['r', 'i', 's', 's']\n",
      "PRED 11: s s r i s s s i s s e\n",
      "PRED SCORE: -0.4372\n",
      "\n",
      "[2021-02-03 01:44:06,606 INFO] \n",
      "SENT 12: ['g', 'e', 'b', 'r', 'a', 'u', 'c', 'h']\n",
      "PRED 12: u u c h r a u c h e r\n",
      "PRED SCORE: -1.4919\n",
      "\n",
      "[2021-02-03 01:44:06,606 INFO] \n",
      "SENT 13: ['g', 'r', 'u', 'b', 'e']\n",
      "PRED 13: b g r u b e n\n",
      "PRED SCORE: -0.0797\n",
      "\n",
      "[2021-02-03 01:44:06,606 INFO] \n",
      "SENT 14: ['k', 'o', 'n', 'z', 'e', 'n', 't', 'r', 'a', 't', 'i', 'o', 'n']\n",
      "PRED 14: r a t i o n z e n t e r\n",
      "PRED SCORE: -0.9809\n",
      "\n",
      "[2021-02-03 01:44:06,607 INFO] \n",
      "SENT 15: ['a', 'u', 'f', 's', 'e', 'h', 'e', 'r']\n",
      "PRED 15: h a u f e r f s e h e r\n",
      "PRED SCORE: -1.1456\n",
      "\n",
      "[2021-02-03 01:44:06,607 INFO] \n",
      "SENT 16: ['u', 'n', 't', 'e', 'r', 'g', 'a', 'n', 'g']\n",
      "PRED 16: r u n g e r g a n g e r\n",
      "PRED SCORE: -0.5142\n",
      "\n",
      "[2021-02-03 01:44:06,607 INFO] \n",
      "SENT 17: ['p', 'r', 'o', 'z', 'e', 'n', 't']\n",
      "PRED 17: p p r o z e n t e\n",
      "PRED SCORE: -1.4213\n",
      "\n",
      "[2021-02-03 01:44:06,607 INFO] \n",
      "SENT 18: ['b', 'e', 'd', 'e', 'u', 't', 'u', 'n', 'g']\n",
      "PRED 18: b u b e u n g e u n g e\n",
      "PRED SCORE: -0.6079\n",
      "\n",
      "[2021-02-03 01:44:06,607 INFO] \n",
      "SENT 19: ['i', 'n', 'h', 'a', 'l', 't']\n",
      "PRED 19: h a l i n h a l t h t h\n",
      "PRED SCORE: -2.4483\n",
      "\n",
      "[2021-02-03 01:44:06,607 INFO] \n",
      "SENT 20: ['j', 'o', 'c', 'h']\n",
      "PRED 20: o c h o c h o h h e\n",
      "PRED SCORE: -0.7073\n",
      "\n",
      "[2021-02-03 01:44:06,607 INFO] \n",
      "SENT 21: ['o', 'p', 'f', 'e', 'r']\n",
      "PRED 21: p o p f e r p f e r\n",
      "PRED SCORE: -0.0415\n",
      "\n",
      "[2021-02-03 01:44:06,607 INFO] \n",
      "SENT 22: ['s', 'c', 'h', 'l', 'u', 's', 's']\n",
      "PRED 22: s l u s s c h l u s s e\n",
      "PRED SCORE: -0.1264\n",
      "\n",
      "[2021-02-03 01:44:06,608 INFO] \n",
      "SENT 23: ['h', 'o', 'c', 'h', 's', 'c', 'h', 'u', 'l', 'e']\n",
      "PRED 23: h c h u c h s u c h s e\n",
      "PRED SCORE: -1.9833\n",
      "\n",
      "[2021-02-03 01:44:06,608 INFO] \n",
      "SENT 24: ['m', 'i', 't', 'g', 'l', 'i', 'e', 'd']\n",
      "PRED 24: k l i e t i e t i e n\n",
      "PRED SCORE: -3.1925\n",
      "\n",
      "[2021-02-03 01:44:06,608 INFO] \n",
      "SENT 25: ['r', 'e', 'g', 'i', 'e', 'r', 'u', 'n', 'g']\n",
      "PRED 25: r e g i e r u n g e r\n",
      "PRED SCORE: -2.0470\n",
      "\n",
      "[2021-02-03 01:44:06,608 INFO] \n",
      "SENT 26: ['r', 'e', 'd', 'e']\n",
      "PRED 26: d e d r e d e r\n",
      "PRED SCORE: -0.6056\n",
      "\n",
      "[2021-02-03 01:44:06,608 INFO] \n",
      "SENT 27: ['k', 'l', 'a', 's', 's', 'e']\n",
      "PRED 27: s e s s a s s e\n",
      "PRED SCORE: -1.5370\n",
      "\n",
      "[2021-02-03 01:44:06,608 INFO] \n",
      "SENT 28: ['j', 'u', 'w', 'e', 'l', 'i', 'e', 'r']\n",
      "PRED 28: e l i e l i e i e r e\n",
      "PRED SCORE: -2.0052\n",
      "\n",
      "[2021-02-03 01:44:06,608 INFO] \n",
      "SENT 29: ['w', 'e', 'r', 't']\n",
      "PRED 29: r e r w e r t e r\n",
      "PRED SCORE: -0.5248\n",
      "\n",
      "[2021-02-03 01:44:06,608 INFO] \n",
      "SENT 30: ['w', 'i', 'r', 'k', 'l', 'i', 'c', 'h', 'k', 'e', 'i', 't']\n",
      "PRED 30: k w i r k i c h k e i t e n\n",
      "PRED SCORE: -0.4188\n",
      "\n",
      "[2021-02-03 01:44:06,730 INFO] \n",
      "SENT 31: ['m', 'a', 'r', 'k', 't', 'a', 'n', 't', 'e', 'i', 'l']\n",
      "PRED 31: m m a n t e i l t e i n t e l\n",
      "PRED SCORE: -0.7568\n",
      "\n",
      "[2021-02-03 01:44:06,730 INFO] \n",
      "SENT 32: ['a', 'n', 't', 'e', 'i', 'l']\n",
      "PRED 32: e n t a n t e l n\n",
      "PRED SCORE: -2.3372\n",
      "\n",
      "[2021-02-03 01:44:06,730 INFO] \n",
      "SENT 33: ['r', 'o', 'h', 's', 't', 'o', 'f', 'f']\n",
      "PRED 33: f t o f r o f s t o f t e n\n",
      "PRED SCORE: -0.4266\n",
      "\n",
      "[2021-02-03 01:44:06,730 INFO] \n",
      "SENT 34: ['b', 'e', 's', 'c', 'h', 'e', 'i', 'd']\n",
      "PRED 34: b e i d e i d e i d e i d e n\n",
      "PRED SCORE: -1.0286\n",
      "\n",
      "[2021-02-03 01:44:06,731 INFO] \n",
      "SENT 35: ['m', 'i', 't', 't', 'w', 'o', 'c', 'h']\n",
      "PRED 35: t m i t w o c h t e\n",
      "PRED SCORE: -0.2517\n",
      "\n",
      "[2021-02-03 01:44:06,731 INFO] \n",
      "SENT 36: ['k', 'o', 'n', 'f', 'e', 'r', 'e', 'n', 'z']\n",
      "PRED 36: f e r k o n z e r\n",
      "PRED SCORE: -1.9861\n",
      "\n",
      "[2021-02-03 01:44:06,731 INFO] \n",
      "SENT 37: ['s', 'e', 'n', 'd', 'u', 'n', 'g']\n",
      "PRED 37: s e n d u n g e n\n",
      "PRED SCORE: -0.4430\n",
      "\n",
      "[2021-02-03 01:44:06,731 INFO] \n",
      "SENT 38: ['v', 'e', 'r', 'l', 'a', 'g']\n",
      "PRED 38: r a g l a g e r l a g e r\n",
      "PRED SCORE: -0.0702\n",
      "\n",
      "[2021-02-03 01:44:06,731 INFO] \n",
      "SENT 39: ['g', 'l', 'e', 'i', 'c', 'h', 'n', 'i', 's']\n",
      "PRED 39: s l i s c h n i s c h n\n",
      "PRED SCORE: -0.5390\n",
      "\n",
      "[2021-02-03 01:44:06,731 INFO] \n",
      "SENT 40: ['d', 'u', 'r', 'c', 'h', 'g', 'a', 'n', 'g']\n",
      "PRED 40: r a n g u r c h u n g e n\n",
      "PRED SCORE: -0.3101\n",
      "\n",
      "[2021-02-03 01:44:06,732 INFO] \n",
      "SENT 41: ['v', 'e', 'r', 'p', 'f', 'l', 'i', 'c', 'h', 't', 'u', 'n', 'g']\n",
      "PRED 41: t l i c h t u n g e r p i c h t e\n",
      "PRED SCORE: -2.0688\n",
      "\n",
      "[2021-02-03 01:44:06,732 INFO] \n",
      "SENT 42: ['b', 'r', 'u', 'd', 'e', 'r']\n",
      "PRED 42: b u d r u d e r\n",
      "PRED SCORE: -0.9068\n",
      "\n",
      "[2021-02-03 01:44:06,732 INFO] \n",
      "SENT 43: ['u', 'n', 'w', 'a', 'h', 'r', 'h', 'e', 'i', 't']\n",
      "PRED 43: h e i t a h r a h e n\n",
      "PRED SCORE: -2.1829\n",
      "\n",
      "[2021-02-03 01:44:06,732 INFO] \n",
      "SENT 44: ['f', 'e', 'r', 'n', 's', 'e', 'h', 'a', 'p', 'p', 'a', 'r', 'a', 't']\n",
      "PRED 44: p a t p a t p a t p h e r n s e h e r\n",
      "PRED SCORE: -1.4366\n",
      "\n",
      "[2021-02-03 01:44:06,732 INFO] \n",
      "SENT 45: ['z', 'e', 'n', 's', 'u', 's']\n",
      "PRED 45: s u z e n s u s e s\n",
      "PRED SCORE: -0.6478\n",
      "\n",
      "[2021-02-03 01:44:06,732 INFO] \n",
      "SENT 46: ['j', 'o', 'u', 'r', 'n', 'a', 'l', 'i', 's', 't']\n",
      "PRED 46: s t a l n a l n u s t e\n",
      "PRED SCORE: -1.1442\n",
      "\n",
      "[2021-02-03 01:44:06,732 INFO] \n",
      "SENT 47: ['l', 'a', 'g', 'e']\n",
      "PRED 47: g l a g e n\n",
      "PRED SCORE: -0.7324\n",
      "\n",
      "[2021-02-03 01:44:06,733 INFO] \n",
      "SENT 48: ['k', 'r', 'a', 'n', 'k', 'h', 'e', 'i', 't']\n",
      "PRED 48: h e i t h h i t h e i t e n\n",
      "PRED SCORE: -1.5626\n",
      "\n",
      "[2021-02-03 01:44:06,733 INFO] \n",
      "SENT 49: ['e', 'i', 'n', 'z', 'e', 'l', 'h', 'e', 'i', 't']\n",
      "PRED 49: z l e i n z e i t e l h e i t e n\n",
      "PRED SCORE: -0.5442\n",
      "\n",
      "[2021-02-03 01:44:06,733 INFO] \n",
      "SENT 50: ['p', 'a', 'p', 'i', 'e', 'r']\n",
      "PRED 50: p a p i e r e r\n",
      "PRED SCORE: -0.3169\n",
      "\n",
      "[2021-02-03 01:44:06,733 INFO] \n",
      "SENT 51: ['s', 'y', 'm', 'm', 'e', 't', 'r', 'i', 'e']\n",
      "PRED 51: s a m m e t m i e r i e n\n",
      "PRED SCORE: -1.1576\n",
      "\n",
      "[2021-02-03 01:44:06,733 INFO] \n",
      "SENT 52: ['f', 'e', 's', 't', 'u', 'n', 'g']\n",
      "PRED 52: b e s t u n g e n\n",
      "PRED SCORE: -1.0882\n",
      "\n",
      "[2021-02-03 01:44:06,733 INFO] \n",
      "SENT 53: ['r', 'e', 'p', 'u', 'b', 'l', 'i', 'k']\n",
      "PRED 53: k l i k e p l e i k e\n",
      "PRED SCORE: -1.9996\n",
      "\n",
      "[2021-02-03 01:44:06,733 INFO] \n",
      "SENT 54: ['m', 'e', 'i', 'n', 'u', 'n', 'g']\n",
      "PRED 54: u m m e i n u n g e n\n",
      "PRED SCORE: -0.0250\n",
      "\n",
      "[2021-02-03 01:44:06,734 INFO] \n",
      "SENT 55: ['j', 'u', 'n', 'i', 'o', 'r']\n",
      "PRED 55: r u r h i o n e\n",
      "PRED SCORE: -1.8627\n",
      "\n",
      "[2021-02-03 01:44:06,734 INFO] \n",
      "SENT 56: ['t', 'e', 'n', 'd', 'e', 'n', 'z']\n",
      "PRED 56: z e n d e n d e n\n",
      "PRED SCORE: -0.4762\n",
      "\n",
      "[2021-02-03 01:44:06,734 INFO] \n",
      "SENT 57: ['v', 'o', 'r', 's', 'c', 'h', 'l', 'a', 'g']\n",
      "PRED 57: r a g l a g e l s c h l a g l e h\n",
      "PRED SCORE: -1.6659\n",
      "\n",
      "[2021-02-03 01:44:06,734 INFO] \n",
      "SENT 58: ['m', 'a', 'g', 'e', 'n']\n",
      "PRED 58: m m a g e n m a g e n\n",
      "PRED SCORE: -0.1817\n",
      "\n",
      "[2021-02-03 01:44:06,734 INFO] \n",
      "SENT 59: ['m', 'i', 'n', 'i', 's', 't', 'e', 'r']\n",
      "PRED 59: s t i n i n s t e r\n",
      "PRED SCORE: -0.9924\n",
      "\n",
      "[2021-02-03 01:44:06,734 INFO] \n",
      "SENT 60: ['s', 't', 'r', 'a', 'n', 'd']\n",
      "PRED 60: s t r a n d r a n d e r\n",
      "PRED SCORE: -0.6952\n",
      "\n",
      "[2021-02-03 01:44:06,942 INFO] \n",
      "SENT 61: ['a', 'n', 'g', 'e', 'b', 'o', 't']\n",
      "PRED 61: b a n g e o o s e\n",
      "PRED SCORE: -1.4691\n",
      "\n",
      "[2021-02-03 01:44:06,943 INFO] \n",
      "SENT 62: ['v', 'o', 'r', 't', 'e', 'i', 'l']\n",
      "PRED 62: r o r t e i r t e i l e n\n",
      "PRED SCORE: -1.9024\n",
      "\n",
      "[2021-02-03 01:44:06,943 INFO] \n",
      "SENT 63: ['v', 'e', 'r', 'l', 'u', 's', 't']\n",
      "PRED 63: s z l u s t e r\n",
      "PRED SCORE: -1.2729\n",
      "\n",
      "[2021-02-03 01:44:06,943 INFO] \n",
      "SENT 64: ['g', 'e', 's', 'e', 't', 'z']\n",
      "PRED 64: s t e t z e t z e\n",
      "PRED SCORE: -1.4647\n",
      "\n",
      "[2021-02-03 01:44:06,943 INFO] \n",
      "SENT 65: ['r', 'e', 't', 't', 'i', 'c', 'h']\n",
      "PRED 65: d r e t i c h t i c h t e\n",
      "PRED SCORE: -2.1976\n",
      "\n",
      "[2021-02-03 01:44:06,943 INFO] \n",
      "SENT 66: ['b', 'o', 'h', 'r', 'u', 'n', 'g']\n",
      "PRED 66: b r u n g e h r u n g e r\n",
      "PRED SCORE: -0.6370\n",
      "\n",
      "[2021-02-03 01:44:06,944 INFO] \n",
      "SENT 67: ['v', 'e', 'r', 'k', 'a', 'u', 'f']\n",
      "PRED 67: u r k a u f e r k a u f e r\n",
      "PRED SCORE: -0.3317\n",
      "\n",
      "[2021-02-03 01:44:06,944 INFO] \n",
      "SENT 68: ['t', 'e', 'm', 'p', 'e', 'r', 'a', 't', 'u', 'r']\n",
      "PRED 68: t m a t p e r a t e r\n",
      "PRED SCORE: -2.4384\n",
      "\n",
      "[2021-02-03 01:44:06,944 INFO] \n",
      "SENT 69: ['r', 'e', 'i', 'h', 'e', 'n', 'f', 'o', 'l', 'g', 'e']\n",
      "PRED 69: f l e i e n f e i h e i e n\n",
      "PRED SCORE: -1.3363\n",
      "\n",
      "[2021-02-03 01:44:06,944 INFO] \n",
      "SENT 70: ['p', 'o', 'l', 'i', 'z', 'e', 'i']\n",
      "PRED 70: z p o w p i z e i e n\n",
      "PRED SCORE: -2.8687\n",
      "\n",
      "[2021-02-03 01:44:06,944 INFO] \n",
      "SENT 71: ['b', 'a', 'y', 'e', 'r']\n",
      "PRED 71: b a a b a n e r\n",
      "PRED SCORE: -1.2787\n",
      "\n",
      "[2021-02-03 01:44:06,945 INFO] \n",
      "SENT 72: ['v', 'o', 'r', 's', 'p', 'i', 'e', 'l']\n",
      "PRED 72: s p i e l s p i e l e\n",
      "PRED SCORE: -1.1810\n",
      "\n",
      "[2021-02-03 01:44:06,945 INFO] \n",
      "SENT 73: ['r', 'a', 'b', 'a', 't', 't']\n",
      "PRED 73: b r a b r a b t t e n\n",
      "PRED SCORE: -0.9763\n",
      "\n",
      "[2021-02-03 01:44:06,945 INFO] \n",
      "SENT 74: ['r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n']\n",
      "PRED 74: t l o t i o n e r\n",
      "PRED SCORE: -3.6845\n",
      "\n",
      "[2021-02-03 01:44:06,945 INFO] \n",
      "SENT 75: ['e', 'm', 'p', 'f', 'a', 'n', 'g']\n",
      "PRED 75: m a n g e m m a n g e m e\n",
      "PRED SCORE: -0.4858\n",
      "\n",
      "[2021-02-03 01:44:06,945 INFO] \n",
      "SENT 76: ['e', 'm', 'p', 'f', 'e', 'h', 'l', 'u', 'n', 'g']\n",
      "PRED 76: u m p f e l m u n g e m n\n",
      "PRED SCORE: -1.5204\n",
      "\n",
      "[2021-02-03 01:44:06,946 INFO] \n",
      "SENT 77: ['h', 'o', 'r', 'i', 'z', 'o', 'n', 't']\n",
      "PRED 77: r o w i z o n t h t e\n",
      "PRED SCORE: -1.7211\n",
      "\n",
      "[2021-02-03 01:44:06,946 INFO] \n",
      "SENT 78: ['p', 'l', 'a', 'n']\n",
      "PRED 78: p l a n p l a n l e p l a n l e\n",
      "PRED SCORE: -1.4282\n",
      "\n",
      "[2021-02-03 01:44:06,946 INFO] \n",
      "SENT 79: ['k', 'a', 'i', 's', 'e', 'r']\n",
      "PRED 79: s e r k a i s e r\n",
      "PRED SCORE: -0.4062\n",
      "\n",
      "[2021-02-03 01:44:06,946 INFO] \n",
      "SENT 80: ['h', 'a', 'n', 'd', 'e', 'l']\n",
      "PRED 80: h a n d a n d e l h e l h e l h e l h e l h e l h e l h e r\n",
      "PRED SCORE: -1.2442\n",
      "\n",
      "[2021-02-03 01:44:06,946 INFO] PRED AVG SCORE: -0.1000, PRED PPL: 1.1052\n",
      "[2021-02-03 01:44:07,799 INFO] Translating shard 0.\n",
      "[2021-02-03 01:44:07,896 INFO] \n",
      "SENT 1: ['FEM', '<s>', 'e', 'i', 'n', 'z', 'e', 'l', 'h', 'e', 'i', 't']\n",
      "PRED 1: e i n z e l h e l h e n\n",
      "PRED SCORE: -0.9409\n",
      "\n",
      "[2021-02-03 01:44:07,896 INFO] \n",
      "SENT 2: ['NTR', '<s>', 's', 't', 'a', 'd', 'i', 'u', 'm']\n",
      "PRED 2: s t a d i u m e\n",
      "PRED SCORE: -0.0102\n",
      "\n",
      "[2021-02-03 01:44:07,897 INFO] \n",
      "SENT 3: ['MAS', '<s>', 'k', 'l', 'a', 'n', 'g']\n",
      "PRED 3: k l a n g e\n",
      "PRED SCORE: -0.0035\n",
      "\n",
      "[2021-02-03 01:44:07,897 INFO] \n",
      "SENT 4: ['FEM', '<s>', 's', 'p', 'u', 'r']\n",
      "PRED 4: s p u r e n\n",
      "PRED SCORE: -0.1198\n",
      "\n",
      "[2021-02-03 01:44:07,908 INFO] \n",
      "SENT 5: ['MAS', '<s>', 'b', 'e', 's', 'c', 'h', 'l', 'u', 's', 's']\n",
      "PRED 5: b e s c h l u s s e\n",
      "PRED SCORE: -0.0057\n",
      "\n",
      "[2021-02-03 01:44:07,909 INFO] \n",
      "SENT 6: ['MAS', '<s>', 'h', 'o', 'c', 'h', 'o', 'f', 'e', 'n']\n",
      "PRED 6: h o c h o e n\n",
      "PRED SCORE: -0.4390\n",
      "\n",
      "[2021-02-03 01:44:07,909 INFO] \n",
      "SENT 7: ['FEM', '<s>', 'h', 'o', 'c', 'h', 's', 'c', 'h', 'u', 'l', 'e']\n",
      "PRED 7: h o c h s c h s e n\n",
      "PRED SCORE: -0.7454\n",
      "\n",
      "[2021-02-03 01:44:07,909 INFO] \n",
      "SENT 8: ['MAS', '<s>', 's', 't', 'a', 'm', 'm', 'b', 'a', 'u', 'm']\n",
      "PRED 8: s t a m m a u m e\n",
      "PRED SCORE: -0.6291\n",
      "\n",
      "[2021-02-03 01:44:07,909 INFO] \n",
      "SENT 9: ['FEM', '<s>', 'm', 'a', 'n', 'u', 'f', 'a', 'k', 't', 'u', 'r']\n",
      "PRED 9: m a n u f t u r t e n\n",
      "PRED SCORE: -0.2440\n",
      "\n",
      "[2021-02-03 01:44:07,909 INFO] \n",
      "SENT 10: ['NTR', '<s>', 'g', 'e', 's', 'e', 't', 'z']\n",
      "PRED 10: g e s e t e\n",
      "PRED SCORE: -0.6380\n",
      "\n",
      "[2021-02-03 01:44:07,910 INFO] \n",
      "SENT 11: ['MAS', '<s>', 'e', 'i', 'n', 'w', 'a', 'n', 'd', 'e', 'r', 'e', 'r']\n",
      "PRED 11: e i n w a n e r e r\n",
      "PRED SCORE: -0.9788\n",
      "\n",
      "[2021-02-03 01:44:07,910 INFO] \n",
      "SENT 12: ['MAS', '<s>', 's', 't', 'r', 'o', 'm']\n",
      "PRED 12: s t r o m e\n",
      "PRED SCORE: -0.0941\n",
      "\n",
      "[2021-02-03 01:44:07,910 INFO] \n",
      "SENT 13: ['MAS', '<s>', 's', 't', 'a', 'a', 't']\n",
      "PRED 13: s t a a t e\n",
      "PRED SCORE: -0.0067\n",
      "\n",
      "[2021-02-03 01:44:07,910 INFO] \n",
      "SENT 14: ['MAS', '<s>', 'b', 'e', 't', 'r', 'i', 'e', 'b']\n",
      "PRED 14: b e t r i e b e b e\n",
      "PRED SCORE: -0.1291\n",
      "\n",
      "[2021-02-03 01:44:07,910 INFO] \n",
      "SENT 15: ['MAS', '<s>', 't', 'y', 'p', 'u', 's']\n",
      "PRED 15: t y p u s\n",
      "PRED SCORE: -0.0059\n",
      "\n",
      "[2021-02-03 01:44:07,911 INFO] \n",
      "SENT 16: ['FEM', '<s>', 'b', 'e', 'r', 'a', 't', 'u', 'n', 'g']\n",
      "PRED 16: b e r a t u n g e n\n",
      "PRED SCORE: -0.0444\n",
      "\n",
      "[2021-02-03 01:44:07,911 INFO] \n",
      "SENT 17: ['MAS', '<s>', 'z', 'e', 'm', 'e', 'n', 't']\n",
      "PRED 17: z e m e n\n",
      "PRED SCORE: -0.0251\n",
      "\n",
      "[2021-02-03 01:44:07,911 INFO] \n",
      "SENT 18: ['NTR', '<s>', 'a', 'r', 'm', 'a', 't', 'u', 'r', 'e', 'n', 'b', 'r', 'e', 't', 't']\n",
      "PRED 18: a r m a t m e r t u r e n\n",
      "PRED SCORE: -1.7382\n",
      "\n",
      "[2021-02-03 01:44:07,911 INFO] \n",
      "SENT 19: ['FEM', '<s>', 'k', 'r', 'i', 's', 'e']\n",
      "PRED 19: k r i s e n\n",
      "PRED SCORE: -0.0017\n",
      "\n",
      "[2021-02-03 01:44:07,912 INFO] \n",
      "SENT 20: ['FEM', '<s>', 'v', 'e', 'r', 'o', 'r', 'd', 'n', 'u', 'n', 'g']\n",
      "PRED 20: v e r o r o r o g e n\n",
      "PRED SCORE: -0.4448\n",
      "\n",
      "[2021-02-03 01:44:07,912 INFO] \n",
      "SENT 21: ['NTR', '<s>', 's', 't', 'a', 'd', 'i', 'o', 'n']\n",
      "PRED 21: s t a d i o n e\n",
      "PRED SCORE: -0.0140\n",
      "\n",
      "[2021-02-03 01:44:07,912 INFO] \n",
      "SENT 22: ['MAS', '<s>', 'k', 'a', 'm', 'p', 'f']\n",
      "PRED 22: k a m p f e\n",
      "PRED SCORE: -0.0033\n",
      "\n",
      "[2021-02-03 01:44:07,912 INFO] \n",
      "SENT 23: ['NTR', '<s>', 'p', 'r', 'o', 'z', 'e', 'n', 't']\n",
      "PRED 23: p r o z e n\n",
      "PRED SCORE: -0.2165\n",
      "\n",
      "[2021-02-03 01:44:07,912 INFO] \n",
      "SENT 24: ['NTR', '<s>', 'e', 'r', 'z', 'e', 'u', 'g', 'n', 'i', 's']\n",
      "PRED 24: e r z e u s i s e\n",
      "PRED SCORE: -0.5921\n",
      "\n",
      "[2021-02-03 01:44:07,913 INFO] \n",
      "SENT 25: ['MAS', '<s>', 'r', 'a', 'h', 'm', 'e', 'n']\n",
      "PRED 25: r a h m e n\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-02-03 01:44:07,913 INFO] \n",
      "SENT 26: ['MAS', '<s>', 'f', 'r', 'a', 'n', 'z', 'o', 's', 'e']\n",
      "PRED 26: f r a n z o s e n\n",
      "PRED SCORE: -0.0257\n",
      "\n",
      "[2021-02-03 01:44:07,913 INFO] \n",
      "SENT 27: ['NTR', '<s>', 'p', 'r', 'o', 'd', 'u', 'k', 't']\n",
      "PRED 27: p r o d u k t e\n",
      "PRED SCORE: -0.3231\n",
      "\n",
      "[2021-02-03 01:44:07,913 INFO] \n",
      "SENT 28: ['MAS', '<s>', 'v', 'o', 'r', 's', 'c', 'h', 'l', 'a', 'g']\n",
      "PRED 28: v o r s c h l e\n",
      "PRED SCORE: -0.2084\n",
      "\n",
      "[2021-02-03 01:44:07,913 INFO] \n",
      "SENT 29: ['NTR', '<s>', 'k', 'o', 'p', 'f', 't', 'u', 'c', 'h']\n",
      "PRED 29: k o p f t u c h e\n",
      "PRED SCORE: -0.2211\n",
      "\n",
      "[2021-02-03 01:44:07,914 INFO] \n",
      "SENT 30: ['MAS', '<s>', 'm', 'i', 't', 't', 'e', 'l', 'p', 'u', 'n', 'k', 't']\n",
      "PRED 30: m i t t e l p u n k e l\n",
      "PRED SCORE: -0.2931\n",
      "\n",
      "[2021-02-03 01:44:08,006 INFO] \n",
      "SENT 31: ['MAS', '<s>', 'm', 'e', 't', 'e', 'r']\n",
      "PRED 31: m e t e r\n",
      "PRED SCORE: -0.0079\n",
      "\n",
      "[2021-02-03 01:44:08,006 INFO] \n",
      "SENT 32: ['FEM', '<s>', 's', 't', 'e', 'i', 'g', 'e', 'r', 'u', 'n', 'g']\n",
      "PRED 32: s t e i g e r u n g e n\n",
      "PRED SCORE: -0.0502\n",
      "\n",
      "[2021-02-03 01:44:08,007 INFO] \n",
      "SENT 33: ['FEM', '<s>', 'a', 'u', 's', 's', 'p', 'r', 'a', 'c', 'h', 'e']\n",
      "PRED 33: a u s p r a c h e n\n",
      "PRED SCORE: -0.0236\n",
      "\n",
      "[2021-02-03 01:44:08,007 INFO] \n",
      "SENT 34: ['MAS', '<s>', 's', 'e', 'n', 'a', 't']\n",
      "PRED 34: s e n a t e\n",
      "PRED SCORE: -0.1757\n",
      "\n",
      "[2021-02-03 01:44:08,007 INFO] \n",
      "SENT 35: ['MAS', '<s>', 'm', 'i', 'n', 'i', 's', 't', 'e', 'r']\n",
      "PRED 35: m i n i s t e r\n",
      "PRED SCORE: -0.0176\n",
      "\n",
      "[2021-02-03 01:44:08,007 INFO] \n",
      "SENT 36: ['FEM', '<s>', 'v', 'e', 'r', 'w', 'a', 'l', 't', 'u', 'n', 'g']\n",
      "PRED 36: v e r w a l t u n g e n\n",
      "PRED SCORE: -0.0059\n",
      "\n",
      "[2021-02-03 01:44:08,008 INFO] \n",
      "SENT 37: ['MAS', '<s>', 's', 't', 'r', 'e', 'i', 't']\n",
      "PRED 37: s t r e i t e\n",
      "PRED SCORE: -0.0273\n",
      "\n",
      "[2021-02-03 01:44:08,008 INFO] \n",
      "SENT 38: ['NTR', '<s>', 'f', 'e', 'l', 'l']\n",
      "PRED 38: f e l l e\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-02-03 01:44:08,008 INFO] \n",
      "SENT 39: ['NTR', '<s>', 'k', 'i', 'l', 'o', 'g', 'r', 'a', 'm', 'm']\n",
      "PRED 39: k i l o m o m a m m e\n",
      "PRED SCORE: -0.8697\n",
      "\n",
      "[2021-02-03 01:44:08,008 INFO] \n",
      "SENT 40: ['MAS', '<s>', 'h', 'e', 'l', 'd']\n",
      "PRED 40: h e l d e\n",
      "PRED SCORE: -0.1055\n",
      "\n",
      "[2021-02-03 01:44:08,008 INFO] \n",
      "SENT 41: ['MAS', '<s>', 'm', 'i', 't', 't', 'w', 'o', 'c', 'h']\n",
      "PRED 41: m i t t w o c h e\n",
      "PRED SCORE: -0.0041\n",
      "\n",
      "[2021-02-03 01:44:08,008 INFO] \n",
      "SENT 42: ['MAS', '<s>', 'b', 'e', 'z', 'i', 'r', 'k']\n",
      "PRED 42: b e z i r e\n",
      "PRED SCORE: -0.6039\n",
      "\n",
      "[2021-02-03 01:44:08,008 INFO] \n",
      "SENT 43: ['MAS', '<s>', 's', 'p', 'r', 'e', 'c', 'h', 'e', 'r']\n",
      "PRED 43: s p r e c h e r\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-02-03 01:44:08,009 INFO] \n",
      "SENT 44: ['MAS', '<s>', 'p', 'a', 'k', 't']\n",
      "PRED 44: p a k t e n\n",
      "PRED SCORE: -0.0058\n",
      "\n",
      "[2021-02-03 01:44:08,009 INFO] \n",
      "SENT 45: ['MAS', '<s>', 's', 't', 'a', 'n', 'd']\n",
      "PRED 45: s t a n d e r\n",
      "PRED SCORE: -0.3325\n",
      "\n",
      "[2021-02-03 01:44:08,009 INFO] \n",
      "SENT 46: ['NTR', '<s>', 'p', 'o', 's', 't', 'f', 'a', 'c', 'h']\n",
      "PRED 46: p o s t a c h e r\n",
      "PRED SCORE: -0.9180\n",
      "\n",
      "[2021-02-03 01:44:08,009 INFO] \n",
      "SENT 47: ['FEM', '<s>', 'b', 'e', 's', 'p', 'r', 'e', 'c', 'h', 'u', 'n', 'g']\n",
      "PRED 47: b e s p r u n g e n\n",
      "PRED SCORE: -0.0255\n",
      "\n",
      "[2021-02-03 01:44:08,009 INFO] \n",
      "SENT 48: ['FEM', '<s>', 'h', 'i', 'l', 'f', 'e']\n",
      "PRED 48: h i l f e n\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-02-03 01:44:08,009 INFO] \n",
      "SENT 49: ['FEM', '<s>', 'f', 'r', 'e', 'u', 'd', 'e']\n",
      "PRED 49: f r e u d e n\n",
      "PRED SCORE: -0.0035\n",
      "\n",
      "[2021-02-03 01:44:08,009 INFO] \n",
      "SENT 50: ['NTR', '<s>', 'b', 'u', 'l', 'l', 'e', 't', 'i', 'n']\n",
      "PRED 50: b u l l e t i n e\n",
      "PRED SCORE: -0.4493\n",
      "\n",
      "[2021-02-03 01:44:08,010 INFO] \n",
      "SENT 51: ['MAS', '<s>', 's', 'c', 'h', 'e', 'l', 'l', 'f', 'i', 's', 'c', 'h']\n",
      "PRED 51: s c h e l l f e\n",
      "PRED SCORE: -0.2846\n",
      "\n",
      "[2021-02-03 01:44:08,010 INFO] \n",
      "SENT 52: ['FEM', '<s>', 'b', 'e', 'd', 'i', 'n', 'g', 'u', 'n', 'g']\n",
      "PRED 52: b e d i n g u n g e n\n",
      "PRED SCORE: -0.0397\n",
      "\n",
      "[2021-02-03 01:44:08,010 INFO] \n",
      "SENT 53: ['FEM', '<s>', 'e', 'p', 'o', 'c', 'h', 'e']\n",
      "PRED 53: e p c h e n\n",
      "PRED SCORE: -0.2489\n",
      "\n",
      "[2021-02-03 01:44:08,010 INFO] \n",
      "SENT 54: ['FEM', '<s>', 's', 'i', 'c', 'h', 'e', 'r', 'h', 'e', 'i', 't']\n",
      "PRED 54: s i c h e r h e r\n",
      "PRED SCORE: -0.0780\n",
      "\n",
      "[2021-02-03 01:44:08,010 INFO] \n",
      "SENT 55: ['MAS', '<s>', 'v', 'e', 'r', 'l', 'a', 'u', 'f']\n",
      "PRED 55: v e r l a u f e\n",
      "PRED SCORE: -0.0011\n",
      "\n",
      "[2021-02-03 01:44:08,010 INFO] \n",
      "SENT 56: ['MAS', '<s>', 'b', 'e', 'i', 't', 'r', 'a', 'g']\n",
      "PRED 56: b e i t r a g e\n",
      "PRED SCORE: -0.2964\n",
      "\n",
      "[2021-02-03 01:44:08,010 INFO] \n",
      "SENT 57: ['MAS', '<s>', 's', 'c', 'h', 'r', 'e', 'i']\n",
      "PRED 57: s c h r e i e\n",
      "PRED SCORE: -0.0455\n",
      "\n",
      "[2021-02-03 01:44:08,011 INFO] \n",
      "SENT 58: ['FEM', '<s>', 'p', 'r', 'o', 'd', 'u', 'k', 't', 'i', 'o', 'n']\n",
      "PRED 58: p r o d u k i o n e n\n",
      "PRED SCORE: -0.1164\n",
      "\n",
      "[2021-02-03 01:44:08,011 INFO] \n",
      "SENT 59: ['FEM', '<s>', 'p', 'o', 'l', 'i', 't', 'i', 'k']\n",
      "PRED 59: p o l i k i k e n\n",
      "PRED SCORE: -0.6255\n",
      "\n",
      "[2021-02-03 01:44:08,011 INFO] \n",
      "SENT 60: ['FEM', '<s>', 'a', 'u', 's', 'w', 'a', 'h', 'l']\n",
      "PRED 60: a u s w a h l e\n",
      "PRED SCORE: -0.0996\n",
      "\n",
      "[2021-02-03 01:44:08,096 INFO] \n",
      "SENT 61: ['NTR', '<s>', 'j', 'a', 'h', 'r', 'z', 'e', 'h', 'n', 't']\n",
      "PRED 61: v a h r e h z e h t e\n",
      "PRED SCORE: -2.6058\n",
      "\n",
      "[2021-02-03 01:44:08,096 INFO] \n",
      "SENT 62: ['MAS', '<s>', 's', 'c', 'h', 'a', 'c', 'h', 't']\n",
      "PRED 62: s c h a c h t e\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-02-03 01:44:08,096 INFO] \n",
      "SENT 63: ['MAS', '<s>', 'a', 'u', 'f', 's', 'i', 'c', 'h', 't', 's', 'r', 'a', 't']\n",
      "PRED 63: a u f s i c h t i c h t e\n",
      "PRED SCORE: -0.7506\n",
      "\n",
      "[2021-02-03 01:44:08,096 INFO] \n",
      "SENT 64: ['FEM', '<s>', 'k', 'u', 'l', 't', 'u', 'r']\n",
      "PRED 64: k u l t u r e n\n",
      "PRED SCORE: -0.0013\n",
      "\n",
      "[2021-02-03 01:44:08,097 INFO] \n",
      "SENT 65: ['NTR', '<s>', 'w', 'o', 'h', 'n', 'z', 'i', 'm', 'm', 'e', 'r']\n",
      "PRED 65: w o h n i m m i m m e r\n",
      "PRED SCORE: -0.0158\n",
      "\n",
      "[2021-02-03 01:44:08,097 INFO] \n",
      "SENT 66: ['MAS', '<s>', 'g', 'e', 'r', 'u', 'c', 'h']\n",
      "PRED 66: g e r u c h e\n",
      "PRED SCORE: -0.0025\n",
      "\n",
      "[2021-02-03 01:44:08,097 INFO] \n",
      "SENT 67: ['NTR', '<s>', 's', 'y', 's', 't', 'e', 'm']\n",
      "PRED 67: s y s t e m\n",
      "PRED SCORE: -0.4852\n",
      "\n",
      "[2021-02-03 01:44:08,097 INFO] \n",
      "SENT 68: ['NTR', '<s>', 'm', 'i', 'n', 'i', 's', 't', 'e', 'r', 'i', 'u', 'm']\n",
      "PRED 68: m i n i s t e r i u m e r\n",
      "PRED SCORE: -0.2494\n",
      "\n",
      "[2021-02-03 01:44:08,097 INFO] \n",
      "SENT 69: ['FEM', '<s>', 'a', 'n', 's', 'c', 'h', 'a', 'u', 'u', 'n', 'g']\n",
      "PRED 69: a n s c h a u n g e n\n",
      "PRED SCORE: -0.0133\n",
      "\n",
      "[2021-02-03 01:44:08,098 INFO] \n",
      "SENT 70: ['MAS', '<s>', 'd', 'i', 'e', 's', 'e', 'l']\n",
      "PRED 70: d i e s e l\n",
      "PRED SCORE: -0.0047\n",
      "\n",
      "[2021-02-03 01:44:08,098 INFO] \n",
      "SENT 71: ['NTR', '<s>', 'g', 'e', 'b', 'i', 'e', 't']\n",
      "PRED 71: g e b i e r\n",
      "PRED SCORE: -0.2611\n",
      "\n",
      "[2021-02-03 01:44:08,098 INFO] \n",
      "SENT 72: ['MAS', '<s>', 'u', 'm', 's', 'a', 't', 'z']\n",
      "PRED 72: u m s a t z e\n",
      "PRED SCORE: -0.0272\n",
      "\n",
      "[2021-02-03 01:44:08,098 INFO] \n",
      "SENT 73: ['NTR', '<s>', 'd', 'o', 'k', 'u', 'm', 'e', 'n', 't']\n",
      "PRED 73: d o k u m e n\n",
      "PRED SCORE: -0.0154\n",
      "\n",
      "[2021-02-03 01:44:08,098 INFO] \n",
      "SENT 74: ['FEM', '<s>', 'm', 'i', 't', 'b', 'e', 's', 't', 'i', 'm', 'm', 'u', 'n', 'g']\n",
      "PRED 74: m i t b e s t i m m e m u n g e n\n",
      "PRED SCORE: -0.7983\n",
      "\n",
      "[2021-02-03 01:44:08,099 INFO] \n",
      "SENT 75: ['NTR', '<s>', 'p', 'l', 'a', 's', 'm', 'a']\n",
      "PRED 75: p l a s m a s m e\n",
      "PRED SCORE: -0.6680\n",
      "\n",
      "[2021-02-03 01:44:08,099 INFO] \n",
      "SENT 76: ['FEM', '<s>', 'f', 'l', 'a', 'g', 'g', 'e']\n",
      "PRED 76: f l a g g e n\n",
      "PRED SCORE: -0.0025\n",
      "\n",
      "[2021-02-03 01:44:08,099 INFO] \n",
      "SENT 77: ['FEM', '<s>', 'p', 'a', 'u', 's', 'e']\n",
      "PRED 77: p a u s e n\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-02-03 01:44:08,099 INFO] \n",
      "SENT 78: ['FEM', '<s>', 's', 'i', 'e', 'g']\n",
      "PRED 78: s i e g e n\n",
      "PRED SCORE: -0.0153\n",
      "\n",
      "[2021-02-03 01:44:08,099 INFO] \n",
      "SENT 79: ['NTR', '<s>', 'a', 'n', 'g', 'e', 'b', 'o', 't']\n",
      "PRED 79: a n g e b o t o\n",
      "PRED SCORE: -0.7877\n",
      "\n",
      "[2021-02-03 01:44:08,099 INFO] \n",
      "SENT 80: ['NTR', '<s>', 'z', 'e', 'n', 't', 'r', 'u', 'm']\n",
      "PRED 80: z e n t r u m e\n",
      "PRED SCORE: -0.0465\n",
      "\n",
      "[2021-02-03 01:44:08,100 INFO] PRED AVG SCORE: -0.0320, PRED PPL: 1.0325\n",
      "[2021-02-03 01:44:08,916 INFO] Translating shard 0.\n",
      "[2021-02-03 01:44:09,163 INFO] \n",
      "SENT 1: ['g', 'e', 's', 'e', 't', 'z']\n",
      "PRED 1: e t g e s e t z e\n",
      "PRED SCORE: -2.0368\n",
      "\n",
      "[2021-02-03 01:44:09,163 INFO] \n",
      "SENT 2: ['p', 'o', 's', 't', 'f', 'a', 'c', 'h']\n",
      "PRED 2: h o s p o s t o s t e a c h t e\n",
      "PRED SCORE: -1.4913\n",
      "\n",
      "[2021-02-03 01:44:09,163 INFO] \n",
      "SENT 3: ['s', 't', 'r', 'o', 'm']\n",
      "PRED 3: s t r o m s t r e\n",
      "PRED SCORE: -0.2820\n",
      "\n",
      "[2021-02-03 01:44:09,163 INFO] \n",
      "SENT 4: ['b', 'e', 't', 'r', 'i', 'e', 'b']\n",
      "PRED 4: b e b b e b e b e r\n",
      "PRED SCORE: -0.9839\n",
      "\n",
      "[2021-02-03 01:44:09,164 INFO] \n",
      "SENT 5: ['m', 'i', 't', 't', 'w', 'o', 'c', 'h']\n",
      "PRED 5: h o m m i t t w i t e\n",
      "PRED SCORE: -0.9725\n",
      "\n",
      "[2021-02-03 01:44:09,164 INFO] \n",
      "SENT 6: ['s', 't', 'a', 'n', 'd']\n",
      "PRED 6: s t a n s t a n d e a n d e r\n",
      "PRED SCORE: -3.5446\n",
      "\n",
      "[2021-02-03 01:44:09,164 INFO] \n",
      "SENT 7: ['f', 'e', 'l', 'l']\n",
      "PRED 7: f l e l f e l\n",
      "PRED SCORE: -0.0056\n",
      "\n",
      "[2021-02-03 01:44:09,164 INFO] \n",
      "SENT 8: ['k', 'u', 'l', 't', 'u', 'r']\n",
      "PRED 8: k u l k u r u s\n",
      "PRED SCORE: -1.2036\n",
      "\n",
      "[2021-02-03 01:44:09,165 INFO] \n",
      "SENT 9: ['m', 'i', 't', 't', 'e', 'l', 'p', 'u', 'n', 'k', 't']\n",
      "PRED 9: n p u n t i t t u n k e l\n",
      "PRED SCORE: -1.6027\n",
      "\n",
      "[2021-02-03 01:44:09,165 INFO] \n",
      "SENT 10: ['a', 'n', 's', 'c', 'h', 'a', 'u', 'u', 'n', 'g']\n",
      "PRED 10: c h a u n s c h a u n g e\n",
      "PRED SCORE: -0.5812\n",
      "\n",
      "[2021-02-03 01:44:09,165 INFO] \n",
      "SENT 11: ['s', 'i', 'e', 'g']\n",
      "PRED 11: s e g s i e\n",
      "PRED SCORE: -0.9432\n",
      "\n",
      "[2021-02-03 01:44:09,165 INFO] \n",
      "SENT 12: ['h', 'i', 'l', 'f', 'e']\n",
      "PRED 12: h e h f e n\n",
      "PRED SCORE: -1.1058\n",
      "\n",
      "[2021-02-03 01:44:09,166 INFO] \n",
      "SENT 13: ['k', 'o', 'p', 'f', 't', 'u', 'c', 'h']\n",
      "PRED 13: f u c f o t f o t e u c h t e\n",
      "PRED SCORE: -1.0140\n",
      "\n",
      "[2021-02-03 01:44:09,167 INFO] \n",
      "SENT 14: ['f', 'l', 'a', 'g', 'g', 'e']\n",
      "PRED 14: g u g l a g g e l\n",
      "PRED SCORE: -2.1533\n",
      "\n",
      "[2021-02-03 01:44:09,167 INFO] \n",
      "SENT 15: ['p', 'r', 'o', 'd', 'u', 'k', 't', 'i', 'o', 'n']\n",
      "PRED 15: d o k p o n o n t i k u s e\n",
      "PRED SCORE: -1.8569\n",
      "\n",
      "[2021-02-03 01:44:09,167 INFO] \n",
      "SENT 16: ['e', 'i', 'n', 'z', 'e', 'l', 'h', 'e', 'i', 't']\n",
      "PRED 16: l i n z e l h e l w e i t e l\n",
      "PRED SCORE: -2.2584\n",
      "\n",
      "[2021-02-03 01:44:09,167 INFO] \n",
      "SENT 17: ['v', 'e', 'r', 'l', 'a', 'u', 'f']\n",
      "PRED 17: f r a u f e r l a u f e r\n",
      "PRED SCORE: -0.1846\n",
      "\n",
      "[2021-02-03 01:44:09,167 INFO] \n",
      "SENT 18: ['k', 'r', 'i', 's', 'e']\n",
      "PRED 18: k e s e r i s e r\n",
      "PRED SCORE: -2.0791\n",
      "\n",
      "[2021-02-03 01:44:09,168 INFO] \n",
      "SENT 19: ['b', 'u', 'l', 'l', 'e', 't', 'i', 'n']\n",
      "PRED 19: l u l b u n g e t e\n",
      "PRED SCORE: -2.3128\n",
      "\n",
      "[2021-02-03 01:44:09,168 INFO] \n",
      "SENT 20: ['z', 'e', 'n', 't', 'r', 'u', 'm']\n",
      "PRED 20: e n z e r t e n\n",
      "PRED SCORE: -2.4085\n",
      "\n",
      "[2021-02-03 01:44:09,168 INFO] \n",
      "SENT 21: ['m', 'i', 't', 'b', 'e', 's', 't', 'i', 'm', 'm', 'u', 'n', 'g']\n",
      "PRED 21: a u m i m u n g e m i m u n g e\n",
      "PRED SCORE: -4.0816\n",
      "\n",
      "[2021-02-03 01:44:09,168 INFO] \n",
      "SENT 22: ['s', 't', 'a', 'd', 'i', 'o', 'n']\n",
      "PRED 22: s t a d s t i o n e\n",
      "PRED SCORE: -1.5429\n",
      "\n",
      "[2021-02-03 01:44:09,169 INFO] \n",
      "SENT 23: ['b', 'e', 'z', 'i', 'r', 'k']\n",
      "PRED 23: r r i r b e b e r\n",
      "PRED SCORE: -0.8190\n",
      "\n",
      "[2021-02-03 01:44:09,169 INFO] \n",
      "SENT 24: ['b', 'e', 'i', 't', 'r', 'a', 'g']\n",
      "PRED 24: b i g b e b i g e r\n",
      "PRED SCORE: -1.1257\n",
      "\n",
      "[2021-02-03 01:44:09,169 INFO] \n",
      "SENT 25: ['s', 'p', 'u', 'r']\n",
      "PRED 25: s p u r s p u r e s\n",
      "PRED SCORE: -0.7232\n",
      "\n",
      "[2021-02-03 01:44:09,169 INFO] \n",
      "SENT 26: ['k', 'a', 'm', 'p', 'f']\n",
      "PRED 26: f a m p a m p f a m f e\n",
      "PRED SCORE: -1.1213\n",
      "\n",
      "[2021-02-03 01:44:09,169 INFO] \n",
      "SENT 27: ['j', 'a', 'h', 'r', 'z', 'e', 'h', 'n', 't']\n",
      "PRED 27: h r a h r a h t e n\n",
      "PRED SCORE: -1.5953\n",
      "\n",
      "[2021-02-03 01:44:09,170 INFO] \n",
      "SENT 28: ['s', 'p', 'r', 'e', 'c', 'h', 'e', 'r']\n",
      "PRED 28: c h r e s p e r e c h e r\n",
      "PRED SCORE: -2.2428\n",
      "\n",
      "[2021-02-03 01:44:09,170 INFO] \n",
      "SENT 29: ['s', 'y', 's', 't', 'e', 'm']\n",
      "PRED 29: s t m y s t e m e m\n",
      "PRED SCORE: -1.2639\n",
      "\n",
      "[2021-02-03 01:44:09,170 INFO] \n",
      "SENT 30: ['f', 'r', 'a', 'n', 'z', 'o', 's', 'e']\n",
      "PRED 30: f o s r a n e n w o s e\n",
      "PRED SCORE: -2.8325\n",
      "\n",
      "[2021-02-03 01:44:09,279 INFO] \n",
      "SENT 31: ['w', 'o', 'h', 'n', 'z', 'i', 'm', 'm', 'e', 'r']\n",
      "PRED 31: z h w o m m o m m o m m e r\n",
      "PRED SCORE: -2.8864\n",
      "\n",
      "[2021-02-03 01:44:09,279 INFO] \n",
      "SENT 32: ['e', 'r', 'z', 'e', 'u', 'g', 'n', 'i', 's']\n",
      "PRED 32: w u g z e u s e u s e r\n",
      "PRED SCORE: -2.6939\n",
      "\n",
      "[2021-02-03 01:44:09,280 INFO] \n",
      "SENT 33: ['b', 'e', 'r', 'a', 't', 'u', 'n', 'g']\n",
      "PRED 33: u r b e b t u n g e r\n",
      "PRED SCORE: -2.4824\n",
      "\n",
      "[2021-02-03 01:44:09,280 INFO] \n",
      "SENT 34: ['p', 'l', 'a', 's', 'm', 'a']\n",
      "PRED 34: s m l a s m a s m e\n",
      "PRED SCORE: -0.7139\n",
      "\n",
      "[2021-02-03 01:44:09,280 INFO] \n",
      "SENT 35: ['a', 'u', 's', 's', 'p', 'r', 'a', 'c', 'h', 'e']\n",
      "PRED 35: s p u s p a u s p a u s e\n",
      "PRED SCORE: -2.3448\n",
      "\n",
      "[2021-02-03 01:44:09,280 INFO] \n",
      "SENT 36: ['v', 'e', 'r', 'w', 'a', 'l', 't', 'u', 'n', 'g']\n",
      "PRED 36: l r e r w a l t u n g e r\n",
      "PRED SCORE: -2.4188\n",
      "\n",
      "[2021-02-03 01:44:09,281 INFO] \n",
      "SENT 37: ['a', 'u', 's', 'w', 'a', 'h', 'l']\n",
      "PRED 37: h l a u s w a u s e\n",
      "PRED SCORE: -0.4138\n",
      "\n",
      "[2021-02-03 01:44:09,281 INFO] \n",
      "SENT 38: ['h', 'e', 'l', 'd']\n",
      "PRED 38: h e l h e l\n",
      "PRED SCORE: -0.0776\n",
      "\n",
      "[2021-02-03 01:44:09,281 INFO] \n",
      "SENT 39: ['s', 'i', 'c', 'h', 'e', 'r', 'h', 'e', 'i', 't']\n",
      "PRED 39: s c h e r h s i c h e r\n",
      "PRED SCORE: -2.1438\n",
      "\n",
      "[2021-02-03 01:44:09,281 INFO] \n",
      "SENT 40: ['h', 'o', 'c', 'h', 's', 'c', 'h', 'u', 'l', 'e']\n",
      "PRED 40: h o c h s c h u l s c h e\n",
      "PRED SCORE: -0.2216\n",
      "\n",
      "[2021-02-03 01:44:09,281 INFO] \n",
      "SENT 41: ['m', 'e', 't', 'e', 'r']\n",
      "PRED 41: r e r m e r\n",
      "PRED SCORE: -1.5801\n",
      "\n",
      "[2021-02-03 01:44:09,282 INFO] \n",
      "SENT 42: ['a', 'r', 'm', 'a', 't', 'u', 'r', 'e', 'n', 'b', 'r', 'e', 't', 't']\n",
      "PRED 42: r t r a t m e m t u r e n\n",
      "PRED SCORE: -2.5730\n",
      "\n",
      "[2021-02-03 01:44:09,282 INFO] \n",
      "SENT 43: ['p', 'o', 'l', 'i', 't', 'i', 'k']\n",
      "PRED 43: w i k p o l i k i k e\n",
      "PRED SCORE: -1.0729\n",
      "\n",
      "[2021-02-03 01:44:09,282 INFO] \n",
      "SENT 44: ['e', 'i', 'n', 'w', 'a', 'n', 'd', 'e', 'r', 'e', 'r']\n",
      "PRED 44: r i n w e r e i e r e i e r\n",
      "PRED SCORE: -1.9842\n",
      "\n",
      "[2021-02-03 01:44:09,282 INFO] \n",
      "SENT 45: ['p', 'a', 'u', 's', 'e']\n",
      "PRED 45: s e u s p a u s e\n",
      "PRED SCORE: -0.2337\n",
      "\n",
      "[2021-02-03 01:44:09,282 INFO] \n",
      "SENT 46: ['t', 'y', 'p', 'u', 's']\n",
      "PRED 46: s t u s y s e\n",
      "PRED SCORE: -0.0751\n",
      "\n",
      "[2021-02-03 01:44:09,283 INFO] \n",
      "SENT 47: ['k', 'i', 'l', 'o', 'g', 'r', 'a', 'm', 'm']\n",
      "PRED 47: k o m k o m i m i m m e\n",
      "PRED SCORE: -2.2225\n",
      "\n",
      "[2021-02-03 01:44:09,283 INFO] \n",
      "SENT 48: ['b', 'e', 's', 'c', 'h', 'l', 'u', 's', 's']\n",
      "PRED 48: h l e b s c h l e u s s e\n",
      "PRED SCORE: -0.7614\n",
      "\n",
      "[2021-02-03 01:44:09,283 INFO] \n",
      "SENT 49: ['a', 'n', 'g', 'e', 'b', 'o', 't']\n",
      "PRED 49: n t a n g e b n o t e r\n",
      "PRED SCORE: -1.2995\n",
      "\n",
      "[2021-02-03 01:44:09,283 INFO] \n",
      "SENT 50: ['b', 'e', 's', 'p', 'r', 'e', 'c', 'h', 'u', 'n', 'g']\n",
      "PRED 50: c h u n b e u n g e\n",
      "PRED SCORE: -2.2785\n",
      "\n",
      "[2021-02-03 01:44:09,283 INFO] \n",
      "SENT 51: ['z', 'e', 'm', 'e', 'n', 't']\n",
      "PRED 51: e m z e m e m e m e\n",
      "PRED SCORE: -1.5752\n",
      "\n",
      "[2021-02-03 01:44:09,284 INFO] \n",
      "SENT 52: ['u', 'm', 's', 'a', 't', 'z']\n",
      "PRED 52: s a u m s a m z e\n",
      "PRED SCORE: -1.5349\n",
      "\n",
      "[2021-02-03 01:44:09,284 INFO] \n",
      "SENT 53: ['p', 'r', 'o', 'd', 'u', 'k', 't']\n",
      "PRED 53: d o k p o k u k u s t e\n",
      "PRED SCORE: -1.0706\n",
      "\n",
      "[2021-02-03 01:44:09,284 INFO] \n",
      "SENT 54: ['h', 'o', 'c', 'h', 'o', 'f', 'e', 'n']\n",
      "PRED 54: h o c h o c h o n e n\n",
      "PRED SCORE: -0.7921\n",
      "\n",
      "[2021-02-03 01:44:09,284 INFO] \n",
      "SENT 55: ['b', 'e', 'd', 'i', 'n', 'g', 'u', 'n', 'g']\n",
      "PRED 55: u d b e u n g e\n",
      "PRED SCORE: -2.6920\n",
      "\n",
      "[2021-02-03 01:44:09,284 INFO] \n",
      "SENT 56: ['a', 'u', 'f', 's', 'i', 'c', 'h', 't', 's', 'r', 'a', 't']\n",
      "PRED 56: s c h t u f s i c h t a u s e\n",
      "PRED SCORE: -2.2180\n",
      "\n",
      "[2021-02-03 01:44:09,285 INFO] \n",
      "SENT 57: ['s', 't', 'e', 'i', 'g', 'e', 'r', 'u', 'n', 'g']\n",
      "PRED 57: s t r e r u g e r u g e r\n",
      "PRED SCORE: -0.8166\n",
      "\n",
      "[2021-02-03 01:44:09,285 INFO] \n",
      "SENT 58: ['s', 't', 'a', 'd', 'i', 'u', 'm']\n",
      "PRED 58: s t a m i u m i u m e\n",
      "PRED SCORE: -1.3018\n",
      "\n",
      "[2021-02-03 01:44:09,285 INFO] \n",
      "SENT 59: ['g', 'e', 'r', 'u', 'c', 'h']\n",
      "PRED 59: c h e r h e r u c h e r\n",
      "PRED SCORE: -0.9564\n",
      "\n",
      "[2021-02-03 01:44:09,285 INFO] \n",
      "SENT 60: ['s', 't', 'a', 'a', 't']\n",
      "PRED 60: s t a a s t a a t e a t e\n",
      "PRED SCORE: -1.3487\n",
      "\n",
      "[2021-02-03 01:44:09,364 INFO] \n",
      "SENT 61: ['d', 'i', 'e', 's', 'e', 'l']\n",
      "PRED 61: s e l d i e l s e l\n",
      "PRED SCORE: -1.1572\n",
      "\n",
      "[2021-02-03 01:44:09,364 INFO] \n",
      "SENT 62: ['m', 'i', 'n', 'i', 's', 't', 'e', 'r', 'i', 'u', 'm']\n",
      "PRED 62: s t r i m s t i u s t e r\n",
      "PRED SCORE: -2.3043\n",
      "\n",
      "[2021-02-03 01:44:09,365 INFO] \n",
      "SENT 63: ['g', 'e', 'b', 'i', 'e', 't']\n",
      "PRED 63: g e b e b i e b e\n",
      "PRED SCORE: -2.0252\n",
      "\n",
      "[2021-02-03 01:44:09,365 INFO] \n",
      "SENT 64: ['e', 'p', 'o', 'c', 'h', 'e']\n",
      "PRED 64: c h e p p o c h e n\n",
      "PRED SCORE: -0.3189\n",
      "\n",
      "[2021-02-03 01:44:09,365 INFO] \n",
      "SENT 65: ['s', 'c', 'h', 'e', 'l', 'l', 'f', 'i', 's', 'c', 'h']\n",
      "PRED 65: c h e l s c h e l l i s c h e\n",
      "PRED SCORE: -1.9805\n",
      "\n",
      "[2021-02-03 01:44:09,365 INFO] \n",
      "SENT 66: ['k', 'l', 'a', 'n', 'g']\n",
      "PRED 66: k l a n g e l a n g e\n",
      "PRED SCORE: -0.8450\n",
      "\n",
      "[2021-02-03 01:44:09,365 INFO] \n",
      "SENT 67: ['d', 'o', 'k', 'u', 'm', 'e', 'n', 't']\n",
      "PRED 67: k n o m d o m u m e n\n",
      "PRED SCORE: -2.7643\n",
      "\n",
      "[2021-02-03 01:44:09,366 INFO] \n",
      "SENT 68: ['s', 'c', 'h', 'a', 'c', 'h', 't']\n",
      "PRED 68: c h a s c h a c h t e\n",
      "PRED SCORE: -1.2040\n",
      "\n",
      "[2021-02-03 01:44:09,366 INFO] \n",
      "SENT 69: ['s', 'c', 'h', 'r', 'e', 'i']\n",
      "PRED 69: h r e s c h r e i e r\n",
      "PRED SCORE: -1.0593\n",
      "\n",
      "[2021-02-03 01:44:09,366 INFO] \n",
      "SENT 70: ['s', 't', 'r', 'e', 'i', 't']\n",
      "PRED 70: s t r i t s t e r\n",
      "PRED SCORE: -0.1439\n",
      "\n",
      "[2021-02-03 01:44:09,366 INFO] \n",
      "SENT 71: ['f', 'r', 'e', 'u', 'd', 'e']\n",
      "PRED 71: d u d r e u d e u d e u e r\n",
      "PRED SCORE: -0.7733\n",
      "\n",
      "[2021-02-03 01:44:09,366 INFO] \n",
      "SENT 72: ['p', 'a', 'k', 't']\n",
      "PRED 72: k a k p a k t e n\n",
      "PRED SCORE: -0.1432\n",
      "\n",
      "[2021-02-03 01:44:09,367 INFO] \n",
      "SENT 73: ['m', 'i', 'n', 'i', 's', 't', 'e', 'r']\n",
      "PRED 73: s t r i s t i s t e r\n",
      "PRED SCORE: -0.4343\n",
      "\n",
      "[2021-02-03 01:44:09,367 INFO] \n",
      "SENT 74: ['m', 'a', 'n', 'u', 'f', 'a', 'k', 't', 'u', 'r']\n",
      "PRED 74: a u a n t u f t u n t e\n",
      "PRED SCORE: -2.3883\n",
      "\n",
      "[2021-02-03 01:44:09,367 INFO] \n",
      "SENT 75: ['s', 'e', 'n', 'a', 't']\n",
      "PRED 75: s e n s e n a t e n\n",
      "PRED SCORE: -0.0991\n",
      "\n",
      "[2021-02-03 01:44:09,367 INFO] \n",
      "SENT 76: ['p', 'r', 'o', 'z', 'e', 'n', 't']\n",
      "PRED 76: n o z p o p p o n e n\n",
      "PRED SCORE: -3.0524\n",
      "\n",
      "[2021-02-03 01:44:09,368 INFO] \n",
      "SENT 77: ['s', 't', 'a', 'm', 'm', 'b', 'a', 'u', 'm']\n",
      "PRED 77: s t a u m s t a u m s e\n",
      "PRED SCORE: -1.2866\n",
      "\n",
      "[2021-02-03 01:44:09,368 INFO] \n",
      "SENT 78: ['v', 'e', 'r', 'o', 'r', 'd', 'n', 'u', 'n', 'g']\n",
      "PRED 78: d o r v e r o r e r o r e r\n",
      "PRED SCORE: -2.3717\n",
      "\n",
      "[2021-02-03 01:44:09,368 INFO] \n",
      "SENT 79: ['r', 'a', 'h', 'm', 'e', 'n']\n",
      "PRED 79: z n a h m a h m e n\n",
      "PRED SCORE: -1.4612\n",
      "\n",
      "[2021-02-03 01:44:09,368 INFO] \n",
      "SENT 80: ['v', 'o', 'r', 's', 'c', 'h', 'l', 'a', 'g']\n",
      "PRED 80: h l o r s c h l o r s e\n",
      "PRED SCORE: -1.5576\n",
      "\n",
      "[2021-02-03 01:44:09,368 INFO] PRED AVG SCORE: -0.1335, PRED PPL: 1.1428\n",
      "[2021-02-03 01:44:10,250 INFO] Translating shard 0.\n",
      "[2021-02-03 01:44:10,347 INFO] \n",
      "SENT 1: ['MAS', '<s>', 'k', 'a', 'm', 'p', 'f']\n",
      "PRED 1: k a m p f e\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-02-03 01:44:10,347 INFO] \n",
      "SENT 2: ['FEM', '<s>', 'e', 'r', 'i', 'n', 'n', 'e', 'r', 'u', 'n', 'g']\n",
      "PRED 2: e r i n n e r i n e n\n",
      "PRED SCORE: -1.5525\n",
      "\n",
      "[2021-02-03 01:44:10,347 INFO] \n",
      "SENT 3: ['MAS', '<s>', 'k', 'a', 't', 'h', 'o', 'l', 'i', 'k']\n",
      "PRED 3: k a t h o i k e\n",
      "PRED SCORE: -0.4456\n",
      "\n",
      "[2021-02-03 01:44:10,348 INFO] \n",
      "SENT 4: ['FEM', '<s>', 's', 'e', 'e', 'm', 'e', 'i', 'l', 'e']\n",
      "PRED 4: s e e m e i l e n\n",
      "PRED SCORE: -0.0488\n",
      "\n",
      "[2021-02-03 01:44:10,348 INFO] \n",
      "SENT 5: ['NTR', '<s>', 'a', 'n', 'g', 'e', 'b', 'o', 't']\n",
      "PRED 5: a n g e b e b o t e n\n",
      "PRED SCORE: -0.7587\n",
      "\n",
      "[2021-02-03 01:44:10,348 INFO] \n",
      "SENT 6: ['MAS', '<s>', 'f', 'o', 't', 'o']\n",
      "PRED 6: f o t o\n",
      "PRED SCORE: -0.0899\n",
      "\n",
      "[2021-02-03 01:44:10,348 INFO] \n",
      "SENT 7: ['MAS', '<s>', 'v', 'e', 'r', 's', 'u', 'c', 'h']\n",
      "PRED 7: v e r s u c h e\n",
      "PRED SCORE: -0.0103\n",
      "\n",
      "[2021-02-03 01:44:10,348 INFO] \n",
      "SENT 8: ['FEM', '<s>', 'b', 'e', 't', 'r', 'a', 'c', 'h', 't', 'u', 'n', 'g']\n",
      "PRED 8: b e t r a c h t u n g e n\n",
      "PRED SCORE: -0.0958\n",
      "\n",
      "[2021-02-03 01:44:10,349 INFO] \n",
      "SENT 9: ['FEM', '<s>', 's', 'p', 'u', 'r']\n",
      "PRED 9: s p u r u s\n",
      "PRED SCORE: -0.0226\n",
      "\n",
      "[2021-02-03 01:44:10,349 INFO] \n",
      "SENT 10: ['MAS', '<s>', 'd', 'i', 'e', 'n', 's', 't']\n",
      "PRED 10: d i e n s t e\n",
      "PRED SCORE: -0.0016\n",
      "\n",
      "[2021-02-03 01:44:10,349 INFO] \n",
      "SENT 11: ['MAS', '<s>', 'i', 'n', 't', 'e', 'r', 'e', 's', 's', 'e', 'n', 't']\n",
      "PRED 11: i n t e r e r t e n\n",
      "PRED SCORE: -0.2551\n",
      "\n",
      "[2021-02-03 01:44:10,349 INFO] \n",
      "SENT 12: ['NTR', '<s>', 'd', 'o', 'k', 'u', 'm', 'e', 'n', 't']\n",
      "PRED 12: d o k u n t e\n",
      "PRED SCORE: -0.1134\n",
      "\n",
      "[2021-02-03 01:44:10,349 INFO] \n",
      "SENT 13: ['NTR', '<s>', 'p', 'a', 'p', 'i', 'e', 'r']\n",
      "PRED 13: p a p i e r\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-02-03 01:44:10,350 INFO] \n",
      "SENT 14: ['MAS', '<s>', 'v', 'o', 'r', 's', 'c', 'h', 'l', 'a', 'g']\n",
      "PRED 14: v o r s c h l a g e\n",
      "PRED SCORE: -0.0044\n",
      "\n",
      "[2021-02-03 01:44:10,350 INFO] \n",
      "SENT 15: ['MAS', '<s>', 'm', 'o', 'n', 'o', 'p', 'o', 'l']\n",
      "PRED 15: m o n o o p e\n",
      "PRED SCORE: -1.1398\n",
      "\n",
      "[2021-02-03 01:44:10,350 INFO] \n",
      "SENT 16: ['FEM', '<s>', 'l', 'a', 'g', 'e']\n",
      "PRED 16: l a g e n\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-02-03 01:44:10,350 INFO] \n",
      "SENT 17: ['MAS', '<s>', 'a', 'p', 'p', 'a', 'r', 'a', 't']\n",
      "PRED 17: a p p a r a t e n\n",
      "PRED SCORE: -0.1040\n",
      "\n",
      "[2021-02-03 01:44:10,350 INFO] \n",
      "SENT 18: ['NTR', '<s>', 't', 'u', 'r', 'n', 'i', 'e', 'r']\n",
      "PRED 18: t u r n i e r\n",
      "PRED SCORE: -0.0120\n",
      "\n",
      "[2021-02-03 01:44:10,351 INFO] \n",
      "SENT 19: ['MAS', '<s>', 'w', 'e', 'r', 't']\n",
      "PRED 19: w e r t e\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-02-03 01:44:10,351 INFO] \n",
      "SENT 20: ['MAS', '<s>', 'a', 'n', 'z', 'u', 'g']\n",
      "PRED 20: a n z u g e\n",
      "PRED SCORE: -0.0024\n",
      "\n",
      "[2021-02-03 01:44:10,351 INFO] \n",
      "SENT 21: ['MAS', '<s>', 'p', 'h', 'y', 's', 'i', 'k', 'e', 'r']\n",
      "PRED 21: p h y s i k e r\n",
      "PRED SCORE: -0.5784\n",
      "\n",
      "[2021-02-03 01:44:10,351 INFO] \n",
      "SENT 22: ['MAS', '<s>', 'l', 'e', 'n', 'k', 'e', 'r']\n",
      "PRED 22: l e n k e r\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-02-03 01:44:10,351 INFO] \n",
      "SENT 23: ['FEM', '<s>', 'a', 'b', 's', 'c', 'h', 'r', 'e', 'i', 'b', 'u', 'n', 'g']\n",
      "PRED 23: a b s c h r e i b e n\n",
      "PRED SCORE: -0.0615\n",
      "\n",
      "[2021-02-03 01:44:10,351 INFO] \n",
      "SENT 24: ['MAS', '<s>', 'j', 'u', 'l', 'i']\n",
      "PRED 24: j u l i e\n",
      "PRED SCORE: -0.4024\n",
      "\n",
      "[2021-02-03 01:44:10,352 INFO] \n",
      "SENT 25: ['FEM', '<s>', 'n', 'a', 'c', 'h', 'n', 'a', 'h', 'm', 'e']\n",
      "PRED 25: n a c h n a h m e n\n",
      "PRED SCORE: -0.0092\n",
      "\n",
      "[2021-02-03 01:44:10,352 INFO] \n",
      "SENT 26: ['MAS', '<s>', 'v', 'o', 'r', 's', 't', 'a', 'n', 'd']\n",
      "PRED 26: v o r s t a n d e\n",
      "PRED SCORE: -0.2753\n",
      "\n",
      "[2021-02-03 01:44:10,352 INFO] \n",
      "SENT 27: ['NTR', '<s>', 'z', 'i', 'e', 'l']\n",
      "PRED 27: z i e l e\n",
      "PRED SCORE: -0.0098\n",
      "\n",
      "[2021-02-03 01:44:10,352 INFO] \n",
      "SENT 28: ['FEM', '<s>', 't', 'a', 's', 't', 'e']\n",
      "PRED 28: t a s t e n\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-02-03 01:44:10,352 INFO] \n",
      "SENT 29: ['FEM', '<s>', 'r', 'e', 'p', 'u', 'b', 'l', 'i', 'k']\n",
      "PRED 29: r e p b i k l i k e n\n",
      "PRED SCORE: -1.6411\n",
      "\n",
      "[2021-02-03 01:44:10,353 INFO] \n",
      "SENT 30: ['MAS', '<s>', 'b', 'e', 's', 't', 'a', 'n', 'd', 't', 'e', 'i', 'l']\n",
      "PRED 30: b e s t a n d e i l e\n",
      "PRED SCORE: -0.9736\n",
      "\n",
      "[2021-02-03 01:44:10,450 INFO] \n",
      "SENT 31: ['FEM', '<s>', 'l', 'i', 't', 'e', 'r', 'a', 't', 'u', 'r']\n",
      "PRED 31: l i t e r a t e r\n",
      "PRED SCORE: -0.1763\n",
      "\n",
      "[2021-02-03 01:44:10,450 INFO] \n",
      "SENT 32: ['MAS', '<s>', 'a', 'u', 's', 'w', 'e', 'g']\n",
      "PRED 32: a u s e g e g\n",
      "PRED SCORE: -0.5767\n",
      "\n",
      "[2021-02-03 01:44:10,450 INFO] \n",
      "SENT 33: ['MAS', '<s>', 'f', 'l', 'u', 'g', 'p', 'l', 'a', 't', 'z']\n",
      "PRED 33: f l u g p l u t z e\n",
      "PRED SCORE: -0.0623\n",
      "\n",
      "[2021-02-03 01:44:10,451 INFO] \n",
      "SENT 34: ['MAS', '<s>', 'm', 'o', 'h', 'r']\n",
      "PRED 34: m o h r e\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-02-03 01:44:10,451 INFO] \n",
      "SENT 35: ['MAS', '<s>', 'b', 'e', 'i', 'f', 'a', 'h', 'r', 'e', 'r']\n",
      "PRED 35: b e i f r e r\n",
      "PRED SCORE: -0.0069\n",
      "\n",
      "[2021-02-03 01:44:10,451 INFO] \n",
      "SENT 36: ['MAS', '<s>', 'a', 'r', 'b', 'e', 'i', 't', 'g', 'e', 'b', 'e', 'r']\n",
      "PRED 36: a r b e i t g e r\n",
      "PRED SCORE: -0.0336\n",
      "\n",
      "[2021-02-03 01:44:10,451 INFO] \n",
      "SENT 37: ['FEM', '<s>', 'e', 'n', 't', 's', 'c', 'h', 'e', 'i', 'd', 'u', 'n', 'g']\n",
      "PRED 37: e n t s c h e n g e i d e r\n",
      "PRED SCORE: -1.5143\n",
      "\n",
      "[2021-02-03 01:44:10,451 INFO] \n",
      "SENT 38: ['MAS', '<s>', 'z', 'e', 'i', 't', 'r', 'a', 'u', 'm']\n",
      "PRED 38: z e i t r a u e\n",
      "PRED SCORE: -0.1012\n",
      "\n",
      "[2021-02-03 01:44:10,452 INFO] \n",
      "SENT 39: ['MAS', '<s>', 's', 'p', 'i', 't', 'z', 'e', 'n', 'r', 'e', 'i', 't', 'e', 'r']\n",
      "PRED 39: s p i t z e r e i t e r\n",
      "PRED SCORE: -0.0116\n",
      "\n",
      "[2021-02-03 01:44:10,452 INFO] \n",
      "SENT 40: ['MAS', '<s>', 'f', 'i', 's', 'k', 'u', 's']\n",
      "PRED 40: f i s k u s\n",
      "PRED SCORE: -0.2690\n",
      "\n",
      "[2021-02-03 01:44:10,452 INFO] \n",
      "SENT 41: ['NTR', '<s>', 'h', 'o', 't', 'e', 'l']\n",
      "PRED 41: h o t e l\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-02-03 01:44:10,452 INFO] \n",
      "SENT 42: ['FEM', '<s>', 'l', 'e', 'b', 'e', 'n', 's', 'w', 'e', 'i', 's', 'e']\n",
      "PRED 42: l e b e n s e i s e n\n",
      "PRED SCORE: -0.0417\n",
      "\n",
      "[2021-02-03 01:44:10,452 INFO] \n",
      "SENT 43: ['MAS', '<s>', 's', 'o', 'h', 'n']\n",
      "PRED 43: s o h n e\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-02-03 01:44:10,453 INFO] \n",
      "SENT 44: ['FEM', '<s>', 's', 'i', 'c', 'h', 'e', 'r', 'h', 'e', 'i', 't']\n",
      "PRED 44: s i c h e r h e i t e n\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-02-03 01:44:10,453 INFO] \n",
      "SENT 45: ['MAS', '<s>', 'u', 'r', 's', 'p', 'r', 'u', 'n', 'g']\n",
      "PRED 45: u r s p r u n g e\n",
      "PRED SCORE: -0.0049\n",
      "\n",
      "[2021-02-03 01:44:10,453 INFO] \n",
      "SENT 46: ['MAS', '<s>', 'd', 'i', 'e', 'n', 's', 't', 'a', 'g']\n",
      "PRED 46: d i e n s t a g e\n",
      "PRED SCORE: -0.3025\n",
      "\n",
      "[2021-02-03 01:44:10,453 INFO] \n",
      "SENT 47: ['FEM', '<s>', 'r', 'e', 'd', 'e']\n",
      "PRED 47: r e d e n\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-02-03 01:44:10,453 INFO] \n",
      "SENT 48: ['FEM', '<s>', 's', 'i', 'e', 'g']\n",
      "PRED 48: s i e g e n\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-02-03 01:44:10,454 INFO] \n",
      "SENT 49: ['MAS', '<s>', 'a', 'u', 'f', 'z', 'u', 'g']\n",
      "PRED 49: a u f z u g e\n",
      "PRED SCORE: -0.0137\n",
      "\n",
      "[2021-02-03 01:44:10,454 INFO] \n",
      "SENT 50: ['NTR', '<s>', 'g', 'e', 's', 'e', 't', 'z']\n",
      "PRED 50: g e s e t z e\n",
      "PRED SCORE: -0.0079\n",
      "\n",
      "[2021-02-03 01:44:10,454 INFO] \n",
      "SENT 51: ['FEM', '<s>', 'w', 'i', 'r', 'k', 'u', 'n', 'g']\n",
      "PRED 51: w i r k u n g e n\n",
      "PRED SCORE: -0.1169\n",
      "\n",
      "[2021-02-03 01:44:10,454 INFO] \n",
      "SENT 52: ['NTR', '<s>', 'g', 'r', 'u', 'n', 'd', 'g', 'e', 's', 'e', 't', 'z']\n",
      "PRED 52: g r u n d e t z e t z e\n",
      "PRED SCORE: -0.4625\n",
      "\n",
      "[2021-02-03 01:44:10,454 INFO] \n",
      "SENT 53: ['FEM', '<s>', 'r', 'o', 'h', 'r']\n",
      "PRED 53: r o h r e n\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-02-03 01:44:10,455 INFO] \n",
      "SENT 54: ['NTR', '<s>', 'e', 'r', 'z', 'e', 'u', 'g', 'n', 'i', 's']\n",
      "PRED 54: e r z e u i s e\n",
      "PRED SCORE: -2.1685\n",
      "\n",
      "[2021-02-03 01:44:10,455 INFO] \n",
      "SENT 55: ['FEM', '<s>', 'v', 'e', 'r', 'w', 'a', 'l', 't', 'u', 'n', 'g']\n",
      "PRED 55: v e r w a l t u n g e n\n",
      "PRED SCORE: -0.0138\n",
      "\n",
      "[2021-02-03 01:44:10,455 INFO] \n",
      "SENT 56: ['FEM', '<s>', 'r', 'e', 'i', 'c', 'h', 'w', 'e', 'i', 't', 'e']\n",
      "PRED 56: r e i c h e i t e n\n",
      "PRED SCORE: -0.1323\n",
      "\n",
      "[2021-02-03 01:44:10,455 INFO] \n",
      "SENT 57: ['NTR', '<s>', 'q', 'u', 'a', 'n', 't']\n",
      "PRED 57: v u a n t e\n",
      "PRED SCORE: -0.5017\n",
      "\n",
      "[2021-02-03 01:44:10,456 INFO] \n",
      "SENT 58: ['FEM', '<s>', 'p', 'o', 'l', 'i', 't', 'i', 'k']\n",
      "PRED 58: p o l i t i k e n\n",
      "PRED SCORE: -0.0462\n",
      "\n",
      "[2021-02-03 01:44:10,456 INFO] \n",
      "SENT 59: ['FEM', '<s>', 'w', 'e', 's', 't', 'e']\n",
      "PRED 59: w e s t e n\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-02-03 01:44:10,456 INFO] \n",
      "SENT 60: ['FEM', '<s>', 'm', 'e', 'l', 'd', 'u', 'n', 'g']\n",
      "PRED 60: m e l d u n g e n\n",
      "PRED SCORE: -0.0868\n",
      "\n",
      "[2021-02-03 01:44:10,527 INFO] \n",
      "SENT 61: ['MAS', '<s>', 'a', 'n', 't', 'r', 'a', 'g']\n",
      "PRED 61: a n t r a g e\n",
      "PRED SCORE: -0.0020\n",
      "\n",
      "[2021-02-03 01:44:10,527 INFO] \n",
      "SENT 62: ['FEM', '<s>', 'k', 'l', 'i', 'n', 'k', 'e']\n",
      "PRED 62: k l i n k e n\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-02-03 01:44:10,527 INFO] \n",
      "SENT 63: ['MAS', '<s>', 's', 'c', 'h', 'l', 'a', 'm', 'm']\n",
      "PRED 63: s c h l a m m e\n",
      "PRED SCORE: -0.0105\n",
      "\n",
      "[2021-02-03 01:44:10,528 INFO] \n",
      "SENT 64: ['FEM', '<s>', 'a', 'b', 'g', 'a', 'b', 'e']\n",
      "PRED 64: a b g a b e n\n",
      "PRED SCORE: -0.0096\n",
      "\n",
      "[2021-02-03 01:44:10,528 INFO] \n",
      "SENT 65: ['NTR', '<s>', 'a', 'd', 'j', 'e', 'k', 't', 'i', 'v']\n",
      "PRED 65: a d e k t i d e k\n",
      "PRED SCORE: -1.3033\n",
      "\n",
      "[2021-02-03 01:44:10,528 INFO] \n",
      "SENT 66: ['NTR', '<s>', 'g', 'l', 'e', 'i', 'c', 'h', 'g', 'e', 'w', 'i', 'c', 'h', 't']\n",
      "PRED 66: g l e i c h t e w i c h t e\n",
      "PRED SCORE: -0.7699\n",
      "\n",
      "[2021-02-03 01:44:10,528 INFO] \n",
      "SENT 67: ['NTR', '<s>', 'l', 'a', 'g', 'e', 'r']\n",
      "PRED 67: l a g e r\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-02-03 01:44:10,529 INFO] \n",
      "SENT 68: ['FEM', '<s>', 's', 't', 'e', 'i', 'g', 'e', 'r', 'u', 'n', 'g']\n",
      "PRED 68: s t e i g e r u n g e n\n",
      "PRED SCORE: -0.1619\n",
      "\n",
      "[2021-02-03 01:44:10,529 INFO] \n",
      "SENT 69: ['MAS', '<s>', 'b', 'l', 'u', 'f', 'f']\n",
      "PRED 69: b l u f f e\n",
      "PRED SCORE: -0.0020\n",
      "\n",
      "[2021-02-03 01:44:10,529 INFO] \n",
      "SENT 70: ['FEM', '<s>', 'k', 'r', 'o', 'n', 'e']\n",
      "PRED 70: k r o n e n\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-02-03 01:44:10,529 INFO] \n",
      "SENT 71: ['FEM', '<s>', 'z', 'u', 'n', 'e', 'i', 'g', 'u', 'n', 'g']\n",
      "PRED 71: z u n e i g u n g e n\n",
      "PRED SCORE: -0.2558\n",
      "\n",
      "[2021-02-03 01:44:10,529 INFO] \n",
      "SENT 72: ['FEM', '<s>', 'p', 'a', 'r', 't', 'e', 'i']\n",
      "PRED 72: p a r t e n\n",
      "PRED SCORE: -0.2559\n",
      "\n",
      "[2021-02-03 01:44:10,530 INFO] \n",
      "SENT 73: ['MAS', '<s>', 'a', 'n', 's', 't', 'i', 'e', 'g']\n",
      "PRED 73: a n s t i e g e\n",
      "PRED SCORE: -0.0050\n",
      "\n",
      "[2021-02-03 01:44:10,530 INFO] \n",
      "SENT 74: ['MAS', '<s>', 'a', 'f', 'f', 'r', 'o', 'n', 't']\n",
      "PRED 74: a f f r o n t e r\n",
      "PRED SCORE: -0.1187\n",
      "\n",
      "[2021-02-03 01:44:10,530 INFO] \n",
      "SENT 75: ['MAS', '<s>', 'k', 'r', 'o', 'p', 'f']\n",
      "PRED 75: k r o p f e\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-02-03 01:44:10,530 INFO] \n",
      "SENT 76: ['FEM', '<s>', 's', 't', 'r', 'a', 'f', 'e']\n",
      "PRED 76: s t r a f e n\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-02-03 01:44:10,530 INFO] \n",
      "SENT 77: ['NTR', '<s>', 'v', 'e', 'r', 's', 'e', 'h', 'e', 'n']\n",
      "PRED 77: v e r s e n\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-02-03 01:44:10,530 INFO] \n",
      "SENT 78: ['FEM', '<s>', 'b', 'e', 'g', 'e', 'g', 'n', 'u', 'n', 'g']\n",
      "PRED 78: b e g e g n u n g e n\n",
      "PRED SCORE: -0.0408\n",
      "\n",
      "[2021-02-03 01:44:10,531 INFO] \n",
      "SENT 79: ['FEM', '<s>', 'p', 'e', 'i', 't', 's', 'c', 'h', 'e']\n",
      "PRED 79: p e i t s c h e n\n",
      "PRED SCORE: -0.0011\n",
      "\n",
      "[2021-02-03 01:44:10,531 INFO] \n",
      "SENT 80: ['MAS', '<s>', 'g', 'e', 'g', 'e', 'n', 's', 'a', 't', 'z']\n",
      "PRED 80: g e g e n s a t z e\n",
      "PRED SCORE: -0.0794\n",
      "\n",
      "[2021-02-03 01:44:10,531 INFO] PRED AVG SCORE: -0.0281, PRED PPL: 1.0285\n",
      "[2021-02-03 01:44:11,350 INFO] Translating shard 0.\n",
      "[2021-02-03 01:44:11,461 INFO] \n",
      "SENT 1: ['e', 'r', 'i', 'n', 'n', 'e', 'r', 'u', 'n', 'g']\n",
      "PRED 1: r e r i n d e r i n e r\n",
      "PRED SCORE: -2.3532\n",
      "\n",
      "[2021-02-03 01:44:11,461 INFO] \n",
      "SENT 2: ['a', 'n', 's', 't', 'i', 'e', 'g']\n",
      "PRED 2: i e r a n s t i e r e\n",
      "PRED SCORE: -1.7161\n",
      "\n",
      "[2021-02-03 01:44:11,462 INFO] \n",
      "SENT 3: ['a', 'f', 'f', 'r', 'o', 'n', 't']\n",
      "PRED 3: f a f f r o f f e r\n",
      "PRED SCORE: -1.4279\n",
      "\n",
      "[2021-02-03 01:44:11,462 INFO] \n",
      "SENT 4: ['w', 'e', 'r', 't']\n",
      "PRED 4: w w e r t e n\n",
      "PRED SCORE: -0.2415\n",
      "\n",
      "[2021-02-03 01:44:11,462 INFO] \n",
      "SENT 5: ['p', 'o', 'l', 'i', 't', 'i', 'k']\n",
      "PRED 5: i t o i t i k i t i k e r\n",
      "PRED SCORE: -1.9172\n",
      "\n",
      "[2021-02-03 01:44:11,462 INFO] \n",
      "SENT 6: ['a', 'r', 'b', 'e', 'i', 't', 'g', 'e', 'b', 'e', 'r']\n",
      "PRED 6: g a r b e i t g e r b e i t e n\n",
      "PRED SCORE: -1.2552\n",
      "\n",
      "[2021-02-03 01:44:11,462 INFO] \n",
      "SENT 7: ['k', 'r', 'o', 'n', 'e']\n",
      "PRED 7: k r o n r o n e n\n",
      "PRED SCORE: -0.2225\n",
      "\n",
      "[2021-02-03 01:44:11,463 INFO] \n",
      "SENT 8: ['a', 'd', 'j', 'e', 'k', 't', 'i', 'v']\n",
      "PRED 8: z a d d e k d e k i d e k i d e k i t e n\n",
      "PRED SCORE: -4.0405\n",
      "\n",
      "[2021-02-03 01:44:11,463 INFO] \n",
      "SENT 9: ['p', 'h', 'y', 's', 'i', 'k', 'e', 'r']\n",
      "PRED 9: k i k h i k h i k e r\n",
      "PRED SCORE: -2.7459\n",
      "\n",
      "[2021-02-03 01:44:11,463 INFO] \n",
      "SENT 10: ['s', 'e', 'e', 'm', 'e', 'i', 'l', 'e']\n",
      "PRED 10: s s e e m e m e i l e e\n",
      "PRED SCORE: -2.1214\n",
      "\n",
      "[2021-02-03 01:44:11,464 INFO] \n",
      "SENT 11: ['a', 'u', 'f', 'z', 'u', 'g']\n",
      "PRED 11: f a u f u f z e\n",
      "PRED SCORE: -0.4729\n",
      "\n",
      "[2021-02-03 01:44:11,464 INFO] \n",
      "SENT 12: ['f', 'o', 't', 'o']\n",
      "PRED 12: f o t f o t e r\n",
      "PRED SCORE: -0.8908\n",
      "\n",
      "[2021-02-03 01:44:11,464 INFO] \n",
      "SENT 13: ['t', 'u', 'r', 'n', 'i', 'e', 'r']\n",
      "PRED 13: i e r n i e r n i e r\n",
      "PRED SCORE: -0.8048\n",
      "\n",
      "[2021-02-03 01:44:11,464 INFO] \n",
      "SENT 14: ['z', 'i', 'e', 'l']\n",
      "PRED 14: z i e l i e l e\n",
      "PRED SCORE: -0.6729\n",
      "\n",
      "[2021-02-03 01:44:11,465 INFO] \n",
      "SENT 15: ['p', 'e', 'i', 't', 's', 'c', 'h', 'e']\n",
      "PRED 15: t t e i t s c h e n\n",
      "PRED SCORE: -0.7602\n",
      "\n",
      "[2021-02-03 01:44:11,465 INFO] \n",
      "SENT 16: ['s', 'i', 'c', 'h', 'e', 'r', 'h', 'e', 'i', 't']\n",
      "PRED 16: h i c h s i c h e r\n",
      "PRED SCORE: -1.2214\n",
      "\n",
      "[2021-02-03 01:44:11,465 INFO] \n",
      "SENT 17: ['f', 'i', 's', 'k', 'u', 's']\n",
      "PRED 17: f i s f u s k e\n",
      "PRED SCORE: -1.1109\n",
      "\n",
      "[2021-02-03 01:44:11,465 INFO] \n",
      "SENT 18: ['m', 'e', 'l', 'd', 'u', 'n', 'g']\n",
      "PRED 18: u m e l d u n g e\n",
      "PRED SCORE: -1.9222\n",
      "\n",
      "[2021-02-03 01:44:11,465 INFO] \n",
      "SENT 19: ['r', 'e', 'p', 'u', 'b', 'l', 'i', 'k']\n",
      "PRED 19: d r e p r e p p u i e\n",
      "PRED SCORE: -2.6080\n",
      "\n",
      "[2021-02-03 01:44:11,465 INFO] \n",
      "SENT 20: ['m', 'o', 'h', 'r']\n",
      "PRED 20: h o h r o h r e\n",
      "PRED SCORE: -0.6935\n",
      "\n",
      "[2021-02-03 01:44:11,466 INFO] \n",
      "SENT 21: ['v', 'e', 'r', 's', 'e', 'h', 'e', 'n']\n",
      "PRED 21: e e r s e h e r s e n\n",
      "PRED SCORE: -1.7167\n",
      "\n",
      "[2021-02-03 01:44:11,466 INFO] \n",
      "SENT 22: ['a', 'p', 'p', 'a', 'r', 'a', 't']\n",
      "PRED 22: p a r p a r a t e n\n",
      "PRED SCORE: -1.5197\n",
      "\n",
      "[2021-02-03 01:44:11,466 INFO] \n",
      "SENT 23: ['s', 'o', 'h', 'n']\n",
      "PRED 23: h o h s o h n e r\n",
      "PRED SCORE: -0.5068\n",
      "\n",
      "[2021-02-03 01:44:11,466 INFO] \n",
      "SENT 24: ['r', 'o', 'h', 'r']\n",
      "PRED 24: r o h r o h r e r\n",
      "PRED SCORE: -0.6752\n",
      "\n",
      "[2021-02-03 01:44:11,466 INFO] \n",
      "SENT 25: ['z', 'u', 'n', 'e', 'i', 'g', 'u', 'n', 'g']\n",
      "PRED 25: i g z u n g u n g e r\n",
      "PRED SCORE: -2.5809\n",
      "\n",
      "[2021-02-03 01:44:11,467 INFO] \n",
      "SENT 26: ['d', 'i', 'e', 'n', 's', 't']\n",
      "PRED 26: t t i e n s t e\n",
      "PRED SCORE: -1.8048\n",
      "\n",
      "[2021-02-03 01:44:11,467 INFO] \n",
      "SENT 27: ['s', 't', 'r', 'a', 'f', 'e']\n",
      "PRED 27: f t r a f t e r\n",
      "PRED SCORE: -0.0678\n",
      "\n",
      "[2021-02-03 01:44:11,467 INFO] \n",
      "SENT 28: ['a', 'n', 'g', 'e', 'b', 'o', 't']\n",
      "PRED 28: g a n g e r o n g e n\n",
      "PRED SCORE: -2.1126\n",
      "\n",
      "[2021-02-03 01:44:11,467 INFO] \n",
      "SENT 29: ['q', 'u', 'a', 'n', 't']\n",
      "PRED 29: n t u i n t u n t e\n",
      "PRED SCORE: -1.2575\n",
      "\n",
      "[2021-02-03 01:44:11,467 INFO] \n",
      "SENT 30: ['v', 'e', 'r', 's', 'u', 'c', 'h']\n",
      "PRED 30: c a c h e r s u c h e\n",
      "PRED SCORE: -1.4663\n",
      "\n",
      "[2021-02-03 01:44:11,582 INFO] \n",
      "SENT 31: ['p', 'a', 'p', 'i', 'e', 'r']\n",
      "PRED 31: i e r i e r e r\n",
      "PRED SCORE: -0.7975\n",
      "\n",
      "[2021-02-03 01:44:11,582 INFO] \n",
      "SENT 32: ['l', 'i', 't', 'e', 'r', 'a', 't', 'u', 'r']\n",
      "PRED 32: t t i t l i t e r i t e r\n",
      "PRED SCORE: -1.9375\n",
      "\n",
      "[2021-02-03 01:44:11,582 INFO] \n",
      "SENT 33: ['i', 'n', 't', 'e', 'r', 'e', 's', 's', 'e', 'n', 't']\n",
      "PRED 33: i n i n t e r s e r t e\n",
      "PRED SCORE: -2.2232\n",
      "\n",
      "[2021-02-03 01:44:11,582 INFO] \n",
      "SENT 34: ['l', 'a', 'g', 'e', 'r']\n",
      "PRED 34: g l a g l a g e r\n",
      "PRED SCORE: -0.1809\n",
      "\n",
      "[2021-02-03 01:44:11,583 INFO] \n",
      "SENT 35: ['u', 'r', 's', 'p', 'r', 'u', 'n', 'g']\n",
      "PRED 35: r u r s p u r u n g e\n",
      "PRED SCORE: -0.8367\n",
      "\n",
      "[2021-02-03 01:44:11,583 INFO] \n",
      "SENT 36: ['b', 'e', 'i', 'f', 'a', 'h', 'r', 'e', 'r']\n",
      "PRED 36: i h b e i f r e i f e r\n",
      "PRED SCORE: -1.6477\n",
      "\n",
      "[2021-02-03 01:44:11,583 INFO] \n",
      "SENT 37: ['s', 'c', 'h', 'l', 'a', 'm', 'm']\n",
      "PRED 37: l a m s c h l a m m e\n",
      "PRED SCORE: -0.3839\n",
      "\n",
      "[2021-02-03 01:44:11,583 INFO] \n",
      "SENT 38: ['k', 'a', 't', 'h', 'o', 'l', 'i', 'k']\n",
      "PRED 38: k a k a t o t i k o t e n\n",
      "PRED SCORE: -1.8439\n",
      "\n",
      "[2021-02-03 01:44:11,583 INFO] \n",
      "SENT 39: ['g', 'e', 'g', 'e', 'n', 's', 'a', 't', 'z']\n",
      "PRED 39: t a g g e n s a t z e r\n",
      "PRED SCORE: -1.1560\n",
      "\n",
      "[2021-02-03 01:44:11,584 INFO] \n",
      "SENT 40: ['w', 'e', 's', 't', 'e']\n",
      "PRED 40: t w e s t e n\n",
      "PRED SCORE: -0.8837\n",
      "\n",
      "[2021-02-03 01:44:11,584 INFO] \n",
      "SENT 41: ['e', 'r', 'z', 'e', 'u', 'g', 'n', 'i', 's']\n",
      "PRED 41: i s e r z e u i s e u e r\n",
      "PRED SCORE: -2.2235\n",
      "\n",
      "[2021-02-03 01:44:11,584 INFO] \n",
      "SENT 42: ['s', 'p', 'u', 'r']\n",
      "PRED 42: r u r p u r e s\n",
      "PRED SCORE: -1.2570\n",
      "\n",
      "[2021-02-03 01:44:11,584 INFO] \n",
      "SENT 43: ['b', 'e', 't', 'r', 'a', 'c', 'h', 't', 'u', 'n', 'g']\n",
      "PRED 43: u t e t r a c h t u n g e\n",
      "PRED SCORE: -2.5279\n",
      "\n",
      "[2021-02-03 01:44:11,584 INFO] \n",
      "SENT 44: ['b', 'e', 'g', 'e', 'g', 'n', 'u', 'n', 'g']\n",
      "PRED 44: n u r b e g e g e g e g e n\n",
      "PRED SCORE: -1.2920\n",
      "\n",
      "[2021-02-03 01:44:11,585 INFO] \n",
      "SENT 45: ['g', 'e', 's', 'e', 't', 'z']\n",
      "PRED 45: z e s g e t z e r\n",
      "PRED SCORE: -1.3186\n",
      "\n",
      "[2021-02-03 01:44:11,585 INFO] \n",
      "SENT 46: ['k', 'l', 'i', 'n', 'k', 'e']\n",
      "PRED 46: k i n l i n k e r\n",
      "PRED SCORE: -1.3590\n",
      "\n",
      "[2021-02-03 01:44:11,585 INFO] \n",
      "SENT 47: ['g', 'r', 'u', 'n', 'd', 'g', 'e', 's', 'e', 't', 'z']\n",
      "PRED 47: d d g e t r u n d e t e t e\n",
      "PRED SCORE: -3.2959\n",
      "\n",
      "[2021-02-03 01:44:11,585 INFO] \n",
      "SENT 48: ['z', 'e', 'i', 't', 'r', 'a', 'u', 'm']\n",
      "PRED 48: i u z e i t e r a t e n\n",
      "PRED SCORE: -2.1720\n",
      "\n",
      "[2021-02-03 01:44:11,585 INFO] \n",
      "SENT 49: ['s', 'i', 'e', 'g']\n",
      "PRED 49: i e g i e g e g e g i e g e\n",
      "PRED SCORE: -0.4877\n",
      "\n",
      "[2021-02-03 01:44:11,586 INFO] \n",
      "SENT 50: ['v', 'o', 'r', 's', 'c', 'h', 'l', 'a', 'g']\n",
      "PRED 50: l a g s c h l a g h l e\n",
      "PRED SCORE: -1.8965\n",
      "\n",
      "[2021-02-03 01:44:11,586 INFO] \n",
      "SENT 51: ['b', 'e', 's', 't', 'a', 'n', 'd', 't', 'e', 'i', 'l']\n",
      "PRED 51: d t e s t e s e s i l e i l e\n",
      "PRED SCORE: -2.7782\n",
      "\n",
      "[2021-02-03 01:44:11,586 INFO] \n",
      "SENT 52: ['h', 'o', 't', 'e', 'l']\n",
      "PRED 52: l o t h e l o t e l n\n",
      "PRED SCORE: -1.0096\n",
      "\n",
      "[2021-02-03 01:44:11,586 INFO] \n",
      "SENT 53: ['m', 'o', 'n', 'o', 'p', 'o', 'l']\n",
      "PRED 53: p o i n o i l o n e r\n",
      "PRED SCORE: -2.0710\n",
      "\n",
      "[2021-02-03 01:44:11,586 INFO] \n",
      "SENT 54: ['t', 'a', 's', 't', 'e']\n",
      "PRED 54: s t e t t e n\n",
      "PRED SCORE: -0.9018\n",
      "\n",
      "[2021-02-03 01:44:11,587 INFO] \n",
      "SENT 55: ['a', 'b', 's', 'c', 'h', 'r', 'e', 'i', 'b', 'u', 'n', 'g']\n",
      "PRED 55: h r e i b s c h r e i b e i s\n",
      "PRED SCORE: -1.9469\n",
      "\n",
      "[2021-02-03 01:44:11,587 INFO] \n",
      "SENT 56: ['l', 'e', 'n', 'k', 'e', 'r']\n",
      "PRED 56: k l e n l e r\n",
      "PRED SCORE: -0.0338\n",
      "\n",
      "[2021-02-03 01:44:11,587 INFO] \n",
      "SENT 57: ['k', 'r', 'o', 'p', 'f']\n",
      "PRED 57: k r o k r o p f e r\n",
      "PRED SCORE: -0.3553\n",
      "\n",
      "[2021-02-03 01:44:11,587 INFO] \n",
      "SENT 58: ['k', 'a', 'm', 'p', 'f']\n",
      "PRED 58: f a m p f a m f e\n",
      "PRED SCORE: -0.6072\n",
      "\n",
      "[2021-02-03 01:44:11,587 INFO] \n",
      "SENT 59: ['g', 'l', 'e', 'i', 'c', 'h', 'g', 'e', 'w', 'i', 'c', 'h', 't']\n",
      "PRED 59: z t e i c h t e i c h t e i c h t e\n",
      "PRED SCORE: -1.4319\n",
      "\n",
      "[2021-02-03 01:44:11,588 INFO] \n",
      "SENT 60: ['s', 't', 'e', 'i', 'g', 'e', 'r', 'u', 'n', 'g']\n",
      "PRED 60: g t e i g e r u i g e r\n",
      "PRED SCORE: -0.1230\n",
      "\n",
      "[2021-02-03 01:44:11,682 INFO] \n",
      "SENT 61: ['a', 'n', 'z', 'u', 'g']\n",
      "PRED 61: n a n z u n z e\n",
      "PRED SCORE: -1.8008\n",
      "\n",
      "[2021-02-03 01:44:11,682 INFO] \n",
      "SENT 62: ['d', 'i', 'e', 'n', 's', 't', 'a', 'g']\n",
      "PRED 62: g t a e n s t a g e n\n",
      "PRED SCORE: -1.1061\n",
      "\n",
      "[2021-02-03 01:44:11,682 INFO] \n",
      "SENT 63: ['f', 'l', 'u', 'g', 'p', 'l', 'a', 't', 'z']\n",
      "PRED 63: l a t z u t z u p z e\n",
      "PRED SCORE: -1.9586\n",
      "\n",
      "[2021-02-03 01:44:11,682 INFO] \n",
      "SENT 64: ['a', 'u', 's', 'w', 'e', 'g']\n",
      "PRED 64: g a u s w a u s e\n",
      "PRED SCORE: -1.5229\n",
      "\n",
      "[2021-02-03 01:44:11,682 INFO] \n",
      "SENT 65: ['b', 'l', 'u', 'f', 'f']\n",
      "PRED 65: f l u f f u f f e\n",
      "PRED SCORE: -0.0266\n",
      "\n",
      "[2021-02-03 01:44:11,683 INFO] \n",
      "SENT 66: ['v', 'e', 'r', 'w', 'a', 'l', 't', 'u', 'n', 'g']\n",
      "PRED 66: l a l t u n g e r w a l t e\n",
      "PRED SCORE: -1.6971\n",
      "\n",
      "[2021-02-03 01:44:11,683 INFO] \n",
      "SENT 67: ['r', 'e', 'd', 'e']\n",
      "PRED 67: r e d r e e\n",
      "PRED SCORE: -1.6101\n",
      "\n",
      "[2021-02-03 01:44:11,684 INFO] \n",
      "SENT 68: ['p', 'a', 'r', 't', 'e', 'i']\n",
      "PRED 68: i r t e i r t e\n",
      "PRED SCORE: -0.9097\n",
      "\n",
      "[2021-02-03 01:44:11,684 INFO] \n",
      "SENT 69: ['a', 'n', 't', 'r', 'a', 'g']\n",
      "PRED 69: d a n t r a g a n e\n",
      "PRED SCORE: -2.3332\n",
      "\n",
      "[2021-02-03 01:44:11,686 INFO] \n",
      "SENT 70: ['s', 'p', 'i', 't', 'z', 'e', 'n', 'r', 'e', 'i', 't', 'e', 'r']\n",
      "PRED 70: i t z e i t e r e i t e r\n",
      "PRED SCORE: -0.7438\n",
      "\n",
      "[2021-02-03 01:44:11,686 INFO] \n",
      "SENT 71: ['d', 'o', 'k', 'u', 'm', 'e', 'n', 't']\n",
      "PRED 71: k t o k d e n t u k e n\n",
      "PRED SCORE: -1.4993\n",
      "\n",
      "[2021-02-03 01:44:11,686 INFO] \n",
      "SENT 72: ['e', 'n', 't', 's', 'c', 'h', 'e', 'i', 'd', 'u', 'n', 'g']\n",
      "PRED 72: z t e n t s c h e n g e n\n",
      "PRED SCORE: -1.7427\n",
      "\n",
      "[2021-02-03 01:44:11,686 INFO] \n",
      "SENT 73: ['l', 'e', 'b', 'e', 'n', 's', 'w', 'e', 'i', 's', 'e']\n",
      "PRED 73: s s e i l e b e n s e i s e\n",
      "PRED SCORE: -1.9595\n",
      "\n",
      "[2021-02-03 01:44:11,686 INFO] \n",
      "SENT 74: ['r', 'e', 'i', 'c', 'h', 'w', 'e', 'i', 't', 'e']\n",
      "PRED 74: z t e i c h e i t e r\n",
      "PRED SCORE: -1.1105\n",
      "\n",
      "[2021-02-03 01:44:11,687 INFO] \n",
      "SENT 75: ['w', 'i', 'r', 'k', 'u', 'n', 'g']\n",
      "PRED 75: i r k u r k u n g e\n",
      "PRED SCORE: -2.3872\n",
      "\n",
      "[2021-02-03 01:44:11,687 INFO] \n",
      "SENT 76: ['a', 'b', 'g', 'a', 'b', 'e']\n",
      "PRED 76: b a b g a b e n\n",
      "PRED SCORE: -0.7611\n",
      "\n",
      "[2021-02-03 01:44:11,687 INFO] \n",
      "SENT 77: ['l', 'a', 'g', 'e']\n",
      "PRED 77: g l a g l e n\n",
      "PRED SCORE: -1.3067\n",
      "\n",
      "[2021-02-03 01:44:11,687 INFO] \n",
      "SENT 78: ['v', 'o', 'r', 's', 't', 'a', 'n', 'd']\n",
      "PRED 78: n t a n d t a r s a n e\n",
      "PRED SCORE: -1.8808\n",
      "\n",
      "[2021-02-03 01:44:11,687 INFO] \n",
      "SENT 79: ['n', 'a', 'c', 'h', 'n', 'a', 'h', 'm', 'e']\n",
      "PRED 79: h a h r a h m a h m e n\n",
      "PRED SCORE: -1.9316\n",
      "\n",
      "[2021-02-03 01:44:11,688 INFO] \n",
      "SENT 80: ['j', 'u', 'l', 'i']\n",
      "PRED 80: l u u l i g e\n",
      "PRED SCORE: -1.6020\n",
      "\n",
      "[2021-02-03 01:44:11,688 INFO] PRED AVG SCORE: -0.1328, PRED PPL: 1.1420\n",
      "[2021-02-03 01:44:12,573 INFO] Translating shard 0.\n",
      "[2021-02-03 01:44:12,686 INFO] \n",
      "SENT 1: ['MAS', '<s>', 'p', 'r', 'i', 'n', 'z']\n",
      "PRED 1: p r i n z e\n",
      "PRED SCORE: -0.0038\n",
      "\n",
      "[2021-02-03 01:44:12,691 INFO] \n",
      "SENT 2: ['NTR', '<s>', 'a', 'n', 'z', 'e', 'i', 'c', 'h', 'e', 'n']\n",
      "PRED 2: a n z e i c h e n\n",
      "PRED SCORE: -0.0624\n",
      "\n",
      "[2021-02-03 01:44:12,691 INFO] \n",
      "SENT 3: ['NTR', '<s>', 'a', 'l', 'p', 'h', 'a']\n",
      "PRED 3: a l p a h e\n",
      "PRED SCORE: -0.5597\n",
      "\n",
      "[2021-02-03 01:44:12,691 INFO] \n",
      "SENT 4: ['FEM', '<s>', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n']\n",
      "PRED 4: r e m o l l u t t e n\n",
      "PRED SCORE: -2.0443\n",
      "\n",
      "[2021-02-03 01:44:12,691 INFO] \n",
      "SENT 5: ['MAS', '<s>', 'a', 'u', 'f', 'e', 'n', 't', 'h', 'a', 'l', 't']\n",
      "PRED 5: a u f e n t a l t e\n",
      "PRED SCORE: -0.1792\n",
      "\n",
      "[2021-02-03 01:44:12,692 INFO] \n",
      "SENT 6: ['MAS', '<s>', 'a', 'n', 't', 'e', 'i', 'l']\n",
      "PRED 6: a n t e i l e\n",
      "PRED SCORE: -0.0285\n",
      "\n",
      "[2021-02-03 01:44:12,692 INFO] \n",
      "SENT 7: ['FEM', '<s>', 'f', 'r', 'e', 'i', 'z', 'e', 'i', 't']\n",
      "PRED 7: f r e i z e i t e n\n",
      "PRED SCORE: -0.0153\n",
      "\n",
      "[2021-02-03 01:44:12,692 INFO] \n",
      "SENT 8: ['MAS', '<s>', 's', 'c', 'h', 'a', 't', 't', 'e', 'n']\n",
      "PRED 8: s c h a t t e n\n",
      "PRED SCORE: -0.0034\n",
      "\n",
      "[2021-02-03 01:44:12,692 INFO] \n",
      "SENT 9: ['MAS', '<s>', 's', 't', 'e', 'l', 'l', 'v', 'e', 'r', 't', 'r', 'e', 't', 'e', 'r']\n",
      "PRED 9: s t e l l l e t l e r r r e r\n",
      "PRED SCORE: -3.2285\n",
      "\n",
      "[2021-02-03 01:44:12,692 INFO] \n",
      "SENT 10: ['FEM', '<s>', 'g', 'r', 'u', 'n', 'd', 'l', 'a', 'g', 'e']\n",
      "PRED 10: g r u n d a g e n\n",
      "PRED SCORE: -1.3303\n",
      "\n",
      "[2021-02-03 01:44:12,693 INFO] \n",
      "SENT 11: ['FEM', '<s>', 'h', 'e', 'r', 'r', 's', 'c', 'h', 'a', 'f', 't']\n",
      "PRED 11: h e r r s c h a f t e\n",
      "PRED SCORE: -0.2334\n",
      "\n",
      "[2021-02-03 01:44:12,693 INFO] \n",
      "SENT 12: ['NTR', '<s>', 'd', 'a', 's', 'e', 'i', 'n']\n",
      "PRED 12: d a s e i n e\n",
      "PRED SCORE: -0.4161\n",
      "\n",
      "[2021-02-03 01:44:12,693 INFO] \n",
      "SENT 13: ['FEM', '<s>', 'h', 'a', 'l', 't', 'u', 'n', 'g']\n",
      "PRED 13: h a l t u n g e n\n",
      "PRED SCORE: -0.0013\n",
      "\n",
      "[2021-02-03 01:44:12,693 INFO] \n",
      "SENT 14: ['MAS', '<s>', 'm', 'i', 't', 't', 'w', 'o', 'c', 'h']\n",
      "PRED 14: m i t t w o c h e n\n",
      "PRED SCORE: -0.7514\n",
      "\n",
      "[2021-02-03 01:44:12,693 INFO] \n",
      "SENT 15: ['FEM', '<s>', 'a', 'u', 'f', 'f', 'a', 's', 's', 'u', 'n', 'g']\n",
      "PRED 15: a u f f u s s u n g e n\n",
      "PRED SCORE: -0.1381\n",
      "\n",
      "[2021-02-03 01:44:12,693 INFO] \n",
      "SENT 16: ['FEM', '<s>', 'e', 'i', 'n', 'r', 'i', 'c', 'h', 't', 'u', 'n', 'g']\n",
      "PRED 16: e i n r i c h t u n g e n\n",
      "PRED SCORE: -0.0213\n",
      "\n",
      "[2021-02-03 01:44:12,694 INFO] \n",
      "SENT 17: ['NTR', '<s>', 'v', 'o', 't', 'u', 'm']\n",
      "PRED 17: v o t u m e\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-02-03 01:44:12,694 INFO] \n",
      "SENT 18: ['MAS', '<s>', 'k', 'a', 'm', 'p', 'f']\n",
      "PRED 18: k a m p f e\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-02-03 01:44:12,694 INFO] \n",
      "SENT 19: ['MAS', '<s>', 'a', 'n', 'l', 'a', 's', 's']\n",
      "PRED 19: a n l a s s e\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-02-03 01:44:12,694 INFO] \n",
      "SENT 20: ['FEM', '<s>', 'r', 'e', 'p', 'u', 'b', 'l', 'i', 'k']\n",
      "PRED 20: r e p u b l i k e n\n",
      "PRED SCORE: -0.1534\n",
      "\n",
      "[2021-02-03 01:44:12,694 INFO] \n",
      "SENT 21: ['MAS', '<s>', 's', 'c', 'h', 'a', 'u', 'e', 'r']\n",
      "PRED 21: s c h a u e r\n",
      "PRED SCORE: -0.0021\n",
      "\n",
      "[2021-02-03 01:44:12,695 INFO] \n",
      "SENT 22: ['MAS', '<s>', 'p', 'r', 'o', 'z', 'e', 's', 's']\n",
      "PRED 22: p r o z e s s\n",
      "PRED SCORE: -0.3886\n",
      "\n",
      "[2021-02-03 01:44:12,695 INFO] \n",
      "SENT 23: ['MAS', '<s>', 'v', 'e', 'r', 's', 'u', 'c', 'h']\n",
      "PRED 23: v e r s u c h e\n",
      "PRED SCORE: -0.0046\n",
      "\n",
      "[2021-02-03 01:44:12,695 INFO] \n",
      "SENT 24: ['FEM', '<s>', 'a', 'p', 'o', 't', 'h', 'e', 'k', 'e']\n",
      "PRED 24: a p o p h e k e n\n",
      "PRED SCORE: -1.0676\n",
      "\n",
      "[2021-02-03 01:44:12,695 INFO] \n",
      "SENT 25: ['FEM', '<s>', 'e', 'r', 's', 'c', 'h', 'e', 'i', 'n', 'u', 'n', 'g']\n",
      "PRED 25: e r s c h e i n g u n g e n\n",
      "PRED SCORE: -0.0326\n",
      "\n",
      "[2021-02-03 01:44:12,695 INFO] \n",
      "SENT 26: ['MAS', '<s>', 'k', 'o', 'n', 'g', 'r', 'e', 's', 's']\n",
      "PRED 26: k o n g e s s e n\n",
      "PRED SCORE: -0.3318\n",
      "\n",
      "[2021-02-03 01:44:12,696 INFO] \n",
      "SENT 27: ['MAS', '<s>', 'a', 'u', 'f', 'z', 'u', 'g']\n",
      "PRED 27: a u f z u g e\n",
      "PRED SCORE: -0.0034\n",
      "\n",
      "[2021-02-03 01:44:12,696 INFO] \n",
      "SENT 28: ['MAS', '<s>', 'r', 'u', 'f']\n",
      "PRED 28: r u f e\n",
      "PRED SCORE: -0.0874\n",
      "\n",
      "[2021-02-03 01:44:12,696 INFO] \n",
      "SENT 29: ['FEM', '<s>', 'r', 'e', 'c', 'h', 'n', 'u', 'n', 'g']\n",
      "PRED 29: r e c h n u n g e n\n",
      "PRED SCORE: -0.0280\n",
      "\n",
      "[2021-02-03 01:44:12,696 INFO] \n",
      "SENT 30: ['MAS', '<s>', 'h', 'e', 'r', 'r', 's', 'c', 'h', 'e', 'r']\n",
      "PRED 30: h e r r s c h e r\n",
      "PRED SCORE: -0.0026\n",
      "\n",
      "[2021-02-03 01:44:12,795 INFO] \n",
      "SENT 31: ['FEM', '<s>', 'e', 'n', 't', 's', 't', 'e', 'h', 'u', 'n', 'g']\n",
      "PRED 31: e n t s t e h u n g e n\n",
      "PRED SCORE: -0.5973\n",
      "\n",
      "[2021-02-03 01:44:12,796 INFO] \n",
      "SENT 32: ['MAS', '<s>', 'u', 'r', 'l', 'a', 'u', 'b']\n",
      "PRED 32: u r l a u b e\n",
      "PRED SCORE: -0.0046\n",
      "\n",
      "[2021-02-03 01:44:12,796 INFO] \n",
      "SENT 33: ['FEM', '<s>', 'r', 'e', 'i', 'h', 'e']\n",
      "PRED 33: r e i h e n\n",
      "PRED SCORE: -0.0141\n",
      "\n",
      "[2021-02-03 01:44:12,796 INFO] \n",
      "SENT 34: ['MAS', '<s>', 'f', 'e', 'i', 'n', 'd']\n",
      "PRED 34: f e i n d e\n",
      "PRED SCORE: -0.0323\n",
      "\n",
      "[2021-02-03 01:44:12,796 INFO] \n",
      "SENT 35: ['FEM', '<s>', 's', 'p', 'a', 'n', 'n', 'u', 'n', 'g']\n",
      "PRED 35: s p p a n n g e n\n",
      "PRED SCORE: -0.6542\n",
      "\n",
      "[2021-02-03 01:44:12,796 INFO] \n",
      "SENT 36: ['MAS', '<s>', 'e', 'r', 'z', 'b', 'i', 's', 'c', 'h', 'o', 'f']\n",
      "PRED 36: e r z i s c h o f e\n",
      "PRED SCORE: -0.0901\n",
      "\n",
      "[2021-02-03 01:44:12,797 INFO] \n",
      "SENT 37: ['MAS', '<s>', 'k', 'o', 'r', 'r', 'e', 's', 'p', 'o', 'n', 'd', 'e', 'n', 't']\n",
      "PRED 37: k o r r e s p o n d e n\n",
      "PRED SCORE: -0.2981\n",
      "\n",
      "[2021-02-03 01:44:12,797 INFO] \n",
      "SENT 38: ['NTR', '<s>', 'g', 'e', 's', 'e', 't', 'z']\n",
      "PRED 38: g e s e t z e\n",
      "PRED SCORE: -0.0139\n",
      "\n",
      "[2021-02-03 01:44:12,797 INFO] \n",
      "SENT 39: ['FEM', '<s>', 'o', 'r', 'g', 'a', 'n', 'i', 's', 'a', 't', 'i', 'o', 'n']\n",
      "PRED 39: o r g a t i s a t i s e\n",
      "PRED SCORE: -0.9603\n",
      "\n",
      "[2021-02-03 01:44:12,797 INFO] \n",
      "SENT 40: ['MAS', '<s>', 'k', 'o', 'm', 'p', 'o', 'n', 'i', 's', 't']\n",
      "PRED 40: k o m p o n t o n t e\n",
      "PRED SCORE: -1.4429\n",
      "\n",
      "[2021-02-03 01:44:12,797 INFO] \n",
      "SENT 41: ['NTR', '<s>', 'b', 'e', 'i', 's', 'p', 'i', 'e', 'l']\n",
      "PRED 41: b e i s p i e l e\n",
      "PRED SCORE: -0.0787\n",
      "\n",
      "[2021-02-03 01:44:12,798 INFO] \n",
      "SENT 42: ['FEM', '<s>', 'f', 'o', 'r', 's', 'c', 'h', 'u', 'n', 'g']\n",
      "PRED 42: f o r s c h u n g e n\n",
      "PRED SCORE: -0.0060\n",
      "\n",
      "[2021-02-03 01:44:12,798 INFO] \n",
      "SENT 43: ['FEM', '<s>', 't', 'a', 't', 's', 'a', 'c', 'h', 'e']\n",
      "PRED 43: t a t s a c h e n\n",
      "PRED SCORE: -0.0013\n",
      "\n",
      "[2021-02-03 01:44:12,798 INFO] \n",
      "SENT 44: ['MAS', '<s>', 'm', 'a', 'n', 't', 'e', 'l']\n",
      "PRED 44: m a n t e l\n",
      "PRED SCORE: -0.0044\n",
      "\n",
      "[2021-02-03 01:44:12,798 INFO] \n",
      "SENT 45: ['FEM', '<s>', 'k', 'o', 'r', 'r', 'e', 'k', 't', 'u', 'r']\n",
      "PRED 45: k o r r e k u r r e n\n",
      "PRED SCORE: -0.7650\n",
      "\n",
      "[2021-02-03 01:44:12,799 INFO] \n",
      "SENT 46: ['MAS', '<s>', 'u', 'n', 'f', 'a', 'l', 'l']\n",
      "PRED 46: u n f a l l e\n",
      "PRED SCORE: -0.0046\n",
      "\n",
      "[2021-02-03 01:44:12,799 INFO] \n",
      "SENT 47: ['FEM', '<s>', 'm', 'a', 's', 's', 'a', 'g', 'e']\n",
      "PRED 47: m a s s a g e n\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-02-03 01:44:12,799 INFO] \n",
      "SENT 48: ['FEM', '<s>', 'f', 'a', 's', 's', 'u', 'n', 'g']\n",
      "PRED 48: f a s s u n g e n\n",
      "PRED SCORE: -0.0033\n",
      "\n",
      "[2021-02-03 01:44:12,799 INFO] \n",
      "SENT 49: ['FEM', '<s>', 'b', 'e', 'd', 'e', 'u', 't', 'u', 'n', 'g']\n",
      "PRED 49: b e d e u n g u n g e n\n",
      "PRED SCORE: -0.6947\n",
      "\n",
      "[2021-02-03 01:44:12,799 INFO] \n",
      "SENT 50: ['MAS', '<s>', 'w', 'i', 'n', 't', 'e', 'r']\n",
      "PRED 50: w i n t e r\n",
      "PRED SCORE: -0.0020\n",
      "\n",
      "[2021-02-03 01:44:12,800 INFO] \n",
      "SENT 51: ['NTR', '<s>', 'h', 'e', 'c', 'k']\n",
      "PRED 51: h e c k e\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-02-03 01:44:12,800 INFO] \n",
      "SENT 52: ['FEM', '<s>', 'v', 'e', 'r', 'b', 'e', 's', 's', 'e', 'r', 'u', 'n', 'g']\n",
      "PRED 52: v e r b e s e r s e r n\n",
      "PRED SCORE: -1.2417\n",
      "\n",
      "[2021-02-03 01:44:12,800 INFO] \n",
      "SENT 53: ['MAS', '<s>', 'b', 'e', 's', 't', 'a', 'n', 'd']\n",
      "PRED 53: b e s t a n d e\n",
      "PRED SCORE: -0.0077\n",
      "\n",
      "[2021-02-03 01:44:12,800 INFO] \n",
      "SENT 54: ['FEM', '<s>', 'o', 'b', 'e', 'r', 's', 'c', 'h', 'u', 'l', 'e']\n",
      "PRED 54: o b e r s c h u l e n\n",
      "PRED SCORE: -0.3644\n",
      "\n",
      "[2021-02-03 01:44:12,800 INFO] \n",
      "SENT 55: ['MAS', '<s>', 's', 'p', 'i', 'e', 'l', 'a', 'u', 't', 'o', 'm', 'a', 't']\n",
      "PRED 55: s p p i e l a t t o t e n\n",
      "PRED SCORE: -1.2404\n",
      "\n",
      "[2021-02-03 01:44:12,801 INFO] \n",
      "SENT 56: ['MAS', '<s>', 'v', 'e', 'r', 'l', 'a', 'u', 'f']\n",
      "PRED 56: v e r l a u f e\n",
      "PRED SCORE: -0.0116\n",
      "\n",
      "[2021-02-03 01:44:12,801 INFO] \n",
      "SENT 57: ['MAS', '<s>', 'k', 'o', 'n', 'z', 'e', 'r', 'n']\n",
      "PRED 57: k o n z e r n\n",
      "PRED SCORE: -0.0357\n",
      "\n",
      "[2021-02-03 01:44:12,801 INFO] \n",
      "SENT 58: ['NTR', '<s>', 's', 'c', 'h', 'l', 'o', 's', 's']\n",
      "PRED 58: s c h l o s s e\n",
      "PRED SCORE: -0.0102\n",
      "\n",
      "[2021-02-03 01:44:12,801 INFO] \n",
      "SENT 59: ['MAS', '<s>', 'f', 'l', 'u', 's', 's']\n",
      "PRED 59: f l u s s e\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-02-03 01:44:12,801 INFO] \n",
      "SENT 60: ['FEM', '<s>', 'g', 'e', 'f', 'a', 'h', 'r']\n",
      "PRED 60: g e f a h r e n\n",
      "PRED SCORE: -0.0120\n",
      "\n",
      "[2021-02-03 01:44:12,863 INFO] \n",
      "SENT 61: ['MAS', '<s>', 'b', 'e', 's', 'u', 'c', 'h']\n",
      "PRED 61: b e s u c h e\n",
      "PRED SCORE: -0.0092\n",
      "\n",
      "[2021-02-03 01:44:12,864 INFO] \n",
      "SENT 62: ['NTR', '<s>', 'g', 'a', 's']\n",
      "PRED 62: g a s e\n",
      "PRED SCORE: -0.0052\n",
      "\n",
      "[2021-02-03 01:44:12,864 INFO] \n",
      "SENT 63: ['MAS', '<s>', 't', 'a', 'n', 'z']\n",
      "PRED 63: t a n z e\n",
      "PRED SCORE: -0.0016\n",
      "\n",
      "[2021-02-03 01:44:12,864 INFO] \n",
      "SENT 64: ['NTR', '<s>', 'm', 'e', 't', 'a', 'l', 'l']\n",
      "PRED 64: m e t a l l e\n",
      "PRED SCORE: -0.0035\n",
      "\n",
      "[2021-02-03 01:44:12,864 INFO] \n",
      "SENT 65: ['FEM', '<s>', 'w', 'e', 'n', 'd', 'u', 'n', 'g']\n",
      "PRED 65: w e n d u n g e n\n",
      "PRED SCORE: -0.0031\n",
      "\n",
      "[2021-02-03 01:44:12,864 INFO] \n",
      "SENT 66: ['NTR', '<s>', 'd', 'o', 'p', 'p', 'e', 'l']\n",
      "PRED 66: d o p p e l\n",
      "PRED SCORE: -0.0588\n",
      "\n",
      "[2021-02-03 01:44:12,865 INFO] \n",
      "SENT 67: ['FEM', '<s>', 'm', 'i', 't', 't', 'e']\n",
      "PRED 67: m i t t e n\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-02-03 01:44:12,865 INFO] \n",
      "SENT 68: ['FEM', '<s>', 'a', 't', 'o', 'm', 'b', 'o', 'm', 'b', 'e']\n",
      "PRED 68: a t o m o m b o n e n\n",
      "PRED SCORE: -0.8767\n",
      "\n",
      "[2021-02-03 01:44:12,865 INFO] \n",
      "SENT 69: ['MAS', '<s>', 'j', 'u', 'l', 'i']\n",
      "PRED 69: j u l i e\n",
      "PRED SCORE: -0.0716\n",
      "\n",
      "[2021-02-03 01:44:12,865 INFO] \n",
      "SENT 70: ['FEM', '<s>', 'r', 'e', 'g', 'i', 'e', 'r', 'u', 'n', 'g']\n",
      "PRED 70: r e g i e r u n g e n\n",
      "PRED SCORE: -0.2975\n",
      "\n",
      "[2021-02-03 01:44:12,865 INFO] \n",
      "SENT 71: ['MAS', '<s>', 'a', 'b', 'b', 'a', 'u']\n",
      "PRED 71: a b b a u u s\n",
      "PRED SCORE: -0.6837\n",
      "\n",
      "[2021-02-03 01:44:12,866 INFO] \n",
      "SENT 72: ['MAS', '<s>', 'k', 'a', 'r', 'a', 'b', 'i', 'n', 'e', 'r']\n",
      "PRED 72: k a r a b i n e r\n",
      "PRED SCORE: -0.1863\n",
      "\n",
      "[2021-02-03 01:44:12,866 INFO] \n",
      "SENT 73: ['FEM', '<s>', 'n', 'a', 'r', 'k', 'o', 's', 'e']\n",
      "PRED 73: n a r k o s e n\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-02-03 01:44:12,866 INFO] \n",
      "SENT 74: ['MAS', '<s>', 'k', 'r', 'i', 't', 'i', 'k', 'e', 'r']\n",
      "PRED 74: k r i t i k e r\n",
      "PRED SCORE: -0.0354\n",
      "\n",
      "[2021-02-03 01:44:12,866 INFO] \n",
      "SENT 75: ['FEM', '<s>', 'e', 'i', 'n', 'h', 'e', 'i', 't']\n",
      "PRED 75: e i n h e i t e n\n",
      "PRED SCORE: -0.0156\n",
      "\n",
      "[2021-02-03 01:44:12,866 INFO] \n",
      "SENT 76: ['NTR', '<s>', 'a', 'b', 'e', 'n', 'd', 'e', 's', 's', 'e', 'n']\n",
      "PRED 76: a b e n d e n d e n\n",
      "PRED SCORE: -0.1131\n",
      "\n",
      "[2021-02-03 01:44:12,867 INFO] \n",
      "SENT 77: ['NTR', '<s>', 'b', 'l', 'i', 'c', 'k', 'f', 'e', 'l', 'd']\n",
      "PRED 77: b l i c k e l d e l\n",
      "PRED SCORE: -0.7782\n",
      "\n",
      "[2021-02-03 01:44:12,867 INFO] \n",
      "SENT 78: ['NTR', '<s>', 'c', 'h', 'o', 'r']\n",
      "PRED 78: c h o r e\n",
      "PRED SCORE: -0.0160\n",
      "\n",
      "[2021-02-03 01:44:12,867 INFO] \n",
      "SENT 79: ['MAS', '<s>', 't', 'y', 'p', 'u', 's']\n",
      "PRED 79: t y p u s\n",
      "PRED SCORE: -0.6463\n",
      "\n",
      "[2021-02-03 01:44:12,867 INFO] \n",
      "SENT 80: ['MAS', '<s>', 'm', 'e', 't', 'e', 'r']\n",
      "PRED 80: m e t e r\n",
      "PRED SCORE: -0.0020\n",
      "\n",
      "[2021-02-03 01:44:12,867 INFO] PRED AVG SCORE: -0.0346, PRED PPL: 1.0352\n",
      "[2021-02-03 01:44:13,717 INFO] Translating shard 0.\n",
      "[2021-02-03 01:44:14,000 INFO] \n",
      "SENT 1: ['a', 'n', 'z', 'e', 'i', 'c', 'h', 'e', 'n']\n",
      "PRED 1: t e i c h e n\n",
      "PRED SCORE: -1.6832\n",
      "\n",
      "[2021-02-03 01:44:14,001 INFO] \n",
      "SENT 2: ['h', 'a', 'l', 't', 'u', 'n', 'g']\n",
      "PRED 2: t u l t u l g u n g e n\n",
      "PRED SCORE: -0.1884\n",
      "\n",
      "[2021-02-03 01:44:14,001 INFO] \n",
      "SENT 3: ['u', 'n', 'f', 'a', 'l', 'l']\n",
      "PRED 3: l u u n l l a l l e n\n",
      "PRED SCORE: -1.0683\n",
      "\n",
      "[2021-02-03 01:44:14,001 INFO] \n",
      "SENT 4: ['b', 'e', 'i', 's', 'p', 'i', 'e', 'l']\n",
      "PRED 4: b i m b i s p e l n\n",
      "PRED SCORE: -1.6412\n",
      "\n",
      "[2021-02-03 01:44:14,001 INFO] \n",
      "SENT 5: ['b', 'e', 'd', 'e', 'u', 't', 'u', 'n', 'g']\n",
      "PRED 5: b u b e u n g e u n g e n\n",
      "PRED SCORE: -0.8055\n",
      "\n",
      "[2021-02-03 01:44:14,002 INFO] \n",
      "SENT 6: ['f', 'l', 'u', 's', 's']\n",
      "PRED 6: s f l u s s e\n",
      "PRED SCORE: -1.3568\n",
      "\n",
      "[2021-02-03 01:44:14,002 INFO] \n",
      "SENT 7: ['a', 'u', 'f', 'z', 'u', 'g']\n",
      "PRED 7: z u u g u g l u g e n\n",
      "PRED SCORE: -2.8830\n",
      "\n",
      "[2021-02-03 01:44:14,002 INFO] \n",
      "SENT 8: ['r', 'u', 'f']\n",
      "PRED 8: f r u f u f f e r\n",
      "PRED SCORE: -1.8980\n",
      "\n",
      "[2021-02-03 01:44:14,002 INFO] \n",
      "SENT 9: ['c', 'h', 'o', 'r']\n",
      "PRED 9: h o r h o r o c h e r\n",
      "PRED SCORE: -2.1978\n",
      "\n",
      "[2021-02-03 01:44:14,002 INFO] \n",
      "SENT 10: ['v', 'e', 'r', 'b', 'e', 's', 's', 'e', 'r', 'u', 'n', 'g']\n",
      "PRED 10: s e m v e r b s e r b s e n\n",
      "PRED SCORE: -0.8769\n",
      "\n",
      "[2021-02-03 01:44:14,002 INFO] \n",
      "SENT 11: ['e', 'n', 't', 's', 't', 'e', 'h', 'u', 'n', 'g']\n",
      "PRED 11: t u e e n t s t u n g e n\n",
      "PRED SCORE: -0.5154\n",
      "\n",
      "[2021-02-03 01:44:14,002 INFO] \n",
      "SENT 12: ['s', 't', 'e', 'l', 'l', 'v', 'e', 'r', 't', 'r', 'e', 't', 'e', 'r']\n",
      "PRED 12: t l e m s t e l r e r r r e r n\n",
      "PRED SCORE: -3.1048\n",
      "\n",
      "[2021-02-03 01:44:14,003 INFO] \n",
      "SENT 13: ['h', 'e', 'c', 'k']\n",
      "PRED 13: c k e c k e n\n",
      "PRED SCORE: -0.3398\n",
      "\n",
      "[2021-02-03 01:44:14,003 INFO] \n",
      "SENT 14: ['s', 'p', 'i', 'e', 'l', 'a', 'u', 't', 'o', 'm', 'a', 't']\n",
      "PRED 14: m i e s p a t t o t t o t e n\n",
      "PRED SCORE: -1.9730\n",
      "\n",
      "[2021-02-03 01:44:14,003 INFO] \n",
      "SENT 15: ['r', 'e', 'c', 'h', 'n', 'u', 'n', 'g']\n",
      "PRED 15: g u c h n u n g e n\n",
      "PRED SCORE: -0.8083\n",
      "\n",
      "[2021-02-03 01:44:14,003 INFO] \n",
      "SENT 16: ['j', 'u', 'l', 'i']\n",
      "PRED 16: m u s b i l l e n\n",
      "PRED SCORE: -2.7950\n",
      "\n",
      "[2021-02-03 01:44:14,003 INFO] \n",
      "SENT 17: ['f', 'a', 's', 's', 'u', 'n', 'g']\n",
      "PRED 17: s u s s u n g e n\n",
      "PRED SCORE: -0.1362\n",
      "\n",
      "[2021-02-03 01:44:14,004 INFO] \n",
      "SENT 18: ['a', 'p', 'o', 't', 'h', 'e', 'k', 'e']\n",
      "PRED 18: t e a p o p o t e n\n",
      "PRED SCORE: -0.9174\n",
      "\n",
      "[2021-02-03 01:44:14,004 INFO] \n",
      "SENT 19: ['g', 'r', 'u', 'n', 'd', 'l', 'a', 'g', 'e']\n",
      "PRED 19: d l a g g u l d u n d e n\n",
      "PRED SCORE: -1.7697\n",
      "\n",
      "[2021-02-03 01:44:14,004 INFO] \n",
      "SENT 20: ['m', 'i', 't', 't', 'e']\n",
      "PRED 20: t i t i t e n\n",
      "PRED SCORE: -0.5864\n",
      "\n",
      "[2021-02-03 01:44:14,004 INFO] \n",
      "SENT 21: ['k', 'o', 'n', 'g', 'r', 'e', 's', 's']\n",
      "PRED 21: k r o k o n g o r e n\n",
      "PRED SCORE: -2.6587\n",
      "\n",
      "[2021-02-03 01:44:14,004 INFO] \n",
      "SENT 22: ['r', 'e', 'g', 'i', 'e', 'r', 'u', 'n', 'g']\n",
      "PRED 22: g i e r e g u n g e r\n",
      "PRED SCORE: -2.4288\n",
      "\n",
      "[2021-02-03 01:44:14,005 INFO] \n",
      "SENT 23: ['r', 'e', 'i', 'h', 'e']\n",
      "PRED 23: h r e i h e n\n",
      "PRED SCORE: -1.5766\n",
      "\n",
      "[2021-02-03 01:44:14,005 INFO] \n",
      "SENT 24: ['n', 'a', 'r', 'k', 'o', 's', 'e']\n",
      "PRED 24: o n k a r k o r k e n\n",
      "PRED SCORE: -1.1823\n",
      "\n",
      "[2021-02-03 01:44:14,005 INFO] \n",
      "SENT 25: ['k', 'o', 'm', 'p', 'o', 'n', 'i', 's', 't']\n",
      "PRED 25: t t o k o l o n i s t e n\n",
      "PRED SCORE: -2.1024\n",
      "\n",
      "[2021-02-03 01:44:14,005 INFO] \n",
      "SENT 26: ['e', 'r', 'z', 'b', 'i', 's', 'c', 'h', 'o', 'f']\n",
      "PRED 26: b i s c h o f z e r\n",
      "PRED SCORE: -1.3890\n",
      "\n",
      "[2021-02-03 01:44:14,005 INFO] \n",
      "SENT 27: ['a', 'l', 'p', 'h', 'a']\n",
      "PRED 27: o a a l p h a l p e n\n",
      "PRED SCORE: -0.6978\n",
      "\n",
      "[2021-02-03 01:44:14,005 INFO] \n",
      "SENT 28: ['a', 'n', 'l', 'a', 's', 's']\n",
      "PRED 28: s s a m l a s s e\n",
      "PRED SCORE: -0.9746\n",
      "\n",
      "[2021-02-03 01:44:14,005 INFO] \n",
      "SENT 29: ['t', 'a', 't', 's', 'a', 'c', 'h', 'e']\n",
      "PRED 29: t a c h a t s a c h e n\n",
      "PRED SCORE: -1.0840\n",
      "\n",
      "[2021-02-03 01:44:14,005 INFO] \n",
      "SENT 30: ['k', 'o', 'n', 'z', 'e', 'r', 'n']\n",
      "PRED 30: z o k o n z o n z e n\n",
      "PRED SCORE: -1.4378\n",
      "\n",
      "[2021-02-03 01:44:14,112 INFO] \n",
      "SENT 31: ['e', 'i', 'n', 'h', 'e', 'i', 't']\n",
      "PRED 31: t e i n h e i t h e n\n",
      "PRED SCORE: -0.4603\n",
      "\n",
      "[2021-02-03 01:44:14,112 INFO] \n",
      "SENT 32: ['f', 'r', 'e', 'i', 'z', 'e', 'i', 't']\n",
      "PRED 32: t f i t z e i t e n\n",
      "PRED SCORE: -1.4813\n",
      "\n",
      "[2021-02-03 01:44:14,113 INFO] \n",
      "SENT 33: ['f', 'e', 'i', 'n', 'd']\n",
      "PRED 33: d f i f d e i n d e n\n",
      "PRED SCORE: -0.5739\n",
      "\n",
      "[2021-02-03 01:44:14,113 INFO] \n",
      "SENT 34: ['a', 'b', 'b', 'a', 'u']\n",
      "PRED 34: b a b b a b b e n\n",
      "PRED SCORE: -0.2623\n",
      "\n",
      "[2021-02-03 01:44:14,113 INFO] \n",
      "SENT 35: ['w', 'i', 'n', 't', 'e', 'r']\n",
      "PRED 35: t i n w i n t e r n\n",
      "PRED SCORE: -0.4353\n",
      "\n",
      "[2021-02-03 01:44:14,113 INFO] \n",
      "SENT 36: ['u', 'r', 'l', 'a', 'u', 'b']\n",
      "PRED 36: b u u l l u r l a u b e n\n",
      "PRED SCORE: -1.0036\n",
      "\n",
      "[2021-02-03 01:44:14,113 INFO] \n",
      "SENT 37: ['k', 'o', 'r', 'r', 'e', 'k', 't', 'u', 'r']\n",
      "PRED 37: t u k o r r e k u r r e n\n",
      "PRED SCORE: -0.5824\n",
      "\n",
      "[2021-02-03 01:44:14,113 INFO] \n",
      "SENT 38: ['g', 'e', 'f', 'a', 'h', 'r']\n",
      "PRED 38: g r a h e f r a h r e n\n",
      "PRED SCORE: -0.6549\n",
      "\n",
      "[2021-02-03 01:44:14,113 INFO] \n",
      "SENT 39: ['w', 'e', 'n', 'd', 'u', 'n', 'g']\n",
      "PRED 39: d w e n d u n g e n\n",
      "PRED SCORE: -0.6827\n",
      "\n",
      "[2021-02-03 01:44:14,113 INFO] \n",
      "SENT 40: ['k', 'r', 'i', 't', 'i', 'k', 'e', 'r']\n",
      "PRED 40: t i k r i k e r i k e n\n",
      "PRED SCORE: -0.4955\n",
      "\n",
      "[2021-02-03 01:44:14,113 INFO] \n",
      "SENT 41: ['s', 'p', 'a', 'n', 'n', 'u', 'n', 'g']\n",
      "PRED 41: g u s p u n g u n g e n\n",
      "PRED SCORE: -1.6076\n",
      "\n",
      "[2021-02-03 01:44:14,113 INFO] \n",
      "SENT 42: ['p', 'r', 'i', 'n', 'z']\n",
      "PRED 42: z p i p p o n z e n\n",
      "PRED SCORE: -1.4598\n",
      "\n",
      "[2021-02-03 01:44:14,114 INFO] \n",
      "SENT 43: ['b', 'l', 'i', 'c', 'k', 'f', 'e', 'l', 'd']\n",
      "PRED 43: d l i c k k e l d e l n\n",
      "PRED SCORE: -1.8379\n",
      "\n",
      "[2021-02-03 01:44:14,114 INFO] \n",
      "SENT 44: ['v', 'e', 'r', 'l', 'a', 'u', 'f']\n",
      "PRED 44: v l a u f l u f l e n\n",
      "PRED SCORE: -2.4477\n",
      "\n",
      "[2021-02-03 01:44:14,114 INFO] \n",
      "SENT 45: ['s', 'c', 'h', 'l', 'o', 's', 's']\n",
      "PRED 45: s c h l o s c h l o s\n",
      "PRED SCORE: -1.2304\n",
      "\n",
      "[2021-02-03 01:44:14,114 INFO] \n",
      "SENT 46: ['a', 'n', 't', 'e', 'i', 'l']\n",
      "PRED 46: t l a r t a n t e n\n",
      "PRED SCORE: -1.7458\n",
      "\n",
      "[2021-02-03 01:44:14,114 INFO] \n",
      "SENT 47: ['m', 'e', 't', 'a', 'l', 'l']\n",
      "PRED 47: m l a l l u l l e n\n",
      "PRED SCORE: -1.0285\n",
      "\n",
      "[2021-02-03 01:44:14,114 INFO] \n",
      "SENT 48: ['r', 'e', 'p', 'u', 'b', 'l', 'i', 'k']\n",
      "PRED 48: p l i k l e p p e n\n",
      "PRED SCORE: -2.5455\n",
      "\n",
      "[2021-02-03 01:44:14,114 INFO] \n",
      "SENT 49: ['s', 'c', 'h', 'a', 'u', 'e', 'r']\n",
      "PRED 49: s c h a u e r s c h a u e r\n",
      "PRED SCORE: -0.9161\n",
      "\n",
      "[2021-02-03 01:44:14,114 INFO] \n",
      "SENT 50: ['e', 'r', 's', 'c', 'h', 'e', 'i', 'n', 'u', 'n', 'g']\n",
      "PRED 50: c h e i n s c h e i n g e n\n",
      "PRED SCORE: -0.9661\n",
      "\n",
      "[2021-02-03 01:44:14,114 INFO] \n",
      "SENT 51: ['g', 'a', 's']\n",
      "PRED 51: g u g g a s e\n",
      "PRED SCORE: -0.3507\n",
      "\n",
      "[2021-02-03 01:44:14,114 INFO] \n",
      "SENT 52: ['d', 'a', 's', 'e', 'i', 'n']\n",
      "PRED 52: d a e a s e i n e n\n",
      "PRED SCORE: -0.9226\n",
      "\n",
      "[2021-02-03 01:44:14,115 INFO] \n",
      "SENT 53: ['v', 'e', 'r', 's', 'u', 'c', 'h']\n",
      "PRED 53: s u c h e r s u c h e r\n",
      "PRED SCORE: -2.4701\n",
      "\n",
      "[2021-02-03 01:44:14,115 INFO] \n",
      "SENT 54: ['f', 'o', 'r', 's', 'c', 'h', 'u', 'n', 'g']\n",
      "PRED 54: c h u r s c h u n g e r\n",
      "PRED SCORE: -1.3967\n",
      "\n",
      "[2021-02-03 01:44:14,115 INFO] \n",
      "SENT 55: ['v', 'o', 't', 'u', 'm']\n",
      "PRED 55: m u m v o t u m e n\n",
      "PRED SCORE: -1.5641\n",
      "\n",
      "[2021-02-03 01:44:14,115 INFO] \n",
      "SENT 56: ['m', 'a', 'n', 't', 'e', 'l']\n",
      "PRED 56: t e m a n t a n t e l n\n",
      "PRED SCORE: -1.7794\n",
      "\n",
      "[2021-02-03 01:44:14,115 INFO] \n",
      "SENT 57: ['h', 'e', 'r', 'r', 's', 'c', 'h', 'e', 'r']\n",
      "PRED 57: h u h e r s c h e r\n",
      "PRED SCORE: -2.3103\n",
      "\n",
      "[2021-02-03 01:44:14,115 INFO] \n",
      "SENT 58: ['p', 'r', 'o', 'z', 'e', 's', 's']\n",
      "PRED 58: z p o p p o l o z e n\n",
      "PRED SCORE: -1.0895\n",
      "\n",
      "[2021-02-03 01:44:14,115 INFO] \n",
      "SENT 59: ['t', 'a', 'n', 'z']\n",
      "PRED 59: t a n z a n z e n\n",
      "PRED SCORE: -0.4743\n",
      "\n",
      "[2021-02-03 01:44:14,115 INFO] \n",
      "SENT 60: ['d', 'o', 'p', 'p', 'e', 'l']\n",
      "PRED 60: d p o p d o p p e l n\n",
      "PRED SCORE: -0.0397\n",
      "\n",
      "[2021-02-03 01:44:14,197 INFO] \n",
      "SENT 61: ['t', 'y', 'p', 'u', 's']\n",
      "PRED 61: p u p t o s t e n\n",
      "PRED SCORE: -1.5841\n",
      "\n",
      "[2021-02-03 01:44:14,197 INFO] \n",
      "SENT 62: ['s', 'c', 'h', 'a', 't', 't', 'e', 'n']\n",
      "PRED 62: t a t s c h a t t e n\n",
      "PRED SCORE: -0.0456\n",
      "\n",
      "[2021-02-03 01:44:14,197 INFO] \n",
      "SENT 63: ['k', 'o', 'r', 'r', 'e', 's', 'p', 'o', 'n', 'd', 'e', 'n', 't']\n",
      "PRED 63: d p o k o r e n d o r o n d e n\n",
      "PRED SCORE: -2.6392\n",
      "\n",
      "[2021-02-03 01:44:14,197 INFO] \n",
      "SENT 64: ['a', 'u', 'f', 'e', 'n', 't', 'h', 'a', 'l', 't']\n",
      "PRED 64: t a u f e n t a u f e n\n",
      "PRED SCORE: -1.0954\n",
      "\n",
      "[2021-02-03 01:44:14,198 INFO] \n",
      "SENT 65: ['m', 'a', 's', 's', 'a', 'g', 'e']\n",
      "PRED 65: s a s s a s s a g e n\n",
      "PRED SCORE: -1.2852\n",
      "\n",
      "[2021-02-03 01:44:14,198 INFO] \n",
      "SENT 66: ['r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n']\n",
      "PRED 66: o m r o t o t l u t e n\n",
      "PRED SCORE: -3.8323\n",
      "\n",
      "[2021-02-03 01:44:14,198 INFO] \n",
      "SENT 67: ['a', 'b', 'e', 'n', 'd', 'e', 's', 's', 'e', 'n']\n",
      "PRED 67: d e a b e n d e n d e n\n",
      "PRED SCORE: -1.0526\n",
      "\n",
      "[2021-02-03 01:44:14,198 INFO] \n",
      "SENT 68: ['b', 'e', 's', 't', 'a', 'n', 'd']\n",
      "PRED 68: d a m b a n d e n\n",
      "PRED SCORE: -1.2307\n",
      "\n",
      "[2021-02-03 01:44:14,198 INFO] \n",
      "SENT 69: ['m', 'e', 't', 'e', 'r']\n",
      "PRED 69: m e m m e t e r n\n",
      "PRED SCORE: -1.0888\n",
      "\n",
      "[2021-02-03 01:44:14,199 INFO] \n",
      "SENT 70: ['e', 'i', 'n', 'r', 'i', 'c', 'h', 't', 'u', 'n', 'g']\n",
      "PRED 70: t u i n i c h t u n g e n\n",
      "PRED SCORE: -1.0220\n",
      "\n",
      "[2021-02-03 01:44:14,199 INFO] \n",
      "SENT 71: ['b', 'e', 's', 'u', 'c', 'h']\n",
      "PRED 71: b u c h e u c h e n\n",
      "PRED SCORE: -0.6484\n",
      "\n",
      "[2021-02-03 01:44:14,199 INFO] \n",
      "SENT 72: ['k', 'a', 'r', 'a', 'b', 'i', 'n', 'e', 'r']\n",
      "PRED 72: k i k a r r a r a b e n\n",
      "PRED SCORE: -2.4557\n",
      "\n",
      "[2021-02-03 01:44:14,199 INFO] \n",
      "SENT 73: ['o', 'r', 'g', 'a', 'n', 'i', 's', 'a', 't', 'i', 'o', 'n']\n",
      "PRED 73: o r l o t i s a t i s a t e n\n",
      "PRED SCORE: -1.9260\n",
      "\n",
      "[2021-02-03 01:44:14,199 INFO] \n",
      "SENT 74: ['g', 'e', 's', 'e', 't', 'z']\n",
      "PRED 74: z z e s e t z e n\n",
      "PRED SCORE: -0.2807\n",
      "\n",
      "[2021-02-03 01:44:14,200 INFO] \n",
      "SENT 75: ['m', 'i', 't', 't', 'w', 'o', 'c', 'h']\n",
      "PRED 75: t z i t i t o t t e n\n",
      "PRED SCORE: -2.2332\n",
      "\n",
      "[2021-02-03 01:44:14,200 INFO] \n",
      "SENT 76: ['a', 't', 'o', 'm', 'b', 'o', 'm', 'b', 'e']\n",
      "PRED 76: m o a b o t b o t e n\n",
      "PRED SCORE: -1.6032\n",
      "\n",
      "[2021-02-03 01:44:14,200 INFO] \n",
      "SENT 77: ['o', 'b', 'e', 'r', 's', 'c', 'h', 'u', 'l', 'e']\n",
      "PRED 77: c h u b e r s c h u l e n\n",
      "PRED SCORE: -0.7186\n",
      "\n",
      "[2021-02-03 01:44:14,200 INFO] \n",
      "SENT 78: ['k', 'a', 'm', 'p', 'f']\n",
      "PRED 78: k f a m a m p f e\n",
      "PRED SCORE: -1.7031\n",
      "\n",
      "[2021-02-03 01:44:14,200 INFO] \n",
      "SENT 79: ['h', 'e', 'r', 'r', 's', 'c', 'h', 'a', 'f', 't']\n",
      "PRED 79: t a h e r s c h a f t e r\n",
      "PRED SCORE: -1.0377\n",
      "\n",
      "[2021-02-03 01:44:14,201 INFO] \n",
      "SENT 80: ['a', 'u', 'f', 'f', 'a', 's', 's', 'u', 'n', 'g']\n",
      "PRED 80: s u u f f u s s u n g e n\n",
      "PRED SCORE: -1.4753\n",
      "\n",
      "[2021-02-03 01:44:14,201 INFO] PRED AVG SCORE: -0.1216, PRED PPL: 1.1293\n",
      "[2021-02-03 01:44:15,103 INFO] Translating shard 0.\n",
      "[2021-02-03 01:44:15,208 INFO] \n",
      "SENT 1: ['MAS', '<s>', 'h', 'e', 'r', 's', 't', 'e', 'l', 'l', 'e', 'r']\n",
      "PRED 1: h e r s t e r l e r\n",
      "PRED SCORE: -0.4872\n",
      "\n",
      "[2021-02-03 01:44:15,208 INFO] \n",
      "SENT 2: ['FEM', '<s>', 'k', 'l', 'a', 's', 's', 'e']\n",
      "PRED 2: k l a s s e n\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-02-03 01:44:15,208 INFO] \n",
      "SENT 3: ['FEM', '<s>', 'a', 'n', 'z', 'a', 'h', 'l', 'u', 'n', 'g']\n",
      "PRED 3: a n z a h l u n g e n\n",
      "PRED SCORE: -0.1812\n",
      "\n",
      "[2021-02-03 01:44:15,209 INFO] \n",
      "SENT 4: ['FEM', '<s>', 'a', 'b', 'b', 'i', 'l', 'd', 'u', 'n', 'g']\n",
      "PRED 4: a b b i l d u n g e n\n",
      "PRED SCORE: -0.7030\n",
      "\n",
      "[2021-02-03 01:44:15,209 INFO] \n",
      "SENT 5: ['MAS', '<s>', 'b', 'a', 'l', 'd', 'r', 'i', 'a', 'n']\n",
      "PRED 5: b a l d r i a n e\n",
      "PRED SCORE: -0.0277\n",
      "\n",
      "[2021-02-03 01:44:15,209 INFO] \n",
      "SENT 6: ['MAS', '<s>', 'r', 'e', 'b', 'e', 'l', 'l']\n",
      "PRED 6: r e b e l\n",
      "PRED SCORE: -0.0110\n",
      "\n",
      "[2021-02-03 01:44:15,209 INFO] \n",
      "SENT 7: ['FEM', '<s>', 'b', 'e', 'z', 'i', 'e', 'h', 'u', 'n', 'g']\n",
      "PRED 7: b e z i e n g e h e n\n",
      "PRED SCORE: -0.6622\n",
      "\n",
      "[2021-02-03 01:44:15,209 INFO] \n",
      "SENT 8: ['FEM', '<s>', 'f', 'o', 'r', 'd', 'e', 'r', 'u', 'n', 'g']\n",
      "PRED 8: f o r d e r u n g e n\n",
      "PRED SCORE: -0.1153\n",
      "\n",
      "[2021-02-03 01:44:15,210 INFO] \n",
      "SENT 9: ['FEM', '<s>', 'p', 'r', 'o', 'd', 'u', 'k', 't', 'i', 'o', 'n']\n",
      "PRED 9: p r o d t i o n t e n\n",
      "PRED SCORE: -1.3280\n",
      "\n",
      "[2021-02-03 01:44:15,210 INFO] \n",
      "SENT 10: ['MAS', '<s>', 's', 't', 'a', 'a', 't']\n",
      "PRED 10: s t a a t e\n",
      "PRED SCORE: -0.0745\n",
      "\n",
      "[2021-02-03 01:44:15,210 INFO] \n",
      "SENT 11: ['MAS', '<s>', 't', 'h', 'r', 'o', 'n']\n",
      "PRED 11: t r o ß o n e n\n",
      "PRED SCORE: -0.7233\n",
      "\n",
      "[2021-02-03 01:44:15,210 INFO] \n",
      "SENT 12: ['FEM', '<s>', 'f', 'r', 'e', 'i', 'h', 'e', 'i', 't']\n",
      "PRED 12: f r e i e i t e n\n",
      "PRED SCORE: -0.2437\n",
      "\n",
      "[2021-02-03 01:44:15,210 INFO] \n",
      "SENT 13: ['FEM', '<s>', 'd', 'a', 't', 'e', 'n', 'v', 'e', 'r', 'a', 'r', 'b', 'e', 'i', 't', 'u', 'n', 'g']\n",
      "PRED 13: d a t e n b e i t e n\n",
      "PRED SCORE: -0.6222\n",
      "\n",
      "[2021-02-03 01:44:15,211 INFO] \n",
      "SENT 14: ['MAS', '<s>', 'v', 'e', 'r', 't', 'r', 'a', 'g']\n",
      "PRED 14: v e r t r a g e\n",
      "PRED SCORE: -0.0078\n",
      "\n",
      "[2021-02-03 01:44:15,211 INFO] \n",
      "SENT 15: ['MAS', '<s>', 'p', 'l', 'a', 'n']\n",
      "PRED 15: p l a n e\n",
      "PRED SCORE: -0.0954\n",
      "\n",
      "[2021-02-03 01:44:15,211 INFO] \n",
      "SENT 16: ['FEM', '<s>', 'p', 'r', 'i', 'n', 'z', 'e', 's', 's', 'i', 'n']\n",
      "PRED 16: p r i n z e s i n e r\n",
      "PRED SCORE: -0.6636\n",
      "\n",
      "[2021-02-03 01:44:15,211 INFO] \n",
      "SENT 17: ['NTR', '<s>', 'b', 'e', 'i', 's', 'p', 'i', 'e', 'l']\n",
      "PRED 17: b e i s p i e l e\n",
      "PRED SCORE: -0.1935\n",
      "\n",
      "[2021-02-03 01:44:15,211 INFO] \n",
      "SENT 18: ['FEM', '<s>', 'z', 'a', 'u', 'b', 'e', 'r', 'e', 'i']\n",
      "PRED 18: z a u b e r e i e n\n",
      "PRED SCORE: -0.0631\n",
      "\n",
      "[2021-02-03 01:44:15,212 INFO] \n",
      "SENT 19: ['MAS', '<s>', 's', 't', 'e', 'l', 'l', 'v', 'e', 'r', 't', 'r', 'e', 't', 'e', 'r']\n",
      "PRED 19: s t e l l e t e r\n",
      "PRED SCORE: -0.2225\n",
      "\n",
      "[2021-02-03 01:44:15,212 INFO] \n",
      "SENT 20: ['MAS', '<s>', 'e', 'i', 'n', 'f', 'l', 'u', 's', 's']\n",
      "PRED 20: e i n f u s s e\n",
      "PRED SCORE: -0.4190\n",
      "\n",
      "[2021-02-03 01:44:15,212 INFO] \n",
      "SENT 21: ['FEM', '<s>', 'e', 'i', 'n', 'h', 'e', 'i', 't']\n",
      "PRED 21: e i n h e i t e n\n",
      "PRED SCORE: -0.0030\n",
      "\n",
      "[2021-02-03 01:44:15,213 INFO] \n",
      "SENT 22: ['FEM', '<s>', 'e', 'p', 'o', 'c', 'h', 'e']\n",
      "PRED 22: e p h o c h e n\n",
      "PRED SCORE: -0.1722\n",
      "\n",
      "[2021-02-03 01:44:15,213 INFO] \n",
      "SENT 23: ['MAS', '<s>', 'f', 'e', 'i', 'n', 'd']\n",
      "PRED 23: f e i n d e\n",
      "PRED SCORE: -0.0058\n",
      "\n",
      "[2021-02-03 01:44:15,213 INFO] \n",
      "SENT 24: ['MAS', '<s>', 'w', 'e', 'r', 't']\n",
      "PRED 24: w e r t e n\n",
      "PRED SCORE: -0.0055\n",
      "\n",
      "[2021-02-03 01:44:15,213 INFO] \n",
      "SENT 25: ['MAS', '<s>', 's', 'o', 'z', 'i', 'a', 'l', 'i', 's', 'm', 'u', 's']\n",
      "PRED 25: s o z i s m e\n",
      "PRED SCORE: -0.0516\n",
      "\n",
      "[2021-02-03 01:44:15,213 INFO] \n",
      "SENT 26: ['MAS', '<s>', 'a', 'n', 'l', 'a', 's', 's']\n",
      "PRED 26: a n l a s s e\n",
      "PRED SCORE: -0.0863\n",
      "\n",
      "[2021-02-03 01:44:15,213 INFO] \n",
      "SENT 27: ['FEM', '<s>', 'r', 'e', 'i', 'h', 'e', 'n', 'f', 'o', 'l', 'g', 'e']\n",
      "PRED 27: r e i h e n f e n\n",
      "PRED SCORE: -1.2228\n",
      "\n",
      "[2021-02-03 01:44:15,214 INFO] \n",
      "SENT 28: ['FEM', '<s>', 'm', 'e', 'i', 'n', 'u', 'n', 'g']\n",
      "PRED 28: m e i n g e n\n",
      "PRED SCORE: -0.3316\n",
      "\n",
      "[2021-02-03 01:44:15,214 INFO] \n",
      "SENT 29: ['MAS', '<s>', 'o', 'f', 'f', 'i', 'z', 'i', 'e', 'r']\n",
      "PRED 29: o f f i e r e\n",
      "PRED SCORE: -0.1492\n",
      "\n",
      "[2021-02-03 01:44:15,214 INFO] \n",
      "SENT 30: ['MAS', '<s>', 'p', 'a', 'r', 't', 'e', 'i', 't', 'a', 'g']\n",
      "PRED 30: p a r t e i t e n\n",
      "PRED SCORE: -0.5952\n",
      "\n",
      "[2021-02-03 01:44:15,310 INFO] \n",
      "SENT 31: ['FEM', '<s>', 'e', 'n', 't', 's', 'c', 'h', 'e', 'i', 'd', 'u', 'n', 'g']\n",
      "PRED 31: e n t s c h e n k e i d e r\n",
      "PRED SCORE: -1.1650\n",
      "\n",
      "[2021-02-03 01:44:15,310 INFO] \n",
      "SENT 32: ['NTR', '<s>', 's', 'c', 'h', 'i', 'c', 'k', 's', 'a', 'l']\n",
      "PRED 32: s c h i c k s e\n",
      "PRED SCORE: -0.0691\n",
      "\n",
      "[2021-02-03 01:44:15,311 INFO] \n",
      "SENT 33: ['FEM', '<s>', 'r', 'e', 'g', 'i', 'e', 'r', 'u', 'n', 'g']\n",
      "PRED 33: r e g i e r e n\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-02-03 01:44:15,311 INFO] \n",
      "SENT 34: ['FEM', '<s>', 'd', 'e', 'u', 't', 'u', 'n', 'g']\n",
      "PRED 34: d e u t u n g e n\n",
      "PRED SCORE: -0.0011\n",
      "\n",
      "[2021-02-03 01:44:15,311 INFO] \n",
      "SENT 35: ['MAS', '<s>', 'd', 'i', 'e', 'n', 's', 't', 'a', 'g']\n",
      "PRED 35: d i e n s t a g t e\n",
      "PRED SCORE: -0.5786\n",
      "\n",
      "[2021-02-03 01:44:15,311 INFO] \n",
      "SENT 36: ['MAS', '<s>', 'r', 'a', 'n', 'g']\n",
      "PRED 36: r a n g e\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-02-03 01:44:15,311 INFO] \n",
      "SENT 37: ['FEM', '<s>', 'g', 'n', 'a', 'd', 'e']\n",
      "PRED 37: g n a d e n\n",
      "PRED SCORE: -0.0013\n",
      "\n",
      "[2021-02-03 01:44:15,312 INFO] \n",
      "SENT 38: ['MAS', '<s>', 'a', 's', 't', 'r', 'o', 'n', 'o', 'm']\n",
      "PRED 38: a s t r o m e\n",
      "PRED SCORE: -0.5618\n",
      "\n",
      "[2021-02-03 01:44:15,312 INFO] \n",
      "SENT 39: ['FEM', '<s>', 'h', 'o', 'c', 'h', 's', 'c', 'h', 'u', 'l', 'e']\n",
      "PRED 39: h o c h s c h u l e n\n",
      "PRED SCORE: -0.0024\n",
      "\n",
      "[2021-02-03 01:44:15,312 INFO] \n",
      "SENT 40: ['MAS', '<s>', 'v', 'e', 'r', 'l', 'a', 'g']\n",
      "PRED 40: v e r l a g e\n",
      "PRED SCORE: -0.0078\n",
      "\n",
      "[2021-02-03 01:44:15,312 INFO] \n",
      "SENT 41: ['NTR', '<s>', 'g', 'e', 'b', 'i', 'e', 't']\n",
      "PRED 41: g e b i e t e\n",
      "PRED SCORE: -0.6312\n",
      "\n",
      "[2021-02-03 01:44:15,313 INFO] \n",
      "SENT 42: ['MAS', '<s>', 'b', 'e', 't', 'r', 'i', 'e', 'b']\n",
      "PRED 42: b e t r i e b e\n",
      "PRED SCORE: -0.0037\n",
      "\n",
      "[2021-02-03 01:44:15,313 INFO] \n",
      "SENT 43: ['FEM', '<s>', 's', 'e', 'e', 'l', 'e']\n",
      "PRED 43: s e e l e n\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-02-03 01:44:15,314 INFO] \n",
      "SENT 44: ['MAS', '<s>', 'v', 'o', 'r', 'w', 'u', 'r', 'f']\n",
      "PRED 44: v o r w u r f e n\n",
      "PRED SCORE: -0.6164\n",
      "\n",
      "[2021-02-03 01:44:15,316 INFO] \n",
      "SENT 45: ['FEM', '<s>', 'd', 'e', 'v', 'i', 's', 'e']\n",
      "PRED 45: d e r i s e n\n",
      "PRED SCORE: -0.0185\n",
      "\n",
      "[2021-02-03 01:44:15,317 INFO] \n",
      "SENT 46: ['FEM', '<s>', 'h', 'i', 'n', 'g', 'a', 'b', 'e']\n",
      "PRED 46: h i n g e n\n",
      "PRED SCORE: -0.0015\n",
      "\n",
      "[2021-02-03 01:44:15,317 INFO] \n",
      "SENT 47: ['MAS', '<s>', 's', 't', 'i', 'l']\n",
      "PRED 47: s t i l e\n",
      "PRED SCORE: -0.0049\n",
      "\n",
      "[2021-02-03 01:44:15,317 INFO] \n",
      "SENT 48: ['FEM', '<s>', 'z', 'e', 'i', 'c', 'h', 'n', 'u', 'n', 'g']\n",
      "PRED 48: z e i c h n u n g e n\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-02-03 01:44:15,317 INFO] \n",
      "SENT 49: ['NTR', '<s>', 'w', 'e', 'r', 'k', 'z', 'e', 'u', 'g']\n",
      "PRED 49: w e r k e u g e\n",
      "PRED SCORE: -0.0212\n",
      "\n",
      "[2021-02-03 01:44:15,317 INFO] \n",
      "SENT 50: ['MAS', '<s>', 'v', 'e', 'r', 'l', 'a', 'u', 'f']\n",
      "PRED 50: v e r l a u f e\n",
      "PRED SCORE: -0.0107\n",
      "\n",
      "[2021-02-03 01:44:15,318 INFO] \n",
      "SENT 51: ['MAS', '<s>', 'k', 'o', 'r', 'r', 'e', 's', 'p', 'o', 'n', 'd', 'e', 'n', 't']\n",
      "PRED 51: k o r r e s e n t o n e n\n",
      "PRED SCORE: -0.7451\n",
      "\n",
      "[2021-02-03 01:44:15,318 INFO] \n",
      "SENT 52: ['FEM', '<s>', 'a', 'u', 't', 'o', 'b', 'a', 'h', 'n']\n",
      "PRED 52: a u t o h n e n\n",
      "PRED SCORE: -0.1665\n",
      "\n",
      "[2021-02-03 01:44:15,318 INFO] \n",
      "SENT 53: ['FEM', '<s>', 'b', 'a', 'u', 's', 't', 'e', 'l', 'l', 'e']\n",
      "PRED 53: b a u s t e l l e n\n",
      "PRED SCORE: -0.0019\n",
      "\n",
      "[2021-02-03 01:44:15,318 INFO] \n",
      "SENT 54: ['MAS', '<s>', 's', 'c', 'h', 'r', 'e', 'i']\n",
      "PRED 54: s c h r e i e n\n",
      "PRED SCORE: -0.1846\n",
      "\n",
      "[2021-02-03 01:44:15,318 INFO] \n",
      "SENT 55: ['MAS', '<s>', 'h', 'a', 'r', 'n']\n",
      "PRED 55: h a r n e\n",
      "PRED SCORE: -0.0042\n",
      "\n",
      "[2021-02-03 01:44:15,318 INFO] \n",
      "SENT 56: ['NTR', '<s>', 'e', 'r', 'z', 'e', 'u', 'g', 'n', 'i', 's']\n",
      "PRED 56: e r z i s z e u g e\n",
      "PRED SCORE: -0.2512\n",
      "\n",
      "[2021-02-03 01:44:15,318 INFO] \n",
      "SENT 57: ['NTR', '<s>', 'p', 'o', 's', 't', 'u', 'l', 'a', 't']\n",
      "PRED 57: p o s t u l a t e n\n",
      "PRED SCORE: -0.4996\n",
      "\n",
      "[2021-02-03 01:44:15,319 INFO] \n",
      "SENT 58: ['MAS', '<s>', 'a', 'n', 's', 'a', 't', 'z']\n",
      "PRED 58: a n s a t z e\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-02-03 01:44:15,319 INFO] \n",
      "SENT 59: ['MAS', '<s>', 'e', 'f', 'f', 'e', 'k', 't']\n",
      "PRED 59: e f f e n\n",
      "PRED SCORE: -0.4542\n",
      "\n",
      "[2021-02-03 01:44:15,319 INFO] \n",
      "SENT 60: ['MAS', '<s>', 'p', 'e', 'd', 'a', 'n', 't']\n",
      "PRED 60: p e d a n t e n\n",
      "PRED SCORE: -0.0575\n",
      "\n",
      "[2021-02-03 01:44:15,399 INFO] \n",
      "SENT 61: ['MAS', '<s>', 'r', 'i', 't', 'u', 's']\n",
      "PRED 61: r i t u s e\n",
      "PRED SCORE: -0.0033\n",
      "\n",
      "[2021-02-03 01:44:15,399 INFO] \n",
      "SENT 62: ['FEM', '<s>', 'p', 'o', 'l', 'i', 'z', 'e', 'i']\n",
      "PRED 62: p o l i z e i z e n\n",
      "PRED SCORE: -0.6414\n",
      "\n",
      "[2021-02-03 01:44:15,399 INFO] \n",
      "SENT 63: ['MAS', '<s>', 'b', 'e', 's', 'c', 'h', 'l', 'u', 's', 's']\n",
      "PRED 63: b e s c h l u s s e\n",
      "PRED SCORE: -0.1655\n",
      "\n",
      "[2021-02-03 01:44:15,400 INFO] \n",
      "SENT 64: ['MAS', '<s>', 'b', 'o', 'd', 'e', 'n']\n",
      "PRED 64: b o d e n\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-02-03 01:44:15,400 INFO] \n",
      "SENT 65: ['MAS', '<s>', 'l', 'u', 'f', 't', 'a', 'n', 'g', 'r', 'i', 'f', 'f']\n",
      "PRED 65: l u f t a n g e\n",
      "PRED SCORE: -0.0019\n",
      "\n",
      "[2021-02-03 01:44:15,400 INFO] \n",
      "SENT 66: ['NTR', '<s>', 'e', 'r', 'l', 'e', 'b', 'n', 'i', 's']\n",
      "PRED 66: e r l e b e n\n",
      "PRED SCORE: -1.1000\n",
      "\n",
      "[2021-02-03 01:44:15,400 INFO] \n",
      "SENT 67: ['MAS', '<s>', 'k', 'a', 'm', 'p', 'f']\n",
      "PRED 67: k a m p e\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-02-03 01:44:15,401 INFO] \n",
      "SENT 68: ['FEM', '<s>', 'f', 'a', 'm', 'i', 'l', 'i', 'e']\n",
      "PRED 68: f a m i e l e n\n",
      "PRED SCORE: -0.5438\n",
      "\n",
      "[2021-02-03 01:44:15,401 INFO] \n",
      "SENT 69: ['MAS', '<s>', 'j', 'u', 'n', 'i', 'o', 'r']\n",
      "PRED 69: j u n i o r e n\n",
      "PRED SCORE: -0.1044\n",
      "\n",
      "[2021-02-03 01:44:15,401 INFO] \n",
      "SENT 70: ['FEM', '<s>', 'a', 'u', 's', 's', 'p', 'r', 'a', 'c', 'h', 'e']\n",
      "PRED 70: a u s p r a c h e n\n",
      "PRED SCORE: -0.0192\n",
      "\n",
      "[2021-02-03 01:44:15,401 INFO] \n",
      "SENT 71: ['FEM', '<s>', 'b', 'r', 'o', 'n', 'z', 'e']\n",
      "PRED 71: b r o n z e n\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-02-03 01:44:15,401 INFO] \n",
      "SENT 72: ['FEM', '<s>', 'f', 'u', 'n', 'k', 't', 'i', 'o', 'n']\n",
      "PRED 72: f u n k i o n e n\n",
      "PRED SCORE: -1.1253\n",
      "\n",
      "[2021-02-03 01:44:15,402 INFO] \n",
      "SENT 73: ['MAS', '<s>', 'a', 'u', 's', 't', 'a', 'u', 's', 'c', 'h']\n",
      "PRED 73: a u s t a u s c h e\n",
      "PRED SCORE: -0.0680\n",
      "\n",
      "[2021-02-03 01:44:15,402 INFO] \n",
      "SENT 74: ['MAS', '<s>', 'd', 'u', 'r', 'c', 'h', 's', 'c', 'h', 'n', 'i', 't', 't']\n",
      "PRED 74: d u r c h n i t t e n\n",
      "PRED SCORE: -0.0454\n",
      "\n",
      "[2021-02-03 01:44:15,402 INFO] \n",
      "SENT 75: ['MAS', '<s>', 'p', 'r', 'e', 'm', 'i', 'e', 'r', 'm', 'i', 'n', 'i', 's', 't', 'e', 'r']\n",
      "PRED 75: p r e m i e r m e\n",
      "PRED SCORE: -0.2783\n",
      "\n",
      "[2021-02-03 01:44:15,402 INFO] \n",
      "SENT 76: ['MAS', '<s>', 'k', 'a', 'n', 'd', 'i', 'd', 'a', 't']\n",
      "PRED 76: k a n d i d t e n\n",
      "PRED SCORE: -0.0117\n",
      "\n",
      "[2021-02-03 01:44:15,403 INFO] \n",
      "SENT 77: ['MAS', '<s>', 's', 'c', 'h', 'l', 'u', 's', 's']\n",
      "PRED 77: s c h l u s s e\n",
      "PRED SCORE: -0.0028\n",
      "\n",
      "[2021-02-03 01:44:15,403 INFO] \n",
      "SENT 78: ['MAS', '<s>', 's', 't', 'e', 'u', 'e', 'r', 'b', 'e', 'r', 'a', 't', 'e', 'r']\n",
      "PRED 78: s t e u e r b a t e r\n",
      "PRED SCORE: -0.0224\n",
      "\n",
      "[2021-02-03 01:44:15,403 INFO] \n",
      "SENT 79: ['FEM', '<s>', 's', 'e', 'u', 'c', 'h', 'e']\n",
      "PRED 79: s e u c h e n\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-02-03 01:44:15,403 INFO] \n",
      "SENT 80: ['NTR', '<s>', 'p', 'r', 'o', 'z', 'e', 'n', 't']\n",
      "PRED 80: p r o z e n t e n\n",
      "PRED SCORE: -0.0049\n",
      "\n",
      "[2021-02-03 01:44:15,403 INFO] PRED AVG SCORE: -0.0295, PRED PPL: 1.0300\n",
      "[2021-02-03 01:44:16,227 INFO] Translating shard 0.\n",
      "[2021-02-03 01:44:16,343 INFO] \n",
      "SENT 1: ['f', 'a', 'm', 'i', 'l', 'i', 'e']\n",
      "PRED 1: f a m i e l i e n\n",
      "PRED SCORE: -1.1543\n",
      "\n",
      "[2021-02-03 01:44:16,344 INFO] \n",
      "SENT 2: ['b', 'e', 's', 'c', 'h', 'l', 'u', 's', 's']\n",
      "PRED 2: w l u s s c h l u s b e s\n",
      "PRED SCORE: -1.0005\n",
      "\n",
      "[2021-02-03 01:44:16,344 INFO] \n",
      "SENT 3: ['w', 'e', 'r', 't']\n",
      "PRED 3: t w e r t e t e r\n",
      "PRED SCORE: -0.9871\n",
      "\n",
      "[2021-02-03 01:44:16,344 INFO] \n",
      "SENT 4: ['p', 'r', 'i', 'n', 'z', 'e', 's', 's', 'i', 'n']\n",
      "PRED 4: e p i n z i n z e s\n",
      "PRED SCORE: -2.1837\n",
      "\n",
      "[2021-02-03 01:44:16,344 INFO] \n",
      "SENT 5: ['s', 'e', 'e', 'l', 'e']\n",
      "PRED 5: l e i s e e l e n\n",
      "PRED SCORE: -2.2700\n",
      "\n",
      "[2021-02-03 01:44:16,345 INFO] \n",
      "SENT 6: ['v', 'e', 'r', 'l', 'a', 'g']\n",
      "PRED 6: r e r l a g e r\n",
      "PRED SCORE: -0.2696\n",
      "\n",
      "[2021-02-03 01:44:16,345 INFO] \n",
      "SENT 7: ['k', 'o', 'r', 'r', 'e', 's', 'p', 'o', 'n', 'd', 'e', 'n', 't']\n",
      "PRED 7: t o h r e s p o h r e n\n",
      "PRED SCORE: -1.5137\n",
      "\n",
      "[2021-02-03 01:44:16,345 INFO] \n",
      "SENT 8: ['h', 'o', 'c', 'h', 's', 'c', 'h', 'u', 'l', 'e']\n",
      "PRED 8: w u h e n e c h s c h o l e n\n",
      "PRED SCORE: -2.5221\n",
      "\n",
      "[2021-02-03 01:44:16,345 INFO] \n",
      "SENT 9: ['d', 'i', 'e', 'n', 's', 't', 'a', 'g']\n",
      "PRED 9: s t a g t a g s t a g e\n",
      "PRED SCORE: -1.3888\n",
      "\n",
      "[2021-02-03 01:44:16,345 INFO] \n",
      "SENT 10: ['m', 'e', 'i', 'n', 'u', 'n', 'g']\n",
      "PRED 10: g i n e i n g e i n g e r\n",
      "PRED SCORE: -1.4156\n",
      "\n",
      "[2021-02-03 01:44:16,346 INFO] \n",
      "SENT 11: ['a', 'n', 's', 'a', 't', 'z']\n",
      "PRED 11: z a n z a t z e\n",
      "PRED SCORE: -0.2479\n",
      "\n",
      "[2021-02-03 01:44:16,346 INFO] \n",
      "SENT 12: ['e', 'n', 't', 's', 'c', 'h', 'e', 'i', 'd', 'u', 'n', 'g']\n",
      "PRED 12: t e n t c h e n t i d e r\n",
      "PRED SCORE: -1.8607\n",
      "\n",
      "[2021-02-03 01:44:16,346 INFO] \n",
      "SENT 13: ['g', 'n', 'a', 'd', 'e']\n",
      "PRED 13: g n a d g e n\n",
      "PRED SCORE: -0.4863\n",
      "\n",
      "[2021-02-03 01:44:16,346 INFO] \n",
      "SENT 14: ['r', 'e', 'i', 'h', 'e', 'n', 'f', 'o', 'l', 'g', 'e']\n",
      "PRED 14: f e n g h o l g e n\n",
      "PRED SCORE: -2.0588\n",
      "\n",
      "[2021-02-03 01:44:16,346 INFO] \n",
      "SENT 15: ['f', 'e', 'i', 'n', 'd']\n",
      "PRED 15: f e i n d e i n d e i e n\n",
      "PRED SCORE: -1.0243\n",
      "\n",
      "[2021-02-03 01:44:16,347 INFO] \n",
      "SENT 16: ['s', 'c', 'h', 'l', 'u', 's', 's']\n",
      "PRED 16: l u s c h l u s c h l u s s e\n",
      "PRED SCORE: -2.1524\n",
      "\n",
      "[2021-02-03 01:44:16,347 INFO] \n",
      "SENT 17: ['s', 'e', 'u', 'c', 'h', 'e']\n",
      "PRED 17: e u s e c h e u c h e n\n",
      "PRED SCORE: -0.0850\n",
      "\n",
      "[2021-02-03 01:44:16,347 INFO] \n",
      "SENT 18: ['d', 'e', 'v', 'i', 's', 'e']\n",
      "PRED 18: p i s d e r i s e i s e i s e r\n",
      "PRED SCORE: -1.8687\n",
      "\n",
      "[2021-02-03 01:44:16,347 INFO] \n",
      "SENT 19: ['s', 't', 'e', 'l', 'l', 'v', 'e', 'r', 't', 'r', 'e', 't', 'e', 'r']\n",
      "PRED 19: e l l e t r e t e r r e t e r\n",
      "PRED SCORE: -1.4247\n",
      "\n",
      "[2021-02-03 01:44:16,347 INFO] \n",
      "SENT 20: ['t', 'h', 'r', 'o', 'n']\n",
      "PRED 20: t r o n t e r\n",
      "PRED SCORE: -1.0533\n",
      "\n",
      "[2021-02-03 01:44:16,348 INFO] \n",
      "SENT 21: ['k', 'a', 'm', 'p', 'f']\n",
      "PRED 21: p a m p a m p e\n",
      "PRED SCORE: -0.7621\n",
      "\n",
      "[2021-02-03 01:44:16,348 INFO] \n",
      "SENT 22: ['o', 'f', 'f', 'i', 'z', 'i', 'e', 'r']\n",
      "PRED 22: f o f f i e r e\n",
      "PRED SCORE: -0.1897\n",
      "\n",
      "[2021-02-03 01:44:16,348 INFO] \n",
      "SENT 23: ['a', 'u', 't', 'o', 'b', 'a', 'h', 'n']\n",
      "PRED 23: t a h a h n e\n",
      "PRED SCORE: -1.0110\n",
      "\n",
      "[2021-02-03 01:44:16,348 INFO] \n",
      "SENT 24: ['r', 'i', 't', 'u', 's']\n",
      "PRED 24: r i t r i t u s t e\n",
      "PRED SCORE: -0.7935\n",
      "\n",
      "[2021-02-03 01:44:16,348 INFO] \n",
      "SENT 25: ['b', 'e', 't', 'r', 'i', 'e', 'b']\n",
      "PRED 25: r e t r i e b e r\n",
      "PRED SCORE: -0.8539\n",
      "\n",
      "[2021-02-03 01:44:16,349 INFO] \n",
      "SENT 26: ['p', 'r', 'o', 'd', 'u', 'k', 't', 'i', 'o', 'n']\n",
      "PRED 26: t r o k i n t e\n",
      "PRED SCORE: -1.4567\n",
      "\n",
      "[2021-02-03 01:44:16,349 INFO] \n",
      "SENT 27: ['r', 'a', 'n', 'g']\n",
      "PRED 27: g r a n g e r\n",
      "PRED SCORE: -0.0251\n",
      "\n",
      "[2021-02-03 01:44:16,349 INFO] \n",
      "SENT 28: ['l', 'u', 'f', 't', 'a', 'n', 'g', 'r', 'i', 'f', 'f']\n",
      "PRED 28: z a f t a n g e r\n",
      "PRED SCORE: -2.0839\n",
      "\n",
      "[2021-02-03 01:44:16,349 INFO] \n",
      "SENT 29: ['e', 'i', 'n', 'h', 'e', 'i', 't']\n",
      "PRED 29: e i n h e i t e n\n",
      "PRED SCORE: -0.0464\n",
      "\n",
      "[2021-02-03 01:44:16,349 INFO] \n",
      "SENT 30: ['s', 't', 'a', 'a', 't']\n",
      "PRED 30: t a a t t a a t t e n\n",
      "PRED SCORE: -0.3432\n",
      "\n",
      "[2021-02-03 01:44:16,466 INFO] \n",
      "SENT 31: ['b', 'a', 'u', 's', 't', 'e', 'l', 'l', 'e']\n",
      "PRED 31: s t a u s t e l l e n\n",
      "PRED SCORE: -0.5308\n",
      "\n",
      "[2021-02-03 01:44:16,467 INFO] \n",
      "SENT 32: ['f', 'o', 'r', 'd', 'e', 'r', 'u', 'n', 'g']\n",
      "PRED 32: g r u r d o r d e r\n",
      "PRED SCORE: -1.8953\n",
      "\n",
      "[2021-02-03 01:44:16,467 INFO] \n",
      "SENT 33: ['s', 'c', 'h', 'i', 'c', 'k', 's', 'a', 'l']\n",
      "PRED 33: k i c h s c h i c k s e\n",
      "PRED SCORE: -1.2698\n",
      "\n",
      "[2021-02-03 01:44:16,467 INFO] \n",
      "SENT 34: ['e', 'i', 'n', 'f', 'l', 'u', 's', 's']\n",
      "PRED 34: w e i n f u s s e\n",
      "PRED SCORE: -1.2666\n",
      "\n",
      "[2021-02-03 01:44:16,468 INFO] \n",
      "SENT 35: ['r', 'e', 'b', 'e', 'l', 'l']\n",
      "PRED 35: e b e b l e b e r\n",
      "PRED SCORE: -1.3291\n",
      "\n",
      "[2021-02-03 01:44:16,468 INFO] \n",
      "SENT 36: ['s', 'c', 'h', 'r', 'e', 'i']\n",
      "PRED 36: c h r e i e r e i e r\n",
      "PRED SCORE: -1.0622\n",
      "\n",
      "[2021-02-03 01:44:16,468 INFO] \n",
      "SENT 37: ['v', 'o', 'r', 'w', 'u', 'r', 'f']\n",
      "PRED 37: y o r f u r f u r f e o r e o r\n",
      "PRED SCORE: -2.3492\n",
      "\n",
      "[2021-02-03 01:44:16,468 INFO] \n",
      "SENT 38: ['b', 'r', 'o', 'n', 'z', 'e']\n",
      "PRED 38: p e r o n z e n\n",
      "PRED SCORE: -0.3128\n",
      "\n",
      "[2021-02-03 01:44:16,468 INFO] \n",
      "SENT 39: ['e', 'r', 'z', 'e', 'u', 'g', 'n', 'i', 's']\n",
      "PRED 39: z e r z e u g e r\n",
      "PRED SCORE: -1.2151\n",
      "\n",
      "[2021-02-03 01:44:16,469 INFO] \n",
      "SENT 40: ['s', 'o', 'z', 'i', 'a', 'l', 'i', 's', 'm', 'u', 's']\n",
      "PRED 40: i a l i s m e\n",
      "PRED SCORE: -2.3422\n",
      "\n",
      "[2021-02-03 01:44:16,469 INFO] \n",
      "SENT 41: ['p', 'e', 'd', 'a', 'n', 't']\n",
      "PRED 41: t p a p e d t e n\n",
      "PRED SCORE: -1.1910\n",
      "\n",
      "[2021-02-03 01:44:16,469 INFO] \n",
      "SENT 42: ['k', 'l', 'a', 's', 's', 'e']\n",
      "PRED 42: l e k l a s s e n\n",
      "PRED SCORE: -0.7452\n",
      "\n",
      "[2021-02-03 01:44:16,469 INFO] \n",
      "SENT 43: ['a', 'u', 's', 't', 'a', 'u', 's', 'c', 'h']\n",
      "PRED 43: s t a u s c h a u s t a u s t e\n",
      "PRED SCORE: -2.5397\n",
      "\n",
      "[2021-02-03 01:44:16,469 INFO] \n",
      "SENT 44: ['s', 't', 'i', 'l']\n",
      "PRED 44: t i l t i l t i l e\n",
      "PRED SCORE: -1.0014\n",
      "\n",
      "[2021-02-03 01:44:16,469 INFO] \n",
      "SENT 45: ['b', 'e', 'i', 's', 'p', 'i', 'e', 'l']\n",
      "PRED 45: p i e b e i e r s b i e s e\n",
      "PRED SCORE: -1.7564\n",
      "\n",
      "[2021-02-03 01:44:16,469 INFO] \n",
      "SENT 46: ['f', 'r', 'e', 'i', 'h', 'e', 'i', 't']\n",
      "PRED 46: f i t r e i t e r\n",
      "PRED SCORE: -0.4011\n",
      "\n",
      "[2021-02-03 01:44:16,470 INFO] \n",
      "SENT 47: ['g', 'e', 'b', 'i', 'e', 't']\n",
      "PRED 47: g g e b e t i e b e\n",
      "PRED SCORE: -1.4672\n",
      "\n",
      "[2021-02-03 01:44:16,470 INFO] \n",
      "SENT 48: ['a', 'b', 'b', 'i', 'l', 'd', 'u', 'n', 'g']\n",
      "PRED 48: g a b d i l d u n g e n\n",
      "PRED SCORE: -0.4239\n",
      "\n",
      "[2021-02-03 01:44:16,470 INFO] \n",
      "SENT 49: ['p', 'r', 'o', 'z', 'e', 'n', 't']\n",
      "PRED 49: t r o p r o z e n\n",
      "PRED SCORE: -0.2052\n",
      "\n",
      "[2021-02-03 01:44:16,470 INFO] \n",
      "SENT 50: ['v', 'e', 'r', 'l', 'a', 'u', 'f']\n",
      "PRED 50: f e r l a u f f e r\n",
      "PRED SCORE: -0.2734\n",
      "\n",
      "[2021-02-03 01:44:16,470 INFO] \n",
      "SENT 51: ['d', 'u', 'r', 'c', 'h', 's', 'c', 'h', 'n', 'i', 't', 't']\n",
      "PRED 51: n n u r d c h n u r t e n\n",
      "PRED SCORE: -1.3265\n",
      "\n",
      "[2021-02-03 01:44:16,470 INFO] \n",
      "SENT 52: ['e', 'p', 'o', 'c', 'h', 'e']\n",
      "PRED 52: e p p o c h e n\n",
      "PRED SCORE: -0.6369\n",
      "\n",
      "[2021-02-03 01:44:16,471 INFO] \n",
      "SENT 53: ['f', 'u', 'n', 'k', 't', 'i', 'o', 'n']\n",
      "PRED 53: k i o n t e\n",
      "PRED SCORE: -1.7594\n",
      "\n",
      "[2021-02-03 01:44:16,471 INFO] \n",
      "SENT 54: ['b', 'e', 'z', 'i', 'e', 'h', 'u', 'n', 'g']\n",
      "PRED 54: g i e b e z e z e r\n",
      "PRED SCORE: -1.7043\n",
      "\n",
      "[2021-02-03 01:44:16,471 INFO] \n",
      "SENT 55: ['p', 'o', 'l', 'i', 'z', 'e', 'i']\n",
      "PRED 55: p i p i z e i z e i z e i z e n\n",
      "PRED SCORE: -2.6471\n",
      "\n",
      "[2021-02-03 01:44:16,471 INFO] \n",
      "SENT 56: ['d', 'a', 't', 'e', 'n', 'v', 'e', 'r', 'a', 'r', 'b', 'e', 'i', 't', 'u', 'n', 'g']\n",
      "PRED 56: a t d e r b a t e r\n",
      "PRED SCORE: -2.2850\n",
      "\n",
      "[2021-02-03 01:44:16,471 INFO] \n",
      "SENT 57: ['w', 'e', 'r', 'k', 'z', 'e', 'u', 'g']\n",
      "PRED 57: e r k e u g k e u g e r\n",
      "PRED SCORE: -1.5226\n",
      "\n",
      "[2021-02-03 01:44:16,471 INFO] \n",
      "SENT 58: ['h', 'e', 'r', 's', 't', 'e', 'l', 'l', 'e', 'r']\n",
      "PRED 58: e r s t e r l e r\n",
      "PRED SCORE: -1.0698\n",
      "\n",
      "[2021-02-03 01:44:16,471 INFO] \n",
      "SENT 59: ['e', 'f', 'f', 'e', 'k', 't']\n",
      "PRED 59: t e f f e k e k e r\n",
      "PRED SCORE: -0.8375\n",
      "\n",
      "[2021-02-03 01:44:16,472 INFO] \n",
      "SENT 60: ['s', 't', 'e', 'u', 'e', 'r', 'b', 'e', 'r', 'a', 't', 'e', 'r']\n",
      "PRED 60: e u b e r b a t e r b a t e r\n",
      "PRED SCORE: -0.8095\n",
      "\n",
      "[2021-02-03 01:44:16,563 INFO] \n",
      "SENT 61: ['r', 'e', 'g', 'i', 'e', 'r', 'u', 'n', 'g']\n",
      "PRED 61: g r e g e g e g e r\n",
      "PRED SCORE: -0.7621\n",
      "\n",
      "[2021-02-03 01:44:16,563 INFO] \n",
      "SENT 62: ['b', 'o', 'd', 'e', 'n']\n",
      "PRED 62: e o b e n b o d e n\n",
      "PRED SCORE: -0.1277\n",
      "\n",
      "[2021-02-03 01:44:16,563 INFO] \n",
      "SENT 63: ['v', 'e', 'r', 't', 'r', 'a', 'g']\n",
      "PRED 63: t r a g t r a g e r\n",
      "PRED SCORE: -0.9900\n",
      "\n",
      "[2021-02-03 01:44:16,563 INFO] \n",
      "SENT 64: ['p', 'o', 's', 't', 'u', 'l', 'a', 't']\n",
      "PRED 64: t u l t u l t u l t e n\n",
      "PRED SCORE: -1.0240\n",
      "\n",
      "[2021-02-03 01:44:16,564 INFO] \n",
      "SENT 65: ['h', 'i', 'n', 'g', 'a', 'b', 'e']\n",
      "PRED 65: g i n e i n e n\n",
      "PRED SCORE: -0.5206\n",
      "\n",
      "[2021-02-03 01:44:16,564 INFO] \n",
      "SENT 66: ['b', 'a', 'l', 'd', 'r', 'i', 'a', 'n']\n",
      "PRED 66: b a l d r i a n d e r\n",
      "PRED SCORE: -1.2519\n",
      "\n",
      "[2021-02-03 01:44:16,564 INFO] \n",
      "SENT 67: ['z', 'a', 'u', 'b', 'e', 'r', 'e', 'i']\n",
      "PRED 67: e r e i u b e r e i u b e r\n",
      "PRED SCORE: -0.8298\n",
      "\n",
      "[2021-02-03 01:44:16,564 INFO] \n",
      "SENT 68: ['a', 'n', 'z', 'a', 'h', 'l', 'u', 'n', 'g']\n",
      "PRED 68: z a h l u n g e n\n",
      "PRED SCORE: -1.3524\n",
      "\n",
      "[2021-02-03 01:44:16,565 INFO] \n",
      "SENT 69: ['j', 'u', 'n', 'i', 'o', 'r']\n",
      "PRED 69: z u n i o r e n\n",
      "PRED SCORE: -1.0492\n",
      "\n",
      "[2021-02-03 01:44:16,565 INFO] \n",
      "SENT 70: ['p', 'r', 'e', 'm', 'i', 'e', 'r', 'm', 'i', 'n', 'i', 's', 't', 'e', 'r']\n",
      "PRED 70: r i p r e m m i e r s t i e r e\n",
      "PRED SCORE: -2.2488\n",
      "\n",
      "[2021-02-03 01:44:16,565 INFO] \n",
      "SENT 71: ['d', 'e', 'u', 't', 'u', 'n', 'g']\n",
      "PRED 71: g u ß d u n g e u n g e n\n",
      "PRED SCORE: -1.1317\n",
      "\n",
      "[2021-02-03 01:44:16,565 INFO] \n",
      "SENT 72: ['h', 'a', 'r', 'n']\n",
      "PRED 72: a r c h a r n e n\n",
      "PRED SCORE: -1.0305\n",
      "\n",
      "[2021-02-03 01:44:16,566 INFO] \n",
      "SENT 73: ['p', 'a', 'r', 't', 'e', 'i', 't', 'a', 'g']\n",
      "PRED 73: t a p t a r t e i t e r\n",
      "PRED SCORE: -1.4791\n",
      "\n",
      "[2021-02-03 01:44:16,566 INFO] \n",
      "SENT 74: ['z', 'e', 'i', 'c', 'h', 'n', 'u', 'n', 'g']\n",
      "PRED 74: g i c h n u n g e n\n",
      "PRED SCORE: -1.8554\n",
      "\n",
      "[2021-02-03 01:44:16,566 INFO] \n",
      "SENT 75: ['p', 'l', 'a', 'n']\n",
      "PRED 75: l a p l a n p e n\n",
      "PRED SCORE: -0.4437\n",
      "\n",
      "[2021-02-03 01:44:16,566 INFO] \n",
      "SENT 76: ['a', 's', 't', 'r', 'o', 'n', 'o', 'm']\n",
      "PRED 76: t a s t r o n t e r\n",
      "PRED SCORE: -0.4161\n",
      "\n",
      "[2021-02-03 01:44:16,566 INFO] \n",
      "SENT 77: ['a', 'n', 'l', 'a', 's', 's']\n",
      "PRED 77: w a n l a s s e\n",
      "PRED SCORE: -0.9029\n",
      "\n",
      "[2021-02-03 01:44:16,567 INFO] \n",
      "SENT 78: ['e', 'r', 'l', 'e', 'b', 'n', 'i', 's']\n",
      "PRED 78: r e r l e b b e b e r\n",
      "PRED SCORE: -2.7459\n",
      "\n",
      "[2021-02-03 01:44:16,567 INFO] \n",
      "SENT 79: ['a', 'u', 's', 's', 'p', 'r', 'a', 'c', 'h', 'e']\n",
      "PRED 79: s p a c h e r\n",
      "PRED SCORE: -1.3832\n",
      "\n",
      "[2021-02-03 01:44:16,567 INFO] \n",
      "SENT 80: ['k', 'a', 'n', 'd', 'i', 'd', 'a', 't']\n",
      "PRED 80: d i d d i d t a n d e n\n",
      "PRED SCORE: -0.8606\n",
      "\n",
      "[2021-02-03 01:44:16,567 INFO] PRED AVG SCORE: -0.1132, PRED PPL: 1.1199\n"
     ]
    }
   ],
   "source": [
    "for datasize in datasizes:\n",
    "  epochs, n_examples, batchsize,  = 100, int(datasize.split('_')[0]), 20\n",
    "  steps = str(int(epochs * n_examples / batchsize))\n",
    "  datadir = f'drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/{datasize}'\n",
    "  rnn_modelpath = f'{outdir}/german_rnn_model_{datasize}'\n",
    "  rnn_trans_args = ' '.join([\n",
    "    '-model '+rnn_modelpath+'_step_'+steps+'.pt',\n",
    "    f'-src {datadir}/german-src-test.txt',\n",
    "    f'-output {outdir}/german-rnn-{datasize}-pred.txt',\n",
    "    '-replace_unk -verbose',\n",
    "    '-beam_size 12'\n",
    "    ])\n",
    "  !python OpenNMT-py/translate.py $rnn_trans_args\n",
    "  \n",
    "  rnn_trans_args = ' '.join([\n",
    "    '-model '+rnn_modelpath+'_step_'+steps+'.pt',\n",
    "    f'-src {datadir}/german-src-test-genderless.txt',\n",
    "    f'-output {outdir}/german-rnn-{datasize}-genderless-pred.txt',\n",
    "    '-replace_unk -verbose',\n",
    "    '-beam_size 12'\n",
    "    ])\n",
    "  !python OpenNMT-py/translate.py $rnn_trans_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBEwBPB5Pcm-"
   },
   "source": [
    "## Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MO5h6yn8PhHi",
    "outputId": "c9f17d4a-b470-446c-9f94-230f05dc2044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Other: 0.0125 [('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.12121212121212122\n",
      "M Accuracy: 0.42424242424242425\n",
      "-s 0.030303030303030304\n",
      "-e 0.18181818181818182\n",
      "-r 0.06060606060606061\n",
      "-n 0.7272727272727273\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.38235294117647056\n",
      "F Accuracy: 0.5294117647058824\n",
      "-s 0.029411764705882353\n",
      "-e 0.20588235294117646\n",
      "-r 0.17647058823529413\n",
      "-n 0.5588235294117647\n",
      "Other: 0.029411764705882353 [('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3076923076923077\n",
      "N Accuracy: 0.5384615384615384\n",
      "-s 0.0\n",
      "-e 0.23076923076923078\n",
      "-r 0.38461538461538464\n",
      "-n 0.38461538461538464\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 240_16\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.5875\n",
      "Test accuracy: 0.7\n",
      "-s 0.0\n",
      "-e 0.3\n",
      "-r 0.25\n",
      "-n 0.4375\n",
      "Other: 0.0125 [('a', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.47368421052631576\n",
      "M Accuracy: 0.6052631578947368\n",
      "-s 0.0\n",
      "-e 0.47368421052631576\n",
      "-r 0.3684210526315789\n",
      "-n 0.13157894736842105\n",
      "Other: 0.02631578947368421 [('a', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8387096774193549\n",
      "F Accuracy: 0.9032258064516129\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.06451612903225806\n",
      "-n 0.9354838709677419\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.2727272727272727\n",
      "N Accuracy: 0.45454545454545453\n",
      "-s 0.0\n",
      "-e 0.5454545454545454\n",
      "-r 0.36363636363636365\n",
      "-n 0.09090909090909091\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.3875\n",
      "Test accuracy: 0.4375\n",
      "-s 0.0\n",
      "-e 0.2\n",
      "-r 0.4\n",
      "-n 0.3875\n",
      "Other: 0.0125 [('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.42105263157894735\n",
      "M Accuracy: 0.4473684210526316\n",
      "-s 0.0\n",
      "-e 0.13157894736842105\n",
      "-r 0.4473684210526316\n",
      "-n 0.39473684210526316\n",
      "Other: 0.02631578947368421 [('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3548387096774194\n",
      "F Accuracy: 0.45161290322580644\n",
      "-s 0.0\n",
      "-e 0.2903225806451613\n",
      "-r 0.3548387096774194\n",
      "-n 0.3548387096774194\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.36363636363636365\n",
      "N Accuracy: 0.36363636363636365\n",
      "-s 0.0\n",
      "-e 0.18181818181818182\n",
      "-r 0.36363636363636365\n",
      "-n 0.45454545454545453\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 240_17\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.5375\n",
      "Test accuracy: 0.7125\n",
      "-s 0.0\n",
      "-e 0.25\n",
      "-r 0.2\n",
      "-n 0.5125\n",
      "Other: 0.0375 [('l', 2), ('a', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6060606060606061\n",
      "M Accuracy: 0.6666666666666666\n",
      "-s 0.0\n",
      "-e 0.42424242424242425\n",
      "-r 0.24242424242424243\n",
      "-n 0.30303030303030304\n",
      "Other: 0.030303030303030304 [('a', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5\n",
      "F Accuracy: 0.9333333333333333\n",
      "-s 0.0\n",
      "-e 0.03333333333333333\n",
      "-r 0.03333333333333333\n",
      "-n 0.9333333333333333\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.47058823529411764\n",
      "N Accuracy: 0.4117647058823529\n",
      "-s 0.0\n",
      "-e 0.29411764705882354\n",
      "-r 0.4117647058823529\n",
      "-n 0.17647058823529413\n",
      "Other: 0.11764705882352941 [('l', 2)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.375\n",
      "Test accuracy: 0.45\n",
      "-s 0.0\n",
      "-e 0.175\n",
      "-r 0.2625\n",
      "-n 0.55\n",
      "Other: 0.0125 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.21212121212121213\n",
      "M Accuracy: 0.3939393939393939\n",
      "-s 0.0\n",
      "-e 0.12121212121212122\n",
      "-r 0.30303030303030304\n",
      "-n 0.5454545454545454\n",
      "Other: 0.030303030303030304 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5333333333333333\n",
      "F Accuracy: 0.4666666666666667\n",
      "-s 0.0\n",
      "-e 0.2\n",
      "-r 0.23333333333333334\n",
      "-n 0.5666666666666667\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.4117647058823529\n",
      "N Accuracy: 0.5294117647058824\n",
      "-s 0.0\n",
      "-e 0.23529411764705882\n",
      "-r 0.23529411764705882\n",
      "-n 0.5294117647058824\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 240_18\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.675\n",
      "Test accuracy: 0.65\n",
      "-s 0.05\n",
      "-e 0.2\n",
      "-r 0.15\n",
      "-n 0.5625\n",
      "Other: 0.0375 [('l', 2), ('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5277777777777778\n",
      "M Accuracy: 0.5\n",
      "-s 0.1111111111111111\n",
      "-e 0.3611111111111111\n",
      "-r 0.2222222222222222\n",
      "-n 0.3055555555555556\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.84375\n",
      "F Accuracy: 0.9375\n",
      "-s 0.0\n",
      "-e 0.03125\n",
      "-r 0.03125\n",
      "-n 0.9375\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.6666666666666666\n",
      "N Accuracy: 0.3333333333333333\n",
      "-s 0.0\n",
      "-e 0.16666666666666666\n",
      "-r 0.25\n",
      "-n 0.3333333333333333\n",
      "Other: 0.25 [('l', 2), ('t', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "Test accuracy: 0.4125\n",
      "-s 0.0375\n",
      "-e 0.1625\n",
      "-r 0.3125\n",
      "-n 0.3625\n",
      "Other: 0.125 [('l', 7), ('t', 2), ('b', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.19444444444444445\n",
      "M Accuracy: 0.3888888888888889\n",
      "-s 0.05555555555555555\n",
      "-e 0.1388888888888889\n",
      "-r 0.2777777777777778\n",
      "-n 0.3888888888888889\n",
      "Other: 0.1388888888888889 [('l', 3), ('t', 1), ('b', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3125\n",
      "F Accuracy: 0.40625\n",
      "-s 0.03125\n",
      "-e 0.15625\n",
      "-r 0.3125\n",
      "-n 0.34375\n",
      "Other: 0.15625 [('l', 4), ('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "N Accuracy: 0.5\n",
      "-s 0.0\n",
      "-e 0.25\n",
      "-r 0.4166666666666667\n",
      "-n 0.3333333333333333\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 240_19\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.55\n",
      "Test accuracy: 0.725\n",
      "-s 0.0375\n",
      "-e 0.475\n",
      "-r 0.15\n",
      "-n 0.3125\n",
      "Other: 0.025 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5853658536585366\n",
      "M Accuracy: 0.7073170731707317\n",
      "-s 0.024390243902439025\n",
      "-e 0.6341463414634146\n",
      "-r 0.24390243902439024\n",
      "-n 0.07317073170731707\n",
      "Other: 0.024390243902439025 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5416666666666666\n",
      "F Accuracy: 0.75\n",
      "-s 0.041666666666666664\n",
      "-e 0.20833333333333334\n",
      "-r 0.0\n",
      "-n 0.75\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.4666666666666667\n",
      "N Accuracy: 0.7333333333333333\n",
      "-s 0.06666666666666667\n",
      "-e 0.4666666666666667\n",
      "-r 0.13333333333333333\n",
      "-n 0.26666666666666666\n",
      "Other: 0.06666666666666667 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.35\n",
      "Test accuracy: 0.4625\n",
      "-s 0.0125\n",
      "-e 0.325\n",
      "-r 0.25\n",
      "-n 0.375\n",
      "Other: 0.0375 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.36585365853658536\n",
      "M Accuracy: 0.5365853658536586\n",
      "-s 0.024390243902439025\n",
      "-e 0.3170731707317073\n",
      "-r 0.21951219512195122\n",
      "-n 0.4146341463414634\n",
      "Other: 0.024390243902439025 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2916666666666667\n",
      "F Accuracy: 0.4166666666666667\n",
      "-s 0.0\n",
      "-e 0.2916666666666667\n",
      "-r 0.25\n",
      "-n 0.375\n",
      "Other: 0.08333333333333333 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.4\n",
      "N Accuracy: 0.3333333333333333\n",
      "-s 0.0\n",
      "-e 0.4\n",
      "-r 0.3333333333333333\n",
      "-n 0.26666666666666666\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 240_20\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6125\n",
      "Test accuracy: 0.5375\n",
      "-s 0.0\n",
      "-e 0.225\n",
      "-r 0.2875\n",
      "-n 0.4\n",
      "Other: 0.0875 [('l', 6), ('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5128205128205128\n",
      "M Accuracy: 0.38461538461538464\n",
      "-s 0.0\n",
      "-e 0.3333333333333333\n",
      "-r 0.38461538461538464\n",
      "-n 0.15384615384615385\n",
      "Other: 0.1282051282051282 [('l', 4), ('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.7333333333333333\n",
      "F Accuracy: 0.8\n",
      "-s 0.0\n",
      "-e 0.06666666666666667\n",
      "-r 0.1\n",
      "-n 0.8\n",
      "Other: 0.03333333333333333 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6363636363636364\n",
      "N Accuracy: 0.36363636363636365\n",
      "-s 0.0\n",
      "-e 0.2727272727272727\n",
      "-r 0.45454545454545453\n",
      "-n 0.18181818181818182\n",
      "Other: 0.09090909090909091 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.225\n",
      "Test accuracy: 0.3125\n",
      "-s 0.0\n",
      "-e 0.1\n",
      "-r 0.55\n",
      "-n 0.3\n",
      "Other: 0.05 [('l', 3), ('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.23076923076923078\n",
      "M Accuracy: 0.38461538461538464\n",
      "-s 0.0\n",
      "-e 0.05128205128205128\n",
      "-r 0.46153846153846156\n",
      "-n 0.46153846153846156\n",
      "Other: 0.02564102564102564 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2\n",
      "F Accuracy: 0.2\n",
      "-s 0.0\n",
      "-e 0.13333333333333333\n",
      "-r 0.6\n",
      "-n 0.16666666666666666\n",
      "Other: 0.1 [('l', 2), ('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2727272727272727\n",
      "N Accuracy: 0.36363636363636365\n",
      "-s 0.0\n",
      "-e 0.18181818181818182\n",
      "-r 0.7272727272727273\n",
      "-n 0.09090909090909091\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 240_21\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6\n",
      "Test accuracy: 0.7\n",
      "-s 0.0\n",
      "-e 0.4375\n",
      "-r 0.0625\n",
      "-n 0.4875\n",
      "Other: 0.0125 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5882352941176471\n",
      "M Accuracy: 0.8235294117647058\n",
      "-s 0.0\n",
      "-e 0.6176470588235294\n",
      "-r 0.11764705882352941\n",
      "-n 0.23529411764705882\n",
      "Other: 0.029411764705882353 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.76\n",
      "F Accuracy: 0.88\n",
      "-s 0.0\n",
      "-e 0.08\n",
      "-r 0.0\n",
      "-n 0.92\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.42857142857142855\n",
      "N Accuracy: 0.2857142857142857\n",
      "-s 0.0\n",
      "-e 0.5714285714285714\n",
      "-r 0.047619047619047616\n",
      "-n 0.38095238095238093\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.3375\n",
      "Test accuracy: 0.3125\n",
      "-s 0.0\n",
      "-e 0.3125\n",
      "-r 0.35\n",
      "-n 0.275\n",
      "Other: 0.0625 [('l', 3), ('m', 1), ('a', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.4117647058823529\n",
      "M Accuracy: 0.20588235294117646\n",
      "-s 0.0\n",
      "-e 0.3235294117647059\n",
      "-r 0.29411764705882354\n",
      "-n 0.2647058823529412\n",
      "Other: 0.11764705882352941 [('l', 2), ('m', 1), ('a', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.24\n",
      "F Accuracy: 0.28\n",
      "-s 0.0\n",
      "-e 0.28\n",
      "-r 0.36\n",
      "-n 0.32\n",
      "Other: 0.04 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3333333333333333\n",
      "N Accuracy: 0.5238095238095238\n",
      "-s 0.0\n",
      "-e 0.3333333333333333\n",
      "-r 0.42857142857142855\n",
      "-n 0.23809523809523808\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 240_22\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.525\n",
      "Test accuracy: 0.6375\n",
      "-s 0.0\n",
      "-e 0.4125\n",
      "-r 0.15\n",
      "-n 0.3875\n",
      "Other: 0.05 [('l', 3), ('z', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5142857142857142\n",
      "M Accuracy: 0.6\n",
      "-s 0.0\n",
      "-e 0.6571428571428571\n",
      "-r 0.2\n",
      "-n 0.05714285714285714\n",
      "Other: 0.08571428571428572 [('l', 2), ('z', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5483870967741935\n",
      "F Accuracy: 0.7419354838709677\n",
      "-s 0.0\n",
      "-e 0.12903225806451613\n",
      "-r 0.06451612903225806\n",
      "-n 0.8064516129032258\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5\n",
      "N Accuracy: 0.5\n",
      "-s 0.0\n",
      "-e 0.42857142857142855\n",
      "-r 0.21428571428571427\n",
      "-n 0.2857142857142857\n",
      "Other: 0.07142857142857142 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.275\n",
      "Test accuracy: 0.525\n",
      "-s 0.0375\n",
      "-e 0.3\n",
      "-r 0.2375\n",
      "-n 0.3625\n",
      "Other: 0.0625 [('l', 3), ('z', 1), ('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.17142857142857143\n",
      "M Accuracy: 0.5714285714285714\n",
      "-s 0.0\n",
      "-e 0.42857142857142855\n",
      "-r 0.14285714285714285\n",
      "-n 0.34285714285714286\n",
      "Other: 0.08571428571428572 [('l', 2), ('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3225806451612903\n",
      "F Accuracy: 0.45161290322580644\n",
      "-s 0.06451612903225806\n",
      "-e 0.1935483870967742\n",
      "-r 0.25806451612903225\n",
      "-n 0.41935483870967744\n",
      "Other: 0.06451612903225806 [('z', 1), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.42857142857142855\n",
      "N Accuracy: 0.5714285714285714\n",
      "-s 0.07142857142857142\n",
      "-e 0.21428571428571427\n",
      "-r 0.42857142857142855\n",
      "-n 0.2857142857142857\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 240_23\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.45\n",
      "Test accuracy: 0.575\n",
      "-s 0.0125\n",
      "-e 0.2\n",
      "-r 0.2125\n",
      "-n 0.4875\n",
      "Other: 0.0875 [('l', 7)]\n",
      "% of inflections (len 1) that match most popular in training: 0.4444444444444444\n",
      "M Accuracy: 0.4166666666666667\n",
      "-s 0.027777777777777776\n",
      "-e 0.2777777777777778\n",
      "-r 0.3055555555555556\n",
      "-n 0.2777777777777778\n",
      "Other: 0.1111111111111111 [('l', 4)]\n",
      "% of inflections (len 1) that match most popular in training: 0.4\n",
      "F Accuracy: 0.8333333333333334\n",
      "-s 0.0\n",
      "-e 0.03333333333333333\n",
      "-r 0.13333333333333333\n",
      "-n 0.8333333333333334\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5714285714285714\n",
      "N Accuracy: 0.42857142857142855\n",
      "-s 0.0\n",
      "-e 0.35714285714285715\n",
      "-r 0.14285714285714285\n",
      "-n 0.2857142857142857\n",
      "Other: 0.21428571428571427 [('l', 3)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.2125\n",
      "Test accuracy: 0.45\n",
      "-s 0.025\n",
      "-e 0.025\n",
      "-r 0.3\n",
      "-n 0.6375\n",
      "Other: 0.0125 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.16666666666666666\n",
      "M Accuracy: 0.3333333333333333\n",
      "-s 0.027777777777777776\n",
      "-e 0.027777777777777776\n",
      "-r 0.3055555555555556\n",
      "-n 0.6111111111111112\n",
      "Other: 0.027777777777777776 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3\n",
      "F Accuracy: 0.5333333333333333\n",
      "-s 0.03333333333333333\n",
      "-e 0.03333333333333333\n",
      "-r 0.3\n",
      "-n 0.6333333333333333\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.14285714285714285\n",
      "N Accuracy: 0.5714285714285714\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.2857142857142857\n",
      "-n 0.7142857142857143\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 240_24\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6\n",
      "Test accuracy: 0.675\n",
      "-s 0.0125\n",
      "-e 0.3\n",
      "-r 0.225\n",
      "-n 0.425\n",
      "Other: 0.0375 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6097560975609756\n",
      "M Accuracy: 0.6097560975609756\n",
      "-s 0.0\n",
      "-e 0.5365853658536586\n",
      "-r 0.2682926829268293\n",
      "-n 0.14634146341463414\n",
      "Other: 0.04878048780487805 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6129032258064516\n",
      "F Accuracy: 0.8709677419354839\n",
      "-s 0.03225806451612903\n",
      "-e 0.0\n",
      "-r 0.0967741935483871\n",
      "-n 0.8709677419354839\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5\n",
      "N Accuracy: 0.25\n",
      "-s 0.0\n",
      "-e 0.25\n",
      "-r 0.5\n",
      "-n 0.125\n",
      "Other: 0.125 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.3\n",
      "Test accuracy: 0.275\n",
      "-s 0.0125\n",
      "-e 0.1375\n",
      "-r 0.5\n",
      "-n 0.25\n",
      "Other: 0.1 [('l', 7), ('h', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2926829268292683\n",
      "M Accuracy: 0.3170731707317073\n",
      "-s 0.0\n",
      "-e 0.07317073170731707\n",
      "-r 0.4634146341463415\n",
      "-n 0.3170731707317073\n",
      "Other: 0.14634146341463414 [('l', 5), ('h', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3870967741935484\n",
      "F Accuracy: 0.25806451612903225\n",
      "-s 0.03225806451612903\n",
      "-e 0.25806451612903225\n",
      "-r 0.45161290322580644\n",
      "-n 0.22580645161290322\n",
      "Other: 0.03225806451612903 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.0\n",
      "N Accuracy: 0.125\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.875\n",
      "-n 0.0\n",
      "Other: 0.125 [('l', 1)]\n",
      "\n",
      "Data type: 300_0\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6875\n",
      "Test accuracy: 0.8\n",
      "-s 0.0\n",
      "-e 0.2875\n",
      "-r 0.1\n",
      "-n 0.5875\n",
      "Other: 0.025 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5357142857142857\n",
      "M Accuracy: 0.75\n",
      "-s 0.0\n",
      "-e 0.6071428571428571\n",
      "-r 0.21428571428571427\n",
      "-n 0.14285714285714285\n",
      "Other: 0.03571428571428571 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.9473684210526315\n",
      "F Accuracy: 1.0\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.0\n",
      "-n 1.0\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.2857142857142857\n",
      "N Accuracy: 0.35714285714285715\n",
      "-s 0.0\n",
      "-e 0.42857142857142855\n",
      "-r 0.14285714285714285\n",
      "-n 0.35714285714285715\n",
      "Other: 0.07142857142857142 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.375\n",
      "Test accuracy: 0.4875\n",
      "-s 0.0\n",
      "-e 0.1375\n",
      "-r 0.2125\n",
      "-n 0.5875\n",
      "Other: 0.0625 [('l', 5)]\n",
      "% of inflections (len 1) that match most popular in training: 0.21428571428571427\n",
      "M Accuracy: 0.4642857142857143\n",
      "-s 0.0\n",
      "-e 0.17857142857142858\n",
      "-r 0.25\n",
      "-n 0.5357142857142857\n",
      "Other: 0.03571428571428571 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5526315789473685\n",
      "F Accuracy: 0.4473684210526316\n",
      "-s 0.0\n",
      "-e 0.10526315789473684\n",
      "-r 0.21052631578947367\n",
      "-n 0.6052631578947368\n",
      "Other: 0.07894736842105263 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.21428571428571427\n",
      "N Accuracy: 0.6428571428571429\n",
      "-s 0.0\n",
      "-e 0.14285714285714285\n",
      "-r 0.14285714285714285\n",
      "-n 0.6428571428571429\n",
      "Other: 0.07142857142857142 [('l', 1)]\n",
      "\n",
      "Data type: 300_1\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.4625\n",
      "Test accuracy: 0.675\n",
      "-s 0.025\n",
      "-e 0.1875\n",
      "-r 0.225\n",
      "-n 0.5375\n",
      "Other: 0.025 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.4857142857142857\n",
      "M Accuracy: 0.5142857142857142\n",
      "-s 0.0\n",
      "-e 0.34285714285714286\n",
      "-r 0.34285714285714286\n",
      "-n 0.2857142857142857\n",
      "Other: 0.02857142857142857 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.45161290322580644\n",
      "F Accuracy: 0.9354838709677419\n",
      "-s 0.03225806451612903\n",
      "-e 0.0\n",
      "-r 0.03225806451612903\n",
      "-n 0.9354838709677419\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.42857142857142855\n",
      "N Accuracy: 0.5\n",
      "-s 0.07142857142857142\n",
      "-e 0.21428571428571427\n",
      "-r 0.35714285714285715\n",
      "-n 0.2857142857142857\n",
      "Other: 0.07142857142857142 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.2875\n",
      "Test accuracy: 0.5\n",
      "-s 0.025\n",
      "-e 0.05\n",
      "-r 0.2125\n",
      "-n 0.6625\n",
      "Other: 0.05 [('l', 3), ('o', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.22857142857142856\n",
      "M Accuracy: 0.45714285714285713\n",
      "-s 0.02857142857142857\n",
      "-e 0.05714285714285714\n",
      "-r 0.22857142857142856\n",
      "-n 0.6285714285714286\n",
      "Other: 0.05714285714285714 [('l', 1), ('o', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.41935483870967744\n",
      "F Accuracy: 0.4838709677419355\n",
      "-s 0.03225806451612903\n",
      "-e 0.0\n",
      "-r 0.1935483870967742\n",
      "-n 0.7419354838709677\n",
      "Other: 0.03225806451612903 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.14285714285714285\n",
      "N Accuracy: 0.6428571428571429\n",
      "-s 0.0\n",
      "-e 0.14285714285714285\n",
      "-r 0.21428571428571427\n",
      "-n 0.5714285714285714\n",
      "Other: 0.07142857142857142 [('l', 1)]\n",
      "\n",
      "Data type: 300_2\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.5125\n",
      "Test accuracy: 0.7125\n",
      "-s 0.0375\n",
      "-e 0.3125\n",
      "-r 0.125\n",
      "-n 0.5\n",
      "Other: 0.025 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5833333333333334\n",
      "M Accuracy: 0.5833333333333334\n",
      "-s 0.08333333333333333\n",
      "-e 0.4722222222222222\n",
      "-r 0.2222222222222222\n",
      "-n 0.2222222222222222\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.45161290322580644\n",
      "F Accuracy: 0.9354838709677419\n",
      "-s 0.0\n",
      "-e 0.06451612903225806\n",
      "-r 0.0\n",
      "-n 0.9354838709677419\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.46153846153846156\n",
      "N Accuracy: 0.5384615384615384\n",
      "-s 0.0\n",
      "-e 0.46153846153846156\n",
      "-r 0.15384615384615385\n",
      "-n 0.23076923076923078\n",
      "Other: 0.15384615384615385 [('l', 2)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.35\n",
      "Test accuracy: 0.4375\n",
      "-s 0.025\n",
      "-e 0.2625\n",
      "-r 0.1625\n",
      "-n 0.525\n",
      "Other: 0.025 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "M Accuracy: 0.5\n",
      "-s 0.027777777777777776\n",
      "-e 0.3055555555555556\n",
      "-r 0.1388888888888889\n",
      "-n 0.4722222222222222\n",
      "Other: 0.05555555555555555 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.45161290322580644\n",
      "F Accuracy: 0.45161290322580644\n",
      "-s 0.03225806451612903\n",
      "-e 0.1935483870967742\n",
      "-r 0.22580645161290322\n",
      "-n 0.5483870967741935\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.38461538461538464\n",
      "N Accuracy: 0.23076923076923078\n",
      "-s 0.0\n",
      "-e 0.3076923076923077\n",
      "-r 0.07692307692307693\n",
      "-n 0.6153846153846154\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 300_3\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6375\n",
      "Test accuracy: 0.7375\n",
      "-s 0.0125\n",
      "-e 0.2625\n",
      "-r 0.175\n",
      "-n 0.5125\n",
      "Other: 0.0375 [('l', 2), ('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6538461538461539\n",
      "M Accuracy: 0.6538461538461539\n",
      "-s 0.0\n",
      "-e 0.4230769230769231\n",
      "-r 0.2692307692307692\n",
      "-n 0.23076923076923078\n",
      "Other: 0.07692307692307693 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.7647058823529411\n",
      "F Accuracy: 0.9117647058823529\n",
      "-s 0.0\n",
      "-e 0.029411764705882353\n",
      "-r 0.029411764705882353\n",
      "-n 0.9411764705882353\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.4\n",
      "N Accuracy: 0.55\n",
      "-s 0.05\n",
      "-e 0.45\n",
      "-r 0.3\n",
      "-n 0.15\n",
      "Other: 0.05 [('t', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.275\n",
      "Test accuracy: 0.2875\n",
      "-s 0.0125\n",
      "-e 0.2125\n",
      "-r 0.375\n",
      "-n 0.2125\n",
      "Other: 0.1875 [('l', 8), ('c', 1), ('p', 1), ('b', 1), ('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.23076923076923078\n",
      "M Accuracy: 0.23076923076923078\n",
      "-s 0.0\n",
      "-e 0.19230769230769232\n",
      "-r 0.5384615384615384\n",
      "-n 0.11538461538461539\n",
      "Other: 0.15384615384615385 [('b', 1), ('u', 1), ('z', 1), ('y', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.35294117647058826\n",
      "F Accuracy: 0.4411764705882353\n",
      "-s 0.029411764705882353\n",
      "-e 0.20588235294117646\n",
      "-r 0.20588235294117646\n",
      "-n 0.3235294117647059\n",
      "Other: 0.23529411764705882 [('l', 6), ('p', 1), ('o', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2\n",
      "N Accuracy: 0.1\n",
      "-s 0.0\n",
      "-e 0.25\n",
      "-r 0.45\n",
      "-n 0.15\n",
      "Other: 0.15 [('l', 2), ('c', 1)]\n",
      "\n",
      "Data type: 300_4\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6375\n",
      "Test accuracy: 0.775\n",
      "-s 0.0\n",
      "-e 0.3125\n",
      "-r 0.15\n",
      "-n 0.5\n",
      "Other: 0.0375 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6363636363636364\n",
      "M Accuracy: 0.8787878787878788\n",
      "-s 0.0\n",
      "-e 0.5454545454545454\n",
      "-r 0.21212121212121213\n",
      "-n 0.18181818181818182\n",
      "Other: 0.06060606060606061 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.78125\n",
      "F Accuracy: 0.875\n",
      "-s 0.0\n",
      "-e 0.03125\n",
      "-r 0.0625\n",
      "-n 0.90625\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.3333333333333333\n",
      "N Accuracy: 0.3333333333333333\n",
      "-s 0.0\n",
      "-e 0.4\n",
      "-r 0.2\n",
      "-n 0.3333333333333333\n",
      "Other: 0.06666666666666667 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.3125\n",
      "Test accuracy: 0.375\n",
      "-s 0.025\n",
      "-e 0.275\n",
      "-r 0.2625\n",
      "-n 0.325\n",
      "Other: 0.1125 [('l', 7), ('u', 1), ('f', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.21212121212121213\n",
      "M Accuracy: 0.3939393939393939\n",
      "-s 0.030303030303030304\n",
      "-e 0.42424242424242425\n",
      "-r 0.24242424242424243\n",
      "-n 0.24242424242424243\n",
      "Other: 0.06060606060606061 [('l', 1), ('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.4375\n",
      "F Accuracy: 0.4375\n",
      "-s 0.03125\n",
      "-e 0.15625\n",
      "-r 0.1875\n",
      "-n 0.46875\n",
      "Other: 0.15625 [('l', 4), ('f', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.26666666666666666\n",
      "N Accuracy: 0.2\n",
      "-s 0.0\n",
      "-e 0.2\n",
      "-r 0.4666666666666667\n",
      "-n 0.2\n",
      "Other: 0.13333333333333333 [('l', 2)]\n",
      "\n",
      "Data type: 300_5\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.5125\n",
      "Test accuracy: 0.6875\n",
      "-s 0.0125\n",
      "-e 0.4\n",
      "-r 0.1375\n",
      "-n 0.4125\n",
      "Other: 0.0375 [('o', 2), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.32432432432432434\n",
      "M Accuracy: 0.5675675675675675\n",
      "-s 0.02702702702702703\n",
      "-e 0.6216216216216216\n",
      "-r 0.16216216216216217\n",
      "-n 0.13513513513513514\n",
      "Other: 0.05405405405405406 [('o', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8928571428571429\n",
      "F Accuracy: 0.9285714285714286\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.0\n",
      "-n 1.0\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.26666666666666666\n",
      "N Accuracy: 0.5333333333333333\n",
      "-s 0.0\n",
      "-e 0.6\n",
      "-r 0.3333333333333333\n",
      "-n 0.0\n",
      "Other: 0.06666666666666667 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.1625\n",
      "Test accuracy: 0.2875\n",
      "-s 0.0\n",
      "-e 0.3875\n",
      "-r 0.4375\n",
      "-n 0.075\n",
      "Other: 0.1 [('o', 3), ('a', 3), ('i', 1), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.1891891891891892\n",
      "M Accuracy: 0.13513513513513514\n",
      "-s 0.0\n",
      "-e 0.35135135135135137\n",
      "-r 0.5135135135135135\n",
      "-n 0.02702702702702703\n",
      "Other: 0.10810810810810811 [('o', 2), ('i', 1), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.14285714285714285\n",
      "F Accuracy: 0.35714285714285715\n",
      "-s 0.0\n",
      "-e 0.39285714285714285\n",
      "-r 0.42857142857142855\n",
      "-n 0.10714285714285714\n",
      "Other: 0.07142857142857142 [('o', 1), ('a', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.13333333333333333\n",
      "N Accuracy: 0.5333333333333333\n",
      "-s 0.0\n",
      "-e 0.4666666666666667\n",
      "-r 0.26666666666666666\n",
      "-n 0.13333333333333333\n",
      "Other: 0.13333333333333333 [('a', 2)]\n",
      "\n",
      "Data type: 300_6\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.75\n",
      "Test accuracy: 0.7875\n",
      "-s 0.0125\n",
      "-e 0.2875\n",
      "-r 0.1375\n",
      "-n 0.55\n",
      "Other: 0.0125 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.7222222222222222\n",
      "M Accuracy: 0.75\n",
      "-s 0.0\n",
      "-e 0.5\n",
      "-r 0.3055555555555556\n",
      "-n 0.16666666666666666\n",
      "Other: 0.027777777777777776 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8571428571428571\n",
      "F Accuracy: 0.9714285714285714\n",
      "-s 0.0\n",
      "-e 0.02857142857142857\n",
      "-r 0.0\n",
      "-n 0.9714285714285714\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.4444444444444444\n",
      "N Accuracy: 0.2222222222222222\n",
      "-s 0.1111111111111111\n",
      "-e 0.4444444444444444\n",
      "-r 0.0\n",
      "-n 0.4444444444444444\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.2625\n",
      "Test accuracy: 0.55\n",
      "-s 0.0125\n",
      "-e 0.1875\n",
      "-r 0.2375\n",
      "-n 0.5125\n",
      "Other: 0.05 [('l', 3), ('i', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.19444444444444445\n",
      "M Accuracy: 0.6111111111111112\n",
      "-s 0.0\n",
      "-e 0.19444444444444445\n",
      "-r 0.16666666666666666\n",
      "-n 0.5833333333333334\n",
      "Other: 0.05555555555555555 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.37142857142857144\n",
      "F Accuracy: 0.5142857142857142\n",
      "-s 0.0\n",
      "-e 0.14285714285714285\n",
      "-r 0.34285714285714286\n",
      "-n 0.45714285714285713\n",
      "Other: 0.05714285714285714 [('l', 1), ('i', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.1111111111111111\n",
      "N Accuracy: 0.4444444444444444\n",
      "-s 0.1111111111111111\n",
      "-e 0.3333333333333333\n",
      "-r 0.1111111111111111\n",
      "-n 0.4444444444444444\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 300_7\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6625\n",
      "Test accuracy: 0.7375\n",
      "-s 0.0125\n",
      "-e 0.3\n",
      "-r 0.1375\n",
      "-n 0.5125\n",
      "Other: 0.0375 [('l', 2), ('h', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5757575757575758\n",
      "M Accuracy: 0.6666666666666666\n",
      "-s 0.030303030303030304\n",
      "-e 0.48484848484848486\n",
      "-r 0.24242424242424243\n",
      "-n 0.18181818181818182\n",
      "Other: 0.06060606060606061 [('h', 1), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8\n",
      "F Accuracy: 0.9333333333333333\n",
      "-s 0.0\n",
      "-e 0.03333333333333333\n",
      "-r 0.03333333333333333\n",
      "-n 0.9333333333333333\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5882352941176471\n",
      "N Accuracy: 0.5294117647058824\n",
      "-s 0.0\n",
      "-e 0.4117647058823529\n",
      "-r 0.11764705882352941\n",
      "-n 0.4117647058823529\n",
      "Other: 0.058823529411764705 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.2625\n",
      "Test accuracy: 0.3125\n",
      "-s 0.0125\n",
      "-e 0.3625\n",
      "-r 0.2125\n",
      "-n 0.325\n",
      "Other: 0.0875 [('l', 2), ('h', 2), ('t', 1), ('u', 1), ('d', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.30303030303030304\n",
      "M Accuracy: 0.45454545454545453\n",
      "-s 0.030303030303030304\n",
      "-e 0.3333333333333333\n",
      "-r 0.18181818181818182\n",
      "-n 0.3939393939393939\n",
      "Other: 0.06060606060606061 [('l', 1), ('h', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.36666666666666664\n",
      "F Accuracy: 0.2\n",
      "-s 0.0\n",
      "-e 0.3333333333333333\n",
      "-r 0.26666666666666666\n",
      "-n 0.36666666666666664\n",
      "Other: 0.03333333333333333 [('h', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.0\n",
      "N Accuracy: 0.23529411764705882\n",
      "-s 0.0\n",
      "-e 0.47058823529411764\n",
      "-r 0.17647058823529413\n",
      "-n 0.11764705882352941\n",
      "Other: 0.23529411764705882 [('t', 1), ('u', 1), ('d', 1), ('l', 1)]\n",
      "\n",
      "Data type: 300_8\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.5375\n",
      "Test accuracy: 0.725\n",
      "-s 0.0\n",
      "-e 0.3375\n",
      "-r 0.1875\n",
      "-n 0.45\n",
      "Other: 0.025 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6153846153846154\n",
      "M Accuracy: 0.6410256410256411\n",
      "-s 0.0\n",
      "-e 0.5641025641025641\n",
      "-r 0.23076923076923078\n",
      "-n 0.15384615384615385\n",
      "Other: 0.05128205128205128 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.4666666666666667\n",
      "F Accuracy: 0.9\n",
      "-s 0.0\n",
      "-e 0.06666666666666667\n",
      "-r 0.0\n",
      "-n 0.9333333333333333\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.45454545454545453\n",
      "N Accuracy: 0.5454545454545454\n",
      "-s 0.0\n",
      "-e 0.2727272727272727\n",
      "-r 0.5454545454545454\n",
      "-n 0.18181818181818182\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.3\n",
      "Test accuracy: 0.45\n",
      "-s 0.0125\n",
      "-e 0.05\n",
      "-r 0.2875\n",
      "-n 0.6\n",
      "Other: 0.05 [('c', 2), ('l', 1), ('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.20512820512820512\n",
      "M Accuracy: 0.48717948717948717\n",
      "-s 0.02564102564102564\n",
      "-e 0.05128205128205128\n",
      "-r 0.3333333333333333\n",
      "-n 0.5384615384615384\n",
      "Other: 0.05128205128205128 [('c', 1), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.43333333333333335\n",
      "F Accuracy: 0.4\n",
      "-s 0.0\n",
      "-e 0.06666666666666667\n",
      "-r 0.2\n",
      "-n 0.6666666666666666\n",
      "Other: 0.06666666666666667 [('t', 1), ('c', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2727272727272727\n",
      "N Accuracy: 0.45454545454545453\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.36363636363636365\n",
      "-n 0.6363636363636364\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 300_9\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.4875\n",
      "Test accuracy: 0.7125\n",
      "-s 0.0125\n",
      "-e 0.5125\n",
      "-r 0.0875\n",
      "-n 0.325\n",
      "Other: 0.0625 [('l', 3), ('u', 1), ('o', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.36585365853658536\n",
      "M Accuracy: 0.6829268292682927\n",
      "-s 0.024390243902439025\n",
      "-e 0.7317073170731707\n",
      "-r 0.14634146341463414\n",
      "-n 0.024390243902439025\n",
      "Other: 0.07317073170731707 [('l', 2), ('o', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.72\n",
      "F Accuracy: 0.8\n",
      "-s 0.0\n",
      "-e 0.08\n",
      "-r 0.04\n",
      "-n 0.84\n",
      "Other: 0.04 [('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.42857142857142855\n",
      "N Accuracy: 0.6428571428571429\n",
      "-s 0.0\n",
      "-e 0.6428571428571429\n",
      "-r 0.0\n",
      "-n 0.2857142857142857\n",
      "Other: 0.07142857142857142 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.1625\n",
      "Test accuracy: 0.3875\n",
      "-s 0.0375\n",
      "-e 0.55\n",
      "-r 0.2375\n",
      "-n 0.125\n",
      "Other: 0.05 [('u', 2), ('i', 1), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.21951219512195122\n",
      "M Accuracy: 0.4146341463414634\n",
      "-s 0.04878048780487805\n",
      "-e 0.5121951219512195\n",
      "-r 0.21951219512195122\n",
      "-n 0.14634146341463414\n",
      "Other: 0.07317073170731707 [('u', 1), ('i', 1), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.12\n",
      "F Accuracy: 0.44\n",
      "-s 0.04\n",
      "-e 0.68\n",
      "-r 0.2\n",
      "-n 0.08\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.07142857142857142\n",
      "N Accuracy: 0.21428571428571427\n",
      "-s 0.0\n",
      "-e 0.42857142857142855\n",
      "-r 0.35714285714285715\n",
      "-n 0.14285714285714285\n",
      "Other: 0.07142857142857142 [('u', 1)]\n",
      "\n",
      "Data type: 300_10\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6\n",
      "Test accuracy: 0.7375\n",
      "-s 0.0125\n",
      "-e 0.325\n",
      "-r 0.2\n",
      "-n 0.4\n",
      "Other: 0.0625 [('l', 3), ('u', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5789473684210527\n",
      "M Accuracy: 0.7105263157894737\n",
      "-s 0.0\n",
      "-e 0.5526315789473685\n",
      "-r 0.23684210526315788\n",
      "-n 0.10526315789473684\n",
      "Other: 0.10526315789473684 [('l', 2), ('u', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6333333333333333\n",
      "F Accuracy: 0.8333333333333334\n",
      "-s 0.0\n",
      "-e 0.1\n",
      "-r 0.06666666666666667\n",
      "-n 0.8333333333333334\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5833333333333334\n",
      "N Accuracy: 0.5833333333333334\n",
      "-s 0.08333333333333333\n",
      "-e 0.16666666666666666\n",
      "-r 0.4166666666666667\n",
      "-n 0.25\n",
      "Other: 0.08333333333333333 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.275\n",
      "Test accuracy: 0.4625\n",
      "-s 0.025\n",
      "-e 0.3125\n",
      "-r 0.2375\n",
      "-n 0.2875\n",
      "Other: 0.1375 [('u', 3), ('l', 3), ('a', 2), ('i', 1), ('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.42105263157894735\n",
      "M Accuracy: 0.47368421052631576\n",
      "-s 0.05263157894736842\n",
      "-e 0.39473684210526316\n",
      "-r 0.18421052631578946\n",
      "-n 0.2631578947368421\n",
      "Other: 0.10526315789473684 [('u', 1), ('a', 1), ('t', 1), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.1\n",
      "F Accuracy: 0.4666666666666667\n",
      "-s 0.0\n",
      "-e 0.26666666666666666\n",
      "-r 0.3\n",
      "-n 0.3\n",
      "Other: 0.13333333333333333 [('i', 1), ('k', 1), ('u', 1), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "N Accuracy: 0.4166666666666667\n",
      "-s 0.0\n",
      "-e 0.16666666666666666\n",
      "-r 0.25\n",
      "-n 0.3333333333333333\n",
      "Other: 0.25 [('l', 1), ('u', 1), ('a', 1)]\n",
      "\n",
      "Data type: 300_11\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.5375\n",
      "Test accuracy: 0.7\n",
      "-s 0.05\n",
      "-e 0.3625\n",
      "-r 0.1625\n",
      "-n 0.4125\n",
      "Other: 0.0125 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.475\n",
      "M Accuracy: 0.6\n",
      "-s 0.025\n",
      "-e 0.6\n",
      "-r 0.25\n",
      "-n 0.125\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.7241379310344828\n",
      "F Accuracy: 0.8620689655172413\n",
      "-s 0.0\n",
      "-e 0.034482758620689655\n",
      "-r 0.06896551724137931\n",
      "-n 0.896551724137931\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.2727272727272727\n",
      "N Accuracy: 0.6363636363636364\n",
      "-s 0.2727272727272727\n",
      "-e 0.36363636363636365\n",
      "-r 0.09090909090909091\n",
      "-n 0.18181818181818182\n",
      "Other: 0.09090909090909091 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.4\n",
      "Test accuracy: 0.375\n",
      "-s 0.0625\n",
      "-e 0.225\n",
      "-r 0.1875\n",
      "-n 0.475\n",
      "Other: 0.05 [('l', 3), ('k', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.35\n",
      "M Accuracy: 0.325\n",
      "-s 0.075\n",
      "-e 0.25\n",
      "-r 0.175\n",
      "-n 0.425\n",
      "Other: 0.075 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5172413793103449\n",
      "F Accuracy: 0.4482758620689655\n",
      "-s 0.06896551724137931\n",
      "-e 0.1724137931034483\n",
      "-r 0.13793103448275862\n",
      "-n 0.6206896551724138\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.2727272727272727\n",
      "N Accuracy: 0.36363636363636365\n",
      "-s 0.0\n",
      "-e 0.2727272727272727\n",
      "-r 0.36363636363636365\n",
      "-n 0.2727272727272727\n",
      "Other: 0.09090909090909091 [('k', 1)]\n",
      "\n",
      "Data type: 300_12\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.65\n",
      "Test accuracy: 0.7125\n",
      "-s 0.0375\n",
      "-e 0.3625\n",
      "-r 0.1625\n",
      "-n 0.4375\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5789473684210527\n",
      "M Accuracy: 0.7631578947368421\n",
      "-s 0.0\n",
      "-e 0.5263157894736842\n",
      "-r 0.2631578947368421\n",
      "-n 0.21052631578947367\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.8214285714285714\n",
      "F Accuracy: 0.8214285714285714\n",
      "-s 0.03571428571428571\n",
      "-e 0.10714285714285714\n",
      "-r 0.03571428571428571\n",
      "-n 0.8214285714285714\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5\n",
      "N Accuracy: 0.35714285714285715\n",
      "-s 0.14285714285714285\n",
      "-e 0.42857142857142855\n",
      "-r 0.14285714285714285\n",
      "-n 0.2857142857142857\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.3\n",
      "Test accuracy: 0.35\n",
      "-s 0.125\n",
      "-e 0.1625\n",
      "-r 0.3125\n",
      "-n 0.325\n",
      "Other: 0.075 [('l', 2), ('i', 1), ('u', 1), ('z', 1), ('a', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2894736842105263\n",
      "M Accuracy: 0.34210526315789475\n",
      "-s 0.15789473684210525\n",
      "-e 0.21052631578947367\n",
      "-r 0.23684210526315788\n",
      "-n 0.34210526315789475\n",
      "Other: 0.05263157894736842 [('l', 1), ('z', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.32142857142857145\n",
      "F Accuracy: 0.32142857142857145\n",
      "-s 0.14285714285714285\n",
      "-e 0.10714285714285714\n",
      "-r 0.35714285714285715\n",
      "-n 0.2857142857142857\n",
      "Other: 0.10714285714285714 [('l', 1), ('i', 1), ('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2857142857142857\n",
      "N Accuracy: 0.42857142857142855\n",
      "-s 0.0\n",
      "-e 0.14285714285714285\n",
      "-r 0.42857142857142855\n",
      "-n 0.35714285714285715\n",
      "Other: 0.07142857142857142 [('a', 1)]\n",
      "\n",
      "Data type: 300_13\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.7125\n",
      "Test accuracy: 0.7375\n",
      "-s 0.0\n",
      "-e 0.2375\n",
      "-r 0.1375\n",
      "-n 0.5625\n",
      "Other: 0.0625 [('l', 4), ('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6363636363636364\n",
      "M Accuracy: 0.6666666666666666\n",
      "-s 0.0\n",
      "-e 0.45454545454545453\n",
      "-r 0.21212121212121213\n",
      "-n 0.24242424242424243\n",
      "Other: 0.09090909090909091 [('l', 2), ('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8823529411764706\n",
      "F Accuracy: 1.0\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.0\n",
      "-n 1.0\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.46153846153846156\n",
      "N Accuracy: 0.23076923076923078\n",
      "-s 0.0\n",
      "-e 0.3076923076923077\n",
      "-r 0.3076923076923077\n",
      "-n 0.23076923076923078\n",
      "Other: 0.15384615384615385 [('l', 2)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.2375\n",
      "Test accuracy: 0.3375\n",
      "-s 0.0\n",
      "-e 0.3375\n",
      "-r 0.3625\n",
      "-n 0.175\n",
      "Other: 0.125 [('l', 6), ('f', 3), ('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.21212121212121213\n",
      "M Accuracy: 0.2727272727272727\n",
      "-s 0.0\n",
      "-e 0.2727272727272727\n",
      "-r 0.45454545454545453\n",
      "-n 0.12121212121212122\n",
      "Other: 0.15151515151515152 [('l', 3), ('u', 1), ('f', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2647058823529412\n",
      "F Accuracy: 0.38235294117647056\n",
      "-s 0.0\n",
      "-e 0.4411764705882353\n",
      "-r 0.2647058823529412\n",
      "-n 0.20588235294117646\n",
      "Other: 0.08823529411764706 [('l', 2), ('f', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.23076923076923078\n",
      "N Accuracy: 0.38461538461538464\n",
      "-s 0.0\n",
      "-e 0.23076923076923078\n",
      "-r 0.38461538461538464\n",
      "-n 0.23076923076923078\n",
      "Other: 0.15384615384615385 [('f', 1), ('l', 1)]\n",
      "\n",
      "Data type: 300_14\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.7\n",
      "Test accuracy: 0.7375\n",
      "-s 0.0125\n",
      "-e 0.2375\n",
      "-r 0.2\n",
      "-n 0.525\n",
      "Other: 0.025 [('g', 1), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6857142857142857\n",
      "M Accuracy: 0.7428571428571429\n",
      "-s 0.02857142857142857\n",
      "-e 0.42857142857142855\n",
      "-r 0.2571428571428571\n",
      "-n 0.2571428571428571\n",
      "Other: 0.02857142857142857 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8518518518518519\n",
      "F Accuracy: 0.9259259259259259\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.07407407407407407\n",
      "-n 0.9259259259259259\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5\n",
      "N Accuracy: 0.4444444444444444\n",
      "-s 0.0\n",
      "-e 0.2222222222222222\n",
      "-r 0.2777777777777778\n",
      "-n 0.4444444444444444\n",
      "Other: 0.05555555555555555 [('g', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.3875\n",
      "Test accuracy: 0.475\n",
      "-s 0.0375\n",
      "-e 0.1625\n",
      "-r 0.3375\n",
      "-n 0.4125\n",
      "Other: 0.05 [('l', 3), ('f', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.4\n",
      "M Accuracy: 0.5428571428571428\n",
      "-s 0.05714285714285714\n",
      "-e 0.2\n",
      "-r 0.34285714285714286\n",
      "-n 0.37142857142857144\n",
      "Other: 0.02857142857142857 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.4444444444444444\n",
      "F Accuracy: 0.4444444444444444\n",
      "-s 0.037037037037037035\n",
      "-e 0.14814814814814814\n",
      "-r 0.25925925925925924\n",
      "-n 0.48148148148148145\n",
      "Other: 0.07407407407407407 [('f', 1), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2777777777777778\n",
      "N Accuracy: 0.3888888888888889\n",
      "-s 0.0\n",
      "-e 0.1111111111111111\n",
      "-r 0.4444444444444444\n",
      "-n 0.3888888888888889\n",
      "Other: 0.05555555555555555 [('l', 1)]\n",
      "\n",
      "Data type: 300_15\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.525\n",
      "Test accuracy: 0.675\n",
      "-s 0.0375\n",
      "-e 0.25\n",
      "-r 0.2125\n",
      "-n 0.4875\n",
      "Other: 0.0125 [('d', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6060606060606061\n",
      "M Accuracy: 0.6666666666666666\n",
      "-s 0.0\n",
      "-e 0.3939393939393939\n",
      "-r 0.30303030303030304\n",
      "-n 0.30303030303030304\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.47058823529411764\n",
      "F Accuracy: 0.7352941176470589\n",
      "-s 0.08823529411764706\n",
      "-e 0.11764705882352941\n",
      "-r 0.058823529411764705\n",
      "-n 0.7352941176470589\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.46153846153846156\n",
      "N Accuracy: 0.5384615384615384\n",
      "-s 0.0\n",
      "-e 0.23076923076923078\n",
      "-r 0.38461538461538464\n",
      "-n 0.3076923076923077\n",
      "Other: 0.07692307692307693 [('d', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.2625\n",
      "Test accuracy: 0.4125\n",
      "-s 0.0625\n",
      "-e 0.15\n",
      "-r 0.225\n",
      "-n 0.45\n",
      "Other: 0.1125 [('l', 4), ('z', 1), ('t', 1), ('c', 1), ('p', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2727272727272727\n",
      "M Accuracy: 0.30303030303030304\n",
      "-s 0.06060606060606061\n",
      "-e 0.2727272727272727\n",
      "-r 0.15151515151515152\n",
      "-n 0.42424242424242425\n",
      "Other: 0.09090909090909091 [('l', 2), ('z', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.23529411764705882\n",
      "F Accuracy: 0.4411764705882353\n",
      "-s 0.08823529411764706\n",
      "-e 0.08823529411764706\n",
      "-r 0.17647058823529413\n",
      "-n 0.47058823529411764\n",
      "Other: 0.17647058823529413 [('l', 2), ('t', 1), ('c', 1), ('p', 1), ('g', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3076923076923077\n",
      "N Accuracy: 0.6153846153846154\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.5384615384615384\n",
      "-n 0.46153846153846156\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 300_16\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.5375\n",
      "Test accuracy: 0.75\n",
      "-s 0.05\n",
      "-e 0.2625\n",
      "-r 0.125\n",
      "-n 0.5625\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.631578947368421\n",
      "M Accuracy: 0.6842105263157895\n",
      "-s 0.07894736842105263\n",
      "-e 0.42105263157894735\n",
      "-r 0.21052631578947367\n",
      "-n 0.2894736842105263\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.3870967741935484\n",
      "F Accuracy: 0.9354838709677419\n",
      "-s 0.03225806451612903\n",
      "-e 0.0\n",
      "-r 0.0\n",
      "-n 0.967741935483871\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.6363636363636364\n",
      "N Accuracy: 0.45454545454545453\n",
      "-s 0.0\n",
      "-e 0.45454545454545453\n",
      "-r 0.18181818181818182\n",
      "-n 0.36363636363636365\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "Test accuracy: 0.4125\n",
      "-s 0.0625\n",
      "-e 0.2125\n",
      "-r 0.25\n",
      "-n 0.375\n",
      "Other: 0.1 [('l', 5), ('g', 1), ('t', 1), ('d', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.34210526315789475\n",
      "M Accuracy: 0.47368421052631576\n",
      "-s 0.02631578947368421\n",
      "-e 0.23684210526315788\n",
      "-r 0.23684210526315788\n",
      "-n 0.42105263157894735\n",
      "Other: 0.07894736842105263 [('l', 2), ('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.1935483870967742\n",
      "F Accuracy: 0.3225806451612903\n",
      "-s 0.06451612903225806\n",
      "-e 0.22580645161290322\n",
      "-r 0.25806451612903225\n",
      "-n 0.3548387096774194\n",
      "Other: 0.0967741935483871 [('l', 2), ('g', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.09090909090909091\n",
      "N Accuracy: 0.45454545454545453\n",
      "-s 0.18181818181818182\n",
      "-e 0.09090909090909091\n",
      "-r 0.2727272727272727\n",
      "-n 0.2727272727272727\n",
      "Other: 0.18181818181818182 [('d', 1), ('l', 1)]\n",
      "\n",
      "Data type: 300_17\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.5625\n",
      "Test accuracy: 0.7375\n",
      "-s 0.0\n",
      "-e 0.3375\n",
      "-r 0.15\n",
      "-n 0.475\n",
      "Other: 0.0375 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6060606060606061\n",
      "M Accuracy: 0.696969696969697\n",
      "-s 0.0\n",
      "-e 0.5757575757575758\n",
      "-r 0.21212121212121213\n",
      "-n 0.21212121212121213\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5333333333333333\n",
      "F Accuracy: 0.9666666666666667\n",
      "-s 0.0\n",
      "-e 0.03333333333333333\n",
      "-r 0.0\n",
      "-n 0.9666666666666667\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5294117647058824\n",
      "N Accuracy: 0.4117647058823529\n",
      "-s 0.0\n",
      "-e 0.4117647058823529\n",
      "-r 0.29411764705882354\n",
      "-n 0.11764705882352941\n",
      "Other: 0.17647058823529413 [('l', 3)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.3125\n",
      "Test accuracy: 0.4\n",
      "-s 0.0375\n",
      "-e 0.3\n",
      "-r 0.2875\n",
      "-n 0.325\n",
      "Other: 0.05 [('l', 4)]\n",
      "% of inflections (len 1) that match most popular in training: 0.36363636363636365\n",
      "M Accuracy: 0.3333333333333333\n",
      "-s 0.0\n",
      "-e 0.24242424242424243\n",
      "-r 0.3333333333333333\n",
      "-n 0.3333333333333333\n",
      "Other: 0.09090909090909091 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.26666666666666666\n",
      "F Accuracy: 0.4666666666666667\n",
      "-s 0.06666666666666667\n",
      "-e 0.3333333333333333\n",
      "-r 0.2\n",
      "-n 0.36666666666666664\n",
      "Other: 0.03333333333333333 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.29411764705882354\n",
      "N Accuracy: 0.4117647058823529\n",
      "-s 0.058823529411764705\n",
      "-e 0.35294117647058826\n",
      "-r 0.35294117647058826\n",
      "-n 0.23529411764705882\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 300_18\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.75\n",
      "Test accuracy: 0.7125\n",
      "-s 0.0125\n",
      "-e 0.3375\n",
      "-r 0.125\n",
      "-n 0.4875\n",
      "Other: 0.0375 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.7777777777777778\n",
      "M Accuracy: 0.6666666666666666\n",
      "-s 0.027777777777777776\n",
      "-e 0.5277777777777778\n",
      "-r 0.16666666666666666\n",
      "-n 0.19444444444444445\n",
      "Other: 0.08333333333333333 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.875\n",
      "F Accuracy: 0.875\n",
      "-s 0.0\n",
      "-e 0.09375\n",
      "-r 0.03125\n",
      "-n 0.875\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.3333333333333333\n",
      "N Accuracy: 0.4166666666666667\n",
      "-s 0.0\n",
      "-e 0.4166666666666667\n",
      "-r 0.25\n",
      "-n 0.3333333333333333\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.3875\n",
      "Test accuracy: 0.4875\n",
      "-s 0.05\n",
      "-e 0.125\n",
      "-r 0.2125\n",
      "-n 0.525\n",
      "Other: 0.0875 [('l', 5), ('i', 1), ('m', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3055555555555556\n",
      "M Accuracy: 0.4444444444444444\n",
      "-s 0.05555555555555555\n",
      "-e 0.1111111111111111\n",
      "-r 0.19444444444444445\n",
      "-n 0.6111111111111112\n",
      "Other: 0.027777777777777776 [('m', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.53125\n",
      "F Accuracy: 0.5\n",
      "-s 0.03125\n",
      "-e 0.09375\n",
      "-r 0.25\n",
      "-n 0.5\n",
      "Other: 0.125 [('l', 3), ('i', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "N Accuracy: 0.5833333333333334\n",
      "-s 0.08333333333333333\n",
      "-e 0.25\n",
      "-r 0.16666666666666666\n",
      "-n 0.3333333333333333\n",
      "Other: 0.16666666666666666 [('l', 2)]\n",
      "\n",
      "Data type: 300_19\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6875\n",
      "Test accuracy: 0.75\n",
      "-s 0.0\n",
      "-e 0.325\n",
      "-r 0.2125\n",
      "-n 0.45\n",
      "Other: 0.0125 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5609756097560976\n",
      "M Accuracy: 0.6341463414634146\n",
      "-s 0.0\n",
      "-e 0.5365853658536586\n",
      "-r 0.3170731707317073\n",
      "-n 0.14634146341463414\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.9166666666666666\n",
      "F Accuracy: 1.0\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.0\n",
      "-n 1.0\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.6666666666666666\n",
      "N Accuracy: 0.6666666666666666\n",
      "-s 0.0\n",
      "-e 0.26666666666666666\n",
      "-r 0.26666666666666666\n",
      "-n 0.4\n",
      "Other: 0.06666666666666667 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.275\n",
      "Test accuracy: 0.4375\n",
      "-s 0.0\n",
      "-e 0.1625\n",
      "-r 0.4125\n",
      "-n 0.3875\n",
      "Other: 0.0375 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2682926829268293\n",
      "M Accuracy: 0.5365853658536586\n",
      "-s 0.0\n",
      "-e 0.12195121951219512\n",
      "-r 0.3902439024390244\n",
      "-n 0.4146341463414634\n",
      "Other: 0.07317073170731707 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3333333333333333\n",
      "F Accuracy: 0.25\n",
      "-s 0.0\n",
      "-e 0.16666666666666666\n",
      "-r 0.4166666666666667\n",
      "-n 0.4166666666666667\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.2\n",
      "N Accuracy: 0.4666666666666667\n",
      "-s 0.0\n",
      "-e 0.26666666666666666\n",
      "-r 0.4666666666666667\n",
      "-n 0.26666666666666666\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 300_20\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.725\n",
      "Test accuracy: 0.775\n",
      "-s 0.0\n",
      "-e 0.3\n",
      "-r 0.1125\n",
      "-n 0.5125\n",
      "Other: 0.075 [('l', 6)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6410256410256411\n",
      "M Accuracy: 0.6923076923076923\n",
      "-s 0.0\n",
      "-e 0.5128205128205128\n",
      "-r 0.15384615384615385\n",
      "-n 0.23076923076923078\n",
      "Other: 0.10256410256410256 [('l', 4)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8333333333333334\n",
      "F Accuracy: 1.0\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.0\n",
      "-n 1.0\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.7272727272727273\n",
      "N Accuracy: 0.45454545454545453\n",
      "-s 0.0\n",
      "-e 0.36363636363636365\n",
      "-r 0.2727272727272727\n",
      "-n 0.18181818181818182\n",
      "Other: 0.18181818181818182 [('l', 2)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.375\n",
      "Test accuracy: 0.575\n",
      "-s 0.0\n",
      "-e 0.1625\n",
      "-r 0.1375\n",
      "-n 0.6625\n",
      "Other: 0.0375 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2564102564102564\n",
      "M Accuracy: 0.6410256410256411\n",
      "-s 0.0\n",
      "-e 0.15384615384615385\n",
      "-r 0.20512820512820512\n",
      "-n 0.6410256410256411\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5666666666666667\n",
      "F Accuracy: 0.5333333333333333\n",
      "-s 0.0\n",
      "-e 0.2\n",
      "-r 0.03333333333333333\n",
      "-n 0.6666666666666666\n",
      "Other: 0.1 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2727272727272727\n",
      "N Accuracy: 0.45454545454545453\n",
      "-s 0.0\n",
      "-e 0.09090909090909091\n",
      "-r 0.18181818181818182\n",
      "-n 0.7272727272727273\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 300_21\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6125\n",
      "Test accuracy: 0.65\n",
      "-s 0.0125\n",
      "-e 0.3375\n",
      "-r 0.1375\n",
      "-n 0.5\n",
      "Other: 0.0125 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5882352941176471\n",
      "M Accuracy: 0.6764705882352942\n",
      "-s 0.0\n",
      "-e 0.5588235294117647\n",
      "-r 0.14705882352941177\n",
      "-n 0.2647058823529412\n",
      "Other: 0.029411764705882353 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8\n",
      "F Accuracy: 0.92\n",
      "-s 0.04\n",
      "-e 0.0\n",
      "-r 0.0\n",
      "-n 0.96\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.42857142857142855\n",
      "N Accuracy: 0.2857142857142857\n",
      "-s 0.0\n",
      "-e 0.38095238095238093\n",
      "-r 0.2857142857142857\n",
      "-n 0.3333333333333333\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.3125\n",
      "Test accuracy: 0.5625\n",
      "-s 0.0\n",
      "-e 0.35\n",
      "-r 0.1\n",
      "-n 0.4875\n",
      "Other: 0.0625 [('l', 2), ('t', 1), ('a', 1), ('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.29411764705882354\n",
      "M Accuracy: 0.6176470588235294\n",
      "-s 0.0\n",
      "-e 0.3235294117647059\n",
      "-r 0.058823529411764705\n",
      "-n 0.5294117647058824\n",
      "Other: 0.08823529411764706 [('t', 1), ('a', 1), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.44\n",
      "F Accuracy: 0.44\n",
      "-s 0.0\n",
      "-e 0.2\n",
      "-r 0.2\n",
      "-n 0.56\n",
      "Other: 0.04 [('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.19047619047619047\n",
      "N Accuracy: 0.6190476190476191\n",
      "-s 0.0\n",
      "-e 0.5714285714285714\n",
      "-r 0.047619047619047616\n",
      "-n 0.3333333333333333\n",
      "Other: 0.047619047619047616 [('l', 1)]\n",
      "\n",
      "Data type: 300_22\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6\n",
      "Test accuracy: 0.7125\n",
      "-s 0.0\n",
      "-e 0.3375\n",
      "-r 0.125\n",
      "-n 0.525\n",
      "Other: 0.0125 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.4857142857142857\n",
      "M Accuracy: 0.5428571428571428\n",
      "-s 0.0\n",
      "-e 0.5142857142857142\n",
      "-r 0.2\n",
      "-n 0.2857142857142857\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.8064516129032258\n",
      "F Accuracy: 0.9354838709677419\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.0\n",
      "-n 1.0\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.42857142857142855\n",
      "N Accuracy: 0.6428571428571429\n",
      "-s 0.0\n",
      "-e 0.6428571428571429\n",
      "-r 0.21428571428571427\n",
      "-n 0.07142857142857142\n",
      "Other: 0.07142857142857142 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.2625\n",
      "Test accuracy: 0.4\n",
      "-s 0.0125\n",
      "-e 0.25\n",
      "-r 0.3\n",
      "-n 0.4\n",
      "Other: 0.0375 [('l', 2), ('m', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2\n",
      "M Accuracy: 0.4\n",
      "-s 0.02857142857142857\n",
      "-e 0.22857142857142856\n",
      "-r 0.22857142857142856\n",
      "-n 0.45714285714285713\n",
      "Other: 0.05714285714285714 [('m', 1), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2903225806451613\n",
      "F Accuracy: 0.3870967741935484\n",
      "-s 0.0\n",
      "-e 0.25806451612903225\n",
      "-r 0.3548387096774194\n",
      "-n 0.3548387096774194\n",
      "Other: 0.03225806451612903 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.35714285714285715\n",
      "N Accuracy: 0.42857142857142855\n",
      "-s 0.0\n",
      "-e 0.2857142857142857\n",
      "-r 0.35714285714285715\n",
      "-n 0.35714285714285715\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 300_23\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.7\n",
      "Test accuracy: 0.825\n",
      "-s 0.0\n",
      "-e 0.325\n",
      "-r 0.1875\n",
      "-n 0.4625\n",
      "Other: 0.025 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6388888888888888\n",
      "M Accuracy: 0.7777777777777778\n",
      "-s 0.0\n",
      "-e 0.5277777777777778\n",
      "-r 0.25\n",
      "-n 0.19444444444444445\n",
      "Other: 0.027777777777777776 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8333333333333334\n",
      "F Accuracy: 0.9333333333333333\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.06666666666666667\n",
      "-n 0.9333333333333333\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5714285714285714\n",
      "N Accuracy: 0.7142857142857143\n",
      "-s 0.0\n",
      "-e 0.5\n",
      "-r 0.2857142857142857\n",
      "-n 0.14285714285714285\n",
      "Other: 0.07142857142857142 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.3125\n",
      "Test accuracy: 0.4375\n",
      "-s 0.0125\n",
      "-e 0.1\n",
      "-r 0.3375\n",
      "-n 0.475\n",
      "Other: 0.075 [('l', 5), ('c', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "M Accuracy: 0.5277777777777778\n",
      "-s 0.027777777777777776\n",
      "-e 0.08333333333333333\n",
      "-r 0.3611111111111111\n",
      "-n 0.4166666666666667\n",
      "Other: 0.1111111111111111 [('l', 3), ('c', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.43333333333333335\n",
      "F Accuracy: 0.43333333333333335\n",
      "-s 0.0\n",
      "-e 0.13333333333333333\n",
      "-r 0.3\n",
      "-n 0.5333333333333333\n",
      "Other: 0.03333333333333333 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.21428571428571427\n",
      "N Accuracy: 0.21428571428571427\n",
      "-s 0.0\n",
      "-e 0.07142857142857142\n",
      "-r 0.35714285714285715\n",
      "-n 0.5\n",
      "Other: 0.07142857142857142 [('l', 1)]\n",
      "\n",
      "Data type: 300_24\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.625\n",
      "Test accuracy: 0.65\n",
      "-s 0.0\n",
      "-e 0.3\n",
      "-r 0.125\n",
      "-n 0.5375\n",
      "Other: 0.0375 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6341463414634146\n",
      "M Accuracy: 0.5609756097560976\n",
      "-s 0.0\n",
      "-e 0.4634146341463415\n",
      "-r 0.1951219512195122\n",
      "-n 0.3170731707317073\n",
      "Other: 0.024390243902439025 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5806451612903226\n",
      "F Accuracy: 0.8709677419354839\n",
      "-s 0.0\n",
      "-e 0.0967741935483871\n",
      "-r 0.03225806451612903\n",
      "-n 0.8709677419354839\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.75\n",
      "N Accuracy: 0.25\n",
      "-s 0.0\n",
      "-e 0.25\n",
      "-r 0.125\n",
      "-n 0.375\n",
      "Other: 0.25 [('l', 2)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.2625\n",
      "Test accuracy: 0.3125\n",
      "-s 0.0125\n",
      "-e 0.2125\n",
      "-r 0.3625\n",
      "-n 0.325\n",
      "Other: 0.0875 [('l', 7)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2682926829268293\n",
      "M Accuracy: 0.2682926829268293\n",
      "-s 0.024390243902439025\n",
      "-e 0.24390243902439024\n",
      "-r 0.2926829268292683\n",
      "-n 0.3170731707317073\n",
      "Other: 0.12195121951219512 [('l', 5)]\n",
      "% of inflections (len 1) that match most popular in training: 0.25806451612903225\n",
      "F Accuracy: 0.3870967741935484\n",
      "-s 0.0\n",
      "-e 0.22580645161290322\n",
      "-r 0.41935483870967744\n",
      "-n 0.3225806451612903\n",
      "Other: 0.03225806451612903 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "N Accuracy: 0.25\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.5\n",
      "-n 0.375\n",
      "Other: 0.125 [('l', 1)]\n",
      "\n",
      "Data type: 360_0\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.75\n",
      "Test accuracy: 0.7875\n",
      "-s 0.0375\n",
      "-e 0.25\n",
      "-r 0.1\n",
      "-n 0.575\n",
      "Other: 0.0375 [('l', 2), ('z', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6428571428571429\n",
      "M Accuracy: 0.7857142857142857\n",
      "-s 0.07142857142857142\n",
      "-e 0.5357142857142857\n",
      "-r 0.21428571428571427\n",
      "-n 0.10714285714285714\n",
      "Other: 0.07142857142857142 [('l', 1), ('z', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.9736842105263158\n",
      "F Accuracy: 0.9736842105263158\n",
      "-s 0.02631578947368421\n",
      "-e 0.0\n",
      "-r 0.0\n",
      "-n 0.9736842105263158\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.35714285714285715\n",
      "N Accuracy: 0.2857142857142857\n",
      "-s 0.0\n",
      "-e 0.35714285714285715\n",
      "-r 0.14285714285714285\n",
      "-n 0.42857142857142855\n",
      "Other: 0.07142857142857142 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.2625\n",
      "Test accuracy: 0.4125\n",
      "-s 0.0375\n",
      "-e 0.3\n",
      "-r 0.2875\n",
      "-n 0.275\n",
      "Other: 0.1 [('l', 4), ('t', 1), ('w', 1), ('z', 1), ('o', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "M Accuracy: 0.42857142857142855\n",
      "-s 0.07142857142857142\n",
      "-e 0.21428571428571427\n",
      "-r 0.42857142857142855\n",
      "-n 0.21428571428571427\n",
      "Other: 0.07142857142857142 [('z', 1), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3157894736842105\n",
      "F Accuracy: 0.3684210526315789\n",
      "-s 0.02631578947368421\n",
      "-e 0.39473684210526316\n",
      "-r 0.21052631578947367\n",
      "-n 0.2894736842105263\n",
      "Other: 0.07894736842105263 [('l', 2), ('w', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.14285714285714285\n",
      "N Accuracy: 0.5\n",
      "-s 0.0\n",
      "-e 0.21428571428571427\n",
      "-r 0.21428571428571427\n",
      "-n 0.35714285714285715\n",
      "Other: 0.21428571428571427 [('t', 1), ('l', 1), ('o', 1)]\n",
      "\n",
      "Data type: 360_1\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6375\n",
      "Test accuracy: 0.8\n",
      "-s 0.0125\n",
      "-e 0.35\n",
      "-r 0.1125\n",
      "-n 0.4875\n",
      "Other: 0.0375 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5714285714285714\n",
      "M Accuracy: 0.7428571428571429\n",
      "-s 0.02857142857142857\n",
      "-e 0.6\n",
      "-r 0.11428571428571428\n",
      "-n 0.2\n",
      "Other: 0.05714285714285714 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8064516129032258\n",
      "F Accuracy: 0.967741935483871\n",
      "-s 0.0\n",
      "-e 0.03225806451612903\n",
      "-r 0.0\n",
      "-n 0.967741935483871\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.42857142857142855\n",
      "N Accuracy: 0.5714285714285714\n",
      "-s 0.0\n",
      "-e 0.42857142857142855\n",
      "-r 0.35714285714285715\n",
      "-n 0.14285714285714285\n",
      "Other: 0.07142857142857142 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.35\n",
      "Test accuracy: 0.5375\n",
      "-s 0.0125\n",
      "-e 0.225\n",
      "-r 0.2375\n",
      "-n 0.4625\n",
      "Other: 0.0625 [('l', 2), ('m', 1), ('c', 1), ('z', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.22857142857142856\n",
      "M Accuracy: 0.4857142857142857\n",
      "-s 0.0\n",
      "-e 0.22857142857142856\n",
      "-r 0.2857142857142857\n",
      "-n 0.42857142857142855\n",
      "Other: 0.05714285714285714 [('l', 1), ('z', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5161290322580645\n",
      "F Accuracy: 0.6129032258064516\n",
      "-s 0.03225806451612903\n",
      "-e 0.22580645161290322\n",
      "-r 0.12903225806451613\n",
      "-n 0.5161290322580645\n",
      "Other: 0.0967741935483871 [('m', 1), ('l', 1), ('c', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2857142857142857\n",
      "N Accuracy: 0.5\n",
      "-s 0.0\n",
      "-e 0.21428571428571427\n",
      "-r 0.35714285714285715\n",
      "-n 0.42857142857142855\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 360_2\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.575\n",
      "Test accuracy: 0.75\n",
      "-s 0.0125\n",
      "-e 0.425\n",
      "-r 0.1375\n",
      "-n 0.4\n",
      "Other: 0.025 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.7222222222222222\n",
      "M Accuracy: 0.75\n",
      "-s 0.0\n",
      "-e 0.6666666666666666\n",
      "-r 0.25\n",
      "-n 0.05555555555555555\n",
      "Other: 0.027777777777777776 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.4838709677419355\n",
      "F Accuracy: 0.8709677419354839\n",
      "-s 0.0\n",
      "-e 0.0967741935483871\n",
      "-r 0.03225806451612903\n",
      "-n 0.8709677419354839\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.38461538461538464\n",
      "N Accuracy: 0.46153846153846156\n",
      "-s 0.07692307692307693\n",
      "-e 0.5384615384615384\n",
      "-r 0.07692307692307693\n",
      "-n 0.23076923076923078\n",
      "Other: 0.07692307692307693 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.2375\n",
      "Test accuracy: 0.2625\n",
      "-s 0.0125\n",
      "-e 0.35\n",
      "-r 0.4125\n",
      "-n 0.1625\n",
      "Other: 0.0625 [('o', 2), ('l', 2), ('p', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2222222222222222\n",
      "M Accuracy: 0.25\n",
      "-s 0.0\n",
      "-e 0.3333333333333333\n",
      "-r 0.4444444444444444\n",
      "-n 0.16666666666666666\n",
      "Other: 0.05555555555555555 [('o', 1), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.22580645161290322\n",
      "F Accuracy: 0.3225806451612903\n",
      "-s 0.03225806451612903\n",
      "-e 0.3225806451612903\n",
      "-r 0.41935483870967744\n",
      "-n 0.16129032258064516\n",
      "Other: 0.06451612903225806 [('p', 1), ('o', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3076923076923077\n",
      "N Accuracy: 0.15384615384615385\n",
      "-s 0.0\n",
      "-e 0.46153846153846156\n",
      "-r 0.3076923076923077\n",
      "-n 0.15384615384615385\n",
      "Other: 0.07692307692307693 [('l', 1)]\n",
      "\n",
      "Data type: 360_3\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6875\n",
      "Test accuracy: 0.7375\n",
      "-s 0.0125\n",
      "-e 0.2125\n",
      "-r 0.175\n",
      "-n 0.575\n",
      "Other: 0.025 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6538461538461539\n",
      "M Accuracy: 0.6923076923076923\n",
      "-s 0.0\n",
      "-e 0.34615384615384615\n",
      "-r 0.23076923076923078\n",
      "-n 0.34615384615384615\n",
      "Other: 0.07692307692307693 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8529411764705882\n",
      "F Accuracy: 0.9117647058823529\n",
      "-s 0.029411764705882353\n",
      "-e 0.0\n",
      "-r 0.029411764705882353\n",
      "-n 0.9411764705882353\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.45\n",
      "N Accuracy: 0.5\n",
      "-s 0.0\n",
      "-e 0.4\n",
      "-r 0.35\n",
      "-n 0.25\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.45\n",
      "Test accuracy: 0.4875\n",
      "-s 0.025\n",
      "-e 0.0625\n",
      "-r 0.1625\n",
      "-n 0.7375\n",
      "Other: 0.0125 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2692307692307692\n",
      "M Accuracy: 0.38461538461538464\n",
      "-s 0.07692307692307693\n",
      "-e 0.038461538461538464\n",
      "-r 0.038461538461538464\n",
      "-n 0.8076923076923077\n",
      "Other: 0.038461538461538464 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6176470588235294\n",
      "F Accuracy: 0.5588235294117647\n",
      "-s 0.0\n",
      "-e 0.11764705882352941\n",
      "-r 0.14705882352941177\n",
      "-n 0.7352941176470589\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.4\n",
      "N Accuracy: 0.5\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.35\n",
      "-n 0.65\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 360_4\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6375\n",
      "Test accuracy: 0.7375\n",
      "-s 0.0125\n",
      "-e 0.4125\n",
      "-r 0.0875\n",
      "-n 0.4375\n",
      "Other: 0.05 [('l', 3), ('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6666666666666666\n",
      "M Accuracy: 0.7575757575757576\n",
      "-s 0.030303030303030304\n",
      "-e 0.5454545454545454\n",
      "-r 0.18181818181818182\n",
      "-n 0.18181818181818182\n",
      "Other: 0.06060606060606061 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.65625\n",
      "F Accuracy: 0.8125\n",
      "-s 0.0\n",
      "-e 0.125\n",
      "-r 0.0\n",
      "-n 0.84375\n",
      "Other: 0.03125 [('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5333333333333333\n",
      "N Accuracy: 0.5333333333333333\n",
      "-s 0.0\n",
      "-e 0.7333333333333333\n",
      "-r 0.06666666666666667\n",
      "-n 0.13333333333333333\n",
      "Other: 0.06666666666666667 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.4\n",
      "Test accuracy: 0.5875\n",
      "-s 0.025\n",
      "-e 0.35\n",
      "-r 0.125\n",
      "-n 0.4625\n",
      "Other: 0.0375 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3333333333333333\n",
      "M Accuracy: 0.5151515151515151\n",
      "-s 0.06060606060606061\n",
      "-e 0.30303030303030304\n",
      "-r 0.21212121212121213\n",
      "-n 0.42424242424242425\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.40625\n",
      "F Accuracy: 0.625\n",
      "-s 0.0\n",
      "-e 0.4375\n",
      "-r 0.0625\n",
      "-n 0.40625\n",
      "Other: 0.09375 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5333333333333333\n",
      "N Accuracy: 0.6666666666666666\n",
      "-s 0.0\n",
      "-e 0.26666666666666666\n",
      "-r 0.06666666666666667\n",
      "-n 0.6666666666666666\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 360_5\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.575\n",
      "Test accuracy: 0.625\n",
      "-s 0.0125\n",
      "-e 0.2875\n",
      "-r 0.225\n",
      "-n 0.45\n",
      "Other: 0.025 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.4594594594594595\n",
      "M Accuracy: 0.4864864864864865\n",
      "-s 0.0\n",
      "-e 0.4864864864864865\n",
      "-r 0.2972972972972973\n",
      "-n 0.1891891891891892\n",
      "Other: 0.02702702702702703 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.7857142857142857\n",
      "F Accuracy: 0.8214285714285714\n",
      "-s 0.03571428571428571\n",
      "-e 0.0\n",
      "-r 0.07142857142857142\n",
      "-n 0.8928571428571429\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.4666666666666667\n",
      "N Accuracy: 0.6\n",
      "-s 0.0\n",
      "-e 0.3333333333333333\n",
      "-r 0.3333333333333333\n",
      "-n 0.26666666666666666\n",
      "Other: 0.06666666666666667 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.2125\n",
      "Test accuracy: 0.3125\n",
      "-s 0.0125\n",
      "-e 0.2125\n",
      "-r 0.5375\n",
      "-n 0.15\n",
      "Other: 0.0875 [('l', 4), ('t', 1), ('o', 1), ('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.16216216216216217\n",
      "M Accuracy: 0.2702702702702703\n",
      "-s 0.02702702702702703\n",
      "-e 0.24324324324324326\n",
      "-r 0.6216216216216216\n",
      "-n 0.08108108108108109\n",
      "Other: 0.02702702702702703 [('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "F Accuracy: 0.42857142857142855\n",
      "-s 0.0\n",
      "-e 0.17857142857142858\n",
      "-r 0.4642857142857143\n",
      "-n 0.21428571428571427\n",
      "Other: 0.14285714285714285 [('l', 2), ('t', 1), ('o', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.26666666666666666\n",
      "N Accuracy: 0.2\n",
      "-s 0.0\n",
      "-e 0.2\n",
      "-r 0.4666666666666667\n",
      "-n 0.2\n",
      "Other: 0.13333333333333333 [('l', 2)]\n",
      "\n",
      "Data type: 360_6\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.675\n",
      "Test accuracy: 0.8625\n",
      "-s 0.0125\n",
      "-e 0.3375\n",
      "-r 0.125\n",
      "-n 0.5125\n",
      "Other: 0.0125 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6111111111111112\n",
      "M Accuracy: 0.8611111111111112\n",
      "-s 0.0\n",
      "-e 0.6111111111111112\n",
      "-r 0.25\n",
      "-n 0.1111111111111111\n",
      "Other: 0.027777777777777776 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8\n",
      "F Accuracy: 1.0\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.0\n",
      "-n 1.0\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.4444444444444444\n",
      "N Accuracy: 0.3333333333333333\n",
      "-s 0.1111111111111111\n",
      "-e 0.5555555555555556\n",
      "-r 0.1111111111111111\n",
      "-n 0.2222222222222222\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.3125\n",
      "Test accuracy: 0.6125\n",
      "-s 0.025\n",
      "-e 0.2\n",
      "-r 0.1625\n",
      "-n 0.4875\n",
      "Other: 0.125 [('l', 3), ('m', 1), ('u', 1), ('f', 1), ('a', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2777777777777778\n",
      "M Accuracy: 0.5833333333333334\n",
      "-s 0.05555555555555555\n",
      "-e 0.2222222222222222\n",
      "-r 0.1388888888888889\n",
      "-n 0.5\n",
      "Other: 0.08333333333333333 [('l', 1), ('m', 1), ('a', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3142857142857143\n",
      "F Accuracy: 0.6\n",
      "-s 0.0\n",
      "-e 0.2\n",
      "-r 0.22857142857142856\n",
      "-n 0.42857142857142855\n",
      "Other: 0.14285714285714285 [('u', 1), ('f', 1), ('h', 1), ('i', 1), ('c', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.4444444444444444\n",
      "N Accuracy: 0.7777777777777778\n",
      "-s 0.0\n",
      "-e 0.1111111111111111\n",
      "-r 0.0\n",
      "-n 0.6666666666666666\n",
      "Other: 0.2222222222222222 [('l', 2)]\n",
      "\n",
      "Data type: 360_7\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.5875\n",
      "Test accuracy: 0.8\n",
      "-s 0.0\n",
      "-e 0.3875\n",
      "-r 0.125\n",
      "-n 0.4875\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.696969696969697\n",
      "M Accuracy: 0.8181818181818182\n",
      "-s 0.0\n",
      "-e 0.6060606060606061\n",
      "-r 0.2727272727272727\n",
      "-n 0.12121212121212122\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5\n",
      "F Accuracy: 0.9666666666666667\n",
      "-s 0.0\n",
      "-e 0.03333333333333333\n",
      "-r 0.0\n",
      "-n 0.9666666666666667\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5294117647058824\n",
      "N Accuracy: 0.47058823529411764\n",
      "-s 0.0\n",
      "-e 0.5882352941176471\n",
      "-r 0.058823529411764705\n",
      "-n 0.35294117647058826\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.4125\n",
      "Test accuracy: 0.4125\n",
      "-s 0.0125\n",
      "-e 0.5625\n",
      "-r 0.175\n",
      "-n 0.25\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.45454545454545453\n",
      "M Accuracy: 0.45454545454545453\n",
      "-s 0.030303030303030304\n",
      "-e 0.696969696969697\n",
      "-r 0.09090909090909091\n",
      "-n 0.18181818181818182\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.36666666666666664\n",
      "F Accuracy: 0.3\n",
      "-s 0.0\n",
      "-e 0.5\n",
      "-r 0.23333333333333334\n",
      "-n 0.26666666666666666\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.4117647058823529\n",
      "N Accuracy: 0.5294117647058824\n",
      "-s 0.0\n",
      "-e 0.4117647058823529\n",
      "-r 0.23529411764705882\n",
      "-n 0.35294117647058826\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 360_8\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6125\n",
      "Test accuracy: 0.75\n",
      "-s 0.0375\n",
      "-e 0.3375\n",
      "-r 0.1625\n",
      "-n 0.375\n",
      "Other: 0.0875 [('l', 6), ('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.7435897435897436\n",
      "M Accuracy: 0.7692307692307693\n",
      "-s 0.02564102564102564\n",
      "-e 0.6153846153846154\n",
      "-r 0.20512820512820512\n",
      "-n 0.02564102564102564\n",
      "Other: 0.1282051282051282 [('l', 4), ('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5666666666666667\n",
      "F Accuracy: 0.8666666666666667\n",
      "-s 0.03333333333333333\n",
      "-e 0.03333333333333333\n",
      "-r 0.0\n",
      "-n 0.9\n",
      "Other: 0.03333333333333333 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2727272727272727\n",
      "N Accuracy: 0.36363636363636365\n",
      "-s 0.09090909090909091\n",
      "-e 0.18181818181818182\n",
      "-r 0.45454545454545453\n",
      "-n 0.18181818181818182\n",
      "Other: 0.09090909090909091 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.3625\n",
      "Test accuracy: 0.5375\n",
      "-s 0.025\n",
      "-e 0.3625\n",
      "-r 0.1625\n",
      "-n 0.3375\n",
      "Other: 0.1125 [('l', 5), ('a', 2), ('h', 1), ('c', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3076923076923077\n",
      "M Accuracy: 0.46153846153846156\n",
      "-s 0.0\n",
      "-e 0.38461538461538464\n",
      "-r 0.1794871794871795\n",
      "-n 0.3076923076923077\n",
      "Other: 0.1282051282051282 [('l', 3), ('a', 1), ('h', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5\n",
      "F Accuracy: 0.6666666666666666\n",
      "-s 0.0\n",
      "-e 0.4\n",
      "-r 0.13333333333333333\n",
      "-n 0.36666666666666664\n",
      "Other: 0.1 [('l', 2), ('c', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.18181818181818182\n",
      "N Accuracy: 0.45454545454545453\n",
      "-s 0.18181818181818182\n",
      "-e 0.18181818181818182\n",
      "-r 0.18181818181818182\n",
      "-n 0.36363636363636365\n",
      "Other: 0.09090909090909091 [('a', 1)]\n",
      "\n",
      "Data type: 360_9\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.5375\n",
      "Test accuracy: 0.7375\n",
      "-s 0.0\n",
      "-e 0.475\n",
      "-r 0.1125\n",
      "-n 0.4\n",
      "Other: 0.0125 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.4878048780487805\n",
      "M Accuracy: 0.7073170731707317\n",
      "-s 0.0\n",
      "-e 0.7560975609756098\n",
      "-r 0.0975609756097561\n",
      "-n 0.12195121951219512\n",
      "Other: 0.024390243902439025 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8\n",
      "F Accuracy: 0.96\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.0\n",
      "-n 1.0\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.21428571428571427\n",
      "N Accuracy: 0.42857142857142855\n",
      "-s 0.0\n",
      "-e 0.5\n",
      "-r 0.35714285714285715\n",
      "-n 0.14285714285714285\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.4\n",
      "Test accuracy: 0.475\n",
      "-s 0.0875\n",
      "-e 0.25\n",
      "-r 0.1\n",
      "-n 0.4875\n",
      "Other: 0.075 [('c', 2), ('l', 2), ('a', 1), ('o', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.36585365853658536\n",
      "M Accuracy: 0.5121951219512195\n",
      "-s 0.12195121951219512\n",
      "-e 0.3170731707317073\n",
      "-r 0.0975609756097561\n",
      "-n 0.3902439024390244\n",
      "Other: 0.07317073170731707 [('l', 2), ('o', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.52\n",
      "F Accuracy: 0.48\n",
      "-s 0.08\n",
      "-e 0.16\n",
      "-r 0.12\n",
      "-n 0.56\n",
      "Other: 0.08 [('c', 1), ('a', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2857142857142857\n",
      "N Accuracy: 0.35714285714285715\n",
      "-s 0.0\n",
      "-e 0.21428571428571427\n",
      "-r 0.07142857142857142\n",
      "-n 0.6428571428571429\n",
      "Other: 0.07142857142857142 [('c', 1)]\n",
      "\n",
      "Data type: 360_10\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6125\n",
      "Test accuracy: 0.8125\n",
      "-s 0.025\n",
      "-e 0.3375\n",
      "-r 0.175\n",
      "-n 0.425\n",
      "Other: 0.0375 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5789473684210527\n",
      "M Accuracy: 0.7105263157894737\n",
      "-s 0.0\n",
      "-e 0.5263157894736842\n",
      "-r 0.2894736842105263\n",
      "-n 0.13157894736842105\n",
      "Other: 0.05263157894736842 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6666666666666666\n",
      "F Accuracy: 0.9\n",
      "-s 0.06666666666666667\n",
      "-e 0.03333333333333333\n",
      "-r 0.0\n",
      "-n 0.9\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5833333333333334\n",
      "N Accuracy: 0.9166666666666666\n",
      "-s 0.0\n",
      "-e 0.5\n",
      "-r 0.25\n",
      "-n 0.16666666666666666\n",
      "Other: 0.08333333333333333 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "Test accuracy: 0.375\n",
      "-s 0.075\n",
      "-e 0.3625\n",
      "-r 0.325\n",
      "-n 0.1625\n",
      "Other: 0.075 [('l', 5), ('f', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3157894736842105\n",
      "M Accuracy: 0.39473684210526316\n",
      "-s 0.07894736842105263\n",
      "-e 0.39473684210526316\n",
      "-r 0.2894736842105263\n",
      "-n 0.21052631578947367\n",
      "Other: 0.02631578947368421 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.16666666666666666\n",
      "F Accuracy: 0.3333333333333333\n",
      "-s 0.1\n",
      "-e 0.4\n",
      "-r 0.3\n",
      "-n 0.13333333333333333\n",
      "Other: 0.06666666666666667 [('l', 1), ('f', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "N Accuracy: 0.4166666666666667\n",
      "-s 0.0\n",
      "-e 0.16666666666666666\n",
      "-r 0.5\n",
      "-n 0.08333333333333333\n",
      "Other: 0.25 [('l', 3)]\n",
      "\n",
      "Data type: 360_11\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.575\n",
      "Test accuracy: 0.825\n",
      "-s 0.0125\n",
      "-e 0.4375\n",
      "-r 0.0875\n",
      "-n 0.4\n",
      "Other: 0.0625 [('l', 4), ('m', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.45\n",
      "M Accuracy: 0.75\n",
      "-s 0.025\n",
      "-e 0.7\n",
      "-r 0.15\n",
      "-n 0.075\n",
      "Other: 0.05 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.7931034482758621\n",
      "F Accuracy: 0.9310344827586207\n",
      "-s 0.0\n",
      "-e 0.034482758620689655\n",
      "-r 0.0\n",
      "-n 0.9655172413793104\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.45454545454545453\n",
      "N Accuracy: 0.8181818181818182\n",
      "-s 0.0\n",
      "-e 0.5454545454545454\n",
      "-r 0.09090909090909091\n",
      "-n 0.09090909090909091\n",
      "Other: 0.2727272727272727 [('l', 2), ('m', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.3125\n",
      "Test accuracy: 0.6\n",
      "-s 0.0\n",
      "-e 0.3125\n",
      "-r 0.125\n",
      "-n 0.5\n",
      "Other: 0.0625 [('l', 4), ('m', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "M Accuracy: 0.625\n",
      "-s 0.0\n",
      "-e 0.325\n",
      "-r 0.125\n",
      "-n 0.5\n",
      "Other: 0.05 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3793103448275862\n",
      "F Accuracy: 0.5172413793103449\n",
      "-s 0.0\n",
      "-e 0.3448275862068966\n",
      "-r 0.10344827586206896\n",
      "-n 0.4482758620689655\n",
      "Other: 0.10344827586206896 [('l', 2), ('m', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.36363636363636365\n",
      "N Accuracy: 0.7272727272727273\n",
      "-s 0.0\n",
      "-e 0.18181818181818182\n",
      "-r 0.18181818181818182\n",
      "-n 0.6363636363636364\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 360_12\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.625\n",
      "Test accuracy: 0.725\n",
      "-s 0.0125\n",
      "-e 0.325\n",
      "-r 0.1375\n",
      "-n 0.5\n",
      "Other: 0.025 [('u', 1), ('f', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5526315789473685\n",
      "M Accuracy: 0.6578947368421053\n",
      "-s 0.0\n",
      "-e 0.5263157894736842\n",
      "-r 0.18421052631578946\n",
      "-n 0.2631578947368421\n",
      "Other: 0.02631578947368421 [('f', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8571428571428571\n",
      "F Accuracy: 0.9642857142857143\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.0\n",
      "-n 0.9642857142857143\n",
      "Other: 0.03571428571428571 [('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.35714285714285715\n",
      "N Accuracy: 0.42857142857142855\n",
      "-s 0.07142857142857142\n",
      "-e 0.42857142857142855\n",
      "-r 0.2857142857142857\n",
      "-n 0.21428571428571427\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.425\n",
      "Test accuracy: 0.475\n",
      "-s 0.025\n",
      "-e 0.125\n",
      "-r 0.25\n",
      "-n 0.5625\n",
      "Other: 0.0375 [('l', 2), ('-', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.39473684210526316\n",
      "M Accuracy: 0.47368421052631576\n",
      "-s 0.05263157894736842\n",
      "-e 0.10526315789473684\n",
      "-r 0.23684210526315788\n",
      "-n 0.6052631578947368\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.42857142857142855\n",
      "F Accuracy: 0.5\n",
      "-s 0.0\n",
      "-e 0.14285714285714285\n",
      "-r 0.25\n",
      "-n 0.5357142857142857\n",
      "Other: 0.07142857142857142 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5\n",
      "N Accuracy: 0.42857142857142855\n",
      "-s 0.0\n",
      "-e 0.14285714285714285\n",
      "-r 0.2857142857142857\n",
      "-n 0.5\n",
      "Other: 0.07142857142857142 [('-', 1)]\n",
      "\n",
      "Data type: 360_13\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.7125\n",
      "Test accuracy: 0.8125\n",
      "-s 0.0\n",
      "-e 0.35\n",
      "-r 0.0875\n",
      "-n 0.55\n",
      "Other: 0.0125 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6363636363636364\n",
      "M Accuracy: 0.8181818181818182\n",
      "-s 0.0\n",
      "-e 0.5757575757575758\n",
      "-r 0.21212121212121213\n",
      "-n 0.18181818181818182\n",
      "Other: 0.030303030303030304 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8235294117647058\n",
      "F Accuracy: 0.9705882352941176\n",
      "-s 0.0\n",
      "-e 0.029411764705882353\n",
      "-r 0.0\n",
      "-n 0.9705882352941176\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.6153846153846154\n",
      "N Accuracy: 0.38461538461538464\n",
      "-s 0.0\n",
      "-e 0.6153846153846154\n",
      "-r 0.0\n",
      "-n 0.38461538461538464\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.425\n",
      "Test accuracy: 0.5125\n",
      "-s 0.0\n",
      "-e 0.2375\n",
      "-r 0.1875\n",
      "-n 0.5375\n",
      "Other: 0.0375 [('l', 2), ('k', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.42424242424242425\n",
      "M Accuracy: 0.6666666666666666\n",
      "-s 0.0\n",
      "-e 0.24242424242424243\n",
      "-r 0.12121212121212122\n",
      "-n 0.6363636363636364\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5\n",
      "F Accuracy: 0.35294117647058826\n",
      "-s 0.0\n",
      "-e 0.17647058823529413\n",
      "-r 0.23529411764705882\n",
      "-n 0.5\n",
      "Other: 0.08823529411764706 [('l', 2), ('k', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.23076923076923078\n",
      "N Accuracy: 0.5384615384615384\n",
      "-s 0.0\n",
      "-e 0.38461538461538464\n",
      "-r 0.23076923076923078\n",
      "-n 0.38461538461538464\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 360_14\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6875\n",
      "Test accuracy: 0.8125\n",
      "-s 0.05\n",
      "-e 0.3125\n",
      "-r 0.1625\n",
      "-n 0.4625\n",
      "Other: 0.0125 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6\n",
      "M Accuracy: 0.8\n",
      "-s 0.05714285714285714\n",
      "-e 0.4857142857142857\n",
      "-r 0.2857142857142857\n",
      "-n 0.14285714285714285\n",
      "Other: 0.02857142857142857 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8888888888888888\n",
      "F Accuracy: 0.9629629629629629\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.037037037037037035\n",
      "-n 0.9629629629629629\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5555555555555556\n",
      "N Accuracy: 0.6111111111111112\n",
      "-s 0.1111111111111111\n",
      "-e 0.4444444444444444\n",
      "-r 0.1111111111111111\n",
      "-n 0.3333333333333333\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "Test accuracy: 0.3875\n",
      "-s 0.05\n",
      "-e 0.2875\n",
      "-r 0.2125\n",
      "-n 0.3125\n",
      "Other: 0.1375 [('l', 4), ('m', 2), ('f', 1), ('o', 1), ('a', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.17142857142857143\n",
      "M Accuracy: 0.3142857142857143\n",
      "-s 0.02857142857142857\n",
      "-e 0.3142857142857143\n",
      "-r 0.2\n",
      "-n 0.2571428571428571\n",
      "Other: 0.2 [('l', 2), ('m', 2), ('o', 1), ('a', 1), ('u', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.37037037037037035\n",
      "F Accuracy: 0.5555555555555556\n",
      "-s 0.037037037037037035\n",
      "-e 0.2962962962962963\n",
      "-r 0.2222222222222222\n",
      "-n 0.4074074074074074\n",
      "Other: 0.037037037037037035 [('c', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2222222222222222\n",
      "N Accuracy: 0.2777777777777778\n",
      "-s 0.1111111111111111\n",
      "-e 0.2222222222222222\n",
      "-r 0.2222222222222222\n",
      "-n 0.2777777777777778\n",
      "Other: 0.16666666666666666 [('l', 2), ('f', 1)]\n",
      "\n",
      "Data type: 360_15\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6\n",
      "Test accuracy: 0.6875\n",
      "-s 0.025\n",
      "-e 0.35\n",
      "-r 0.15\n",
      "-n 0.425\n",
      "Other: 0.05 [('l', 4)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5757575757575758\n",
      "M Accuracy: 0.6363636363636364\n",
      "-s 0.030303030303030304\n",
      "-e 0.5454545454545454\n",
      "-r 0.24242424242424243\n",
      "-n 0.12121212121212122\n",
      "Other: 0.06060606060606061 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6176470588235294\n",
      "F Accuracy: 0.7647058823529411\n",
      "-s 0.029411764705882353\n",
      "-e 0.11764705882352941\n",
      "-r 0.058823529411764705\n",
      "-n 0.7647058823529411\n",
      "Other: 0.029411764705882353 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6153846153846154\n",
      "N Accuracy: 0.6153846153846154\n",
      "-s 0.0\n",
      "-e 0.46153846153846156\n",
      "-r 0.15384615384615385\n",
      "-n 0.3076923076923077\n",
      "Other: 0.07692307692307693 [('l', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.3375\n",
      "Test accuracy: 0.4\n",
      "-s 0.05\n",
      "-e 0.3375\n",
      "-r 0.125\n",
      "-n 0.325\n",
      "Other: 0.1625 [('l', 11), ('t', 1), ('i', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2727272727272727\n",
      "M Accuracy: 0.3333333333333333\n",
      "-s 0.09090909090909091\n",
      "-e 0.3333333333333333\n",
      "-r 0.030303030303030304\n",
      "-n 0.3333333333333333\n",
      "Other: 0.21212121212121213 [('l', 6), ('i', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.38235294117647056\n",
      "F Accuracy: 0.5\n",
      "-s 0.029411764705882353\n",
      "-e 0.35294117647058826\n",
      "-r 0.14705882352941177\n",
      "-n 0.35294117647058826\n",
      "Other: 0.11764705882352941 [('l', 3), ('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.38461538461538464\n",
      "N Accuracy: 0.3076923076923077\n",
      "-s 0.0\n",
      "-e 0.3076923076923077\n",
      "-r 0.3076923076923077\n",
      "-n 0.23076923076923078\n",
      "Other: 0.15384615384615385 [('l', 2)]\n",
      "\n",
      "Data type: 360_16\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.425\n",
      "Test accuracy: 0.775\n",
      "-s 0.0125\n",
      "-e 0.3625\n",
      "-r 0.175\n",
      "-n 0.4375\n",
      "Other: 0.0125 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5\n",
      "M Accuracy: 0.7368421052631579\n",
      "-s 0.02631578947368421\n",
      "-e 0.631578947368421\n",
      "-r 0.18421052631578946\n",
      "-n 0.13157894736842105\n",
      "Other: 0.02631578947368421 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.3548387096774194\n",
      "F Accuracy: 0.9032258064516129\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.06451612903225806\n",
      "-n 0.9354838709677419\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.36363636363636365\n",
      "N Accuracy: 0.5454545454545454\n",
      "-s 0.0\n",
      "-e 0.45454545454545453\n",
      "-r 0.45454545454545453\n",
      "-n 0.09090909090909091\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "Test accuracy: 0.4\n",
      "-s 0.0375\n",
      "-e 0.075\n",
      "-r 0.35\n",
      "-n 0.5125\n",
      "Other: 0.025 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.23684210526315788\n",
      "M Accuracy: 0.4473684210526316\n",
      "-s 0.02631578947368421\n",
      "-e 0.07894736842105263\n",
      "-r 0.34210526315789475\n",
      "-n 0.5263157894736842\n",
      "Other: 0.02631578947368421 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.1935483870967742\n",
      "F Accuracy: 0.2903225806451613\n",
      "-s 0.06451612903225806\n",
      "-e 0.0967741935483871\n",
      "-r 0.3870967741935484\n",
      "-n 0.41935483870967744\n",
      "Other: 0.03225806451612903 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.45454545454545453\n",
      "N Accuracy: 0.5454545454545454\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.2727272727272727\n",
      "-n 0.7272727272727273\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 360_17\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.65\n",
      "Test accuracy: 0.6875\n",
      "-s 0.025\n",
      "-e 0.3\n",
      "-r 0.1375\n",
      "-n 0.5125\n",
      "Other: 0.025 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5454545454545454\n",
      "M Accuracy: 0.6363636363636364\n",
      "-s 0.0\n",
      "-e 0.5454545454545454\n",
      "-r 0.21212121212121213\n",
      "-n 0.24242424242424243\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.8333333333333334\n",
      "F Accuracy: 0.9333333333333333\n",
      "-s 0.0\n",
      "-e 0.06666666666666667\n",
      "-r 0.0\n",
      "-n 0.9333333333333333\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5294117647058824\n",
      "N Accuracy: 0.35294117647058826\n",
      "-s 0.11764705882352941\n",
      "-e 0.23529411764705882\n",
      "-r 0.23529411764705882\n",
      "-n 0.29411764705882354\n",
      "Other: 0.11764705882352941 [('l', 2)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.45\n",
      "Test accuracy: 0.525\n",
      "-s 0.05\n",
      "-e 0.1375\n",
      "-r 0.2125\n",
      "-n 0.5375\n",
      "Other: 0.0625 [('g', 1), ('k', 1), ('i', 1), ('l', 1), ('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.36363636363636365\n",
      "M Accuracy: 0.48484848484848486\n",
      "-s 0.06060606060606061\n",
      "-e 0.15151515151515152\n",
      "-r 0.21212121212121213\n",
      "-n 0.5151515151515151\n",
      "Other: 0.06060606060606061 [('k', 1), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6\n",
      "F Accuracy: 0.5\n",
      "-s 0.03333333333333333\n",
      "-e 0.03333333333333333\n",
      "-r 0.26666666666666666\n",
      "-n 0.6\n",
      "Other: 0.06666666666666667 [('g', 1), ('i', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.35294117647058826\n",
      "N Accuracy: 0.6470588235294118\n",
      "-s 0.058823529411764705\n",
      "-e 0.29411764705882354\n",
      "-r 0.11764705882352941\n",
      "-n 0.47058823529411764\n",
      "Other: 0.058823529411764705 [('t', 1)]\n",
      "\n",
      "Data type: 360_18\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6625\n",
      "Test accuracy: 0.7375\n",
      "-s 0.0\n",
      "-e 0.45\n",
      "-r 0.075\n",
      "-n 0.4375\n",
      "Other: 0.0375 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6944444444444444\n",
      "M Accuracy: 0.6666666666666666\n",
      "-s 0.0\n",
      "-e 0.6388888888888888\n",
      "-r 0.1111111111111111\n",
      "-n 0.16666666666666666\n",
      "Other: 0.08333333333333333 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.75\n",
      "F Accuracy: 0.8125\n",
      "-s 0.0\n",
      "-e 0.15625\n",
      "-r 0.03125\n",
      "-n 0.8125\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.3333333333333333\n",
      "N Accuracy: 0.75\n",
      "-s 0.0\n",
      "-e 0.6666666666666666\n",
      "-r 0.08333333333333333\n",
      "-n 0.25\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.35\n",
      "Test accuracy: 0.4375\n",
      "-s 0.0\n",
      "-e 0.1875\n",
      "-r 0.2\n",
      "-n 0.575\n",
      "Other: 0.0375 [('m', 1), ('l', 1), ('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.2777777777777778\n",
      "M Accuracy: 0.4166666666666667\n",
      "-s 0.0\n",
      "-e 0.2222222222222222\n",
      "-r 0.16666666666666666\n",
      "-n 0.5555555555555556\n",
      "Other: 0.05555555555555555 [('m', 1), ('t', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.46875\n",
      "F Accuracy: 0.4375\n",
      "-s 0.0\n",
      "-e 0.21875\n",
      "-r 0.21875\n",
      "-n 0.53125\n",
      "Other: 0.03125 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "N Accuracy: 0.5\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.25\n",
      "-n 0.75\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 360_19\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6375\n",
      "Test accuracy: 0.8125\n",
      "-s 0.025\n",
      "-e 0.4\n",
      "-r 0.1625\n",
      "-n 0.4\n",
      "Other: 0.0125 [('m', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5121951219512195\n",
      "M Accuracy: 0.7317073170731707\n",
      "-s 0.04878048780487805\n",
      "-e 0.6341463414634146\n",
      "-r 0.21951219512195122\n",
      "-n 0.07317073170731707\n",
      "Other: 0.024390243902439025 [('m', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.9166666666666666\n",
      "F Accuracy: 1.0\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.0\n",
      "-n 1.0\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5333333333333333\n",
      "N Accuracy: 0.7333333333333333\n",
      "-s 0.0\n",
      "-e 0.4\n",
      "-r 0.26666666666666666\n",
      "-n 0.3333333333333333\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.275\n",
      "Test accuracy: 0.375\n",
      "-s 0.0\n",
      "-e 0.3625\n",
      "-r 0.3875\n",
      "-n 0.225\n",
      "Other: 0.025 [('g', 1), ('m', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.34146341463414637\n",
      "M Accuracy: 0.34146341463414637\n",
      "-s 0.0\n",
      "-e 0.3170731707317073\n",
      "-r 0.36585365853658536\n",
      "-n 0.2926829268292683\n",
      "Other: 0.024390243902439025 [('g', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.20833333333333334\n",
      "F Accuracy: 0.4166666666666667\n",
      "-s 0.0\n",
      "-e 0.375\n",
      "-r 0.4583333333333333\n",
      "-n 0.16666666666666666\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.2\n",
      "N Accuracy: 0.4\n",
      "-s 0.0\n",
      "-e 0.4666666666666667\n",
      "-r 0.3333333333333333\n",
      "-n 0.13333333333333333\n",
      "Other: 0.06666666666666667 [('m', 1)]\n",
      "\n",
      "Data type: 360_20\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.7125\n",
      "Test accuracy: 0.775\n",
      "-s 0.0\n",
      "-e 0.3375\n",
      "-r 0.1125\n",
      "-n 0.5375\n",
      "Other: 0.0125 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6666666666666666\n",
      "M Accuracy: 0.6410256410256411\n",
      "-s 0.0\n",
      "-e 0.5128205128205128\n",
      "-r 0.1282051282051282\n",
      "-n 0.3333333333333333\n",
      "Other: 0.02564102564102564 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8333333333333334\n",
      "F Accuracy: 1.0\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.0\n",
      "-n 1.0\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5454545454545454\n",
      "N Accuracy: 0.6363636363636364\n",
      "-s 0.0\n",
      "-e 0.6363636363636364\n",
      "-r 0.36363636363636365\n",
      "-n 0.0\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.2125\n",
      "Test accuracy: 0.475\n",
      "-s 0.0125\n",
      "-e 0.3125\n",
      "-r 0.3125\n",
      "-n 0.3125\n",
      "Other: 0.05 [('l', 2), ('h', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.23076923076923078\n",
      "M Accuracy: 0.48717948717948717\n",
      "-s 0.02564102564102564\n",
      "-e 0.358974358974359\n",
      "-r 0.28205128205128205\n",
      "-n 0.3076923076923077\n",
      "Other: 0.02564102564102564 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.26666666666666666\n",
      "F Accuracy: 0.43333333333333335\n",
      "-s 0.0\n",
      "-e 0.23333333333333334\n",
      "-r 0.3333333333333333\n",
      "-n 0.3333333333333333\n",
      "Other: 0.1 [('h', 2), ('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.0\n",
      "N Accuracy: 0.5454545454545454\n",
      "-s 0.0\n",
      "-e 0.36363636363636365\n",
      "-r 0.36363636363636365\n",
      "-n 0.2727272727272727\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 360_21\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6375\n",
      "Test accuracy: 0.7125\n",
      "-s 0.0125\n",
      "-e 0.425\n",
      "-r 0.125\n",
      "-n 0.3875\n",
      "Other: 0.05 [('l', 2), ('m', 1), ('o', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6470588235294118\n",
      "M Accuracy: 0.7941176470588235\n",
      "-s 0.029411764705882353\n",
      "-e 0.6176470588235294\n",
      "-r 0.14705882352941177\n",
      "-n 0.14705882352941177\n",
      "Other: 0.058823529411764705 [('l', 2)]\n",
      "% of inflections (len 1) that match most popular in training: 0.76\n",
      "F Accuracy: 0.88\n",
      "-s 0.0\n",
      "-e 0.04\n",
      "-r 0.04\n",
      "-n 0.92\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.47619047619047616\n",
      "N Accuracy: 0.38095238095238093\n",
      "-s 0.0\n",
      "-e 0.5714285714285714\n",
      "-r 0.19047619047619047\n",
      "-n 0.14285714285714285\n",
      "Other: 0.09523809523809523 [('m', 1), ('o', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "Test accuracy: 0.35\n",
      "-s 0.025\n",
      "-e 0.4625\n",
      "-r 0.2875\n",
      "-n 0.1375\n",
      "Other: 0.0875 [('l', 6), ('m', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.23529411764705882\n",
      "M Accuracy: 0.38235294117647056\n",
      "-s 0.058823529411764705\n",
      "-e 0.5588235294117647\n",
      "-r 0.2647058823529412\n",
      "-n 0.08823529411764706\n",
      "Other: 0.029411764705882353 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.16\n",
      "F Accuracy: 0.36\n",
      "-s 0.0\n",
      "-e 0.4\n",
      "-r 0.32\n",
      "-n 0.16\n",
      "Other: 0.12 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.38095238095238093\n",
      "N Accuracy: 0.2857142857142857\n",
      "-s 0.0\n",
      "-e 0.38095238095238093\n",
      "-r 0.2857142857142857\n",
      "-n 0.19047619047619047\n",
      "Other: 0.14285714285714285 [('l', 2), ('m', 1)]\n",
      "\n",
      "Data type: 360_22\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.5875\n",
      "Test accuracy: 0.75\n",
      "-s 0.025\n",
      "-e 0.3875\n",
      "-r 0.1375\n",
      "-n 0.4\n",
      "Other: 0.05 [('o', 1), ('g', 1), ('l', 1), ('k', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.37142857142857144\n",
      "M Accuracy: 0.7428571428571429\n",
      "-s 0.02857142857142857\n",
      "-e 0.6857142857142857\n",
      "-r 0.17142857142857143\n",
      "-n 0.05714285714285714\n",
      "Other: 0.05714285714285714 [('o', 1), ('g', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8064516129032258\n",
      "F Accuracy: 0.8387096774193549\n",
      "-s 0.03225806451612903\n",
      "-e 0.0\n",
      "-r 0.06451612903225806\n",
      "-n 0.9032258064516129\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.6428571428571429\n",
      "N Accuracy: 0.5714285714285714\n",
      "-s 0.0\n",
      "-e 0.5\n",
      "-r 0.21428571428571427\n",
      "-n 0.14285714285714285\n",
      "Other: 0.14285714285714285 [('l', 1), ('k', 1)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.3\n",
      "Test accuracy: 0.425\n",
      "-s 0.025\n",
      "-e 0.4125\n",
      "-r 0.3125\n",
      "-n 0.25\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.2571428571428571\n",
      "M Accuracy: 0.45714285714285713\n",
      "-s 0.0\n",
      "-e 0.5142857142857142\n",
      "-r 0.2857142857142857\n",
      "-n 0.2\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.3870967741935484\n",
      "F Accuracy: 0.4838709677419355\n",
      "-s 0.06451612903225806\n",
      "-e 0.3225806451612903\n",
      "-r 0.2903225806451613\n",
      "-n 0.3225806451612903\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.21428571428571427\n",
      "N Accuracy: 0.21428571428571427\n",
      "-s 0.0\n",
      "-e 0.35714285714285715\n",
      "-r 0.42857142857142855\n",
      "-n 0.21428571428571427\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 360_23\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.7125\n",
      "Test accuracy: 0.8125\n",
      "-s 0.0375\n",
      "-e 0.3875\n",
      "-r 0.0875\n",
      "-n 0.45\n",
      "Other: 0.0375 [('l', 3)]\n",
      "% of inflections (len 1) that match most popular in training: 0.6666666666666666\n",
      "M Accuracy: 0.75\n",
      "-s 0.08333333333333333\n",
      "-e 0.5277777777777778\n",
      "-r 0.19444444444444445\n",
      "-n 0.16666666666666666\n",
      "Other: 0.027777777777777776 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.8333333333333334\n",
      "F Accuracy: 0.9333333333333333\n",
      "-s 0.0\n",
      "-e 0.06666666666666667\n",
      "-r 0.0\n",
      "-n 0.9333333333333333\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5714285714285714\n",
      "N Accuracy: 0.7142857142857143\n",
      "-s 0.0\n",
      "-e 0.7142857142857143\n",
      "-r 0.0\n",
      "-n 0.14285714285714285\n",
      "Other: 0.14285714285714285 [('l', 2)]\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.4625\n",
      "Test accuracy: 0.5125\n",
      "-s 0.0125\n",
      "-e 0.05\n",
      "-r 0.1125\n",
      "-n 0.825\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.2222222222222222\n",
      "M Accuracy: 0.4166666666666667\n",
      "-s 0.0\n",
      "-e 0.05555555555555555\n",
      "-r 0.19444444444444445\n",
      "-n 0.75\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.8333333333333334\n",
      "F Accuracy: 0.5333333333333333\n",
      "-s 0.03333333333333333\n",
      "-e 0.0\n",
      "-r 0.06666666666666667\n",
      "-n 0.9\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.2857142857142857\n",
      "N Accuracy: 0.7142857142857143\n",
      "-s 0.0\n",
      "-e 0.14285714285714285\n",
      "-r 0.0\n",
      "-n 0.8571428571428571\n",
      "Other: 0.0 []\n",
      "\n",
      "Data type: 360_24\n",
      "\n",
      "\n",
      "rnn regular_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.6375\n",
      "Test accuracy: 0.775\n",
      "-s 0.0\n",
      "-e 0.375\n",
      "-r 0.0625\n",
      "-n 0.55\n",
      "Other: 0.0125 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.7073170731707317\n",
      "M Accuracy: 0.6829268292682927\n",
      "-s 0.0\n",
      "-e 0.6097560975609756\n",
      "-r 0.07317073170731707\n",
      "-n 0.2926829268292683\n",
      "Other: 0.024390243902439025 [('l', 1)]\n",
      "% of inflections (len 1) that match most popular in training: 0.5806451612903226\n",
      "F Accuracy: 0.9354838709677419\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.06451612903225806\n",
      "-n 0.9354838709677419\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.5\n",
      "N Accuracy: 0.625\n",
      "-s 0.0\n",
      "-e 0.625\n",
      "-r 0.0\n",
      "-n 0.375\n",
      "Other: 0.0 []\n",
      "\n",
      "\n",
      "rnn genderless_test results\n",
      "% of inflections (len 1) that match most popular in training: 0.275\n",
      "Test accuracy: 0.475\n",
      "-s 0.025\n",
      "-e 0.225\n",
      "-r 0.3875\n",
      "-n 0.3625\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.3170731707317073\n",
      "M Accuracy: 0.43902439024390244\n",
      "-s 0.0\n",
      "-e 0.1951219512195122\n",
      "-r 0.43902439024390244\n",
      "-n 0.36585365853658536\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.22580645161290322\n",
      "F Accuracy: 0.5483870967741935\n",
      "-s 0.06451612903225806\n",
      "-e 0.3225806451612903\n",
      "-r 0.2903225806451613\n",
      "-n 0.3225806451612903\n",
      "Other: 0.0 []\n",
      "% of inflections (len 1) that match most popular in training: 0.25\n",
      "N Accuracy: 0.375\n",
      "-s 0.0\n",
      "-e 0.0\n",
      "-r 0.5\n",
      "-n 0.5\n",
      "Other: 0.0 []\n",
      "\n",
      "data_type, RNN, condition, test_subset, acc, freq, s, e, r, n, others, others_most_common\n",
      "60_0,RNN,regular_test,All,0.5,0.3,0.0,0.1375,0.05,0.6,0.2125,[('l' 14) ('-' 3)]\n",
      "60_0,RNN,regular_test,M,0.32142857142857145,0.32142857142857145,0.0,0.32142857142857145,0.03571428571428571,0.35714285714285715,0.2857142857142857,[('l' 7) ('-' 1)]\n",
      "60_0,RNN,regular_test,F,0.7631578947368421,0.3684210526315789,0.0,0.0,0.05263157894736842,0.7631578947368421,0.18421052631578946,[('l' 5) ('-' 2)]\n",
      "60_0,RNN,regular_test,N,0.14285714285714285,0.07142857142857142,0.0,0.14285714285714285,0.07142857142857142,0.6428571428571429,0.14285714285714285,[('l' 2)]\n",
      "60_0,RNN,genderless_test,All,0.5375,0.2375,0.0,0.1875,0.075,0.675,0.0625,[('l' 4) ('-' 1)]\n",
      "60_0,RNN,genderless_test,M,0.5357142857142857,0.10714285714285714,0.0,0.21428571428571427,0.07142857142857142,0.6071428571428571,0.10714285714285714,[('l' 2) ('-' 1)]\n",
      "60_0,RNN,genderless_test,F,0.5263157894736842,0.3157894736842105,0.0,0.18421052631578946,0.10526315789473684,0.6842105263157895,0.02631578947368421,[('l' 1)]\n",
      "60_0,RNN,genderless_test,N,0.5714285714285714,0.2857142857142857,0.0,0.14285714285714285,0.0,0.7857142857142857,0.07142857142857142,[('l' 1)]\n",
      "60_1,RNN,regular_test,All,0.5625,0.3625,0.0,0.475,0.05,0.45,0.025,[('l' 2)]\n",
      "60_1,RNN,regular_test,M,0.5714285714285714,0.3142857142857143,0.0,0.7142857142857143,0.0,0.22857142857142856,0.05714285714285714,[('l' 2)]\n",
      "60_1,RNN,regular_test,F,0.6451612903225806,0.3870967741935484,0.0,0.25806451612903225,0.0967741935483871,0.6451612903225806,0.0,[]\n",
      "60_1,RNN,regular_test,N,0.35714285714285715,0.42857142857142855,0.0,0.35714285714285715,0.07142857142857142,0.5714285714285714,0.0,[]\n",
      "60_1,RNN,genderless_test,All,0.5,0.2,0.0,0.4125,0.05,0.5125,0.025,[('-' 1) ('u' 1)]\n",
      "60_1,RNN,genderless_test,M,0.45714285714285713,0.14285714285714285,0.0,0.37142857142857144,0.11428571428571428,0.4857142857142857,0.02857142857142857,[('-' 1)]\n",
      "60_1,RNN,genderless_test,F,0.5806451612903226,0.22580645161290322,0.0,0.41935483870967744,0.0,0.5806451612903226,0.0,[]\n",
      "60_1,RNN,genderless_test,N,0.42857142857142855,0.2857142857142857,0.0,0.5,0.0,0.42857142857142855,0.07142857142857142,[('u' 1)]\n",
      "60_2,RNN,regular_test,All,0.525,0.3125,0.0,0.1625,0.0,0.8,0.0375,[('l' 2) ('-' 1)]\n",
      "60_2,RNN,regular_test,M,0.3055555555555556,0.2777777777777778,0.0,0.3055555555555556,0.0,0.6666666666666666,0.027777777777777776,[('l' 1)]\n",
      "60_2,RNN,regular_test,F,0.967741935483871,0.3870967741935484,0.0,0.03225806451612903,0.0,0.967741935483871,0.0,[]\n",
      "60_2,RNN,regular_test,N,0.07692307692307693,0.23076923076923078,0.0,0.07692307692307693,0.0,0.7692307692307693,0.15384615384615385,[('-' 1) ('l' 1)]\n",
      "60_2,RNN,genderless_test,All,0.45,0.225,0.0,0.175,0.0,0.825,0.0,[]\n",
      "60_2,RNN,genderless_test,M,0.4722222222222222,0.1388888888888889,0.0,0.2222222222222222,0.0,0.7777777777777778,0.0,[]\n",
      "60_2,RNN,genderless_test,F,0.41935483870967744,0.3225806451612903,0.0,0.12903225806451613,0.0,0.8709677419354839,0.0,[]\n",
      "60_2,RNN,genderless_test,N,0.46153846153846156,0.23076923076923078,0.0,0.15384615384615385,0.0,0.8461538461538461,0.0,[]\n",
      "60_3,RNN,regular_test,All,0.4875,0.2625,0.0,0.15,0.175,0.5875,0.0875,[('l' 7)]\n",
      "60_3,RNN,regular_test,M,0.46153846153846156,0.19230769230769232,0.0,0.38461538461538464,0.11538461538461539,0.34615384615384615,0.15384615384615385,[('l' 4)]\n",
      "60_3,RNN,regular_test,F,0.7058823529411765,0.38235294117647056,0.0,0.058823529411764705,0.14705882352941177,0.7352941176470589,0.058823529411764705,[('l' 2)]\n",
      "60_3,RNN,regular_test,N,0.15,0.15,0.0,0.0,0.3,0.65,0.05,[('l' 1)]\n",
      "60_3,RNN,genderless_test,All,0.425,0.15,0.0,0.0875,0.125,0.75,0.0375,[('l' 3)]\n",
      "60_3,RNN,genderless_test,M,0.38461538461538464,0.0,0.0,0.07692307692307693,0.15384615384615385,0.7307692307692307,0.038461538461538464,[('l' 1)]\n",
      "60_3,RNN,genderless_test,F,0.47058823529411764,0.29411764705882354,0.0,0.058823529411764705,0.08823529411764706,0.8235294117647058,0.029411764705882353,[('l' 1)]\n",
      "60_3,RNN,genderless_test,N,0.4,0.1,0.0,0.15,0.15,0.65,0.05,[('l' 1)]\n",
      "60_4,RNN,regular_test,All,0.45,0.2125,0.0,0.025,0.075,0.9,0.0,[]\n",
      "60_4,RNN,regular_test,M,0.21212121212121213,0.21212121212121213,0.0,0.06060606060606061,0.09090909090909091,0.8484848484848485,0.0,[]\n",
      "60_4,RNN,regular_test,F,0.875,0.25,0.0,0.0,0.09375,0.90625,0.0,[]\n",
      "60_4,RNN,regular_test,N,0.06666666666666667,0.13333333333333333,0.0,0.0,0.0,1.0,0.0,[]\n",
      "60_4,RNN,genderless_test,All,0.45,0.1875,0.0,0.0,0.05,0.95,0.0,[]\n",
      "60_4,RNN,genderless_test,M,0.45454545454545453,0.18181818181818182,0.0,0.0,0.06060606060606061,0.9393939393939394,0.0,[]\n",
      "60_4,RNN,genderless_test,F,0.4375,0.21875,0.0,0.0,0.0625,0.9375,0.0,[]\n",
      "60_4,RNN,genderless_test,N,0.4666666666666667,0.13333333333333333,0.0,0.0,0.0,1.0,0.0,[]\n",
      "60_5,RNN,regular_test,All,0.4625,0.225,0.0,0.1875,0.1,0.7125,0.0,[]\n",
      "60_5,RNN,regular_test,M,0.40540540540540543,0.24324324324324326,0.0,0.24324324324324326,0.08108108108108109,0.6756756756756757,0.0,[]\n",
      "60_5,RNN,regular_test,F,0.75,0.21428571428571427,0.0,0.14285714285714285,0.10714285714285714,0.75,0.0,[]\n",
      "60_5,RNN,regular_test,N,0.06666666666666667,0.2,0.0,0.13333333333333333,0.13333333333333333,0.7333333333333333,0.0,[]\n",
      "60_5,RNN,genderless_test,All,0.525,0.15,0.0125,0.1625,0.125,0.6875,0.0125,[('l' 1)]\n",
      "60_5,RNN,genderless_test,M,0.5135135135135135,0.08108108108108109,0.02702702702702703,0.1891891891891892,0.1891891891891892,0.5945945945945946,0.0,[]\n",
      "60_5,RNN,genderless_test,F,0.42857142857142855,0.17857142857142858,0.0,0.10714285714285714,0.07142857142857142,0.7857142857142857,0.03571428571428571,[('l' 1)]\n",
      "60_5,RNN,genderless_test,N,0.7333333333333333,0.26666666666666666,0.0,0.2,0.06666666666666667,0.7333333333333333,0.0,[]\n",
      "60_6,RNN,regular_test,All,0.575,0.2375,0.0,0.0375,0.025,0.925,0.0125,[('l' 1)]\n",
      "60_6,RNN,regular_test,M,0.2222222222222222,0.1388888888888889,0.0,0.05555555555555555,0.05555555555555555,0.8611111111111112,0.027777777777777776,[('l' 1)]\n",
      "60_6,RNN,regular_test,F,1.0,0.3142857142857143,0.0,0.0,0.0,1.0,0.0,[]\n",
      "60_6,RNN,regular_test,N,0.3333333333333333,0.3333333333333333,0.0,0.1111111111111111,0.0,0.8888888888888888,0.0,[]\n",
      "60_6,RNN,genderless_test,All,0.5125,0.2,0.0125,0.0125,0.0,0.925,0.05,[('c' 1) ('h' 1) ('l' 1) ('a' 1)]\n",
      "60_6,RNN,genderless_test,M,0.4444444444444444,0.08333333333333333,0.027777777777777776,0.027777777777777776,0.0,0.8888888888888888,0.05555555555555555,[('c' 1) ('l' 1)]\n",
      "60_6,RNN,genderless_test,F,0.6,0.3142857142857143,0.0,0.0,0.0,1.0,0.0,[]\n",
      "60_6,RNN,genderless_test,N,0.4444444444444444,0.2222222222222222,0.0,0.0,0.0,0.7777777777777778,0.2222222222222222,[('h' 1) ('a' 1)]\n",
      "60_7,RNN,regular_test,All,0.3875,0.2375,0.0125,0.225,0.125,0.625,0.0125,[('l' 1)]\n",
      "60_7,RNN,regular_test,M,0.36363636363636365,0.30303030303030304,0.0,0.18181818181818182,0.15151515151515152,0.6363636363636364,0.030303030303030304,[('l' 1)]\n",
      "60_7,RNN,regular_test,F,0.5,0.26666666666666666,0.03333333333333333,0.3333333333333333,0.13333333333333333,0.5,0.0,[]\n",
      "60_7,RNN,regular_test,N,0.23529411764705882,0.058823529411764705,0.0,0.11764705882352941,0.058823529411764705,0.8235294117647058,0.0,[]\n",
      "60_7,RNN,genderless_test,All,0.3625,0.1875,0.0125,0.225,0.0625,0.7,0.0,[]\n",
      "60_7,RNN,genderless_test,M,0.36363636363636365,0.18181818181818182,0.0,0.36363636363636365,0.09090909090909091,0.5454545454545454,0.0,[]\n",
      "60_7,RNN,genderless_test,F,0.4,0.23333333333333334,0.0,0.06666666666666667,0.06666666666666667,0.8666666666666667,0.0,[]\n",
      "60_7,RNN,genderless_test,N,0.29411764705882354,0.11764705882352941,0.058823529411764705,0.23529411764705882,0.0,0.7058823529411765,0.0,[]\n",
      "60_8,RNN,regular_test,All,0.4625,0.275,0.0,0.35,0.1375,0.4,0.1125,[('l' 4) ('a' 2) ('-' 2) ('z' 1)]\n",
      "60_8,RNN,regular_test,M,0.46153846153846156,0.23076923076923078,0.0,0.4358974358974359,0.1282051282051282,0.28205128205128205,0.15384615384615385,[('l' 3) ('a' 2) ('-' 1)]\n",
      "60_8,RNN,regular_test,F,0.5333333333333333,0.4,0.0,0.23333333333333334,0.2,0.5333333333333333,0.03333333333333333,[('z' 1)]\n",
      "60_8,RNN,regular_test,N,0.2727272727272727,0.09090909090909091,0.0,0.36363636363636365,0.0,0.45454545454545453,0.18181818181818182,[('l' 1) ('-' 1)]\n",
      "60_8,RNN,genderless_test,All,0.4625,0.175,0.0,0.4,0.1375,0.4,0.0625,[('l' 3) ('z' 1) ('c' 1)]\n",
      "60_8,RNN,genderless_test,M,0.4358974358974359,0.1282051282051282,0.0,0.38461538461538464,0.10256410256410256,0.41025641025641024,0.10256410256410256,[('l' 3) ('c' 1)]\n",
      "60_8,RNN,genderless_test,F,0.5333333333333333,0.23333333333333334,0.0,0.43333333333333335,0.2,0.3333333333333333,0.03333333333333333,[('z' 1)]\n",
      "60_8,RNN,genderless_test,N,0.36363636363636365,0.18181818181818182,0.0,0.36363636363636365,0.09090909090909091,0.5454545454545454,0.0,[]\n",
      "60_9,RNN,regular_test,All,0.4375,0.2875,0.0,0.325,0.15,0.5125,0.0125,[('l' 1)]\n",
      "60_9,RNN,regular_test,M,0.4146341463414634,0.3170731707317073,0.0,0.36585365853658536,0.0975609756097561,0.5121951219512195,0.024390243902439025,[('l' 1)]\n",
      "60_9,RNN,regular_test,F,0.56,0.32,0.0,0.32,0.16,0.52,0.0,[]\n",
      "60_9,RNN,regular_test,N,0.2857142857142857,0.14285714285714285,0.0,0.21428571428571427,0.2857142857142857,0.5,0.0,[]\n",
      "60_9,RNN,genderless_test,All,0.3,0.125,0.0,0.2625,0.35,0.3875,0.0,[]\n",
      "60_9,RNN,genderless_test,M,0.34146341463414637,0.14634146341463414,0.0,0.24390243902439024,0.3170731707317073,0.43902439024390244,0.0,[]\n",
      "60_9,RNN,genderless_test,F,0.24,0.12,0.0,0.28,0.4,0.32,0.0,[]\n",
      "60_9,RNN,genderless_test,N,0.2857142857142857,0.07142857142857142,0.0,0.2857142857142857,0.35714285714285715,0.35714285714285715,0.0,[]\n",
      "60_10,RNN,regular_test,All,0.4625,0.3875,0.0,0.25,0.25,0.5,0.0,[]\n",
      "60_10,RNN,regular_test,M,0.47368421052631576,0.3157894736842105,0.0,0.2894736842105263,0.21052631578947367,0.5,0.0,[]\n",
      "60_10,RNN,regular_test,F,0.5333333333333333,0.5333333333333333,0.0,0.2,0.26666666666666666,0.5333333333333333,0.0,[]\n",
      "60_10,RNN,regular_test,N,0.25,0.25,0.0,0.25,0.3333333333333333,0.4166666666666667,0.0,[]\n",
      "60_10,RNN,genderless_test,All,0.425,0.2375,0.0,0.2125,0.25,0.5375,0.0,[]\n",
      "60_10,RNN,genderless_test,M,0.5,0.18421052631578946,0.0,0.23684210526315788,0.21052631578947367,0.5526315789473685,0.0,[]\n",
      "60_10,RNN,genderless_test,F,0.3333333333333333,0.3,0.0,0.16666666666666666,0.3,0.5333333333333333,0.0,[]\n",
      "60_10,RNN,genderless_test,N,0.4166666666666667,0.25,0.0,0.25,0.25,0.5,0.0,[]\n",
      "60_11,RNN,regular_test,All,0.375,0.4,0.0125,0.375,0.275,0.3375,0.0,[]\n",
      "60_11,RNN,regular_test,M,0.425,0.325,0.025,0.375,0.275,0.325,0.0,[]\n",
      "60_11,RNN,regular_test,F,0.3448275862068966,0.4827586206896552,0.0,0.41379310344827586,0.2413793103448276,0.3448275862068966,0.0,[]\n",
      "60_11,RNN,regular_test,N,0.2727272727272727,0.45454545454545453,0.0,0.2727272727272727,0.36363636363636365,0.36363636363636365,0.0,[]\n",
      "60_11,RNN,genderless_test,All,0.3375,0.2125,0.0,0.2625,0.325,0.4125,0.0,[]\n",
      "60_11,RNN,genderless_test,M,0.35,0.2,0.0,0.3,0.35,0.35,0.0,[]\n",
      "60_11,RNN,genderless_test,F,0.3448275862068966,0.20689655172413793,0.0,0.20689655172413793,0.3448275862068966,0.4482758620689655,0.0,[]\n",
      "60_11,RNN,genderless_test,N,0.2727272727272727,0.2727272727272727,0.0,0.2727272727272727,0.18181818181818182,0.5454545454545454,0.0,[]\n",
      "60_12,RNN,regular_test,All,0.575,0.3125,0.0,0.075,0.0625,0.85,0.0125,[('l' 1)]\n",
      "60_12,RNN,regular_test,M,0.42105263157894735,0.2894736842105263,0.0,0.13157894736842105,0.13157894736842105,0.7368421052631579,0.0,[]\n",
      "60_12,RNN,regular_test,F,1.0,0.35714285714285715,0.0,0.0,0.0,1.0,0.0,[]\n",
      "60_12,RNN,regular_test,N,0.14285714285714285,0.2857142857142857,0.0,0.07142857142857142,0.0,0.8571428571428571,0.07142857142857142,[('l' 1)]\n",
      "60_12,RNN,genderless_test,All,0.5625,0.2875,0.0,0.0,0.05,0.95,0.0,[]\n",
      "60_12,RNN,genderless_test,M,0.5789473684210527,0.21052631578947367,0.0,0.0,0.02631578947368421,0.9736842105263158,0.0,[]\n",
      "60_12,RNN,genderless_test,F,0.5714285714285714,0.35714285714285715,0.0,0.0,0.07142857142857142,0.9285714285714286,0.0,[]\n",
      "60_12,RNN,genderless_test,N,0.5,0.35714285714285715,0.0,0.0,0.07142857142857142,0.9285714285714286,0.0,[]\n",
      "60_13,RNN,regular_test,All,0.3375,0.275,0.0,0.025,0.425,0.5125,0.0375,[('-' 2) ('l' 1)]\n",
      "60_13,RNN,regular_test,M,0.21212121212121213,0.3333333333333333,0.0,0.030303030303030304,0.42424242424242425,0.5151515151515151,0.030303030303030304,[('l' 1)]\n",
      "60_13,RNN,regular_test,F,0.5,0.2647058823529412,0.0,0.029411764705882353,0.4117647058823529,0.5,0.058823529411764705,[('-' 2)]\n",
      "60_13,RNN,regular_test,N,0.23076923076923078,0.15384615384615385,0.0,0.0,0.46153846153846156,0.5384615384615384,0.0,[]\n",
      "60_13,RNN,genderless_test,All,0.375,0.225,0.0,0.0,0.4375,0.55,0.0125,[('l' 1)]\n",
      "60_13,RNN,genderless_test,M,0.45454545454545453,0.21212121212121213,0.0,0.0,0.45454545454545453,0.5454545454545454,0.0,[]\n",
      "60_13,RNN,genderless_test,F,0.35294117647058826,0.23529411764705882,0.0,0.0,0.35294117647058826,0.6470588235294118,0.0,[]\n",
      "60_13,RNN,genderless_test,N,0.23076923076923078,0.23076923076923078,0.0,0.0,0.6153846153846154,0.3076923076923077,0.07692307692307693,[('l' 1)]\n",
      "60_14,RNN,regular_test,All,0.375,0.2375,0.125,0.4125,0.1375,0.275,0.05,[('l' 3) ('-' 1)]\n",
      "60_14,RNN,regular_test,M,0.5428571428571428,0.34285714285714286,0.11428571428571428,0.42857142857142855,0.22857142857142856,0.2,0.02857142857142857,[('-' 1)]\n",
      "60_14,RNN,regular_test,F,0.2962962962962963,0.1111111111111111,0.14814814814814814,0.4444444444444444,0.037037037037037035,0.2962962962962963,0.07407407407407407,[('l' 2)]\n",
      "60_14,RNN,regular_test,N,0.16666666666666666,0.2222222222222222,0.1111111111111111,0.3333333333333333,0.1111111111111111,0.3888888888888889,0.05555555555555555,[('l' 1)]\n",
      "60_14,RNN,genderless_test,All,0.3875,0.1625,0.0375,0.5125,0.1375,0.275,0.0375,[('l' 2) ('-' 1)]\n",
      "60_14,RNN,genderless_test,M,0.45714285714285713,0.2,0.02857142857142857,0.34285714285714286,0.14285714285714285,0.4,0.08571428571428572,[('l' 2) ('-' 1)]\n",
      "60_14,RNN,genderless_test,F,0.25925925925925924,0.1111111111111111,0.07407407407407407,0.5555555555555556,0.1111111111111111,0.25925925925925924,0.0,[]\n",
      "60_14,RNN,genderless_test,N,0.4444444444444444,0.16666666666666666,0.0,0.7777777777777778,0.16666666666666666,0.05555555555555555,0.0,[]\n",
      "60_15,RNN,regular_test,All,0.475,0.1625,0.0,0.0875,0.1,0.775,0.0375,[('l' 3)]\n",
      "60_15,RNN,regular_test,M,0.2727272727272727,0.12121212121212122,0.0,0.15151515151515152,0.09090909090909091,0.7272727272727273,0.030303030303030304,[('l' 1)]\n",
      "60_15,RNN,regular_test,F,0.7941176470588235,0.2647058823529412,0.0,0.029411764705882353,0.11764705882352941,0.7941176470588235,0.058823529411764705,[('l' 2)]\n",
      "60_15,RNN,regular_test,N,0.15384615384615385,0.0,0.0,0.07692307692307693,0.07692307692307693,0.8461538461538461,0.0,[]\n",
      "60_15,RNN,genderless_test,All,0.475,0.15,0.0,0.075,0.0875,0.8,0.0375,[('l' 3)]\n",
      "60_15,RNN,genderless_test,M,0.48484848484848486,0.030303030303030304,0.0,0.09090909090909091,0.09090909090909091,0.7878787878787878,0.030303030303030304,[('l' 1)]\n",
      "60_15,RNN,genderless_test,F,0.5,0.3235294117647059,0.0,0.0,0.08823529411764706,0.8529411764705882,0.058823529411764705,[('l' 2)]\n",
      "60_15,RNN,genderless_test,N,0.38461538461538464,0.0,0.0,0.23076923076923078,0.07692307692307693,0.6923076923076923,0.0,[]\n",
      "60_16,RNN,regular_test,All,0.5125,0.2375,0.0,0.1125,0.175,0.7,0.0125,[('-' 1)]\n",
      "60_16,RNN,regular_test,M,0.34210526315789475,0.21052631578947367,0.0,0.18421052631578946,0.21052631578947367,0.6052631578947368,0.0,[]\n",
      "60_16,RNN,regular_test,F,0.8709677419354839,0.3225806451612903,0.0,0.03225806451612903,0.06451612903225806,0.9032258064516129,0.0,[]\n",
      "60_16,RNN,regular_test,N,0.09090909090909091,0.09090909090909091,0.0,0.09090909090909091,0.36363636363636365,0.45454545454545453,0.09090909090909091,[('-' 1)]\n",
      "60_16,RNN,genderless_test,All,0.4625,0.1875,0.0,0.0,0.0875,0.9125,0.0,[]\n",
      "60_16,RNN,genderless_test,M,0.42105263157894735,0.10526315789473684,0.0,0.0,0.05263157894736842,0.9473684210526315,0.0,[]\n",
      "60_16,RNN,genderless_test,F,0.4838709677419355,0.3225806451612903,0.0,0.0,0.0967741935483871,0.9032258064516129,0.0,[]\n",
      "60_16,RNN,genderless_test,N,0.5454545454545454,0.09090909090909091,0.0,0.0,0.18181818181818182,0.8181818181818182,0.0,[]\n",
      "60_17,RNN,regular_test,All,0.4875,0.2875,0.0125,0.6,0.075,0.2875,0.025,[('l' 2)]\n",
      "60_17,RNN,regular_test,M,0.5757575757575758,0.2727272727272727,0.0,0.6666666666666666,0.09090909090909091,0.24242424242424243,0.0,[]\n",
      "60_17,RNN,regular_test,F,0.4,0.3,0.0,0.5333333333333333,0.06666666666666667,0.4,0.0,[]\n",
      "60_17,RNN,regular_test,N,0.47058823529411764,0.29411764705882354,0.058823529411764705,0.5882352941176471,0.058823529411764705,0.17647058823529413,0.11764705882352941,[('l' 2)]\n",
      "60_17,RNN,genderless_test,All,0.4375,0.175,0.0,0.475,0.0625,0.4375,0.025,[('l' 2)]\n",
      "60_17,RNN,genderless_test,M,0.42424242424242425,0.15151515151515152,0.0,0.42424242424242425,0.06060606060606061,0.48484848484848486,0.030303030303030304,[('l' 1)]\n",
      "60_17,RNN,genderless_test,F,0.5,0.13333333333333333,0.0,0.4,0.1,0.5,0.0,[]\n",
      "60_17,RNN,genderless_test,N,0.35294117647058826,0.29411764705882354,0.0,0.7058823529411765,0.0,0.23529411764705882,0.058823529411764705,[('l' 1)]\n",
      "60_18,RNN,regular_test,All,0.5125,0.25,0.0,0.25,0.0,0.725,0.025,[('l' 1) ('-' 1)]\n",
      "60_18,RNN,regular_test,M,0.3888888888888889,0.1388888888888889,0.0,0.2777777777777778,0.0,0.7222222222222222,0.0,[]\n",
      "60_18,RNN,regular_test,F,0.75,0.40625,0.0,0.21875,0.0,0.75,0.03125,[('-' 1)]\n",
      "60_18,RNN,regular_test,N,0.25,0.16666666666666666,0.0,0.25,0.0,0.6666666666666666,0.08333333333333333,[('l' 1)]\n",
      "60_18,RNN,genderless_test,All,0.475,0.2625,0.0,0.2375,0.0,0.75,0.0125,[('-' 1)]\n",
      "60_18,RNN,genderless_test,M,0.4722222222222222,0.08333333333333333,0.0,0.3611111111111111,0.0,0.6111111111111112,0.027777777777777776,[('-' 1)]\n",
      "60_18,RNN,genderless_test,F,0.5,0.40625,0.0,0.125,0.0,0.875,0.0,[]\n",
      "60_18,RNN,genderless_test,N,0.4166666666666667,0.4166666666666667,0.0,0.16666666666666666,0.0,0.8333333333333334,0.0,[]\n",
      "60_19,RNN,regular_test,All,0.4875,0.3375,0.025,0.25,0.1,0.55,0.075,[('l' 6)]\n",
      "60_19,RNN,regular_test,M,0.36585365853658536,0.24390243902439024,0.04878048780487805,0.2926829268292683,0.14634146341463414,0.4146341463414634,0.0975609756097561,[('l' 4)]\n",
      "60_19,RNN,regular_test,F,0.75,0.4583333333333333,0.0,0.20833333333333334,0.0,0.75,0.041666666666666664,[('l' 1)]\n",
      "60_19,RNN,regular_test,N,0.4,0.4,0.0,0.2,0.13333333333333333,0.6,0.06666666666666667,[('l' 1)]\n",
      "60_19,RNN,genderless_test,All,0.4875,0.2125,0.0125,0.275,0.075,0.575,0.0625,[('l' 5)]\n",
      "60_19,RNN,genderless_test,M,0.4634146341463415,0.14634146341463414,0.0,0.3170731707317073,0.07317073170731707,0.5365853658536586,0.07317073170731707,[('l' 3)]\n",
      "60_19,RNN,genderless_test,F,0.5416666666666666,0.3333333333333333,0.0,0.16666666666666666,0.125,0.6666666666666666,0.041666666666666664,[('l' 1)]\n",
      "60_19,RNN,genderless_test,N,0.4666666666666667,0.2,0.06666666666666667,0.3333333333333333,0.0,0.5333333333333333,0.06666666666666667,[('l' 1)]\n",
      "60_20,RNN,regular_test,All,0.3875,0.275,0.0,0.325,0.1625,0.4,0.1125,[('l' 9)]\n",
      "60_20,RNN,regular_test,M,0.358974358974359,0.3076923076923077,0.0,0.4358974358974359,0.15384615384615385,0.3333333333333333,0.07692307692307693,[('l' 3)]\n",
      "60_20,RNN,regular_test,F,0.5,0.26666666666666666,0.0,0.26666666666666666,0.13333333333333333,0.5,0.1,[('l' 3)]\n",
      "60_20,RNN,regular_test,N,0.18181818181818182,0.18181818181818182,0.0,0.09090909090909091,0.2727272727272727,0.36363636363636365,0.2727272727272727,[('l' 3)]\n",
      "60_20,RNN,genderless_test,All,0.4375,0.1875,0.0,0.225,0.15,0.55,0.075,[('l' 6)]\n",
      "60_20,RNN,genderless_test,M,0.48717948717948717,0.1282051282051282,0.0,0.20512820512820512,0.1794871794871795,0.48717948717948717,0.1282051282051282,[('l' 5)]\n",
      "60_20,RNN,genderless_test,F,0.4,0.23333333333333334,0.0,0.3,0.1,0.5666666666666667,0.03333333333333333,[('l' 1)]\n",
      "60_20,RNN,genderless_test,N,0.36363636363636365,0.2727272727272727,0.0,0.09090909090909091,0.18181818181818182,0.7272727272727273,0.0,[]\n",
      "60_21,RNN,regular_test,All,0.45,0.2375,0.0,0.1875,0.0125,0.7375,0.0625,[('l' 5)]\n",
      "60_21,RNN,regular_test,M,0.3235294117647059,0.2647058823529412,0.0,0.29411764705882354,0.0,0.6764705882352942,0.029411764705882353,[('l' 1)]\n",
      "60_21,RNN,regular_test,F,0.8,0.36,0.0,0.04,0.04,0.84,0.08,[('l' 2)]\n",
      "60_21,RNN,regular_test,N,0.23809523809523808,0.047619047619047616,0.0,0.19047619047619047,0.0,0.7142857142857143,0.09523809523809523,[('l' 2)]\n",
      "60_21,RNN,genderless_test,All,0.4,0.225,0.0,0.1625,0.0125,0.8125,0.0125,[('l' 1)]\n",
      "60_21,RNN,genderless_test,M,0.4117647058823529,0.17647058823529413,0.0,0.11764705882352941,0.0,0.8823529411764706,0.0,[]\n",
      "60_21,RNN,genderless_test,F,0.44,0.32,0.0,0.28,0.04,0.64,0.04,[('l' 1)]\n",
      "60_21,RNN,genderless_test,N,0.3333333333333333,0.19047619047619047,0.0,0.09523809523809523,0.0,0.9047619047619048,0.0,[]\n",
      "60_22,RNN,regular_test,All,0.475,0.25,0.0,0.075,0.1125,0.775,0.0375,[('g' 2) ('i' 1)]\n",
      "60_22,RNN,regular_test,M,0.2857142857142857,0.08571428571428572,0.0,0.11428571428571428,0.14285714285714285,0.6571428571428571,0.08571428571428572,[('g' 2) ('i' 1)]\n",
      "60_22,RNN,regular_test,F,0.8387096774193549,0.41935483870967744,0.0,0.03225806451612903,0.06451612903225806,0.9032258064516129,0.0,[]\n",
      "60_22,RNN,regular_test,N,0.14285714285714285,0.2857142857142857,0.0,0.07142857142857142,0.14285714285714285,0.7857142857142857,0.0,[]\n",
      "60_22,RNN,genderless_test,All,0.4375,0.1875,0.0,0.0,0.0875,0.9125,0.0,[]\n",
      "60_22,RNN,genderless_test,M,0.37142857142857144,0.05714285714285714,0.0,0.0,0.08571428571428572,0.9142857142857143,0.0,[]\n",
      "60_22,RNN,genderless_test,F,0.45161290322580644,0.3548387096774194,0.0,0.0,0.0967741935483871,0.9032258064516129,0.0,[]\n",
      "60_22,RNN,genderless_test,N,0.5714285714285714,0.14285714285714285,0.0,0.0,0.07142857142857142,0.9285714285714286,0.0,[]\n",
      "60_23,RNN,regular_test,All,0.5,0.3,0.025,0.4,0.025,0.5375,0.0125,[('l' 1)]\n",
      "60_23,RNN,regular_test,M,0.3888888888888889,0.25,0.027777777777777776,0.5,0.027777777777777776,0.4166666666666667,0.027777777777777776,[('l' 1)]\n",
      "60_23,RNN,regular_test,F,0.7,0.3,0.0,0.3,0.0,0.7,0.0,[]\n",
      "60_23,RNN,regular_test,N,0.35714285714285715,0.42857142857142855,0.07142857142857142,0.35714285714285715,0.07142857142857142,0.5,0.0,[]\n",
      "60_23,RNN,genderless_test,All,0.475,0.2125,0.0,0.1375,0.0,0.8625,0.0,[]\n",
      "60_23,RNN,genderless_test,M,0.5277777777777778,0.1111111111111111,0.0,0.1388888888888889,0.0,0.8611111111111112,0.0,[]\n",
      "60_23,RNN,genderless_test,F,0.4,0.3,0.0,0.1,0.0,0.9,0.0,[]\n",
      "60_23,RNN,genderless_test,N,0.5,0.2857142857142857,0.0,0.21428571428571427,0.0,0.7857142857142857,0.0,[]\n",
      "60_24,RNN,regular_test,All,0.5,0.25,0.0,0.1625,0.1125,0.725,0.0,[]\n",
      "60_24,RNN,regular_test,M,0.34146341463414637,0.17073170731707318,0.0,0.24390243902439024,0.17073170731707318,0.5853658536585366,0.0,[]\n",
      "60_24,RNN,regular_test,F,0.8387096774193549,0.3548387096774194,0.0,0.0967741935483871,0.06451612903225806,0.8387096774193549,0.0,[]\n",
      "60_24,RNN,regular_test,N,0.0,0.25,0.0,0.0,0.0,1.0,0.0,[]\n",
      "60_24,RNN,genderless_test,All,0.45,0.2,0.0,0.025,0.0875,0.8875,0.0,[]\n",
      "60_24,RNN,genderless_test,M,0.5121951219512195,0.07317073170731707,0.0,0.024390243902439025,0.04878048780487805,0.926829268292683,0.0,[]\n",
      "60_24,RNN,genderless_test,F,0.41935483870967744,0.3870967741935484,0.0,0.03225806451612903,0.0967741935483871,0.8709677419354839,0.0,[]\n",
      "60_24,RNN,genderless_test,N,0.25,0.125,0.0,0.0,0.25,0.75,0.0,[]\n",
      "120_0,RNN,regular_test,All,0.5875,0.425,0.0125,0.2875,0.15,0.4375,0.1125,[('l' 8) ('i' 1)]\n",
      "120_0,RNN,regular_test,M,0.5357142857142857,0.39285714285714285,0.0,0.4642857142857143,0.17857142857142858,0.17857142857142858,0.17857142857142858,[('l' 4) ('i' 1)]\n",
      "120_0,RNN,regular_test,F,0.7894736842105263,0.5263157894736842,0.0,0.10526315789473684,0.05263157894736842,0.7894736842105263,0.05263157894736842,[('l' 2)]\n",
      "120_0,RNN,regular_test,N,0.14285714285714285,0.21428571428571427,0.07142857142857142,0.42857142857142855,0.35714285714285715,0.0,0.14285714285714285,[('l' 2)]\n",
      "120_0,RNN,genderless_test,All,0.2375,0.225,0.0,0.3,0.4625,0.1375,0.1,[('l' 7) ('o' 1)]\n",
      "120_0,RNN,genderless_test,M,0.25,0.39285714285714285,0.0,0.35714285714285715,0.42857142857142855,0.10714285714285714,0.10714285714285714,[('l' 3)]\n",
      "120_0,RNN,genderless_test,F,0.21052631578947367,0.15789473684210525,0.0,0.2631578947368421,0.5,0.13157894736842105,0.10526315789473684,[('l' 4)]\n",
      "120_0,RNN,genderless_test,N,0.2857142857142857,0.07142857142857142,0.0,0.2857142857142857,0.42857142857142855,0.21428571428571427,0.07142857142857142,[('o' 1)]\n",
      "120_1,RNN,regular_test,All,0.5375,0.4625,0.0,0.1125,0.1625,0.7,0.025,[('b' 2)]\n",
      "120_1,RNN,regular_test,M,0.2857142857142857,0.4,0.0,0.17142857142857143,0.14285714285714285,0.6285714285714286,0.05714285714285714,[('b' 2)]\n",
      "120_1,RNN,regular_test,F,0.9354838709677419,0.5806451612903226,0.0,0.06451612903225806,0.0,0.9354838709677419,0.0,[]\n",
      "120_1,RNN,regular_test,N,0.2857142857142857,0.35714285714285715,0.0,0.07142857142857142,0.5714285714285714,0.35714285714285715,0.0,[]\n",
      "120_1,RNN,genderless_test,All,0.4625,0.325,0.0,0.0,0.1375,0.85,0.0125,[('l' 1)]\n",
      "120_1,RNN,genderless_test,M,0.5714285714285714,0.2571428571428571,0.0,0.0,0.11428571428571428,0.8857142857142857,0.0,[]\n",
      "120_1,RNN,genderless_test,F,0.3548387096774194,0.41935483870967744,0.0,0.0,0.16129032258064516,0.8064516129032258,0.03225806451612903,[('l' 1)]\n",
      "120_1,RNN,genderless_test,N,0.42857142857142855,0.2857142857142857,0.0,0.0,0.14285714285714285,0.8571428571428571,0.0,[]\n",
      "120_2,RNN,regular_test,All,0.475,0.375,0.0,0.2875,0.1625,0.4625,0.0875,[('l' 7)]\n",
      "120_2,RNN,regular_test,M,0.3611111111111111,0.3055555555555556,0.0,0.4722222222222222,0.19444444444444445,0.19444444444444445,0.1388888888888889,[('l' 5)]\n",
      "120_2,RNN,regular_test,F,0.7419354838709677,0.4838709677419355,0.0,0.16129032258064516,0.06451612903225806,0.7419354838709677,0.03225806451612903,[('l' 1)]\n",
      "120_2,RNN,regular_test,N,0.15384615384615385,0.3076923076923077,0.0,0.07692307692307693,0.3076923076923077,0.5384615384615384,0.07692307692307693,[('l' 1)]\n",
      "120_2,RNN,genderless_test,All,0.375,0.275,0.0,0.1125,0.25,0.5625,0.075,[('l' 6)]\n",
      "120_2,RNN,genderless_test,M,0.3333333333333333,0.1111111111111111,0.0,0.08333333333333333,0.2777777777777778,0.5277777777777778,0.1111111111111111,[('l' 4)]\n",
      "120_2,RNN,genderless_test,F,0.4838709677419355,0.3870967741935484,0.0,0.12903225806451613,0.1935483870967742,0.6774193548387096,0.0,[]\n",
      "120_2,RNN,genderless_test,N,0.23076923076923078,0.46153846153846156,0.0,0.15384615384615385,0.3076923076923077,0.38461538461538464,0.15384615384615385,[('l' 2)]\n",
      "120_3,RNN,regular_test,All,0.3875,0.4125,0.0125,0.2,0.1875,0.5125,0.0875,[('-' 4) ('l' 3)]\n",
      "120_3,RNN,regular_test,M,0.38461538461538464,0.3076923076923077,0.038461538461538464,0.2692307692307692,0.2692307692307692,0.3076923076923077,0.11538461538461539,[('-' 2) ('l' 1)]\n",
      "120_3,RNN,regular_test,F,0.5588235294117647,0.5,0.0,0.17647058823529413,0.17647058823529413,0.5882352941176471,0.058823529411764705,[('l' 1) ('-' 1)]\n",
      "120_3,RNN,regular_test,N,0.1,0.4,0.0,0.15,0.1,0.65,0.1,[('l' 1) ('-' 1)]\n",
      "120_3,RNN,genderless_test,All,0.2625,0.2625,0.025,0.1625,0.25,0.5125,0.05,[('l' 4)]\n",
      "120_3,RNN,genderless_test,M,0.3076923076923077,0.23076923076923078,0.038461538461538464,0.15384615384615385,0.19230769230769232,0.5384615384615384,0.07692307692307693,[('l' 2)]\n",
      "120_3,RNN,genderless_test,F,0.20588235294117646,0.3235294117647059,0.029411764705882353,0.17647058823529413,0.3235294117647059,0.47058823529411764,0.0,[]\n",
      "120_3,RNN,genderless_test,N,0.3,0.2,0.0,0.15,0.2,0.55,0.1,[('l' 2)]\n",
      "120_4,RNN,regular_test,All,0.4875,0.475,0.0,0.1625,0.175,0.575,0.0875,[('l' 6) ('-' 1)]\n",
      "120_4,RNN,regular_test,M,0.3939393939393939,0.45454545454545453,0.0,0.18181818181818182,0.24242424242424243,0.3939393939393939,0.18181818181818182,[('l' 5) ('-' 1)]\n",
      "120_4,RNN,regular_test,F,0.75,0.5625,0.0,0.15625,0.0625,0.78125,0.0,[]\n",
      "120_4,RNN,regular_test,N,0.13333333333333333,0.3333333333333333,0.0,0.13333333333333333,0.26666666666666666,0.5333333333333333,0.06666666666666667,[('l' 1)]\n",
      "120_4,RNN,genderless_test,All,0.375,0.3875,0.0,0.1125,0.2875,0.55,0.05,[('l' 4)]\n",
      "120_4,RNN,genderless_test,M,0.24242424242424243,0.36363636363636365,0.0,0.15151515151515152,0.2727272727272727,0.5454545454545454,0.030303030303030304,[('l' 1)]\n",
      "120_4,RNN,genderless_test,F,0.4375,0.4375,0.0,0.0625,0.34375,0.5,0.09375,[('l' 3)]\n",
      "120_4,RNN,genderless_test,N,0.5333333333333333,0.3333333333333333,0.0,0.13333333333333333,0.2,0.6666666666666666,0.0,[]\n",
      "120_5,RNN,regular_test,All,0.45,0.5125,0.0625,0.15,0.1,0.5875,0.1,[('l' 8)]\n",
      "120_5,RNN,regular_test,M,0.2702702702702703,0.4864864864864865,0.08108108108108109,0.1891891891891892,0.08108108108108109,0.5405405405405406,0.10810810810810811,[('l' 4)]\n",
      "120_5,RNN,regular_test,F,0.75,0.6785714285714286,0.03571428571428571,0.07142857142857142,0.10714285714285714,0.7857142857142857,0.0,[]\n",
      "120_5,RNN,regular_test,N,0.3333333333333333,0.26666666666666666,0.06666666666666667,0.2,0.13333333333333333,0.3333333333333333,0.26666666666666666,[('l' 4)]\n",
      "120_5,RNN,genderless_test,All,0.4375,0.45,0.0375,0.075,0.125,0.7125,0.05,[('l' 3) ('g' 1)]\n",
      "120_5,RNN,genderless_test,M,0.32432432432432434,0.40540540540540543,0.02702702702702703,0.05405405405405406,0.13513513513513514,0.7297297297297297,0.05405405405405406,[('l' 2)]\n",
      "120_5,RNN,genderless_test,F,0.4642857142857143,0.6428571428571429,0.03571428571428571,0.03571428571428571,0.10714285714285714,0.75,0.07142857142857142,[('g' 1) ('l' 1)]\n",
      "120_5,RNN,genderless_test,N,0.6666666666666666,0.2,0.06666666666666667,0.2,0.13333333333333333,0.6,0.0,[]\n",
      "120_6,RNN,regular_test,All,0.4875,0.4875,0.0,0.2875,0.1,0.5625,0.05,[('l' 3) ('m' 1)]\n",
      "120_6,RNN,regular_test,M,0.3888888888888889,0.4722222222222222,0.0,0.3333333333333333,0.16666666666666666,0.4722222222222222,0.027777777777777776,[('m' 1)]\n",
      "120_6,RNN,regular_test,F,0.6571428571428571,0.4857142857142857,0.0,0.2571428571428571,0.02857142857142857,0.6571428571428571,0.05714285714285714,[('l' 2)]\n",
      "120_6,RNN,regular_test,N,0.2222222222222222,0.5555555555555556,0.0,0.2222222222222222,0.1111111111111111,0.5555555555555556,0.1111111111111111,[('l' 1)]\n",
      "120_6,RNN,genderless_test,All,0.425,0.275,0.0125,0.2125,0.25,0.5125,0.0125,[('l' 1)]\n",
      "120_6,RNN,genderless_test,M,0.4166666666666667,0.2777777777777778,0.0,0.16666666666666666,0.3055555555555556,0.5277777777777778,0.0,[]\n",
      "120_6,RNN,genderless_test,F,0.4,0.2857142857142857,0.0,0.22857142857142856,0.17142857142857143,0.5714285714285714,0.02857142857142857,[('l' 1)]\n",
      "120_6,RNN,genderless_test,N,0.5555555555555556,0.2222222222222222,0.1111111111111111,0.3333333333333333,0.3333333333333333,0.2222222222222222,0.0,[]\n",
      "120_7,RNN,regular_test,All,0.55,0.5375,0.0125,0.225,0.275,0.475,0.0125,[('-' 1)]\n",
      "120_7,RNN,regular_test,M,0.5151515151515151,0.6363636363636364,0.0,0.24242424242424243,0.36363636363636365,0.36363636363636365,0.030303030303030304,[('-' 1)]\n",
      "120_7,RNN,regular_test,F,0.7,0.4666666666666667,0.0,0.13333333333333333,0.16666666666666666,0.7,0.0,[]\n",
      "120_7,RNN,regular_test,N,0.35294117647058826,0.47058823529411764,0.058823529411764705,0.35294117647058826,0.29411764705882354,0.29411764705882354,0.0,[]\n",
      "120_7,RNN,genderless_test,All,0.375,0.225,0.0,0.2125,0.425,0.35,0.0125,[('l' 1)]\n",
      "120_7,RNN,genderless_test,M,0.3333333333333333,0.18181818181818182,0.0,0.12121212121212122,0.42424242424242425,0.45454545454545453,0.0,[]\n",
      "120_7,RNN,genderless_test,F,0.43333333333333335,0.3333333333333333,0.0,0.26666666666666666,0.4,0.3,0.03333333333333333,[('l' 1)]\n",
      "120_7,RNN,genderless_test,N,0.35294117647058826,0.11764705882352941,0.0,0.29411764705882354,0.47058823529411764,0.23529411764705882,0.0,[]\n",
      "120_8,RNN,regular_test,All,0.4625,0.4125,0.0375,0.1375,0.2375,0.5375,0.05,[('l' 3) ('h' 1)]\n",
      "120_8,RNN,regular_test,M,0.23076923076923078,0.28205128205128205,0.05128205128205128,0.1282051282051282,0.358974358974359,0.41025641025641024,0.05128205128205128,[('h' 1) ('l' 1)]\n",
      "120_8,RNN,regular_test,F,0.8,0.6,0.03333333333333333,0.1,0.03333333333333333,0.8333333333333334,0.0,[]\n",
      "120_8,RNN,regular_test,N,0.36363636363636365,0.36363636363636365,0.0,0.2727272727272727,0.36363636363636365,0.18181818181818182,0.18181818181818182,[('l' 2)]\n",
      "120_8,RNN,genderless_test,All,0.4,0.3375,0.0125,0.175,0.25,0.525,0.0375,[('l' 2) ('f' 1)]\n",
      "120_8,RNN,genderless_test,M,0.46153846153846156,0.358974358974359,0.0,0.20512820512820512,0.20512820512820512,0.5641025641025641,0.02564102564102564,[('f' 1)]\n",
      "120_8,RNN,genderless_test,F,0.3,0.36666666666666664,0.03333333333333333,0.16666666666666666,0.26666666666666666,0.5,0.03333333333333333,[('l' 1)]\n",
      "120_8,RNN,genderless_test,N,0.45454545454545453,0.18181818181818182,0.0,0.09090909090909091,0.36363636363636365,0.45454545454545453,0.09090909090909091,[('l' 1)]\n",
      "120_9,RNN,regular_test,All,0.425,0.5,0.0,0.2625,0.15,0.575,0.0125,[('a' 1)]\n",
      "120_9,RNN,regular_test,M,0.3170731707317073,0.36585365853658536,0.0,0.34146341463414637,0.14634146341463414,0.5121951219512195,0.0,[]\n",
      "120_9,RNN,regular_test,F,0.64,0.68,0.0,0.08,0.2,0.68,0.04,[('a' 1)]\n",
      "120_9,RNN,regular_test,N,0.35714285714285715,0.5714285714285714,0.0,0.35714285714285715,0.07142857142857142,0.5714285714285714,0.0,[]\n",
      "120_9,RNN,genderless_test,All,0.325,0.3375,0.0,0.25,0.1625,0.525,0.0625,[('t' 3) ('o' 1) ('a' 1)]\n",
      "120_9,RNN,genderless_test,M,0.4634146341463415,0.3170731707317073,0.0,0.24390243902439024,0.21951219512195122,0.43902439024390244,0.0975609756097561,[('t' 2) ('o' 1) ('a' 1)]\n",
      "120_9,RNN,genderless_test,F,0.12,0.4,0.0,0.32,0.12,0.56,0.0,[]\n",
      "120_9,RNN,genderless_test,N,0.2857142857142857,0.2857142857142857,0.0,0.14285714285714285,0.07142857142857142,0.7142857142857143,0.07142857142857142,[('t' 1)]\n",
      "120_10,RNN,regular_test,All,0.6,0.3625,0.0125,0.1875,0.225,0.55,0.025,[('l' 2)]\n",
      "120_10,RNN,regular_test,M,0.47368421052631576,0.3157894736842105,0.02631578947368421,0.2894736842105263,0.34210526315789475,0.3157894736842105,0.02631578947368421,[('l' 1)]\n",
      "120_10,RNN,regular_test,F,0.8666666666666667,0.43333333333333335,0.0,0.1,0.03333333333333333,0.8666666666666667,0.0,[]\n",
      "120_10,RNN,regular_test,N,0.3333333333333333,0.3333333333333333,0.0,0.08333333333333333,0.3333333333333333,0.5,0.08333333333333333,[('l' 1)]\n",
      "120_10,RNN,genderless_test,All,0.4125,0.275,0.0,0.1625,0.2875,0.475,0.075,[('l' 4) ('-' 2)]\n",
      "120_10,RNN,genderless_test,M,0.4473684210526316,0.18421052631578946,0.0,0.13157894736842105,0.2894736842105263,0.47368421052631576,0.10526315789473684,[('l' 2) ('-' 2)]\n",
      "120_10,RNN,genderless_test,F,0.4,0.4,0.0,0.26666666666666666,0.26666666666666666,0.4666666666666667,0.0,[]\n",
      "120_10,RNN,genderless_test,N,0.3333333333333333,0.25,0.0,0.0,0.3333333333333333,0.5,0.16666666666666666,[('l' 2)]\n",
      "120_11,RNN,regular_test,All,0.525,0.45,0.025,0.3,0.1,0.5375,0.0375,[('l' 3)]\n",
      "120_11,RNN,regular_test,M,0.325,0.35,0.05,0.4,0.15,0.325,0.075,[('l' 3)]\n",
      "120_11,RNN,regular_test,F,0.7931034482758621,0.6551724137931034,0.0,0.13793103448275862,0.034482758620689655,0.8275862068965517,0.0,[]\n",
      "120_11,RNN,regular_test,N,0.5454545454545454,0.2727272727272727,0.0,0.36363636363636365,0.09090909090909091,0.5454545454545454,0.0,[]\n",
      "120_11,RNN,genderless_test,All,0.3375,0.2125,0.0,0.2375,0.275,0.4625,0.025,[('l' 2)]\n",
      "120_11,RNN,genderless_test,M,0.375,0.2,0.0,0.225,0.25,0.5,0.025,[('l' 1)]\n",
      "120_11,RNN,genderless_test,F,0.3103448275862069,0.27586206896551724,0.0,0.20689655172413793,0.3448275862068966,0.41379310344827586,0.034482758620689655,[('l' 1)]\n",
      "120_11,RNN,genderless_test,N,0.2727272727272727,0.09090909090909091,0.0,0.36363636363636365,0.18181818181818182,0.45454545454545453,0.0,[]\n",
      "120_12,RNN,regular_test,All,0.55,0.4375,0.0,0.1875,0.15,0.6625,0.0,[]\n",
      "120_12,RNN,regular_test,M,0.5,0.3684210526315789,0.0,0.2631578947368421,0.23684210526315788,0.5,0.0,[]\n",
      "120_12,RNN,regular_test,F,0.8214285714285714,0.4642857142857143,0.0,0.14285714285714285,0.03571428571428571,0.8214285714285714,0.0,[]\n",
      "120_12,RNN,regular_test,N,0.14285714285714285,0.5714285714285714,0.0,0.07142857142857142,0.14285714285714285,0.7857142857142857,0.0,[]\n",
      "120_12,RNN,genderless_test,All,0.525,0.3875,0.0,0.0375,0.0875,0.8625,0.0125,[('a' 1)]\n",
      "120_12,RNN,genderless_test,M,0.5263157894736842,0.2894736842105263,0.0,0.05263157894736842,0.05263157894736842,0.8947368421052632,0.0,[]\n",
      "120_12,RNN,genderless_test,F,0.5357142857142857,0.4642857142857143,0.0,0.03571428571428571,0.14285714285714285,0.8214285714285714,0.0,[]\n",
      "120_12,RNN,genderless_test,N,0.5,0.5,0.0,0.0,0.07142857142857142,0.8571428571428571,0.07142857142857142,[('a' 1)]\n",
      "120_13,RNN,regular_test,All,0.4125,0.45,0.0,0.0875,0.2875,0.5375,0.0875,[('l' 7)]\n",
      "120_13,RNN,regular_test,M,0.21212121212121213,0.3939393939393939,0.0,0.12121212121212122,0.3333333333333333,0.45454545454545453,0.09090909090909091,[('l' 3)]\n",
      "120_13,RNN,regular_test,F,0.7058823529411765,0.47058823529411764,0.0,0.029411764705882353,0.20588235294117646,0.7058823529411765,0.058823529411764705,[('l' 2)]\n",
      "120_13,RNN,regular_test,N,0.15384615384615385,0.5384615384615384,0.0,0.15384615384615385,0.38461538461538464,0.3076923076923077,0.15384615384615385,[('l' 2)]\n",
      "120_13,RNN,genderless_test,All,0.4625,0.2,0.0,0.05,0.35,0.55,0.05,[('l' 4)]\n",
      "120_13,RNN,genderless_test,M,0.5151515151515151,0.18181818181818182,0.0,0.09090909090909091,0.24242424242424243,0.6363636363636364,0.030303030303030304,[('l' 1)]\n",
      "120_13,RNN,genderless_test,F,0.4411764705882353,0.2647058823529412,0.0,0.029411764705882353,0.38235294117647056,0.5294117647058824,0.058823529411764705,[('l' 2)]\n",
      "120_13,RNN,genderless_test,N,0.38461538461538464,0.07692307692307693,0.0,0.0,0.5384615384615384,0.38461538461538464,0.07692307692307693,[('l' 1)]\n",
      "120_14,RNN,regular_test,All,0.6,0.5125,0.025,0.15,0.1125,0.7,0.0125,[('l' 1)]\n",
      "120_14,RNN,regular_test,M,0.45714285714285713,0.4857142857142857,0.0,0.22857142857142856,0.2,0.5714285714285714,0.0,[]\n",
      "120_14,RNN,regular_test,F,0.9259259259259259,0.48148148148148145,0.0,0.037037037037037035,0.037037037037037035,0.9259259259259259,0.0,[]\n",
      "120_14,RNN,regular_test,N,0.3888888888888889,0.6111111111111112,0.1111111111111111,0.16666666666666666,0.05555555555555555,0.6111111111111112,0.05555555555555555,[('l' 1)]\n",
      "120_14,RNN,genderless_test,All,0.4125,0.2875,0.05,0.0375,0.2125,0.65,0.05,[('l' 4)]\n",
      "120_14,RNN,genderless_test,M,0.3142857142857143,0.22857142857142856,0.02857142857142857,0.05714285714285714,0.2571428571428571,0.5714285714285714,0.08571428571428572,[('l' 3)]\n",
      "120_14,RNN,genderless_test,F,0.4444444444444444,0.2962962962962963,0.037037037037037035,0.037037037037037035,0.18518518518518517,0.7407407407407407,0.0,[]\n",
      "120_14,RNN,genderless_test,N,0.5555555555555556,0.3888888888888889,0.1111111111111111,0.0,0.16666666666666666,0.6666666666666666,0.05555555555555555,[('l' 1)]\n",
      "120_15,RNN,regular_test,All,0.525,0.475,0.025,0.25,0.0625,0.6,0.0625,[('l' 5)]\n",
      "120_15,RNN,regular_test,M,0.36363636363636365,0.2727272727272727,0.030303030303030304,0.24242424242424243,0.09090909090909091,0.5757575757575758,0.06060606060606061,[('l' 2)]\n",
      "120_15,RNN,regular_test,F,0.7058823529411765,0.6176470588235294,0.029411764705882353,0.2647058823529412,0.0,0.7058823529411765,0.0,[]\n",
      "120_15,RNN,regular_test,N,0.46153846153846156,0.6153846153846154,0.0,0.23076923076923078,0.15384615384615385,0.38461538461538464,0.23076923076923078,[('l' 3)]\n",
      "120_15,RNN,genderless_test,All,0.5125,0.325,0.0125,0.1875,0.05,0.7,0.05,[('l' 4)]\n",
      "120_15,RNN,genderless_test,M,0.5454545454545454,0.24242424242424243,0.030303030303030304,0.09090909090909091,0.06060606060606061,0.7575757575757576,0.06060606060606061,[('l' 2)]\n",
      "120_15,RNN,genderless_test,F,0.5588235294117647,0.4411764705882353,0.0,0.29411764705882354,0.0,0.7058823529411765,0.0,[]\n",
      "120_15,RNN,genderless_test,N,0.3076923076923077,0.23076923076923078,0.0,0.15384615384615385,0.15384615384615385,0.5384615384615384,0.15384615384615385,[('l' 2)]\n",
      "120_16,RNN,regular_test,All,0.575,0.5125,0.0125,0.2625,0.2625,0.45,0.0125,[('d' 1)]\n",
      "120_16,RNN,regular_test,M,0.47368421052631576,0.5,0.02631578947368421,0.3157894736842105,0.3684210526315789,0.2631578947368421,0.02631578947368421,[('d' 1)]\n",
      "120_16,RNN,regular_test,F,0.7096774193548387,0.5806451612903226,0.0,0.1935483870967742,0.06451612903225806,0.7419354838709677,0.0,[]\n",
      "120_16,RNN,regular_test,N,0.5454545454545454,0.36363636363636365,0.0,0.2727272727272727,0.45454545454545453,0.2727272727272727,0.0,[]\n",
      "120_16,RNN,genderless_test,All,0.3625,0.25,0.05,0.2,0.325,0.4,0.025,[('i' 1) ('l' 1)]\n",
      "120_16,RNN,genderless_test,M,0.42105263157894735,0.2631578947368421,0.05263157894736842,0.21052631578947367,0.2631578947368421,0.4473684210526316,0.02631578947368421,[('i' 1)]\n",
      "120_16,RNN,genderless_test,F,0.3225806451612903,0.2903225806451613,0.06451612903225806,0.16129032258064516,0.3225806451612903,0.41935483870967744,0.03225806451612903,[('l' 1)]\n",
      "120_16,RNN,genderless_test,N,0.2727272727272727,0.09090909090909091,0.0,0.2727272727272727,0.5454545454545454,0.18181818181818182,0.0,[]\n",
      "120_17,RNN,regular_test,All,0.5125,0.575,0.0,0.1875,0.1875,0.5875,0.0375,[('l' 3)]\n",
      "120_17,RNN,regular_test,M,0.45454545454545453,0.5757575757575758,0.0,0.21212121212121213,0.2727272727272727,0.5151515151515151,0.0,[]\n",
      "120_17,RNN,regular_test,F,0.7666666666666667,0.5,0.0,0.16666666666666666,0.06666666666666667,0.7666666666666667,0.0,[]\n",
      "120_17,RNN,regular_test,N,0.17647058823529413,0.7058823529411765,0.0,0.17647058823529413,0.23529411764705882,0.4117647058823529,0.17647058823529413,[('l' 3)]\n",
      "120_17,RNN,genderless_test,All,0.375,0.3375,0.0,0.2125,0.25,0.4875,0.05,[('l' 3) ('z' 1)]\n",
      "120_17,RNN,genderless_test,M,0.36363636363636365,0.2727272727272727,0.0,0.30303030303030304,0.24242424242424243,0.36363636363636365,0.09090909090909091,[('l' 2) ('z' 1)]\n",
      "120_17,RNN,genderless_test,F,0.4666666666666667,0.4,0.0,0.16666666666666666,0.2,0.6,0.03333333333333333,[('l' 1)]\n",
      "120_17,RNN,genderless_test,N,0.23529411764705882,0.35294117647058826,0.0,0.11764705882352941,0.35294117647058826,0.5294117647058824,0.0,[]\n",
      "120_18,RNN,regular_test,All,0.5125,0.5,0.0125,0.0875,0.0625,0.7375,0.1,[('-' 5) ('l' 3)]\n",
      "120_18,RNN,regular_test,M,0.3055555555555556,0.2777777777777778,0.027777777777777776,0.1388888888888889,0.1111111111111111,0.5833333333333334,0.1388888888888889,[('-' 3) ('l' 2)]\n",
      "120_18,RNN,regular_test,F,0.875,0.6875,0.0,0.03125,0.0,0.875,0.09375,[('-' 2) ('l' 1)]\n",
      "120_18,RNN,regular_test,N,0.16666666666666666,0.6666666666666666,0.0,0.08333333333333333,0.08333333333333333,0.8333333333333334,0.0,[]\n",
      "120_18,RNN,genderless_test,All,0.5875,0.5375,0.0,0.1125,0.0625,0.825,0.0,[]\n",
      "120_18,RNN,genderless_test,M,0.5833333333333334,0.3611111111111111,0.0,0.1388888888888889,0.08333333333333333,0.7777777777777778,0.0,[]\n",
      "120_18,RNN,genderless_test,F,0.6875,0.75,0.0,0.0625,0.03125,0.90625,0.0,[]\n",
      "120_18,RNN,genderless_test,N,0.3333333333333333,0.5,0.0,0.16666666666666666,0.08333333333333333,0.75,0.0,[]\n",
      "120_19,RNN,regular_test,All,0.5375,0.575,0.025,0.2125,0.1125,0.575,0.075,[('l' 6)]\n",
      "120_19,RNN,regular_test,M,0.4146341463414634,0.43902439024390244,0.024390243902439025,0.3170731707317073,0.1951219512195122,0.4146341463414634,0.04878048780487805,[('l' 2)]\n",
      "120_19,RNN,regular_test,F,0.875,0.8333333333333334,0.041666666666666664,0.08333333333333333,0.0,0.875,0.0,[]\n",
      "120_19,RNN,regular_test,N,0.3333333333333333,0.5333333333333333,0.0,0.13333333333333333,0.06666666666666667,0.5333333333333333,0.26666666666666666,[('l' 4)]\n",
      "120_19,RNN,genderless_test,All,0.4375,0.35,0.05,0.075,0.2375,0.6,0.0375,[('l' 3)]\n",
      "120_19,RNN,genderless_test,M,0.4634146341463415,0.2682926829268293,0.0975609756097561,0.024390243902439025,0.1951219512195122,0.6585365853658537,0.024390243902439025,[('l' 1)]\n",
      "120_19,RNN,genderless_test,F,0.375,0.4583333333333333,0.0,0.125,0.2916666666666667,0.5,0.08333333333333333,[('l' 2)]\n",
      "120_19,RNN,genderless_test,N,0.4666666666666667,0.4,0.0,0.13333333333333333,0.26666666666666666,0.6,0.0,[]\n",
      "120_20,RNN,regular_test,All,0.525,0.4375,0.025,0.075,0.2375,0.625,0.0375,[('l' 3)]\n",
      "120_20,RNN,regular_test,M,0.28205128205128205,0.38461538461538464,0.02564102564102564,0.15384615384615385,0.3076923076923077,0.4358974358974359,0.07692307692307693,[('l' 3)]\n",
      "120_20,RNN,regular_test,F,0.9333333333333333,0.5,0.03333333333333333,0.0,0.03333333333333333,0.9333333333333333,0.0,[]\n",
      "120_20,RNN,regular_test,N,0.2727272727272727,0.45454545454545453,0.0,0.0,0.5454545454545454,0.45454545454545453,0.0,[]\n",
      "120_20,RNN,genderless_test,All,0.45,0.2375,0.0125,0.075,0.2,0.6625,0.05,[('l' 3) ('t' 1)]\n",
      "120_20,RNN,genderless_test,M,0.358974358974359,0.23076923076923078,0.0,0.07692307692307693,0.1794871794871795,0.6923076923076923,0.05128205128205128,[('l' 2)]\n",
      "120_20,RNN,genderless_test,F,0.5333333333333333,0.3,0.03333333333333333,0.1,0.2,0.6,0.06666666666666667,[('t' 1) ('l' 1)]\n",
      "120_20,RNN,genderless_test,N,0.5454545454545454,0.09090909090909091,0.0,0.0,0.2727272727272727,0.7272727272727273,0.0,[]\n",
      "120_21,RNN,regular_test,All,0.425,0.4375,0.0,0.2,0.1375,0.6,0.0625,[('l' 5)]\n",
      "120_21,RNN,regular_test,M,0.38235294117647056,0.5294117647058824,0.0,0.20588235294117646,0.17647058823529413,0.5588235294117647,0.058823529411764705,[('l' 2)]\n",
      "120_21,RNN,regular_test,F,0.6,0.32,0.0,0.24,0.04,0.64,0.08,[('l' 2)]\n",
      "120_21,RNN,regular_test,N,0.2857142857142857,0.42857142857142855,0.0,0.14285714285714285,0.19047619047619047,0.6190476190476191,0.047619047619047616,[('l' 1)]\n",
      "120_21,RNN,genderless_test,All,0.425,0.325,0.0125,0.15,0.1125,0.6625,0.0625,[('l' 5)]\n",
      "120_21,RNN,genderless_test,M,0.47058823529411764,0.3235294117647059,0.029411764705882353,0.20588235294117646,0.08823529411764706,0.5882352941176471,0.08823529411764706,[('l' 3)]\n",
      "120_21,RNN,genderless_test,F,0.36,0.36,0.0,0.12,0.08,0.72,0.08,[('l' 2)]\n",
      "120_21,RNN,genderless_test,N,0.42857142857142855,0.2857142857142857,0.0,0.09523809523809523,0.19047619047619047,0.7142857142857143,0.0,[]\n",
      "120_22,RNN,regular_test,All,0.5125,0.4625,0.0,0.3,0.0375,0.6125,0.05,[('l' 4)]\n",
      "120_22,RNN,regular_test,M,0.4,0.4857142857142857,0.0,0.34285714285714286,0.05714285714285714,0.5714285714285714,0.02857142857142857,[('l' 1)]\n",
      "120_22,RNN,regular_test,F,0.7419354838709677,0.5161290322580645,0.0,0.22580645161290322,0.0,0.7419354838709677,0.03225806451612903,[('l' 1)]\n",
      "120_22,RNN,regular_test,N,0.2857142857142857,0.2857142857142857,0.0,0.35714285714285715,0.07142857142857142,0.42857142857142855,0.14285714285714285,[('l' 2)]\n",
      "120_22,RNN,genderless_test,All,0.425,0.25,0.0125,0.1625,0.1125,0.6125,0.1,[('l' 8)]\n",
      "120_22,RNN,genderless_test,M,0.3142857142857143,0.2,0.02857142857142857,0.14285714285714285,0.17142857142857143,0.6,0.05714285714285714,[('l' 2)]\n",
      "120_22,RNN,genderless_test,F,0.5483870967741935,0.2903225806451613,0.0,0.22580645161290322,0.0967741935483871,0.5806451612903226,0.0967741935483871,[('l' 3)]\n",
      "120_22,RNN,genderless_test,N,0.42857142857142855,0.2857142857142857,0.0,0.07142857142857142,0.0,0.7142857142857143,0.21428571428571427,[('l' 3)]\n",
      "120_23,RNN,regular_test,All,0.65,0.5125,0.0,0.2625,0.1375,0.55,0.05,[('l' 3) ('h' 1)]\n",
      "120_23,RNN,regular_test,M,0.5833333333333334,0.5555555555555556,0.0,0.3888888888888889,0.2222222222222222,0.3611111111111111,0.027777777777777776,[('l' 1)]\n",
      "120_23,RNN,regular_test,F,0.8333333333333334,0.5333333333333333,0.0,0.1,0.06666666666666667,0.8333333333333334,0.0,[]\n",
      "120_23,RNN,regular_test,N,0.42857142857142855,0.35714285714285715,0.0,0.2857142857142857,0.07142857142857142,0.42857142857142855,0.21428571428571427,[('l' 2) ('h' 1)]\n",
      "120_23,RNN,genderless_test,All,0.5,0.3,0.0125,0.1875,0.175,0.5875,0.0375,[('l' 2) ('h' 1)]\n",
      "120_23,RNN,genderless_test,M,0.4722222222222222,0.3055555555555556,0.0,0.16666666666666666,0.1388888888888889,0.6944444444444444,0.0,[]\n",
      "120_23,RNN,genderless_test,F,0.6333333333333333,0.3333333333333333,0.03333333333333333,0.23333333333333334,0.16666666666666666,0.5,0.06666666666666667,[('l' 2)]\n",
      "120_23,RNN,genderless_test,N,0.2857142857142857,0.21428571428571427,0.0,0.14285714285714285,0.2857142857142857,0.5,0.07142857142857142,[('h' 1)]\n",
      "120_24,RNN,regular_test,All,0.5625,0.4625,0.0375,0.2,0.05,0.675,0.0375,[('l' 2) ('m' 1)]\n",
      "120_24,RNN,regular_test,M,0.34146341463414637,0.3902439024390244,0.04878048780487805,0.3902439024390244,0.07317073170731707,0.4146341463414634,0.07317073170731707,[('l' 2) ('m' 1)]\n",
      "120_24,RNN,regular_test,F,1.0,0.6129032258064516,0.0,0.0,0.0,1.0,0.0,[]\n",
      "120_24,RNN,regular_test,N,0.0,0.25,0.125,0.0,0.125,0.75,0.0,[]\n",
      "120_24,RNN,genderless_test,All,0.5125,0.4,0.0125,0.075,0.1375,0.7,0.075,[('l' 6)]\n",
      "120_24,RNN,genderless_test,M,0.5365853658536586,0.3170731707317073,0.0,0.024390243902439025,0.12195121951219512,0.8048780487804879,0.04878048780487805,[('l' 2)]\n",
      "120_24,RNN,genderless_test,F,0.4838709677419355,0.5161290322580645,0.03225806451612903,0.12903225806451613,0.0967741935483871,0.6129032258064516,0.12903225806451613,[('l' 4)]\n",
      "120_24,RNN,genderless_test,N,0.5,0.375,0.0,0.125,0.375,0.5,0.0,[]\n",
      "180_0,RNN,regular_test,All,0.6125,0.6,0.0125,0.1875,0.2125,0.5875,0.0,[]\n",
      "180_0,RNN,regular_test,M,0.42857142857142855,0.42857142857142855,0.03571428571428571,0.32142857142857145,0.32142857142857145,0.32142857142857145,0.0,[]\n",
      "180_0,RNN,regular_test,F,0.8947368421052632,0.8421052631578947,0.0,0.05263157894736842,0.05263157894736842,0.8947368421052632,0.0,[]\n",
      "180_0,RNN,regular_test,N,0.21428571428571427,0.2857142857142857,0.0,0.2857142857142857,0.42857142857142855,0.2857142857142857,0.0,[]\n",
      "180_0,RNN,genderless_test,All,0.45,0.3375,0.0125,0.175,0.2625,0.55,0.0,[]\n",
      "180_0,RNN,genderless_test,M,0.39285714285714285,0.25,0.0,0.10714285714285714,0.21428571428571427,0.6785714285714286,0.0,[]\n",
      "180_0,RNN,genderless_test,F,0.47368421052631576,0.4473684210526316,0.02631578947368421,0.21052631578947367,0.3157894736842105,0.4473684210526316,0.0,[]\n",
      "180_0,RNN,genderless_test,N,0.5,0.21428571428571427,0.0,0.21428571428571427,0.21428571428571427,0.5714285714285714,0.0,[]\n",
      "180_1,RNN,regular_test,All,0.575,0.5375,0.0125,0.2,0.175,0.5625,0.05,[('l' 3) ('b' 1)]\n",
      "180_1,RNN,regular_test,M,0.4857142857142857,0.5142857142857142,0.02857142857142857,0.2857142857142857,0.2,0.42857142857142855,0.05714285714285714,[('b' 1) ('l' 1)]\n",
      "180_1,RNN,regular_test,F,0.7741935483870968,0.6774193548387096,0.0,0.0967741935483871,0.12903225806451613,0.7741935483870968,0.0,[]\n",
      "180_1,RNN,regular_test,N,0.35714285714285715,0.2857142857142857,0.0,0.21428571428571427,0.21428571428571427,0.42857142857142855,0.14285714285714285,[('l' 2)]\n",
      "180_1,RNN,genderless_test,All,0.4125,0.3125,0.0,0.225,0.325,0.375,0.075,[('l' 5) ('ß' 1)]\n",
      "180_1,RNN,genderless_test,M,0.4,0.2857142857142857,0.0,0.2571428571428571,0.2857142857142857,0.37142857142857144,0.08571428571428572,[('l' 3)]\n",
      "180_1,RNN,genderless_test,F,0.3870967741935484,0.3870967741935484,0.0,0.22580645161290322,0.3225806451612903,0.41935483870967744,0.03225806451612903,[('ß' 1)]\n",
      "180_1,RNN,genderless_test,N,0.5,0.21428571428571427,0.0,0.14285714285714285,0.42857142857142855,0.2857142857142857,0.14285714285714285,[('l' 2)]\n",
      "180_2,RNN,regular_test,All,0.625,0.525,0.05,0.2875,0.1375,0.4125,0.1125,[('l' 8) ('o' 1)]\n",
      "180_2,RNN,regular_test,M,0.6111111111111112,0.5555555555555556,0.1111111111111111,0.4444444444444444,0.19444444444444445,0.16666666666666666,0.08333333333333333,[('l' 2) ('o' 1)]\n",
      "180_2,RNN,regular_test,F,0.7741935483870968,0.5483870967741935,0.0,0.12903225806451613,0.06451612903225806,0.7741935483870968,0.03225806451612903,[('l' 1)]\n",
      "180_2,RNN,regular_test,N,0.3076923076923077,0.38461538461538464,0.0,0.23076923076923078,0.15384615384615385,0.23076923076923078,0.38461538461538464,[('l' 5)]\n",
      "180_2,RNN,genderless_test,All,0.3625,0.2375,0.0125,0.175,0.425,0.3125,0.075,[('l' 5) ('g' 1)]\n",
      "180_2,RNN,genderless_test,M,0.3611111111111111,0.19444444444444445,0.0,0.1388888888888889,0.3611111111111111,0.3888888888888889,0.1111111111111111,[('l' 3) ('g' 1)]\n",
      "180_2,RNN,genderless_test,F,0.3225806451612903,0.22580645161290322,0.0,0.25806451612903225,0.4838709677419355,0.1935483870967742,0.06451612903225806,[('l' 2)]\n",
      "180_2,RNN,genderless_test,N,0.46153846153846156,0.38461538461538464,0.07692307692307693,0.07692307692307693,0.46153846153846156,0.38461538461538464,0.0,[]\n",
      "180_3,RNN,regular_test,All,0.6375,0.5375,0.0375,0.3,0.125,0.475,0.0625,[('l' 5)]\n",
      "180_3,RNN,regular_test,M,0.5769230769230769,0.5384615384615384,0.038461538461538464,0.46153846153846156,0.23076923076923078,0.19230769230769232,0.07692307692307693,[('l' 2)]\n",
      "180_3,RNN,regular_test,F,0.7647058823529411,0.5,0.029411764705882353,0.08823529411764706,0.058823529411764705,0.7941176470588235,0.029411764705882353,[('l' 1)]\n",
      "180_3,RNN,regular_test,N,0.5,0.6,0.05,0.45,0.1,0.3,0.1,[('l' 2)]\n",
      "180_3,RNN,genderless_test,All,0.325,0.2125,0.05,0.1125,0.4125,0.3625,0.0625,[('l' 4) ('k' 1)]\n",
      "180_3,RNN,genderless_test,M,0.34615384615384615,0.2692307692307692,0.038461538461538464,0.15384615384615385,0.38461538461538464,0.38461538461538464,0.038461538461538464,[('l' 1)]\n",
      "180_3,RNN,genderless_test,F,0.2647058823529412,0.14705882352941177,0.058823529411764705,0.058823529411764705,0.5,0.2647058823529412,0.11764705882352941,[('l' 3) ('k' 1)]\n",
      "180_3,RNN,genderless_test,N,0.4,0.25,0.05,0.15,0.3,0.5,0.0,[]\n",
      "180_4,RNN,regular_test,All,0.4875,0.5,0.075,0.3875,0.1625,0.35,0.025,[('l' 2)]\n",
      "180_4,RNN,regular_test,M,0.5757575757575758,0.48484848484848486,0.09090909090909091,0.5151515151515151,0.18181818181818182,0.21212121212121213,0.0,[]\n",
      "180_4,RNN,regular_test,F,0.5,0.53125,0.09375,0.28125,0.15625,0.46875,0.0,[]\n",
      "180_4,RNN,regular_test,N,0.26666666666666666,0.4666666666666667,0.0,0.3333333333333333,0.13333333333333333,0.4,0.13333333333333333,[('l' 2)]\n",
      "180_4,RNN,genderless_test,All,0.3,0.2875,0.0625,0.2375,0.2625,0.4375,0.0,[]\n",
      "180_4,RNN,genderless_test,M,0.3333333333333333,0.21212121212121213,0.0,0.18181818181818182,0.36363636363636365,0.45454545454545453,0.0,[]\n",
      "180_4,RNN,genderless_test,F,0.28125,0.3125,0.0625,0.3125,0.15625,0.46875,0.0,[]\n",
      "180_4,RNN,genderless_test,N,0.26666666666666666,0.4,0.2,0.2,0.26666666666666666,0.3333333333333333,0.0,[]\n",
      "180_5,RNN,regular_test,All,0.5625,0.575,0.0125,0.1875,0.175,0.55,0.075,[('l' 5) ('m' 1)]\n",
      "180_5,RNN,regular_test,M,0.4594594594594595,0.40540540540540543,0.0,0.2972972972972973,0.2972972972972973,0.35135135135135137,0.05405405405405406,[('m' 1) ('l' 1)]\n",
      "180_5,RNN,regular_test,F,0.8571428571428571,0.7857142857142857,0.03571428571428571,0.03571428571428571,0.0,0.9285714285714286,0.0,[]\n",
      "180_5,RNN,regular_test,N,0.26666666666666666,0.6,0.0,0.2,0.2,0.3333333333333333,0.26666666666666666,[('l' 4)]\n",
      "180_5,RNN,genderless_test,All,0.3,0.35,0.0125,0.0375,0.45,0.425,0.075,[('l' 4) ('k' 1) ('m' 1)]\n",
      "180_5,RNN,genderless_test,M,0.35135135135135137,0.43243243243243246,0.0,0.05405405405405406,0.4864864864864865,0.40540540540540543,0.05405405405405406,[('l' 2)]\n",
      "180_5,RNN,genderless_test,F,0.32142857142857145,0.39285714285714285,0.0,0.0,0.42857142857142855,0.5,0.07142857142857142,[('k' 1) ('m' 1)]\n",
      "180_5,RNN,genderless_test,N,0.13333333333333333,0.06666666666666667,0.06666666666666667,0.06666666666666667,0.4,0.3333333333333333,0.13333333333333333,[('l' 2)]\n",
      "180_6,RNN,regular_test,All,0.6625,0.525,0.075,0.1875,0.1625,0.5625,0.0125,[('l' 1)]\n",
      "180_6,RNN,regular_test,M,0.5555555555555556,0.6111111111111112,0.08333333333333333,0.25,0.3333333333333333,0.3055555555555556,0.027777777777777776,[('l' 1)]\n",
      "180_6,RNN,regular_test,F,0.8571428571428571,0.42857142857142855,0.05714285714285714,0.08571428571428572,0.0,0.8571428571428571,0.0,[]\n",
      "180_6,RNN,regular_test,N,0.3333333333333333,0.5555555555555556,0.1111111111111111,0.3333333333333333,0.1111111111111111,0.4444444444444444,0.0,[]\n",
      "180_6,RNN,genderless_test,All,0.4125,0.2875,0.05,0.1875,0.3625,0.3125,0.0875,[('l' 6) ('h' 1)]\n",
      "180_6,RNN,genderless_test,M,0.4166666666666667,0.3611111111111111,0.027777777777777776,0.19444444444444445,0.3888888888888889,0.3611111111111111,0.027777777777777776,[('l' 1)]\n",
      "180_6,RNN,genderless_test,F,0.45714285714285713,0.2571428571428571,0.02857142857142857,0.14285714285714285,0.3142857142857143,0.34285714285714286,0.17142857142857143,[('l' 5) ('h' 1)]\n",
      "180_6,RNN,genderless_test,N,0.2222222222222222,0.1111111111111111,0.2222222222222222,0.3333333333333333,0.4444444444444444,0.0,0.0,[]\n",
      "180_7,RNN,regular_test,All,0.4875,0.4375,0.0375,0.0875,0.4875,0.3625,0.025,[('l' 2)]\n",
      "180_7,RNN,regular_test,M,0.42424242424242425,0.5757575757575758,0.0,0.15151515151515152,0.696969696969697,0.15151515151515152,0.0,[]\n",
      "180_7,RNN,regular_test,F,0.7,0.3333333333333333,0.1,0.0,0.16666666666666666,0.7,0.03333333333333333,[('l' 1)]\n",
      "180_7,RNN,regular_test,N,0.23529411764705882,0.35294117647058826,0.0,0.11764705882352941,0.6470588235294118,0.17647058823529413,0.058823529411764705,[('l' 1)]\n",
      "180_7,RNN,genderless_test,All,0.25,0.3,0.0375,0.0875,0.7,0.1375,0.0375,[('l' 2) ('t' 1)]\n",
      "180_7,RNN,genderless_test,M,0.21212121212121213,0.42424242424242425,0.030303030303030304,0.030303030303030304,0.6666666666666666,0.21212121212121213,0.06060606060606061,[('t' 1) ('l' 1)]\n",
      "180_7,RNN,genderless_test,F,0.36666666666666664,0.16666666666666666,0.03333333333333333,0.13333333333333333,0.7,0.1,0.03333333333333333,[('l' 1)]\n",
      "180_7,RNN,genderless_test,N,0.11764705882352941,0.29411764705882354,0.058823529411764705,0.11764705882352941,0.7647058823529411,0.058823529411764705,0.0,[]\n",
      "180_8,RNN,regular_test,All,0.6,0.5125,0.0125,0.2625,0.15,0.5,0.075,[('l' 6)]\n",
      "180_8,RNN,regular_test,M,0.46153846153846156,0.5128205128205128,0.02564102564102564,0.4358974358974359,0.2564102564102564,0.20512820512820512,0.07692307692307693,[('l' 3)]\n",
      "180_8,RNN,regular_test,F,0.8666666666666667,0.5,0.0,0.06666666666666667,0.0,0.9,0.03333333333333333,[('l' 1)]\n",
      "180_8,RNN,regular_test,N,0.36363636363636365,0.5454545454545454,0.0,0.18181818181818182,0.18181818181818182,0.45454545454545453,0.18181818181818182,[('l' 2)]\n",
      "180_8,RNN,genderless_test,All,0.45,0.35,0.0,0.1625,0.15,0.65,0.0375,[('l' 3)]\n",
      "180_8,RNN,genderless_test,M,0.358974358974359,0.1794871794871795,0.0,0.20512820512820512,0.15384615384615385,0.6153846153846154,0.02564102564102564,[('l' 1)]\n",
      "180_8,RNN,genderless_test,F,0.5,0.6,0.0,0.1,0.13333333333333333,0.7666666666666667,0.0,[]\n",
      "180_8,RNN,genderless_test,N,0.6363636363636364,0.2727272727272727,0.0,0.18181818181818182,0.18181818181818182,0.45454545454545453,0.18181818181818182,[('l' 2)]\n",
      "180_9,RNN,regular_test,All,0.625,0.65,0.025,0.25,0.075,0.575,0.075,[('l' 3) ('-' 1) ('y' 1) ('u' 1)]\n",
      "180_9,RNN,regular_test,M,0.5365853658536586,0.5853658536585366,0.024390243902439025,0.36585365853658536,0.14634146341463414,0.3902439024390244,0.07317073170731707,[('-' 1) ('l' 1) ('y' 1)]\n",
      "180_9,RNN,regular_test,F,0.92,0.76,0.0,0.04,0.0,0.96,0.0,[]\n",
      "180_9,RNN,regular_test,N,0.35714285714285715,0.6428571428571429,0.07142857142857142,0.2857142857142857,0.0,0.42857142857142855,0.21428571428571427,[('l' 2) ('u' 1)]\n",
      "180_9,RNN,genderless_test,All,0.4625,0.4375,0.0,0.1375,0.175,0.6625,0.025,[('l' 1) ('u' 1)]\n",
      "180_9,RNN,genderless_test,M,0.4634146341463415,0.3902439024390244,0.0,0.12195121951219512,0.14634146341463414,0.7317073170731707,0.0,[]\n",
      "180_9,RNN,genderless_test,F,0.48,0.52,0.0,0.2,0.16,0.6,0.04,[('l' 1)]\n",
      "180_9,RNN,genderless_test,N,0.42857142857142855,0.42857142857142855,0.0,0.07142857142857142,0.2857142857142857,0.5714285714285714,0.07142857142857142,[('u' 1)]\n",
      "180_10,RNN,regular_test,All,0.65,0.575,0.0,0.1625,0.25,0.5125,0.075,[('l' 6)]\n",
      "180_10,RNN,regular_test,M,0.5,0.5526315789473685,0.0,0.2894736842105263,0.34210526315789475,0.2631578947368421,0.10526315789473684,[('l' 4)]\n",
      "180_10,RNN,regular_test,F,0.9,0.6,0.0,0.0,0.1,0.9,0.0,[]\n",
      "180_10,RNN,regular_test,N,0.5,0.5833333333333334,0.0,0.16666666666666666,0.3333333333333333,0.3333333333333333,0.16666666666666666,[('l' 2)]\n",
      "180_10,RNN,genderless_test,All,0.425,0.3125,0.025,0.0125,0.3125,0.65,0.0,[]\n",
      "180_10,RNN,genderless_test,M,0.47368421052631576,0.18421052631578946,0.02631578947368421,0.0,0.3157894736842105,0.6578947368421053,0.0,[]\n",
      "180_10,RNN,genderless_test,F,0.4,0.4666666666666667,0.03333333333333333,0.03333333333333333,0.16666666666666666,0.7666666666666667,0.0,[]\n",
      "180_10,RNN,genderless_test,N,0.3333333333333333,0.3333333333333333,0.0,0.0,0.6666666666666666,0.3333333333333333,0.0,[]\n",
      "180_11,RNN,regular_test,All,0.5125,0.4625,0.0,0.225,0.275,0.4625,0.0375,[('l' 2) ('-' 1)]\n",
      "180_11,RNN,regular_test,M,0.275,0.375,0.0,0.325,0.425,0.225,0.025,[('-' 1)]\n",
      "180_11,RNN,regular_test,F,0.8275862068965517,0.4827586206896552,0.0,0.06896551724137931,0.06896551724137931,0.8620689655172413,0.0,[]\n",
      "180_11,RNN,regular_test,N,0.5454545454545454,0.7272727272727273,0.0,0.2727272727272727,0.2727272727272727,0.2727272727272727,0.18181818181818182,[('l' 2)]\n",
      "180_11,RNN,genderless_test,All,0.35,0.2625,0.0125,0.2,0.2875,0.4625,0.0375,[('l' 2) ('c' 1)]\n",
      "180_11,RNN,genderless_test,M,0.45,0.275,0.025,0.15,0.35,0.45,0.025,[('c' 1)]\n",
      "180_11,RNN,genderless_test,F,0.27586206896551724,0.3103448275862069,0.0,0.27586206896551724,0.20689655172413793,0.4827586206896552,0.034482758620689655,[('l' 1)]\n",
      "180_11,RNN,genderless_test,N,0.18181818181818182,0.09090909090909091,0.0,0.18181818181818182,0.2727272727272727,0.45454545454545453,0.09090909090909091,[('l' 1)]\n",
      "180_12,RNN,regular_test,All,0.625,0.6,0.0,0.175,0.1875,0.625,0.0125,[('l' 1)]\n",
      "180_12,RNN,regular_test,M,0.5263157894736842,0.631578947368421,0.0,0.2631578947368421,0.3157894736842105,0.42105263157894735,0.0,[]\n",
      "180_12,RNN,regular_test,F,0.9285714285714286,0.42857142857142855,0.0,0.0,0.07142857142857142,0.9285714285714286,0.0,[]\n",
      "180_12,RNN,regular_test,N,0.2857142857142857,0.8571428571428571,0.0,0.2857142857142857,0.07142857142857142,0.5714285714285714,0.07142857142857142,[('l' 1)]\n",
      "180_12,RNN,genderless_test,All,0.3625,0.225,0.0,0.125,0.425,0.4375,0.0125,[('l' 1)]\n",
      "180_12,RNN,genderless_test,M,0.42105263157894735,0.23684210526315788,0.0,0.10526315789473684,0.34210526315789475,0.5526315789473685,0.0,[]\n",
      "180_12,RNN,genderless_test,F,0.32142857142857145,0.14285714285714285,0.0,0.17857142857142858,0.5357142857142857,0.2857142857142857,0.0,[]\n",
      "180_12,RNN,genderless_test,N,0.2857142857142857,0.35714285714285715,0.0,0.07142857142857142,0.42857142857142855,0.42857142857142855,0.07142857142857142,[('l' 1)]\n",
      "180_13,RNN,regular_test,All,0.525,0.4,0.025,0.25,0.1875,0.475,0.0625,[('l' 4) ('i' 1)]\n",
      "180_13,RNN,regular_test,M,0.42424242424242425,0.42424242424242425,0.0,0.45454545454545453,0.18181818181818182,0.3333333333333333,0.030303030303030304,[('l' 1)]\n",
      "180_13,RNN,regular_test,F,0.7647058823529411,0.4411764705882353,0.029411764705882353,0.08823529411764706,0.11764705882352941,0.7647058823529411,0.0,[]\n",
      "180_13,RNN,regular_test,N,0.15384615384615385,0.23076923076923078,0.07692307692307693,0.15384615384615385,0.38461538461538464,0.07692307692307693,0.3076923076923077,[('l' 3) ('i' 1)]\n",
      "180_13,RNN,genderless_test,All,0.325,0.1625,0.075,0.05,0.375,0.425,0.075,[('l' 5) ('t' 1)]\n",
      "180_13,RNN,genderless_test,M,0.3333333333333333,0.09090909090909091,0.06060606060606061,0.06060606060606061,0.2727272727272727,0.5151515151515151,0.09090909090909091,[('l' 3)]\n",
      "180_13,RNN,genderless_test,F,0.38235294117647056,0.2647058823529412,0.08823529411764706,0.058823529411764705,0.38235294117647056,0.4411764705882353,0.029411764705882353,[('l' 1)]\n",
      "180_13,RNN,genderless_test,N,0.15384615384615385,0.07692307692307693,0.07692307692307693,0.0,0.6153846153846154,0.15384615384615385,0.15384615384615385,[('l' 1) ('t' 1)]\n",
      "180_14,RNN,regular_test,All,0.625,0.45,0.0,0.175,0.275,0.5,0.05,[('l' 4)]\n",
      "180_14,RNN,regular_test,M,0.5428571428571428,0.4857142857142857,0.0,0.3142857142857143,0.4,0.2,0.08571428571428572,[('l' 3)]\n",
      "180_14,RNN,regular_test,F,0.9629629629629629,0.4074074074074074,0.0,0.0,0.037037037037037035,0.9629629629629629,0.0,[]\n",
      "180_14,RNN,regular_test,N,0.2777777777777778,0.4444444444444444,0.0,0.16666666666666666,0.3888888888888889,0.3888888888888889,0.05555555555555555,[('l' 1)]\n",
      "180_14,RNN,genderless_test,All,0.325,0.3125,0.0,0.1625,0.425,0.35,0.0625,[('l' 2) ('a' 2) ('t' 1)]\n",
      "180_14,RNN,genderless_test,M,0.22857142857142856,0.37142857142857144,0.0,0.2,0.4,0.3142857142857143,0.08571428571428572,[('l' 2) ('t' 1)]\n",
      "180_14,RNN,genderless_test,F,0.48148148148148145,0.25925925925925924,0.0,0.18518518518518517,0.4074074074074074,0.37037037037037035,0.037037037037037035,[('a' 1)]\n",
      "180_14,RNN,genderless_test,N,0.2777777777777778,0.2777777777777778,0.0,0.05555555555555555,0.5,0.3888888888888889,0.05555555555555555,[('a' 1)]\n",
      "180_15,RNN,regular_test,All,0.675,0.5375,0.0,0.15,0.1875,0.625,0.0375,[('l' 3)]\n",
      "180_15,RNN,regular_test,M,0.5151515151515151,0.5454545454545454,0.0,0.21212121212121213,0.30303030303030304,0.42424242424242425,0.06060606060606061,[('l' 2)]\n",
      "180_15,RNN,regular_test,F,0.8823529411764706,0.5882352941176471,0.0,0.058823529411764705,0.058823529411764705,0.8823529411764706,0.0,[]\n",
      "180_15,RNN,regular_test,N,0.5384615384615384,0.38461538461538464,0.0,0.23076923076923078,0.23076923076923078,0.46153846153846156,0.07692307692307693,[('l' 1)]\n",
      "180_15,RNN,genderless_test,All,0.55,0.3375,0.0,0.025,0.3,0.6625,0.0125,[('l' 1)]\n",
      "180_15,RNN,genderless_test,M,0.5151515151515151,0.24242424242424243,0.0,0.0,0.2727272727272727,0.7272727272727273,0.0,[]\n",
      "180_15,RNN,genderless_test,F,0.6176470588235294,0.4117647058823529,0.0,0.058823529411764705,0.23529411764705882,0.6764705882352942,0.029411764705882353,[('l' 1)]\n",
      "180_15,RNN,genderless_test,N,0.46153846153846156,0.38461538461538464,0.0,0.0,0.5384615384615384,0.46153846153846156,0.0,[]\n",
      "180_16,RNN,regular_test,All,0.625,0.5875,0.0,0.275,0.1125,0.5875,0.025,[('l' 2)]\n",
      "180_16,RNN,regular_test,M,0.47368421052631576,0.42105263157894735,0.0,0.42105263157894735,0.21052631578947367,0.34210526315789475,0.02631578947368421,[('l' 1)]\n",
      "180_16,RNN,regular_test,F,0.9354838709677419,0.8387096774193549,0.0,0.03225806451612903,0.0,0.967741935483871,0.0,[]\n",
      "180_16,RNN,regular_test,N,0.2727272727272727,0.45454545454545453,0.0,0.45454545454545453,0.09090909090909091,0.36363636363636365,0.09090909090909091,[('l' 1)]\n",
      "180_16,RNN,genderless_test,All,0.5625,0.4125,0.0,0.275,0.1,0.575,0.05,[('l' 3) ('a' 1)]\n",
      "180_16,RNN,genderless_test,M,0.5,0.2894736842105263,0.0,0.39473684210526316,0.05263157894736842,0.5263157894736842,0.02631578947368421,[('l' 1)]\n",
      "180_16,RNN,genderless_test,F,0.6129032258064516,0.4838709677419355,0.0,0.1935483870967742,0.12903225806451613,0.6129032258064516,0.06451612903225806,[('a' 1) ('l' 1)]\n",
      "180_16,RNN,genderless_test,N,0.6363636363636364,0.6363636363636364,0.0,0.09090909090909091,0.18181818181818182,0.6363636363636364,0.09090909090909091,[('l' 1)]\n",
      "180_17,RNN,regular_test,All,0.5,0.5625,0.0,0.275,0.325,0.3375,0.0625,[('l' 5)]\n",
      "180_17,RNN,regular_test,M,0.42424242424242425,0.5757575757575758,0.0,0.21212121212121213,0.5454545454545454,0.18181818181818182,0.06060606060606061,[('l' 2)]\n",
      "180_17,RNN,regular_test,F,0.6,0.6,0.0,0.36666666666666664,0.03333333333333333,0.6,0.0,[]\n",
      "180_17,RNN,regular_test,N,0.47058823529411764,0.47058823529411764,0.0,0.23529411764705882,0.4117647058823529,0.17647058823529413,0.17647058823529413,[('l' 3)]\n",
      "180_17,RNN,genderless_test,All,0.3,0.2625,0.0125,0.175,0.5375,0.25,0.025,[('l' 2)]\n",
      "180_17,RNN,genderless_test,M,0.30303030303030304,0.3939393939393939,0.0,0.15151515151515152,0.48484848484848486,0.3333333333333333,0.030303030303030304,[('l' 1)]\n",
      "180_17,RNN,genderless_test,F,0.26666666666666666,0.13333333333333333,0.03333333333333333,0.26666666666666666,0.6,0.1,0.0,[]\n",
      "180_17,RNN,genderless_test,N,0.35294117647058826,0.23529411764705882,0.0,0.058823529411764705,0.5294117647058824,0.35294117647058826,0.058823529411764705,[('l' 1)]\n",
      "180_18,RNN,regular_test,All,0.6625,0.6125,0.0125,0.375,0.175,0.3875,0.05,[('l' 3) ('g' 1)]\n",
      "180_18,RNN,regular_test,M,0.5833333333333334,0.5833333333333334,0.0,0.5833333333333334,0.25,0.1111111111111111,0.05555555555555555,[('l' 1) ('g' 1)]\n",
      "180_18,RNN,regular_test,F,0.84375,0.84375,0.0,0.09375,0.03125,0.84375,0.03125,[('l' 1)]\n",
      "180_18,RNN,regular_test,N,0.4166666666666667,0.08333333333333333,0.08333333333333333,0.5,0.3333333333333333,0.0,0.08333333333333333,[('l' 1)]\n",
      "180_18,RNN,genderless_test,All,0.4375,0.2625,0.025,0.2,0.3375,0.275,0.1625,[('l' 9) ('g' 2) ('h' 1) ('t' 1)]\n",
      "180_18,RNN,genderless_test,M,0.2777777777777778,0.2222222222222222,0.027777777777777776,0.19444444444444445,0.3888888888888889,0.25,0.1388888888888889,[('l' 3) ('g' 2)]\n",
      "180_18,RNN,genderless_test,F,0.5625,0.3125,0.03125,0.1875,0.3125,0.3125,0.15625,[('l' 5)]\n",
      "180_18,RNN,genderless_test,N,0.5833333333333334,0.25,0.0,0.25,0.25,0.25,0.25,[('h' 1) ('t' 1) ('l' 1)]\n",
      "180_19,RNN,regular_test,All,0.6625,0.6,0.05,0.2625,0.2875,0.3875,0.0125,[('l' 1)]\n",
      "180_19,RNN,regular_test,M,0.4878048780487805,0.5121951219512195,0.07317073170731707,0.36585365853658536,0.43902439024390244,0.0975609756097561,0.024390243902439025,[('l' 1)]\n",
      "180_19,RNN,regular_test,F,0.875,0.625,0.041666666666666664,0.041666666666666664,0.041666666666666664,0.875,0.0,[]\n",
      "180_19,RNN,regular_test,N,0.8,0.8,0.0,0.3333333333333333,0.26666666666666666,0.4,0.0,[]\n",
      "180_19,RNN,genderless_test,All,0.2875,0.25,0.05,0.1,0.625,0.2125,0.0125,[('m' 1)]\n",
      "180_19,RNN,genderless_test,M,0.2926829268292683,0.2682926829268293,0.024390243902439025,0.0975609756097561,0.6097560975609756,0.24390243902439024,0.024390243902439025,[('m' 1)]\n",
      "180_19,RNN,genderless_test,F,0.25,0.08333333333333333,0.08333333333333333,0.125,0.625,0.16666666666666666,0.0,[]\n",
      "180_19,RNN,genderless_test,N,0.3333333333333333,0.4666666666666667,0.06666666666666667,0.06666666666666667,0.6666666666666666,0.2,0.0,[]\n",
      "180_20,RNN,regular_test,All,0.4625,0.4625,0.0,0.25,0.4625,0.2375,0.05,[('l' 4)]\n",
      "180_20,RNN,regular_test,M,0.46153846153846156,0.46153846153846156,0.0,0.38461538461538464,0.4358974358974359,0.07692307692307693,0.10256410256410256,[('l' 4)]\n",
      "180_20,RNN,regular_test,F,0.4666666666666667,0.43333333333333335,0.0,0.1,0.43333333333333335,0.4666666666666667,0.0,[]\n",
      "180_20,RNN,regular_test,N,0.45454545454545453,0.5454545454545454,0.0,0.18181818181818182,0.6363636363636364,0.18181818181818182,0.0,[]\n",
      "180_20,RNN,genderless_test,All,0.225,0.2375,0.0125,0.125,0.625,0.175,0.0625,[('l' 4) ('u' 1)]\n",
      "180_20,RNN,genderless_test,M,0.23076923076923078,0.15384615384615385,0.0,0.10256410256410256,0.6923076923076923,0.1282051282051282,0.07692307692307693,[('l' 3)]\n",
      "180_20,RNN,genderless_test,F,0.2,0.26666666666666666,0.03333333333333333,0.16666666666666666,0.5333333333333333,0.2,0.06666666666666667,[('l' 1) ('u' 1)]\n",
      "180_20,RNN,genderless_test,N,0.2727272727272727,0.45454545454545453,0.0,0.09090909090909091,0.6363636363636364,0.2727272727272727,0.0,[]\n",
      "180_21,RNN,regular_test,All,0.45,0.6,0.0,0.0875,0.175,0.7375,0.0,[]\n",
      "180_21,RNN,regular_test,M,0.29411764705882354,0.5,0.0,0.20588235294117646,0.20588235294117646,0.5882352941176471,0.0,[]\n",
      "180_21,RNN,regular_test,F,0.88,0.8,0.0,0.0,0.08,0.92,0.0,[]\n",
      "180_21,RNN,regular_test,N,0.19047619047619047,0.5238095238095238,0.0,0.0,0.23809523809523808,0.7619047619047619,0.0,[]\n",
      "180_21,RNN,genderless_test,All,0.3625,0.5,0.0,0.025,0.1375,0.7625,0.075,[('c' 2) ('h' 1) ('m' 1) ('l' 1) ('u' 1)]\n",
      "180_21,RNN,genderless_test,M,0.47058823529411764,0.47058823529411764,0.0,0.0,0.14705882352941177,0.8235294117647058,0.029411764705882353,[('c' 1)]\n",
      "180_21,RNN,genderless_test,F,0.28,0.52,0.0,0.04,0.12,0.68,0.16,[('h' 1) ('m' 1) ('l' 1) ('c' 1)]\n",
      "180_21,RNN,genderless_test,N,0.2857142857142857,0.5238095238095238,0.0,0.047619047619047616,0.14285714285714285,0.7619047619047619,0.047619047619047616,[('u' 1)]\n",
      "180_22,RNN,regular_test,All,0.5,0.5,0.0,0.175,0.2875,0.5125,0.025,[('l' 2)]\n",
      "180_22,RNN,regular_test,M,0.4,0.5142857142857142,0.0,0.34285714285714286,0.3142857142857143,0.3142857142857143,0.02857142857142857,[('l' 1)]\n",
      "180_22,RNN,regular_test,F,0.7419354838709677,0.5483870967741935,0.0,0.03225806451612903,0.1935483870967742,0.7741935483870968,0.0,[]\n",
      "180_22,RNN,regular_test,N,0.21428571428571427,0.35714285714285715,0.0,0.07142857142857142,0.42857142857142855,0.42857142857142855,0.07142857142857142,[('l' 1)]\n",
      "180_22,RNN,genderless_test,All,0.3625,0.275,0.0,0.1125,0.325,0.525,0.0375,[('l' 2) ('t' 1)]\n",
      "180_22,RNN,genderless_test,M,0.37142857142857144,0.2571428571428571,0.0,0.08571428571428572,0.34285714285714286,0.5142857142857142,0.05714285714285714,[('t' 1) ('l' 1)]\n",
      "180_22,RNN,genderless_test,F,0.3870967741935484,0.2903225806451613,0.0,0.16129032258064516,0.2903225806451613,0.5483870967741935,0.0,[]\n",
      "180_22,RNN,genderless_test,N,0.2857142857142857,0.2857142857142857,0.0,0.07142857142857142,0.35714285714285715,0.5,0.07142857142857142,[('l' 1)]\n",
      "180_23,RNN,regular_test,All,0.7,0.6625,0.0125,0.375,0.175,0.3875,0.05,[('l' 4)]\n",
      "180_23,RNN,regular_test,M,0.6388888888888888,0.6111111111111112,0.0,0.5833333333333334,0.25,0.1388888888888889,0.027777777777777776,[('l' 1)]\n",
      "180_23,RNN,regular_test,F,0.8,0.8,0.0,0.13333333333333333,0.03333333333333333,0.8,0.03333333333333333,[('l' 1)]\n",
      "180_23,RNN,regular_test,N,0.6428571428571429,0.5,0.07142857142857142,0.35714285714285715,0.2857142857142857,0.14285714285714285,0.14285714285714285,[('l' 2)]\n",
      "180_23,RNN,genderless_test,All,0.5,0.3,0.025,0.2625,0.275,0.3875,0.05,[('l' 3) ('a' 1)]\n",
      "180_23,RNN,genderless_test,M,0.3888888888888889,0.2777777777777778,0.05555555555555555,0.2777777777777778,0.25,0.3611111111111111,0.05555555555555555,[('l' 1) ('a' 1)]\n",
      "180_23,RNN,genderless_test,F,0.6,0.36666666666666664,0.0,0.23333333333333334,0.26666666666666666,0.43333333333333335,0.06666666666666667,[('l' 2)]\n",
      "180_23,RNN,genderless_test,N,0.5714285714285714,0.21428571428571427,0.0,0.2857142857142857,0.35714285714285715,0.35714285714285715,0.0,[]\n",
      "180_24,RNN,regular_test,All,0.525,0.5,0.0,0.2125,0.325,0.45,0.0125,[('l' 1)]\n",
      "180_24,RNN,regular_test,M,0.4146341463414634,0.4634146341463415,0.0,0.3170731707317073,0.4146341463414634,0.24390243902439024,0.024390243902439025,[('l' 1)]\n",
      "180_24,RNN,regular_test,F,0.7419354838709677,0.6129032258064516,0.0,0.06451612903225806,0.1935483870967742,0.7419354838709677,0.0,[]\n",
      "180_24,RNN,regular_test,N,0.25,0.25,0.0,0.25,0.375,0.375,0.0,[]\n",
      "180_24,RNN,genderless_test,All,0.2875,0.2625,0.0125,0.0625,0.5625,0.275,0.0875,[('l' 6) ('i' 1)]\n",
      "180_24,RNN,genderless_test,M,0.34146341463414637,0.21951219512195122,0.0,0.12195121951219512,0.5853658536585366,0.1951219512195122,0.0975609756097561,[('l' 3) ('i' 1)]\n",
      "180_24,RNN,genderless_test,F,0.2903225806451613,0.3225806451612903,0.03225806451612903,0.0,0.4838709677419355,0.3870967741935484,0.0967741935483871,[('l' 3)]\n",
      "180_24,RNN,genderless_test,N,0.0,0.25,0.0,0.0,0.75,0.25,0.0,[]\n",
      "240_0,RNN,regular_test,All,0.6625,0.575,0.0,0.175,0.2,0.575,0.05,[('l' 4)]\n",
      "240_0,RNN,regular_test,M,0.6071428571428571,0.6428571428571429,0.0,0.42857142857142855,0.32142857142857145,0.17857142857142858,0.07142857142857142,[('l' 2)]\n",
      "240_0,RNN,regular_test,F,0.9210526315789473,0.5526315789473685,0.0,0.0,0.07894736842105263,0.9210526315789473,0.0,[]\n",
      "240_0,RNN,regular_test,N,0.07142857142857142,0.5,0.0,0.14285714285714285,0.2857142857142857,0.42857142857142855,0.14285714285714285,[('l' 2)]\n",
      "240_0,RNN,genderless_test,All,0.4,0.1875,0.0,0.0375,0.475,0.475,0.0125,[('l' 1)]\n",
      "240_0,RNN,genderless_test,M,0.5,0.10714285714285714,0.0,0.0,0.39285714285714285,0.6071428571428571,0.0,[]\n",
      "240_0,RNN,genderless_test,F,0.42105263157894735,0.21052631578947367,0.0,0.05263157894736842,0.5,0.42105263157894735,0.02631578947368421,[('l' 1)]\n",
      "240_0,RNN,genderless_test,N,0.14285714285714285,0.2857142857142857,0.0,0.07142857142857142,0.5714285714285714,0.35714285714285715,0.0,[]\n",
      "240_1,RNN,regular_test,All,0.6375,0.475,0.0,0.4375,0.125,0.4,0.0375,[('l' 2) ('u' 1)]\n",
      "240_1,RNN,regular_test,M,0.6,0.4,0.0,0.5714285714285714,0.2,0.2,0.02857142857142857,[('u' 1)]\n",
      "240_1,RNN,regular_test,F,0.7096774193548387,0.5806451612903226,0.0,0.25806451612903225,0.0,0.7096774193548387,0.03225806451612903,[('l' 1)]\n",
      "240_1,RNN,regular_test,N,0.5714285714285714,0.42857142857142855,0.0,0.5,0.21428571428571427,0.21428571428571427,0.07142857142857142,[('l' 1)]\n",
      "240_1,RNN,genderless_test,All,0.3875,0.2875,0.0375,0.2875,0.3,0.25,0.125,[('l' 10)]\n",
      "240_1,RNN,genderless_test,M,0.34285714285714286,0.2571428571428571,0.02857142857142857,0.2571428571428571,0.34285714285714286,0.2,0.17142857142857143,[('l' 6)]\n",
      "240_1,RNN,genderless_test,F,0.4838709677419355,0.3548387096774194,0.03225806451612903,0.25806451612903225,0.25806451612903225,0.3870967741935484,0.06451612903225806,[('l' 2)]\n",
      "240_1,RNN,genderless_test,N,0.2857142857142857,0.21428571428571427,0.07142857142857142,0.42857142857142855,0.2857142857142857,0.07142857142857142,0.14285714285714285,[('l' 2)]\n",
      "240_2,RNN,regular_test,All,0.7125,0.5625,0.0,0.3125,0.1125,0.5375,0.0375,[('l' 2) ('t' 1)]\n",
      "240_2,RNN,regular_test,M,0.6111111111111112,0.6111111111111112,0.0,0.6111111111111112,0.19444444444444445,0.19444444444444445,0.0,[]\n",
      "240_2,RNN,regular_test,F,1.0,0.5161290322580645,0.0,0.0,0.0,1.0,0.0,[]\n",
      "240_2,RNN,regular_test,N,0.3076923076923077,0.5384615384615384,0.0,0.23076923076923078,0.15384615384615385,0.38461538461538464,0.23076923076923078,[('l' 2) ('t' 1)]\n",
      "240_2,RNN,genderless_test,All,0.475,0.3,0.025,0.1625,0.0875,0.7,0.025,[('l' 2)]\n",
      "240_2,RNN,genderless_test,M,0.4722222222222222,0.19444444444444445,0.05555555555555555,0.19444444444444445,0.08333333333333333,0.6666666666666666,0.0,[]\n",
      "240_2,RNN,genderless_test,F,0.4838709677419355,0.3548387096774194,0.0,0.12903225806451613,0.12903225806451613,0.7096774193548387,0.03225806451612903,[('l' 1)]\n",
      "240_2,RNN,genderless_test,N,0.46153846153846156,0.46153846153846156,0.0,0.15384615384615385,0.0,0.7692307692307693,0.07692307692307693,[('l' 1)]\n",
      "240_3,RNN,regular_test,All,0.675,0.7375,0.025,0.2875,0.2,0.4625,0.025,[('l' 2)]\n",
      "240_3,RNN,regular_test,M,0.6923076923076923,0.6923076923076923,0.038461538461538464,0.5,0.23076923076923078,0.15384615384615385,0.07692307692307693,[('l' 2)]\n",
      "240_3,RNN,regular_test,F,0.7941176470588235,0.8529411764705882,0.029411764705882353,0.0,0.14705882352941177,0.8235294117647058,0.0,[]\n",
      "240_3,RNN,regular_test,N,0.45,0.6,0.0,0.5,0.25,0.25,0.0,[]\n",
      "240_3,RNN,genderless_test,All,0.425,0.3375,0.025,0.0875,0.25,0.575,0.0625,[('l' 3) ('u' 1) ('m' 1)]\n",
      "240_3,RNN,genderless_test,M,0.4230769230769231,0.15384615384615385,0.07692307692307693,0.07692307692307693,0.3076923076923077,0.46153846153846156,0.07692307692307693,[('u' 1) ('l' 1)]\n",
      "240_3,RNN,genderless_test,F,0.4411764705882353,0.5588235294117647,0.0,0.14705882352941177,0.23529411764705882,0.5882352941176471,0.029411764705882353,[('l' 1)]\n",
      "240_3,RNN,genderless_test,N,0.4,0.2,0.0,0.0,0.2,0.7,0.1,[('m' 1) ('l' 1)]\n",
      "240_4,RNN,regular_test,All,0.6625,0.6,0.025,0.1875,0.2,0.5375,0.05,[('l' 3) ('u' 1)]\n",
      "240_4,RNN,regular_test,M,0.6060606060606061,0.5151515151515151,0.030303030303030304,0.3939393939393939,0.24242424242424243,0.24242424242424243,0.09090909090909091,[('l' 2) ('u' 1)]\n",
      "240_4,RNN,regular_test,F,0.9375,0.78125,0.0,0.0,0.03125,0.96875,0.0,[]\n",
      "240_4,RNN,regular_test,N,0.2,0.4,0.06666666666666667,0.13333333333333333,0.4666666666666667,0.26666666666666666,0.06666666666666667,[('l' 1)]\n",
      "240_4,RNN,genderless_test,All,0.4,0.3625,0.025,0.0625,0.2625,0.5875,0.0625,[('l' 5)]\n",
      "240_4,RNN,genderless_test,M,0.30303030303030304,0.2727272727272727,0.0,0.0,0.2727272727272727,0.6363636363636364,0.09090909090909091,[('l' 3)]\n",
      "240_4,RNN,genderless_test,F,0.40625,0.5,0.0625,0.09375,0.28125,0.5,0.0625,[('l' 2)]\n",
      "240_4,RNN,genderless_test,N,0.6,0.26666666666666666,0.0,0.13333333333333333,0.2,0.6666666666666666,0.0,[]\n",
      "240_5,RNN,regular_test,All,0.5875,0.5875,0.0125,0.2625,0.225,0.45,0.05,[('l' 4)]\n",
      "240_5,RNN,regular_test,M,0.4594594594594595,0.5405405405405406,0.02702702702702703,0.3783783783783784,0.24324324324324326,0.2972972972972973,0.05405405405405406,[('l' 2)]\n",
      "240_5,RNN,regular_test,F,0.8214285714285714,0.75,0.0,0.0,0.10714285714285714,0.8928571428571429,0.0,[]\n",
      "240_5,RNN,regular_test,N,0.4666666666666667,0.4,0.0,0.4666666666666667,0.4,0.0,0.13333333333333333,[('l' 2)]\n",
      "240_5,RNN,genderless_test,All,0.45,0.25,0.025,0.2,0.3875,0.3125,0.075,[('l' 6)]\n",
      "240_5,RNN,genderless_test,M,0.5405405405405406,0.2972972972972973,0.0,0.08108108108108109,0.5135135135135135,0.35135135135135137,0.05405405405405406,[('l' 2)]\n",
      "240_5,RNN,genderless_test,F,0.2857142857142857,0.25,0.03571428571428571,0.25,0.32142857142857145,0.2857142857142857,0.10714285714285714,[('l' 3)]\n",
      "240_5,RNN,genderless_test,N,0.5333333333333333,0.13333333333333333,0.06666666666666667,0.4,0.2,0.26666666666666666,0.06666666666666667,[('l' 1)]\n",
      "240_6,RNN,regular_test,All,0.7125,0.6375,0.0125,0.3125,0.2,0.4375,0.0375,[('l' 3)]\n",
      "240_6,RNN,regular_test,M,0.6944444444444444,0.5277777777777778,0.0,0.5277777777777778,0.3611111111111111,0.05555555555555555,0.05555555555555555,[('l' 2)]\n",
      "240_6,RNN,regular_test,F,0.8857142857142857,0.8285714285714286,0.02857142857142857,0.05714285714285714,0.0,0.8857142857142857,0.02857142857142857,[('l' 1)]\n",
      "240_6,RNN,regular_test,N,0.1111111111111111,0.3333333333333333,0.0,0.4444444444444444,0.3333333333333333,0.2222222222222222,0.0,[]\n",
      "240_6,RNN,genderless_test,All,0.5375,0.4,0.0125,0.15,0.325,0.45,0.0625,[('l' 5)]\n",
      "240_6,RNN,genderless_test,M,0.5277777777777778,0.4166666666666667,0.0,0.25,0.25,0.4722222222222222,0.027777777777777776,[('l' 1)]\n",
      "240_6,RNN,genderless_test,F,0.5428571428571428,0.37142857142857144,0.02857142857142857,0.08571428571428572,0.3142857142857143,0.45714285714285713,0.11428571428571428,[('l' 4)]\n",
      "240_6,RNN,genderless_test,N,0.5555555555555556,0.4444444444444444,0.0,0.0,0.6666666666666666,0.3333333333333333,0.0,[]\n",
      "240_7,RNN,regular_test,All,0.625,0.725,0.0375,0.1625,0.2625,0.5,0.0375,[('l' 2) ('t' 1)]\n",
      "240_7,RNN,regular_test,M,0.6060606060606061,0.6666666666666666,0.06060606060606061,0.30303030303030304,0.3939393939393939,0.24242424242424243,0.0,[]\n",
      "240_7,RNN,regular_test,F,0.8333333333333334,0.8333333333333334,0.03333333333333333,0.0,0.13333333333333333,0.8333333333333334,0.0,[]\n",
      "240_7,RNN,regular_test,N,0.29411764705882354,0.6470588235294118,0.0,0.17647058823529413,0.23529411764705882,0.4117647058823529,0.17647058823529413,[('l' 2) ('t' 1)]\n",
      "240_7,RNN,genderless_test,All,0.45,0.4375,0.0875,0.05,0.2625,0.5125,0.0875,[('t' 2) ('k' 1) ('z' 1) ('l' 1) ('d' 1)]\n",
      "240_7,RNN,genderless_test,M,0.5151515151515151,0.30303030303030304,0.15151515151515152,0.0,0.2727272727272727,0.48484848484848486,0.09090909090909091,[('k' 1) ('d' 1) ('g' 1)]\n",
      "240_7,RNN,genderless_test,F,0.4,0.5333333333333333,0.03333333333333333,0.06666666666666667,0.26666666666666666,0.5333333333333333,0.1,[('t' 2) ('l' 1)]\n",
      "240_7,RNN,genderless_test,N,0.4117647058823529,0.5294117647058824,0.058823529411764705,0.11764705882352941,0.23529411764705882,0.5294117647058824,0.058823529411764705,[('z' 1)]\n",
      "240_8,RNN,regular_test,All,0.6125,0.5625,0.0,0.275,0.0875,0.5875,0.05,[('l' 3) ('u' 1)]\n",
      "240_8,RNN,regular_test,M,0.41025641025641024,0.358974358974359,0.0,0.46153846153846156,0.07692307692307693,0.358974358974359,0.10256410256410256,[('l' 3) ('u' 1)]\n",
      "240_8,RNN,regular_test,F,0.9333333333333333,0.8666666666666667,0.0,0.03333333333333333,0.0,0.9666666666666667,0.0,[]\n",
      "240_8,RNN,regular_test,N,0.45454545454545453,0.45454545454545453,0.0,0.2727272727272727,0.36363636363636365,0.36363636363636365,0.0,[]\n",
      "240_8,RNN,genderless_test,All,0.475,0.4875,0.025,0.075,0.0875,0.775,0.0375,[('o' 1) ('l' 1) ('u' 1)]\n",
      "240_8,RNN,genderless_test,M,0.4358974358974359,0.3076923076923077,0.0,0.1282051282051282,0.10256410256410256,0.717948717948718,0.05128205128205128,[('l' 1) ('u' 1)]\n",
      "240_8,RNN,genderless_test,F,0.5333333333333333,0.7666666666666667,0.06666666666666667,0.03333333333333333,0.06666666666666667,0.8333333333333334,0.0,[]\n",
      "240_8,RNN,genderless_test,N,0.45454545454545453,0.36363636363636365,0.0,0.0,0.09090909090909091,0.8181818181818182,0.09090909090909091,[('o' 1)]\n",
      "240_9,RNN,regular_test,All,0.725,0.5125,0.0125,0.4,0.0875,0.475,0.025,[('l' 1) ('o' 1)]\n",
      "240_9,RNN,regular_test,M,0.6585365853658537,0.5853658536585366,0.0,0.6585365853658537,0.0975609756097561,0.1951219512195122,0.04878048780487805,[('l' 1) ('o' 1)]\n",
      "240_9,RNN,regular_test,F,0.96,0.4,0.0,0.0,0.0,1.0,0.0,[]\n",
      "240_9,RNN,regular_test,N,0.5,0.5,0.07142857142857142,0.35714285714285715,0.21428571428571427,0.35714285714285715,0.0,[]\n",
      "240_9,RNN,genderless_test,All,0.45,0.2875,0.0375,0.1875,0.1875,0.5875,0.0,[]\n",
      "240_9,RNN,genderless_test,M,0.3902439024390244,0.24390243902439024,0.04878048780487805,0.14634146341463414,0.1951219512195122,0.6097560975609756,0.0,[]\n",
      "240_9,RNN,genderless_test,F,0.48,0.36,0.04,0.16,0.2,0.6,0.0,[]\n",
      "240_9,RNN,genderless_test,N,0.5714285714285714,0.2857142857142857,0.0,0.35714285714285715,0.14285714285714285,0.5,0.0,[]\n",
      "240_10,RNN,regular_test,All,0.7375,0.6,0.0125,0.2125,0.2125,0.5,0.0625,[('l' 4) ('a' 1)]\n",
      "240_10,RNN,regular_test,M,0.631578947368421,0.6578947368421053,0.02631578947368421,0.42105263157894735,0.2894736842105263,0.15789473684210525,0.10526315789473684,[('l' 3) ('a' 1)]\n",
      "240_10,RNN,regular_test,F,1.0,0.5333333333333333,0.0,0.0,0.0,1.0,0.0,[]\n",
      "240_10,RNN,regular_test,N,0.4166666666666667,0.5833333333333334,0.0,0.08333333333333333,0.5,0.3333333333333333,0.08333333333333333,[('l' 1)]\n",
      "240_10,RNN,genderless_test,All,0.375,0.3125,0.1,0.1125,0.325,0.425,0.0375,[('m' 1) ('l' 1) ('p' 1)]\n",
      "240_10,RNN,genderless_test,M,0.3157894736842105,0.2894736842105263,0.13157894736842105,0.10526315789473684,0.34210526315789475,0.39473684210526316,0.02631578947368421,[('m' 1)]\n",
      "240_10,RNN,genderless_test,F,0.43333333333333335,0.3333333333333333,0.06666666666666667,0.06666666666666667,0.3,0.5333333333333333,0.03333333333333333,[('l' 1)]\n",
      "240_10,RNN,genderless_test,N,0.4166666666666667,0.3333333333333333,0.08333333333333333,0.25,0.3333333333333333,0.25,0.08333333333333333,[('p' 1)]\n",
      "240_11,RNN,regular_test,All,0.6,0.45,0.025,0.35,0.175,0.4,0.05,[('l' 4)]\n",
      "240_11,RNN,regular_test,M,0.425,0.25,0.05,0.55,0.225,0.125,0.05,[('l' 2)]\n",
      "240_11,RNN,regular_test,F,0.8620689655172413,0.7241379310344828,0.0,0.034482758620689655,0.06896551724137931,0.896551724137931,0.0,[]\n",
      "240_11,RNN,regular_test,N,0.5454545454545454,0.45454545454545453,0.0,0.45454545454545453,0.2727272727272727,0.09090909090909091,0.18181818181818182,[('l' 2)]\n",
      "240_11,RNN,genderless_test,All,0.3375,0.325,0.025,0.25,0.3125,0.3625,0.05,[('l' 1) ('i' 1) ('m' 1) ('t' 1)]\n",
      "240_11,RNN,genderless_test,M,0.475,0.4,0.0,0.25,0.25,0.4,0.1,[('l' 1) ('i' 1) ('m' 1) ('t' 1)]\n",
      "240_11,RNN,genderless_test,F,0.1724137931034483,0.1724137931034483,0.034482758620689655,0.3103448275862069,0.3793103448275862,0.27586206896551724,0.0,[]\n",
      "240_11,RNN,genderless_test,N,0.2727272727272727,0.45454545454545453,0.09090909090909091,0.09090909090909091,0.36363636363636365,0.45454545454545453,0.0,[]\n",
      "240_12,RNN,regular_test,All,0.6625,0.4875,0.0125,0.2375,0.2375,0.5,0.0125,[('i' 1)]\n",
      "240_12,RNN,regular_test,M,0.6578947368421053,0.5263157894736842,0.0,0.4473684210526316,0.2894736842105263,0.23684210526315788,0.02631578947368421,[('i' 1)]\n",
      "240_12,RNN,regular_test,F,0.8928571428571429,0.42857142857142855,0.03571428571428571,0.0,0.07142857142857142,0.8928571428571429,0.0,[]\n",
      "240_12,RNN,regular_test,N,0.21428571428571427,0.5,0.0,0.14285714285714285,0.42857142857142855,0.42857142857142855,0.0,[]\n",
      "240_12,RNN,genderless_test,All,0.4125,0.3,0.0625,0.125,0.2625,0.475,0.075,[('v' 2) ('i' 1) ('c' 1) ('k' 1) ('b' 1)]\n",
      "240_12,RNN,genderless_test,M,0.3157894736842105,0.23684210526315788,0.07894736842105263,0.05263157894736842,0.34210526315789475,0.47368421052631576,0.05263157894736842,[('i' 1) ('v' 1)]\n",
      "240_12,RNN,genderless_test,F,0.5,0.35714285714285715,0.07142857142857142,0.14285714285714285,0.21428571428571427,0.42857142857142855,0.14285714285714285,[('v' 1) ('c' 1) ('k' 1) ('b' 1)]\n",
      "240_12,RNN,genderless_test,N,0.5,0.35714285714285715,0.0,0.2857142857142857,0.14285714285714285,0.5714285714285714,0.0,[]\n",
      "240_13,RNN,regular_test,All,0.7375,0.5625,0.0125,0.3,0.1125,0.5625,0.0125,[('l' 1)]\n",
      "240_13,RNN,regular_test,M,0.6666666666666666,0.6666666666666666,0.0,0.48484848484848486,0.24242424242424243,0.24242424242424243,0.030303030303030304,[('l' 1)]\n",
      "240_13,RNN,regular_test,F,0.9411764705882353,0.5294117647058824,0.0,0.058823529411764705,0.0,0.9411764705882353,0.0,[]\n",
      "240_13,RNN,regular_test,N,0.38461538461538464,0.38461538461538464,0.07692307692307693,0.46153846153846156,0.07692307692307693,0.38461538461538464,0.0,[]\n",
      "240_13,RNN,genderless_test,All,0.4625,0.325,0.0125,0.1875,0.3125,0.4375,0.05,[('u' 2) ('f' 2)]\n",
      "240_13,RNN,genderless_test,M,0.42424242424242425,0.30303030303030304,0.030303030303030304,0.15151515151515152,0.42424242424242425,0.36363636363636365,0.030303030303030304,[('f' 1)]\n",
      "240_13,RNN,genderless_test,F,0.6176470588235294,0.4117647058823529,0.0,0.23529411764705882,0.14705882352941177,0.5882352941176471,0.029411764705882353,[('u' 1)]\n",
      "240_13,RNN,genderless_test,N,0.15384615384615385,0.15384615384615385,0.0,0.15384615384615385,0.46153846153846156,0.23076923076923078,0.15384615384615385,[('u' 1) ('f' 1)]\n",
      "240_14,RNN,regular_test,All,0.5375,0.575,0.0125,0.175,0.3125,0.475,0.025,[('l' 2)]\n",
      "240_14,RNN,regular_test,M,0.4857142857142857,0.5428571428571428,0.02857142857142857,0.2857142857142857,0.3142857142857143,0.3142857142857143,0.05714285714285714,[('l' 2)]\n",
      "240_14,RNN,regular_test,F,0.8518518518518519,0.8518518518518519,0.0,0.037037037037037035,0.1111111111111111,0.8518518518518519,0.0,[]\n",
      "240_14,RNN,regular_test,N,0.16666666666666666,0.2222222222222222,0.0,0.16666666666666666,0.6111111111111112,0.2222222222222222,0.0,[]\n",
      "240_14,RNN,genderless_test,All,0.425,0.3625,0.0375,0.05,0.2,0.7,0.0125,[('a' 1)]\n",
      "240_14,RNN,genderless_test,M,0.42857142857142855,0.2857142857142857,0.05714285714285714,0.05714285714285714,0.11428571428571428,0.7428571428571429,0.02857142857142857,[('a' 1)]\n",
      "240_14,RNN,genderless_test,F,0.4074074074074074,0.5185185185185185,0.037037037037037035,0.07407407407407407,0.2962962962962963,0.5925925925925926,0.0,[]\n",
      "240_14,RNN,genderless_test,N,0.4444444444444444,0.2777777777777778,0.0,0.0,0.2222222222222222,0.7777777777777778,0.0,[]\n",
      "240_15,RNN,regular_test,All,0.7375,0.5125,0.0125,0.1375,0.1625,0.675,0.0125,[('u' 1)]\n",
      "240_15,RNN,regular_test,M,0.6060606060606061,0.5454545454545454,0.030303030303030304,0.24242424242424243,0.2727272727272727,0.42424242424242425,0.030303030303030304,[('u' 1)]\n",
      "240_15,RNN,regular_test,F,0.9705882352941176,0.5,0.0,0.0,0.029411764705882353,0.9705882352941176,0.0,[]\n",
      "240_15,RNN,regular_test,N,0.46153846153846156,0.46153846153846156,0.0,0.23076923076923078,0.23076923076923078,0.5384615384615384,0.0,[]\n",
      "240_15,RNN,genderless_test,All,0.4875,0.2625,0.025,0.2,0.1625,0.6,0.0125,[('t' 1)]\n",
      "240_15,RNN,genderless_test,M,0.42424242424242425,0.12121212121212122,0.030303030303030304,0.18181818181818182,0.06060606060606061,0.7272727272727273,0.0,[]\n",
      "240_15,RNN,genderless_test,F,0.5294117647058824,0.38235294117647056,0.029411764705882353,0.20588235294117646,0.17647058823529413,0.5588235294117647,0.029411764705882353,[('t' 1)]\n",
      "240_15,RNN,genderless_test,N,0.5384615384615384,0.3076923076923077,0.0,0.23076923076923078,0.38461538461538464,0.38461538461538464,0.0,[]\n",
      "240_16,RNN,regular_test,All,0.7,0.5875,0.0,0.3,0.25,0.4375,0.0125,[('a' 1)]\n",
      "240_16,RNN,regular_test,M,0.6052631578947368,0.47368421052631576,0.0,0.47368421052631576,0.3684210526315789,0.13157894736842105,0.02631578947368421,[('a' 1)]\n",
      "240_16,RNN,regular_test,F,0.9032258064516129,0.8387096774193549,0.0,0.0,0.06451612903225806,0.9354838709677419,0.0,[]\n",
      "240_16,RNN,regular_test,N,0.45454545454545453,0.2727272727272727,0.0,0.5454545454545454,0.36363636363636365,0.09090909090909091,0.0,[]\n",
      "240_16,RNN,genderless_test,All,0.4375,0.3875,0.0,0.2,0.4,0.3875,0.0125,[('t' 1)]\n",
      "240_16,RNN,genderless_test,M,0.4473684210526316,0.42105263157894735,0.0,0.13157894736842105,0.4473684210526316,0.39473684210526316,0.02631578947368421,[('t' 1)]\n",
      "240_16,RNN,genderless_test,F,0.45161290322580644,0.3548387096774194,0.0,0.2903225806451613,0.3548387096774194,0.3548387096774194,0.0,[]\n",
      "240_16,RNN,genderless_test,N,0.36363636363636365,0.36363636363636365,0.0,0.18181818181818182,0.36363636363636365,0.45454545454545453,0.0,[]\n",
      "240_17,RNN,regular_test,All,0.7125,0.5375,0.0,0.25,0.2,0.5125,0.0375,[('l' 2) ('a' 1)]\n",
      "240_17,RNN,regular_test,M,0.6666666666666666,0.6060606060606061,0.0,0.42424242424242425,0.24242424242424243,0.30303030303030304,0.030303030303030304,[('a' 1)]\n",
      "240_17,RNN,regular_test,F,0.9333333333333333,0.5,0.0,0.03333333333333333,0.03333333333333333,0.9333333333333333,0.0,[]\n",
      "240_17,RNN,regular_test,N,0.4117647058823529,0.47058823529411764,0.0,0.29411764705882354,0.4117647058823529,0.17647058823529413,0.11764705882352941,[('l' 2)]\n",
      "240_17,RNN,genderless_test,All,0.45,0.375,0.0,0.175,0.2625,0.55,0.0125,[('l' 1)]\n",
      "240_17,RNN,genderless_test,M,0.3939393939393939,0.21212121212121213,0.0,0.12121212121212122,0.30303030303030304,0.5454545454545454,0.030303030303030304,[('l' 1)]\n",
      "240_17,RNN,genderless_test,F,0.4666666666666667,0.5333333333333333,0.0,0.2,0.23333333333333334,0.5666666666666667,0.0,[]\n",
      "240_17,RNN,genderless_test,N,0.5294117647058824,0.4117647058823529,0.0,0.23529411764705882,0.23529411764705882,0.5294117647058824,0.0,[]\n",
      "240_18,RNN,regular_test,All,0.65,0.675,0.05,0.2,0.15,0.5625,0.0375,[('l' 2) ('t' 1)]\n",
      "240_18,RNN,regular_test,M,0.5,0.5277777777777778,0.1111111111111111,0.3611111111111111,0.2222222222222222,0.3055555555555556,0.0,[]\n",
      "240_18,RNN,regular_test,F,0.9375,0.84375,0.0,0.03125,0.03125,0.9375,0.0,[]\n",
      "240_18,RNN,regular_test,N,0.3333333333333333,0.6666666666666666,0.0,0.16666666666666666,0.25,0.3333333333333333,0.25,[('l' 2) ('t' 1)]\n",
      "240_18,RNN,genderless_test,All,0.4125,0.25,0.0375,0.1625,0.3125,0.3625,0.125,[('l' 7) ('t' 2) ('b' 1)]\n",
      "240_18,RNN,genderless_test,M,0.3888888888888889,0.19444444444444445,0.05555555555555555,0.1388888888888889,0.2777777777777778,0.3888888888888889,0.1388888888888889,[('l' 3) ('t' 1) ('b' 1)]\n",
      "240_18,RNN,genderless_test,F,0.40625,0.3125,0.03125,0.15625,0.3125,0.34375,0.15625,[('l' 4) ('t' 1)]\n",
      "240_18,RNN,genderless_test,N,0.5,0.25,0.0,0.25,0.4166666666666667,0.3333333333333333,0.0,[]\n",
      "240_19,RNN,regular_test,All,0.725,0.55,0.0375,0.475,0.15,0.3125,0.025,[('l' 2)]\n",
      "240_19,RNN,regular_test,M,0.7073170731707317,0.5853658536585366,0.024390243902439025,0.6341463414634146,0.24390243902439024,0.07317073170731707,0.024390243902439025,[('l' 1)]\n",
      "240_19,RNN,regular_test,F,0.75,0.5416666666666666,0.041666666666666664,0.20833333333333334,0.0,0.75,0.0,[]\n",
      "240_19,RNN,regular_test,N,0.7333333333333333,0.4666666666666667,0.06666666666666667,0.4666666666666667,0.13333333333333333,0.26666666666666666,0.06666666666666667,[('l' 1)]\n",
      "240_19,RNN,genderless_test,All,0.4625,0.35,0.0125,0.325,0.25,0.375,0.0375,[('l' 3)]\n",
      "240_19,RNN,genderless_test,M,0.5365853658536586,0.36585365853658536,0.024390243902439025,0.3170731707317073,0.21951219512195122,0.4146341463414634,0.024390243902439025,[('l' 1)]\n",
      "240_19,RNN,genderless_test,F,0.4166666666666667,0.2916666666666667,0.0,0.2916666666666667,0.25,0.375,0.08333333333333333,[('l' 2)]\n",
      "240_19,RNN,genderless_test,N,0.3333333333333333,0.4,0.0,0.4,0.3333333333333333,0.26666666666666666,0.0,[]\n",
      "240_20,RNN,regular_test,All,0.5375,0.6125,0.0,0.225,0.2875,0.4,0.0875,[('l' 6) ('u' 1)]\n",
      "240_20,RNN,regular_test,M,0.38461538461538464,0.5128205128205128,0.0,0.3333333333333333,0.38461538461538464,0.15384615384615385,0.1282051282051282,[('l' 4) ('u' 1)]\n",
      "240_20,RNN,regular_test,F,0.8,0.7333333333333333,0.0,0.06666666666666667,0.1,0.8,0.03333333333333333,[('l' 1)]\n",
      "240_20,RNN,regular_test,N,0.36363636363636365,0.6363636363636364,0.0,0.2727272727272727,0.45454545454545453,0.18181818181818182,0.09090909090909091,[('l' 1)]\n",
      "240_20,RNN,genderless_test,All,0.3125,0.225,0.0,0.1,0.55,0.3,0.05,[('l' 3) ('u' 1)]\n",
      "240_20,RNN,genderless_test,M,0.38461538461538464,0.23076923076923078,0.0,0.05128205128205128,0.46153846153846156,0.46153846153846156,0.02564102564102564,[('l' 1)]\n",
      "240_20,RNN,genderless_test,F,0.2,0.2,0.0,0.13333333333333333,0.6,0.16666666666666666,0.1,[('l' 2) ('u' 1)]\n",
      "240_20,RNN,genderless_test,N,0.36363636363636365,0.2727272727272727,0.0,0.18181818181818182,0.7272727272727273,0.09090909090909091,0.0,[]\n",
      "240_21,RNN,regular_test,All,0.7,0.6,0.0,0.4375,0.0625,0.4875,0.0125,[('l' 1)]\n",
      "240_21,RNN,regular_test,M,0.8235294117647058,0.5882352941176471,0.0,0.6176470588235294,0.11764705882352941,0.23529411764705882,0.029411764705882353,[('l' 1)]\n",
      "240_21,RNN,regular_test,F,0.88,0.76,0.0,0.08,0.0,0.92,0.0,[]\n",
      "240_21,RNN,regular_test,N,0.2857142857142857,0.42857142857142855,0.0,0.5714285714285714,0.047619047619047616,0.38095238095238093,0.0,[]\n",
      "240_21,RNN,genderless_test,All,0.3125,0.3375,0.0,0.3125,0.35,0.275,0.0625,[('l' 3) ('m' 1) ('a' 1)]\n",
      "240_21,RNN,genderless_test,M,0.20588235294117646,0.4117647058823529,0.0,0.3235294117647059,0.29411764705882354,0.2647058823529412,0.11764705882352941,[('l' 2) ('m' 1) ('a' 1)]\n",
      "240_21,RNN,genderless_test,F,0.28,0.24,0.0,0.28,0.36,0.32,0.04,[('l' 1)]\n",
      "240_21,RNN,genderless_test,N,0.5238095238095238,0.3333333333333333,0.0,0.3333333333333333,0.42857142857142855,0.23809523809523808,0.0,[]\n",
      "240_22,RNN,regular_test,All,0.6375,0.525,0.0,0.4125,0.15,0.3875,0.05,[('l' 3) ('z' 1)]\n",
      "240_22,RNN,regular_test,M,0.6,0.5142857142857142,0.0,0.6571428571428571,0.2,0.05714285714285714,0.08571428571428572,[('l' 2) ('z' 1)]\n",
      "240_22,RNN,regular_test,F,0.7419354838709677,0.5483870967741935,0.0,0.12903225806451613,0.06451612903225806,0.8064516129032258,0.0,[]\n",
      "240_22,RNN,regular_test,N,0.5,0.5,0.0,0.42857142857142855,0.21428571428571427,0.2857142857142857,0.07142857142857142,[('l' 1)]\n",
      "240_22,RNN,genderless_test,All,0.525,0.275,0.0375,0.3,0.2375,0.3625,0.0625,[('l' 3) ('z' 1) ('t' 1)]\n",
      "240_22,RNN,genderless_test,M,0.5714285714285714,0.17142857142857143,0.0,0.42857142857142855,0.14285714285714285,0.34285714285714286,0.08571428571428572,[('l' 2) ('t' 1)]\n",
      "240_22,RNN,genderless_test,F,0.45161290322580644,0.3225806451612903,0.06451612903225806,0.1935483870967742,0.25806451612903225,0.41935483870967744,0.06451612903225806,[('z' 1) ('l' 1)]\n",
      "240_22,RNN,genderless_test,N,0.5714285714285714,0.42857142857142855,0.07142857142857142,0.21428571428571427,0.42857142857142855,0.2857142857142857,0.0,[]\n",
      "240_23,RNN,regular_test,All,0.575,0.45,0.0125,0.2,0.2125,0.4875,0.0875,[('l' 7)]\n",
      "240_23,RNN,regular_test,M,0.4166666666666667,0.4444444444444444,0.027777777777777776,0.2777777777777778,0.3055555555555556,0.2777777777777778,0.1111111111111111,[('l' 4)]\n",
      "240_23,RNN,regular_test,F,0.8333333333333334,0.4,0.0,0.03333333333333333,0.13333333333333333,0.8333333333333334,0.0,[]\n",
      "240_23,RNN,regular_test,N,0.42857142857142855,0.5714285714285714,0.0,0.35714285714285715,0.14285714285714285,0.2857142857142857,0.21428571428571427,[('l' 3)]\n",
      "240_23,RNN,genderless_test,All,0.45,0.2125,0.025,0.025,0.3,0.6375,0.0125,[('l' 1)]\n",
      "240_23,RNN,genderless_test,M,0.3333333333333333,0.16666666666666666,0.027777777777777776,0.027777777777777776,0.3055555555555556,0.6111111111111112,0.027777777777777776,[('l' 1)]\n",
      "240_23,RNN,genderless_test,F,0.5333333333333333,0.3,0.03333333333333333,0.03333333333333333,0.3,0.6333333333333333,0.0,[]\n",
      "240_23,RNN,genderless_test,N,0.5714285714285714,0.14285714285714285,0.0,0.0,0.2857142857142857,0.7142857142857143,0.0,[]\n",
      "240_24,RNN,regular_test,All,0.675,0.6,0.0125,0.3,0.225,0.425,0.0375,[('l' 3)]\n",
      "240_24,RNN,regular_test,M,0.6097560975609756,0.6097560975609756,0.0,0.5365853658536586,0.2682926829268293,0.14634146341463414,0.04878048780487805,[('l' 2)]\n",
      "240_24,RNN,regular_test,F,0.8709677419354839,0.6129032258064516,0.03225806451612903,0.0,0.0967741935483871,0.8709677419354839,0.0,[]\n",
      "240_24,RNN,regular_test,N,0.25,0.5,0.0,0.25,0.5,0.125,0.125,[('l' 1)]\n",
      "240_24,RNN,genderless_test,All,0.275,0.3,0.0125,0.1375,0.5,0.25,0.1,[('l' 7) ('h' 1)]\n",
      "240_24,RNN,genderless_test,M,0.3170731707317073,0.2926829268292683,0.0,0.07317073170731707,0.4634146341463415,0.3170731707317073,0.14634146341463414,[('l' 5) ('h' 1)]\n",
      "240_24,RNN,genderless_test,F,0.25806451612903225,0.3870967741935484,0.03225806451612903,0.25806451612903225,0.45161290322580644,0.22580645161290322,0.03225806451612903,[('l' 1)]\n",
      "240_24,RNN,genderless_test,N,0.125,0.0,0.0,0.0,0.875,0.0,0.125,[('l' 1)]\n",
      "300_0,RNN,regular_test,All,0.8,0.6875,0.0,0.2875,0.1,0.5875,0.025,[('l' 2)]\n",
      "300_0,RNN,regular_test,M,0.75,0.5357142857142857,0.0,0.6071428571428571,0.21428571428571427,0.14285714285714285,0.03571428571428571,[('l' 1)]\n",
      "300_0,RNN,regular_test,F,1.0,0.9473684210526315,0.0,0.0,0.0,1.0,0.0,[]\n",
      "300_0,RNN,regular_test,N,0.35714285714285715,0.2857142857142857,0.0,0.42857142857142855,0.14285714285714285,0.35714285714285715,0.07142857142857142,[('l' 1)]\n",
      "300_0,RNN,genderless_test,All,0.4875,0.375,0.0,0.1375,0.2125,0.5875,0.0625,[('l' 5)]\n",
      "300_0,RNN,genderless_test,M,0.4642857142857143,0.21428571428571427,0.0,0.17857142857142858,0.25,0.5357142857142857,0.03571428571428571,[('l' 1)]\n",
      "300_0,RNN,genderless_test,F,0.4473684210526316,0.5526315789473685,0.0,0.10526315789473684,0.21052631578947367,0.6052631578947368,0.07894736842105263,[('l' 3)]\n",
      "300_0,RNN,genderless_test,N,0.6428571428571429,0.21428571428571427,0.0,0.14285714285714285,0.14285714285714285,0.6428571428571429,0.07142857142857142,[('l' 1)]\n",
      "300_1,RNN,regular_test,All,0.675,0.4625,0.025,0.1875,0.225,0.5375,0.025,[('l' 2)]\n",
      "300_1,RNN,regular_test,M,0.5142857142857142,0.4857142857142857,0.0,0.34285714285714286,0.34285714285714286,0.2857142857142857,0.02857142857142857,[('l' 1)]\n",
      "300_1,RNN,regular_test,F,0.9354838709677419,0.45161290322580644,0.03225806451612903,0.0,0.03225806451612903,0.9354838709677419,0.0,[]\n",
      "300_1,RNN,regular_test,N,0.5,0.42857142857142855,0.07142857142857142,0.21428571428571427,0.35714285714285715,0.2857142857142857,0.07142857142857142,[('l' 1)]\n",
      "300_1,RNN,genderless_test,All,0.5,0.2875,0.025,0.05,0.2125,0.6625,0.05,[('l' 3) ('o' 1)]\n",
      "300_1,RNN,genderless_test,M,0.45714285714285713,0.22857142857142856,0.02857142857142857,0.05714285714285714,0.22857142857142856,0.6285714285714286,0.05714285714285714,[('l' 1) ('o' 1)]\n",
      "300_1,RNN,genderless_test,F,0.4838709677419355,0.41935483870967744,0.03225806451612903,0.0,0.1935483870967742,0.7419354838709677,0.03225806451612903,[('l' 1)]\n",
      "300_1,RNN,genderless_test,N,0.6428571428571429,0.14285714285714285,0.0,0.14285714285714285,0.21428571428571427,0.5714285714285714,0.07142857142857142,[('l' 1)]\n",
      "300_2,RNN,regular_test,All,0.7125,0.5125,0.0375,0.3125,0.125,0.5,0.025,[('l' 2)]\n",
      "300_2,RNN,regular_test,M,0.5833333333333334,0.5833333333333334,0.08333333333333333,0.4722222222222222,0.2222222222222222,0.2222222222222222,0.0,[]\n",
      "300_2,RNN,regular_test,F,0.9354838709677419,0.45161290322580644,0.0,0.06451612903225806,0.0,0.9354838709677419,0.0,[]\n",
      "300_2,RNN,regular_test,N,0.5384615384615384,0.46153846153846156,0.0,0.46153846153846156,0.15384615384615385,0.23076923076923078,0.15384615384615385,[('l' 2)]\n",
      "300_2,RNN,genderless_test,All,0.4375,0.35,0.025,0.2625,0.1625,0.525,0.025,[('l' 2)]\n",
      "300_2,RNN,genderless_test,M,0.5,0.25,0.027777777777777776,0.3055555555555556,0.1388888888888889,0.4722222222222222,0.05555555555555555,[('l' 2)]\n",
      "300_2,RNN,genderless_test,F,0.45161290322580644,0.45161290322580644,0.03225806451612903,0.1935483870967742,0.22580645161290322,0.5483870967741935,0.0,[]\n",
      "300_2,RNN,genderless_test,N,0.23076923076923078,0.38461538461538464,0.0,0.3076923076923077,0.07692307692307693,0.6153846153846154,0.0,[]\n",
      "300_3,RNN,regular_test,All,0.7375,0.6375,0.0125,0.2625,0.175,0.5125,0.0375,[('l' 2) ('t' 1)]\n",
      "300_3,RNN,regular_test,M,0.6538461538461539,0.6538461538461539,0.0,0.4230769230769231,0.2692307692307692,0.23076923076923078,0.07692307692307693,[('l' 2)]\n",
      "300_3,RNN,regular_test,F,0.9117647058823529,0.7647058823529411,0.0,0.029411764705882353,0.029411764705882353,0.9411764705882353,0.0,[]\n",
      "300_3,RNN,regular_test,N,0.55,0.4,0.05,0.45,0.3,0.15,0.05,[('t' 1)]\n",
      "300_3,RNN,genderless_test,All,0.2875,0.275,0.0125,0.2125,0.375,0.2125,0.1875,[('l' 8) ('c' 1) ('p' 1) ('b' 1) ('u' 1)]\n",
      "300_3,RNN,genderless_test,M,0.23076923076923078,0.23076923076923078,0.0,0.19230769230769232,0.5384615384615384,0.11538461538461539,0.15384615384615385,[('b' 1) ('u' 1) ('z' 1) ('y' 1)]\n",
      "300_3,RNN,genderless_test,F,0.4411764705882353,0.35294117647058826,0.029411764705882353,0.20588235294117646,0.20588235294117646,0.3235294117647059,0.23529411764705882,[('l' 6) ('p' 1) ('o' 1)]\n",
      "300_3,RNN,genderless_test,N,0.1,0.2,0.0,0.25,0.45,0.15,0.15,[('l' 2) ('c' 1)]\n",
      "300_4,RNN,regular_test,All,0.775,0.6375,0.0,0.3125,0.15,0.5,0.0375,[('l' 3)]\n",
      "300_4,RNN,regular_test,M,0.8787878787878788,0.6363636363636364,0.0,0.5454545454545454,0.21212121212121213,0.18181818181818182,0.06060606060606061,[('l' 2)]\n",
      "300_4,RNN,regular_test,F,0.875,0.78125,0.0,0.03125,0.0625,0.90625,0.0,[]\n",
      "300_4,RNN,regular_test,N,0.3333333333333333,0.3333333333333333,0.0,0.4,0.2,0.3333333333333333,0.06666666666666667,[('l' 1)]\n",
      "300_4,RNN,genderless_test,All,0.375,0.3125,0.025,0.275,0.2625,0.325,0.1125,[('l' 7) ('u' 1) ('f' 1)]\n",
      "300_4,RNN,genderless_test,M,0.3939393939393939,0.21212121212121213,0.030303030303030304,0.42424242424242425,0.24242424242424243,0.24242424242424243,0.06060606060606061,[('l' 1) ('u' 1)]\n",
      "300_4,RNN,genderless_test,F,0.4375,0.4375,0.03125,0.15625,0.1875,0.46875,0.15625,[('l' 4) ('f' 1)]\n",
      "300_4,RNN,genderless_test,N,0.2,0.26666666666666666,0.0,0.2,0.4666666666666667,0.2,0.13333333333333333,[('l' 2)]\n",
      "300_5,RNN,regular_test,All,0.6875,0.5125,0.0125,0.4,0.1375,0.4125,0.0375,[('o' 2) ('l' 1)]\n",
      "300_5,RNN,regular_test,M,0.5675675675675675,0.32432432432432434,0.02702702702702703,0.6216216216216216,0.16216216216216217,0.13513513513513514,0.05405405405405406,[('o' 2)]\n",
      "300_5,RNN,regular_test,F,0.9285714285714286,0.8928571428571429,0.0,0.0,0.0,1.0,0.0,[]\n",
      "300_5,RNN,regular_test,N,0.5333333333333333,0.26666666666666666,0.0,0.6,0.3333333333333333,0.0,0.06666666666666667,[('l' 1)]\n",
      "300_5,RNN,genderless_test,All,0.2875,0.1625,0.0,0.3875,0.4375,0.075,0.1,[('o' 3) ('a' 3) ('i' 1) ('l' 1)]\n",
      "300_5,RNN,genderless_test,M,0.13513513513513514,0.1891891891891892,0.0,0.35135135135135137,0.5135135135135135,0.02702702702702703,0.10810810810810811,[('o' 2) ('i' 1) ('l' 1)]\n",
      "300_5,RNN,genderless_test,F,0.35714285714285715,0.14285714285714285,0.0,0.39285714285714285,0.42857142857142855,0.10714285714285714,0.07142857142857142,[('o' 1) ('a' 1)]\n",
      "300_5,RNN,genderless_test,N,0.5333333333333333,0.13333333333333333,0.0,0.4666666666666667,0.26666666666666666,0.13333333333333333,0.13333333333333333,[('a' 2)]\n",
      "300_6,RNN,regular_test,All,0.7875,0.75,0.0125,0.2875,0.1375,0.55,0.0125,[('l' 1)]\n",
      "300_6,RNN,regular_test,M,0.75,0.7222222222222222,0.0,0.5,0.3055555555555556,0.16666666666666666,0.027777777777777776,[('l' 1)]\n",
      "300_6,RNN,regular_test,F,0.9714285714285714,0.8571428571428571,0.0,0.02857142857142857,0.0,0.9714285714285714,0.0,[]\n",
      "300_6,RNN,regular_test,N,0.2222222222222222,0.4444444444444444,0.1111111111111111,0.4444444444444444,0.0,0.4444444444444444,0.0,[]\n",
      "300_6,RNN,genderless_test,All,0.55,0.2625,0.0125,0.1875,0.2375,0.5125,0.05,[('l' 3) ('i' 1)]\n",
      "300_6,RNN,genderless_test,M,0.6111111111111112,0.19444444444444445,0.0,0.19444444444444445,0.16666666666666666,0.5833333333333334,0.05555555555555555,[('l' 2)]\n",
      "300_6,RNN,genderless_test,F,0.5142857142857142,0.37142857142857144,0.0,0.14285714285714285,0.34285714285714286,0.45714285714285713,0.05714285714285714,[('l' 1) ('i' 1)]\n",
      "300_6,RNN,genderless_test,N,0.4444444444444444,0.1111111111111111,0.1111111111111111,0.3333333333333333,0.1111111111111111,0.4444444444444444,0.0,[]\n",
      "300_7,RNN,regular_test,All,0.7375,0.6625,0.0125,0.3,0.1375,0.5125,0.0375,[('l' 2) ('h' 1)]\n",
      "300_7,RNN,regular_test,M,0.6666666666666666,0.5757575757575758,0.030303030303030304,0.48484848484848486,0.24242424242424243,0.18181818181818182,0.06060606060606061,[('h' 1) ('l' 1)]\n",
      "300_7,RNN,regular_test,F,0.9333333333333333,0.8,0.0,0.03333333333333333,0.03333333333333333,0.9333333333333333,0.0,[]\n",
      "300_7,RNN,regular_test,N,0.5294117647058824,0.5882352941176471,0.0,0.4117647058823529,0.11764705882352941,0.4117647058823529,0.058823529411764705,[('l' 1)]\n",
      "300_7,RNN,genderless_test,All,0.3125,0.2625,0.0125,0.3625,0.2125,0.325,0.0875,[('l' 2) ('h' 2) ('t' 1) ('u' 1) ('d' 1)]\n",
      "300_7,RNN,genderless_test,M,0.45454545454545453,0.30303030303030304,0.030303030303030304,0.3333333333333333,0.18181818181818182,0.3939393939393939,0.06060606060606061,[('l' 1) ('h' 1)]\n",
      "300_7,RNN,genderless_test,F,0.2,0.36666666666666664,0.0,0.3333333333333333,0.26666666666666666,0.36666666666666664,0.03333333333333333,[('h' 1)]\n",
      "300_7,RNN,genderless_test,N,0.23529411764705882,0.0,0.0,0.47058823529411764,0.17647058823529413,0.11764705882352941,0.23529411764705882,[('t' 1) ('u' 1) ('d' 1) ('l' 1)]\n",
      "300_8,RNN,regular_test,All,0.725,0.5375,0.0,0.3375,0.1875,0.45,0.025,[('l' 2)]\n",
      "300_8,RNN,regular_test,M,0.6410256410256411,0.6153846153846154,0.0,0.5641025641025641,0.23076923076923078,0.15384615384615385,0.05128205128205128,[('l' 2)]\n",
      "300_8,RNN,regular_test,F,0.9,0.4666666666666667,0.0,0.06666666666666667,0.0,0.9333333333333333,0.0,[]\n",
      "300_8,RNN,regular_test,N,0.5454545454545454,0.45454545454545453,0.0,0.2727272727272727,0.5454545454545454,0.18181818181818182,0.0,[]\n",
      "300_8,RNN,genderless_test,All,0.45,0.3,0.0125,0.05,0.2875,0.6,0.05,[('c' 2) ('l' 1) ('t' 1)]\n",
      "300_8,RNN,genderless_test,M,0.48717948717948717,0.20512820512820512,0.02564102564102564,0.05128205128205128,0.3333333333333333,0.5384615384615384,0.05128205128205128,[('c' 1) ('l' 1)]\n",
      "300_8,RNN,genderless_test,F,0.4,0.43333333333333335,0.0,0.06666666666666667,0.2,0.6666666666666666,0.06666666666666667,[('t' 1) ('c' 1)]\n",
      "300_8,RNN,genderless_test,N,0.45454545454545453,0.2727272727272727,0.0,0.0,0.36363636363636365,0.6363636363636364,0.0,[]\n",
      "300_9,RNN,regular_test,All,0.7125,0.4875,0.0125,0.5125,0.0875,0.325,0.0625,[('l' 3) ('u' 1) ('o' 1)]\n",
      "300_9,RNN,regular_test,M,0.6829268292682927,0.36585365853658536,0.024390243902439025,0.7317073170731707,0.14634146341463414,0.024390243902439025,0.07317073170731707,[('l' 2) ('o' 1)]\n",
      "300_9,RNN,regular_test,F,0.8,0.72,0.0,0.08,0.04,0.84,0.04,[('u' 1)]\n",
      "300_9,RNN,regular_test,N,0.6428571428571429,0.42857142857142855,0.0,0.6428571428571429,0.0,0.2857142857142857,0.07142857142857142,[('l' 1)]\n",
      "300_9,RNN,genderless_test,All,0.3875,0.1625,0.0375,0.55,0.2375,0.125,0.05,[('u' 2) ('i' 1) ('l' 1)]\n",
      "300_9,RNN,genderless_test,M,0.4146341463414634,0.21951219512195122,0.04878048780487805,0.5121951219512195,0.21951219512195122,0.14634146341463414,0.07317073170731707,[('u' 1) ('i' 1) ('l' 1)]\n",
      "300_9,RNN,genderless_test,F,0.44,0.12,0.04,0.68,0.2,0.08,0.0,[]\n",
      "300_9,RNN,genderless_test,N,0.21428571428571427,0.07142857142857142,0.0,0.42857142857142855,0.35714285714285715,0.14285714285714285,0.07142857142857142,[('u' 1)]\n",
      "300_10,RNN,regular_test,All,0.7375,0.6,0.0125,0.325,0.2,0.4,0.0625,[('l' 3) ('u' 2)]\n",
      "300_10,RNN,regular_test,M,0.7105263157894737,0.5789473684210527,0.0,0.5526315789473685,0.23684210526315788,0.10526315789473684,0.10526315789473684,[('l' 2) ('u' 2)]\n",
      "300_10,RNN,regular_test,F,0.8333333333333334,0.6333333333333333,0.0,0.1,0.06666666666666667,0.8333333333333334,0.0,[]\n",
      "300_10,RNN,regular_test,N,0.5833333333333334,0.5833333333333334,0.08333333333333333,0.16666666666666666,0.4166666666666667,0.25,0.08333333333333333,[('l' 1)]\n",
      "300_10,RNN,genderless_test,All,0.4625,0.275,0.025,0.3125,0.2375,0.2875,0.1375,[('u' 3) ('l' 3) ('a' 2) ('i' 1) ('t' 1)]\n",
      "300_10,RNN,genderless_test,M,0.47368421052631576,0.42105263157894735,0.05263157894736842,0.39473684210526316,0.18421052631578946,0.2631578947368421,0.10526315789473684,[('u' 1) ('a' 1) ('t' 1) ('l' 1)]\n",
      "300_10,RNN,genderless_test,F,0.4666666666666667,0.1,0.0,0.26666666666666666,0.3,0.3,0.13333333333333333,[('i' 1) ('k' 1) ('u' 1) ('l' 1)]\n",
      "300_10,RNN,genderless_test,N,0.4166666666666667,0.25,0.0,0.16666666666666666,0.25,0.3333333333333333,0.25,[('l' 1) ('u' 1) ('a' 1)]\n",
      "300_11,RNN,regular_test,All,0.7,0.5375,0.05,0.3625,0.1625,0.4125,0.0125,[('l' 1)]\n",
      "300_11,RNN,regular_test,M,0.6,0.475,0.025,0.6,0.25,0.125,0.0,[]\n",
      "300_11,RNN,regular_test,F,0.8620689655172413,0.7241379310344828,0.0,0.034482758620689655,0.06896551724137931,0.896551724137931,0.0,[]\n",
      "300_11,RNN,regular_test,N,0.6363636363636364,0.2727272727272727,0.2727272727272727,0.36363636363636365,0.09090909090909091,0.18181818181818182,0.09090909090909091,[('l' 1)]\n",
      "300_11,RNN,genderless_test,All,0.375,0.4,0.0625,0.225,0.1875,0.475,0.05,[('l' 3) ('k' 1)]\n",
      "300_11,RNN,genderless_test,M,0.325,0.35,0.075,0.25,0.175,0.425,0.075,[('l' 3)]\n",
      "300_11,RNN,genderless_test,F,0.4482758620689655,0.5172413793103449,0.06896551724137931,0.1724137931034483,0.13793103448275862,0.6206896551724138,0.0,[]\n",
      "300_11,RNN,genderless_test,N,0.36363636363636365,0.2727272727272727,0.0,0.2727272727272727,0.36363636363636365,0.2727272727272727,0.09090909090909091,[('k' 1)]\n",
      "300_12,RNN,regular_test,All,0.7125,0.65,0.0375,0.3625,0.1625,0.4375,0.0,[]\n",
      "300_12,RNN,regular_test,M,0.7631578947368421,0.5789473684210527,0.0,0.5263157894736842,0.2631578947368421,0.21052631578947367,0.0,[]\n",
      "300_12,RNN,regular_test,F,0.8214285714285714,0.8214285714285714,0.03571428571428571,0.10714285714285714,0.03571428571428571,0.8214285714285714,0.0,[]\n",
      "300_12,RNN,regular_test,N,0.35714285714285715,0.5,0.14285714285714285,0.42857142857142855,0.14285714285714285,0.2857142857142857,0.0,[]\n",
      "300_12,RNN,genderless_test,All,0.35,0.3,0.125,0.1625,0.3125,0.325,0.075,[('l' 2) ('i' 1) ('u' 1) ('z' 1) ('a' 1)]\n",
      "300_12,RNN,genderless_test,M,0.34210526315789475,0.2894736842105263,0.15789473684210525,0.21052631578947367,0.23684210526315788,0.34210526315789475,0.05263157894736842,[('l' 1) ('z' 1)]\n",
      "300_12,RNN,genderless_test,F,0.32142857142857145,0.32142857142857145,0.14285714285714285,0.10714285714285714,0.35714285714285715,0.2857142857142857,0.10714285714285714,[('l' 1) ('i' 1) ('u' 1)]\n",
      "300_12,RNN,genderless_test,N,0.42857142857142855,0.2857142857142857,0.0,0.14285714285714285,0.42857142857142855,0.35714285714285715,0.07142857142857142,[('a' 1)]\n",
      "300_13,RNN,regular_test,All,0.7375,0.7125,0.0,0.2375,0.1375,0.5625,0.0625,[('l' 4) ('u' 1)]\n",
      "300_13,RNN,regular_test,M,0.6666666666666666,0.6363636363636364,0.0,0.45454545454545453,0.21212121212121213,0.24242424242424243,0.09090909090909091,[('l' 2) ('u' 1)]\n",
      "300_13,RNN,regular_test,F,1.0,0.8823529411764706,0.0,0.0,0.0,1.0,0.0,[]\n",
      "300_13,RNN,regular_test,N,0.23076923076923078,0.46153846153846156,0.0,0.3076923076923077,0.3076923076923077,0.23076923076923078,0.15384615384615385,[('l' 2)]\n",
      "300_13,RNN,genderless_test,All,0.3375,0.2375,0.0,0.3375,0.3625,0.175,0.125,[('l' 6) ('f' 3) ('u' 1)]\n",
      "300_13,RNN,genderless_test,M,0.2727272727272727,0.21212121212121213,0.0,0.2727272727272727,0.45454545454545453,0.12121212121212122,0.15151515151515152,[('l' 3) ('u' 1) ('f' 1)]\n",
      "300_13,RNN,genderless_test,F,0.38235294117647056,0.2647058823529412,0.0,0.4411764705882353,0.2647058823529412,0.20588235294117646,0.08823529411764706,[('l' 2) ('f' 1)]\n",
      "300_13,RNN,genderless_test,N,0.38461538461538464,0.23076923076923078,0.0,0.23076923076923078,0.38461538461538464,0.23076923076923078,0.15384615384615385,[('f' 1) ('l' 1)]\n",
      "300_14,RNN,regular_test,All,0.7375,0.7,0.0125,0.2375,0.2,0.525,0.025,[('g' 1) ('l' 1)]\n",
      "300_14,RNN,regular_test,M,0.7428571428571429,0.6857142857142857,0.02857142857142857,0.42857142857142855,0.2571428571428571,0.2571428571428571,0.02857142857142857,[('l' 1)]\n",
      "300_14,RNN,regular_test,F,0.9259259259259259,0.8518518518518519,0.0,0.0,0.07407407407407407,0.9259259259259259,0.0,[]\n",
      "300_14,RNN,regular_test,N,0.4444444444444444,0.5,0.0,0.2222222222222222,0.2777777777777778,0.4444444444444444,0.05555555555555555,[('g' 1)]\n",
      "300_14,RNN,genderless_test,All,0.475,0.3875,0.0375,0.1625,0.3375,0.4125,0.05,[('l' 3) ('f' 1)]\n",
      "300_14,RNN,genderless_test,M,0.5428571428571428,0.4,0.05714285714285714,0.2,0.34285714285714286,0.37142857142857144,0.02857142857142857,[('l' 1)]\n",
      "300_14,RNN,genderless_test,F,0.4444444444444444,0.4444444444444444,0.037037037037037035,0.14814814814814814,0.25925925925925924,0.48148148148148145,0.07407407407407407,[('f' 1) ('l' 1)]\n",
      "300_14,RNN,genderless_test,N,0.3888888888888889,0.2777777777777778,0.0,0.1111111111111111,0.4444444444444444,0.3888888888888889,0.05555555555555555,[('l' 1)]\n",
      "300_15,RNN,regular_test,All,0.675,0.525,0.0375,0.25,0.2125,0.4875,0.0125,[('d' 1)]\n",
      "300_15,RNN,regular_test,M,0.6666666666666666,0.6060606060606061,0.0,0.3939393939393939,0.30303030303030304,0.30303030303030304,0.0,[]\n",
      "300_15,RNN,regular_test,F,0.7352941176470589,0.47058823529411764,0.08823529411764706,0.11764705882352941,0.058823529411764705,0.7352941176470589,0.0,[]\n",
      "300_15,RNN,regular_test,N,0.5384615384615384,0.46153846153846156,0.0,0.23076923076923078,0.38461538461538464,0.3076923076923077,0.07692307692307693,[('d' 1)]\n",
      "300_15,RNN,genderless_test,All,0.4125,0.2625,0.0625,0.15,0.225,0.45,0.1125,[('l' 4) ('z' 1) ('t' 1) ('c' 1) ('p' 1)]\n",
      "300_15,RNN,genderless_test,M,0.30303030303030304,0.2727272727272727,0.06060606060606061,0.2727272727272727,0.15151515151515152,0.42424242424242425,0.09090909090909091,[('l' 2) ('z' 1)]\n",
      "300_15,RNN,genderless_test,F,0.4411764705882353,0.23529411764705882,0.08823529411764706,0.08823529411764706,0.17647058823529413,0.47058823529411764,0.17647058823529413,[('l' 2) ('t' 1) ('c' 1) ('p' 1) ('g' 1)]\n",
      "300_15,RNN,genderless_test,N,0.6153846153846154,0.3076923076923077,0.0,0.0,0.5384615384615384,0.46153846153846156,0.0,[]\n",
      "300_16,RNN,regular_test,All,0.75,0.5375,0.05,0.2625,0.125,0.5625,0.0,[]\n",
      "300_16,RNN,regular_test,M,0.6842105263157895,0.631578947368421,0.07894736842105263,0.42105263157894735,0.21052631578947367,0.2894736842105263,0.0,[]\n",
      "300_16,RNN,regular_test,F,0.9354838709677419,0.3870967741935484,0.03225806451612903,0.0,0.0,0.967741935483871,0.0,[]\n",
      "300_16,RNN,regular_test,N,0.45454545454545453,0.6363636363636364,0.0,0.45454545454545453,0.18181818181818182,0.36363636363636365,0.0,[]\n",
      "300_16,RNN,genderless_test,All,0.4125,0.25,0.0625,0.2125,0.25,0.375,0.1,[('l' 5) ('g' 1) ('t' 1) ('d' 1)]\n",
      "300_16,RNN,genderless_test,M,0.47368421052631576,0.34210526315789475,0.02631578947368421,0.23684210526315788,0.23684210526315788,0.42105263157894735,0.07894736842105263,[('l' 2) ('t' 1)]\n",
      "300_16,RNN,genderless_test,F,0.3225806451612903,0.1935483870967742,0.06451612903225806,0.22580645161290322,0.25806451612903225,0.3548387096774194,0.0967741935483871,[('l' 2) ('g' 1)]\n",
      "300_16,RNN,genderless_test,N,0.45454545454545453,0.09090909090909091,0.18181818181818182,0.09090909090909091,0.2727272727272727,0.2727272727272727,0.18181818181818182,[('d' 1) ('l' 1)]\n",
      "300_17,RNN,regular_test,All,0.7375,0.5625,0.0,0.3375,0.15,0.475,0.0375,[('l' 3)]\n",
      "300_17,RNN,regular_test,M,0.696969696969697,0.6060606060606061,0.0,0.5757575757575758,0.21212121212121213,0.21212121212121213,0.0,[]\n",
      "300_17,RNN,regular_test,F,0.9666666666666667,0.5333333333333333,0.0,0.03333333333333333,0.0,0.9666666666666667,0.0,[]\n",
      "300_17,RNN,regular_test,N,0.4117647058823529,0.5294117647058824,0.0,0.4117647058823529,0.29411764705882354,0.11764705882352941,0.17647058823529413,[('l' 3)]\n",
      "300_17,RNN,genderless_test,All,0.4,0.3125,0.0375,0.3,0.2875,0.325,0.05,[('l' 4)]\n",
      "300_17,RNN,genderless_test,M,0.3333333333333333,0.36363636363636365,0.0,0.24242424242424243,0.3333333333333333,0.3333333333333333,0.09090909090909091,[('l' 3)]\n",
      "300_17,RNN,genderless_test,F,0.4666666666666667,0.26666666666666666,0.06666666666666667,0.3333333333333333,0.2,0.36666666666666664,0.03333333333333333,[('l' 1)]\n",
      "300_17,RNN,genderless_test,N,0.4117647058823529,0.29411764705882354,0.058823529411764705,0.35294117647058826,0.35294117647058826,0.23529411764705882,0.0,[]\n",
      "300_18,RNN,regular_test,All,0.7125,0.75,0.0125,0.3375,0.125,0.4875,0.0375,[('l' 3)]\n",
      "300_18,RNN,regular_test,M,0.6666666666666666,0.7777777777777778,0.027777777777777776,0.5277777777777778,0.16666666666666666,0.19444444444444445,0.08333333333333333,[('l' 3)]\n",
      "300_18,RNN,regular_test,F,0.875,0.875,0.0,0.09375,0.03125,0.875,0.0,[]\n",
      "300_18,RNN,regular_test,N,0.4166666666666667,0.3333333333333333,0.0,0.4166666666666667,0.25,0.3333333333333333,0.0,[]\n",
      "300_18,RNN,genderless_test,All,0.4875,0.3875,0.05,0.125,0.2125,0.525,0.0875,[('l' 5) ('i' 1) ('m' 1)]\n",
      "300_18,RNN,genderless_test,M,0.4444444444444444,0.3055555555555556,0.05555555555555555,0.1111111111111111,0.19444444444444445,0.6111111111111112,0.027777777777777776,[('m' 1)]\n",
      "300_18,RNN,genderless_test,F,0.5,0.53125,0.03125,0.09375,0.25,0.5,0.125,[('l' 3) ('i' 1)]\n",
      "300_18,RNN,genderless_test,N,0.5833333333333334,0.25,0.08333333333333333,0.25,0.16666666666666666,0.3333333333333333,0.16666666666666666,[('l' 2)]\n",
      "300_19,RNN,regular_test,All,0.75,0.6875,0.0,0.325,0.2125,0.45,0.0125,[('l' 1)]\n",
      "300_19,RNN,regular_test,M,0.6341463414634146,0.5609756097560976,0.0,0.5365853658536586,0.3170731707317073,0.14634146341463414,0.0,[]\n",
      "300_19,RNN,regular_test,F,1.0,0.9166666666666666,0.0,0.0,0.0,1.0,0.0,[]\n",
      "300_19,RNN,regular_test,N,0.6666666666666666,0.6666666666666666,0.0,0.26666666666666666,0.26666666666666666,0.4,0.06666666666666667,[('l' 1)]\n",
      "300_19,RNN,genderless_test,All,0.4375,0.275,0.0,0.1625,0.4125,0.3875,0.0375,[('l' 3)]\n",
      "300_19,RNN,genderless_test,M,0.5365853658536586,0.2682926829268293,0.0,0.12195121951219512,0.3902439024390244,0.4146341463414634,0.07317073170731707,[('l' 3)]\n",
      "300_19,RNN,genderless_test,F,0.25,0.3333333333333333,0.0,0.16666666666666666,0.4166666666666667,0.4166666666666667,0.0,[]\n",
      "300_19,RNN,genderless_test,N,0.4666666666666667,0.2,0.0,0.26666666666666666,0.4666666666666667,0.26666666666666666,0.0,[]\n",
      "300_20,RNN,regular_test,All,0.775,0.725,0.0,0.3,0.1125,0.5125,0.075,[('l' 6)]\n",
      "300_20,RNN,regular_test,M,0.6923076923076923,0.6410256410256411,0.0,0.5128205128205128,0.15384615384615385,0.23076923076923078,0.10256410256410256,[('l' 4)]\n",
      "300_20,RNN,regular_test,F,1.0,0.8333333333333334,0.0,0.0,0.0,1.0,0.0,[]\n",
      "300_20,RNN,regular_test,N,0.45454545454545453,0.7272727272727273,0.0,0.36363636363636365,0.2727272727272727,0.18181818181818182,0.18181818181818182,[('l' 2)]\n",
      "300_20,RNN,genderless_test,All,0.575,0.375,0.0,0.1625,0.1375,0.6625,0.0375,[('l' 3)]\n",
      "300_20,RNN,genderless_test,M,0.6410256410256411,0.2564102564102564,0.0,0.15384615384615385,0.20512820512820512,0.6410256410256411,0.0,[]\n",
      "300_20,RNN,genderless_test,F,0.5333333333333333,0.5666666666666667,0.0,0.2,0.03333333333333333,0.6666666666666666,0.1,[('l' 3)]\n",
      "300_20,RNN,genderless_test,N,0.45454545454545453,0.2727272727272727,0.0,0.09090909090909091,0.18181818181818182,0.7272727272727273,0.0,[]\n",
      "300_21,RNN,regular_test,All,0.65,0.6125,0.0125,0.3375,0.1375,0.5,0.0125,[('l' 1)]\n",
      "300_21,RNN,regular_test,M,0.6764705882352942,0.5882352941176471,0.0,0.5588235294117647,0.14705882352941177,0.2647058823529412,0.029411764705882353,[('l' 1)]\n",
      "300_21,RNN,regular_test,F,0.92,0.8,0.04,0.0,0.0,0.96,0.0,[]\n",
      "300_21,RNN,regular_test,N,0.2857142857142857,0.42857142857142855,0.0,0.38095238095238093,0.2857142857142857,0.3333333333333333,0.0,[]\n",
      "300_21,RNN,genderless_test,All,0.5625,0.3125,0.0,0.35,0.1,0.4875,0.0625,[('l' 2) ('t' 1) ('a' 1) ('u' 1)]\n",
      "300_21,RNN,genderless_test,M,0.6176470588235294,0.29411764705882354,0.0,0.3235294117647059,0.058823529411764705,0.5294117647058824,0.08823529411764706,[('t' 1) ('a' 1) ('l' 1)]\n",
      "300_21,RNN,genderless_test,F,0.44,0.44,0.0,0.2,0.2,0.56,0.04,[('u' 1)]\n",
      "300_21,RNN,genderless_test,N,0.6190476190476191,0.19047619047619047,0.0,0.5714285714285714,0.047619047619047616,0.3333333333333333,0.047619047619047616,[('l' 1)]\n",
      "300_22,RNN,regular_test,All,0.7125,0.6,0.0,0.3375,0.125,0.525,0.0125,[('l' 1)]\n",
      "300_22,RNN,regular_test,M,0.5428571428571428,0.4857142857142857,0.0,0.5142857142857142,0.2,0.2857142857142857,0.0,[]\n",
      "300_22,RNN,regular_test,F,0.9354838709677419,0.8064516129032258,0.0,0.0,0.0,1.0,0.0,[]\n",
      "300_22,RNN,regular_test,N,0.6428571428571429,0.42857142857142855,0.0,0.6428571428571429,0.21428571428571427,0.07142857142857142,0.07142857142857142,[('l' 1)]\n",
      "300_22,RNN,genderless_test,All,0.4,0.2625,0.0125,0.25,0.3,0.4,0.0375,[('l' 2) ('m' 1)]\n",
      "300_22,RNN,genderless_test,M,0.4,0.2,0.02857142857142857,0.22857142857142856,0.22857142857142856,0.45714285714285713,0.05714285714285714,[('m' 1) ('l' 1)]\n",
      "300_22,RNN,genderless_test,F,0.3870967741935484,0.2903225806451613,0.0,0.25806451612903225,0.3548387096774194,0.3548387096774194,0.03225806451612903,[('l' 1)]\n",
      "300_22,RNN,genderless_test,N,0.42857142857142855,0.35714285714285715,0.0,0.2857142857142857,0.35714285714285715,0.35714285714285715,0.0,[]\n",
      "300_23,RNN,regular_test,All,0.825,0.7,0.0,0.325,0.1875,0.4625,0.025,[('l' 2)]\n",
      "300_23,RNN,regular_test,M,0.7777777777777778,0.6388888888888888,0.0,0.5277777777777778,0.25,0.19444444444444445,0.027777777777777776,[('l' 1)]\n",
      "300_23,RNN,regular_test,F,0.9333333333333333,0.8333333333333334,0.0,0.0,0.06666666666666667,0.9333333333333333,0.0,[]\n",
      "300_23,RNN,regular_test,N,0.7142857142857143,0.5714285714285714,0.0,0.5,0.2857142857142857,0.14285714285714285,0.07142857142857142,[('l' 1)]\n",
      "300_23,RNN,genderless_test,All,0.4375,0.3125,0.0125,0.1,0.3375,0.475,0.075,[('l' 5) ('c' 1)]\n",
      "300_23,RNN,genderless_test,M,0.5277777777777778,0.25,0.027777777777777776,0.08333333333333333,0.3611111111111111,0.4166666666666667,0.1111111111111111,[('l' 3) ('c' 1)]\n",
      "300_23,RNN,genderless_test,F,0.43333333333333335,0.43333333333333335,0.0,0.13333333333333333,0.3,0.5333333333333333,0.03333333333333333,[('l' 1)]\n",
      "300_23,RNN,genderless_test,N,0.21428571428571427,0.21428571428571427,0.0,0.07142857142857142,0.35714285714285715,0.5,0.07142857142857142,[('l' 1)]\n",
      "300_24,RNN,regular_test,All,0.65,0.625,0.0,0.3,0.125,0.5375,0.0375,[('l' 3)]\n",
      "300_24,RNN,regular_test,M,0.5609756097560976,0.6341463414634146,0.0,0.4634146341463415,0.1951219512195122,0.3170731707317073,0.024390243902439025,[('l' 1)]\n",
      "300_24,RNN,regular_test,F,0.8709677419354839,0.5806451612903226,0.0,0.0967741935483871,0.03225806451612903,0.8709677419354839,0.0,[]\n",
      "300_24,RNN,regular_test,N,0.25,0.75,0.0,0.25,0.125,0.375,0.25,[('l' 2)]\n",
      "300_24,RNN,genderless_test,All,0.3125,0.2625,0.0125,0.2125,0.3625,0.325,0.0875,[('l' 7)]\n",
      "300_24,RNN,genderless_test,M,0.2682926829268293,0.2682926829268293,0.024390243902439025,0.24390243902439024,0.2926829268292683,0.3170731707317073,0.12195121951219512,[('l' 5)]\n",
      "300_24,RNN,genderless_test,F,0.3870967741935484,0.25806451612903225,0.0,0.22580645161290322,0.41935483870967744,0.3225806451612903,0.03225806451612903,[('l' 1)]\n",
      "300_24,RNN,genderless_test,N,0.25,0.25,0.0,0.0,0.5,0.375,0.125,[('l' 1)]\n",
      "360_0,RNN,regular_test,All,0.7875,0.75,0.0375,0.25,0.1,0.575,0.0375,[('l' 2) ('z' 1)]\n",
      "360_0,RNN,regular_test,M,0.7857142857142857,0.6428571428571429,0.07142857142857142,0.5357142857142857,0.21428571428571427,0.10714285714285714,0.07142857142857142,[('l' 1) ('z' 1)]\n",
      "360_0,RNN,regular_test,F,0.9736842105263158,0.9736842105263158,0.02631578947368421,0.0,0.0,0.9736842105263158,0.0,[]\n",
      "360_0,RNN,regular_test,N,0.2857142857142857,0.35714285714285715,0.0,0.35714285714285715,0.14285714285714285,0.42857142857142855,0.07142857142857142,[('l' 1)]\n",
      "360_0,RNN,genderless_test,All,0.4125,0.2625,0.0375,0.3,0.2875,0.275,0.1,[('l' 4) ('t' 1) ('w' 1) ('z' 1) ('o' 1)]\n",
      "360_0,RNN,genderless_test,M,0.42857142857142855,0.25,0.07142857142857142,0.21428571428571427,0.42857142857142855,0.21428571428571427,0.07142857142857142,[('z' 1) ('l' 1)]\n",
      "360_0,RNN,genderless_test,F,0.3684210526315789,0.3157894736842105,0.02631578947368421,0.39473684210526316,0.21052631578947367,0.2894736842105263,0.07894736842105263,[('l' 2) ('w' 1)]\n",
      "360_0,RNN,genderless_test,N,0.5,0.14285714285714285,0.0,0.21428571428571427,0.21428571428571427,0.35714285714285715,0.21428571428571427,[('t' 1) ('l' 1) ('o' 1)]\n",
      "360_1,RNN,regular_test,All,0.8,0.6375,0.0125,0.35,0.1125,0.4875,0.0375,[('l' 3)]\n",
      "360_1,RNN,regular_test,M,0.7428571428571429,0.5714285714285714,0.02857142857142857,0.6,0.11428571428571428,0.2,0.05714285714285714,[('l' 2)]\n",
      "360_1,RNN,regular_test,F,0.967741935483871,0.8064516129032258,0.0,0.03225806451612903,0.0,0.967741935483871,0.0,[]\n",
      "360_1,RNN,regular_test,N,0.5714285714285714,0.42857142857142855,0.0,0.42857142857142855,0.35714285714285715,0.14285714285714285,0.07142857142857142,[('l' 1)]\n",
      "360_1,RNN,genderless_test,All,0.5375,0.35,0.0125,0.225,0.2375,0.4625,0.0625,[('l' 2) ('m' 1) ('c' 1) ('z' 1)]\n",
      "360_1,RNN,genderless_test,M,0.4857142857142857,0.22857142857142856,0.0,0.22857142857142856,0.2857142857142857,0.42857142857142855,0.05714285714285714,[('l' 1) ('z' 1)]\n",
      "360_1,RNN,genderless_test,F,0.6129032258064516,0.5161290322580645,0.03225806451612903,0.22580645161290322,0.12903225806451613,0.5161290322580645,0.0967741935483871,[('m' 1) ('l' 1) ('c' 1)]\n",
      "360_1,RNN,genderless_test,N,0.5,0.2857142857142857,0.0,0.21428571428571427,0.35714285714285715,0.42857142857142855,0.0,[]\n",
      "360_2,RNN,regular_test,All,0.75,0.575,0.0125,0.425,0.1375,0.4,0.025,[('l' 2)]\n",
      "360_2,RNN,regular_test,M,0.75,0.7222222222222222,0.0,0.6666666666666666,0.25,0.05555555555555555,0.027777777777777776,[('l' 1)]\n",
      "360_2,RNN,regular_test,F,0.8709677419354839,0.4838709677419355,0.0,0.0967741935483871,0.03225806451612903,0.8709677419354839,0.0,[]\n",
      "360_2,RNN,regular_test,N,0.46153846153846156,0.38461538461538464,0.07692307692307693,0.5384615384615384,0.07692307692307693,0.23076923076923078,0.07692307692307693,[('l' 1)]\n",
      "360_2,RNN,genderless_test,All,0.2625,0.2375,0.0125,0.35,0.4125,0.1625,0.0625,[('o' 2) ('l' 2) ('p' 1)]\n",
      "360_2,RNN,genderless_test,M,0.25,0.2222222222222222,0.0,0.3333333333333333,0.4444444444444444,0.16666666666666666,0.05555555555555555,[('o' 1) ('l' 1)]\n",
      "360_2,RNN,genderless_test,F,0.3225806451612903,0.22580645161290322,0.03225806451612903,0.3225806451612903,0.41935483870967744,0.16129032258064516,0.06451612903225806,[('p' 1) ('o' 1)]\n",
      "360_2,RNN,genderless_test,N,0.15384615384615385,0.3076923076923077,0.0,0.46153846153846156,0.3076923076923077,0.15384615384615385,0.07692307692307693,[('l' 1)]\n",
      "360_3,RNN,regular_test,All,0.7375,0.6875,0.0125,0.2125,0.175,0.575,0.025,[('l' 2)]\n",
      "360_3,RNN,regular_test,M,0.6923076923076923,0.6538461538461539,0.0,0.34615384615384615,0.23076923076923078,0.34615384615384615,0.07692307692307693,[('l' 2)]\n",
      "360_3,RNN,regular_test,F,0.9117647058823529,0.8529411764705882,0.029411764705882353,0.0,0.029411764705882353,0.9411764705882353,0.0,[]\n",
      "360_3,RNN,regular_test,N,0.5,0.45,0.0,0.4,0.35,0.25,0.0,[]\n",
      "360_3,RNN,genderless_test,All,0.4875,0.45,0.025,0.0625,0.1625,0.7375,0.0125,[('l' 1)]\n",
      "360_3,RNN,genderless_test,M,0.38461538461538464,0.2692307692307692,0.07692307692307693,0.038461538461538464,0.038461538461538464,0.8076923076923077,0.038461538461538464,[('l' 1)]\n",
      "360_3,RNN,genderless_test,F,0.5588235294117647,0.6176470588235294,0.0,0.11764705882352941,0.14705882352941177,0.7352941176470589,0.0,[]\n",
      "360_3,RNN,genderless_test,N,0.5,0.4,0.0,0.0,0.35,0.65,0.0,[]\n",
      "360_4,RNN,regular_test,All,0.7375,0.6375,0.0125,0.4125,0.0875,0.4375,0.05,[('l' 3) ('u' 1)]\n",
      "360_4,RNN,regular_test,M,0.7575757575757576,0.6666666666666666,0.030303030303030304,0.5454545454545454,0.18181818181818182,0.18181818181818182,0.06060606060606061,[('l' 2)]\n",
      "360_4,RNN,regular_test,F,0.8125,0.65625,0.0,0.125,0.0,0.84375,0.03125,[('u' 1)]\n",
      "360_4,RNN,regular_test,N,0.5333333333333333,0.5333333333333333,0.0,0.7333333333333333,0.06666666666666667,0.13333333333333333,0.06666666666666667,[('l' 1)]\n",
      "360_4,RNN,genderless_test,All,0.5875,0.4,0.025,0.35,0.125,0.4625,0.0375,[('l' 3)]\n",
      "360_4,RNN,genderless_test,M,0.5151515151515151,0.3333333333333333,0.06060606060606061,0.30303030303030304,0.21212121212121213,0.42424242424242425,0.0,[]\n",
      "360_4,RNN,genderless_test,F,0.625,0.40625,0.0,0.4375,0.0625,0.40625,0.09375,[('l' 3)]\n",
      "360_4,RNN,genderless_test,N,0.6666666666666666,0.5333333333333333,0.0,0.26666666666666666,0.06666666666666667,0.6666666666666666,0.0,[]\n",
      "360_5,RNN,regular_test,All,0.625,0.575,0.0125,0.2875,0.225,0.45,0.025,[('l' 2)]\n",
      "360_5,RNN,regular_test,M,0.4864864864864865,0.4594594594594595,0.0,0.4864864864864865,0.2972972972972973,0.1891891891891892,0.02702702702702703,[('l' 1)]\n",
      "360_5,RNN,regular_test,F,0.8214285714285714,0.7857142857142857,0.03571428571428571,0.0,0.07142857142857142,0.8928571428571429,0.0,[]\n",
      "360_5,RNN,regular_test,N,0.6,0.4666666666666667,0.0,0.3333333333333333,0.3333333333333333,0.26666666666666666,0.06666666666666667,[('l' 1)]\n",
      "360_5,RNN,genderless_test,All,0.3125,0.2125,0.0125,0.2125,0.5375,0.15,0.0875,[('l' 4) ('t' 1) ('o' 1) ('u' 1)]\n",
      "360_5,RNN,genderless_test,M,0.2702702702702703,0.16216216216216217,0.02702702702702703,0.24324324324324326,0.6216216216216216,0.08108108108108109,0.02702702702702703,[('u' 1)]\n",
      "360_5,RNN,genderless_test,F,0.42857142857142855,0.25,0.0,0.17857142857142858,0.4642857142857143,0.21428571428571427,0.14285714285714285,[('l' 2) ('t' 1) ('o' 1)]\n",
      "360_5,RNN,genderless_test,N,0.2,0.26666666666666666,0.0,0.2,0.4666666666666667,0.2,0.13333333333333333,[('l' 2)]\n",
      "360_6,RNN,regular_test,All,0.8625,0.675,0.0125,0.3375,0.125,0.5125,0.0125,[('l' 1)]\n",
      "360_6,RNN,regular_test,M,0.8611111111111112,0.6111111111111112,0.0,0.6111111111111112,0.25,0.1111111111111111,0.027777777777777776,[('l' 1)]\n",
      "360_6,RNN,regular_test,F,1.0,0.8,0.0,0.0,0.0,1.0,0.0,[]\n",
      "360_6,RNN,regular_test,N,0.3333333333333333,0.4444444444444444,0.1111111111111111,0.5555555555555556,0.1111111111111111,0.2222222222222222,0.0,[]\n",
      "360_6,RNN,genderless_test,All,0.6125,0.3125,0.025,0.2,0.1625,0.4875,0.125,[('l' 3) ('m' 1) ('u' 1) ('f' 1) ('a' 1)]\n",
      "360_6,RNN,genderless_test,M,0.5833333333333334,0.2777777777777778,0.05555555555555555,0.2222222222222222,0.1388888888888889,0.5,0.08333333333333333,[('l' 1) ('m' 1) ('a' 1)]\n",
      "360_6,RNN,genderless_test,F,0.6,0.3142857142857143,0.0,0.2,0.22857142857142856,0.42857142857142855,0.14285714285714285,[('u' 1) ('f' 1) ('h' 1) ('i' 1) ('c' 1)]\n",
      "360_6,RNN,genderless_test,N,0.7777777777777778,0.4444444444444444,0.0,0.1111111111111111,0.0,0.6666666666666666,0.2222222222222222,[('l' 2)]\n",
      "360_7,RNN,regular_test,All,0.8,0.5875,0.0,0.3875,0.125,0.4875,0.0,[]\n",
      "360_7,RNN,regular_test,M,0.8181818181818182,0.696969696969697,0.0,0.6060606060606061,0.2727272727272727,0.12121212121212122,0.0,[]\n",
      "360_7,RNN,regular_test,F,0.9666666666666667,0.5,0.0,0.03333333333333333,0.0,0.9666666666666667,0.0,[]\n",
      "360_7,RNN,regular_test,N,0.47058823529411764,0.5294117647058824,0.0,0.5882352941176471,0.058823529411764705,0.35294117647058826,0.0,[]\n",
      "360_7,RNN,genderless_test,All,0.4125,0.4125,0.0125,0.5625,0.175,0.25,0.0,[]\n",
      "360_7,RNN,genderless_test,M,0.45454545454545453,0.45454545454545453,0.030303030303030304,0.696969696969697,0.09090909090909091,0.18181818181818182,0.0,[]\n",
      "360_7,RNN,genderless_test,F,0.3,0.36666666666666664,0.0,0.5,0.23333333333333334,0.26666666666666666,0.0,[]\n",
      "360_7,RNN,genderless_test,N,0.5294117647058824,0.4117647058823529,0.0,0.4117647058823529,0.23529411764705882,0.35294117647058826,0.0,[]\n",
      "360_8,RNN,regular_test,All,0.75,0.6125,0.0375,0.3375,0.1625,0.375,0.0875,[('l' 6) ('t' 1)]\n",
      "360_8,RNN,regular_test,M,0.7692307692307693,0.7435897435897436,0.02564102564102564,0.6153846153846154,0.20512820512820512,0.02564102564102564,0.1282051282051282,[('l' 4) ('t' 1)]\n",
      "360_8,RNN,regular_test,F,0.8666666666666667,0.5666666666666667,0.03333333333333333,0.03333333333333333,0.0,0.9,0.03333333333333333,[('l' 1)]\n",
      "360_8,RNN,regular_test,N,0.36363636363636365,0.2727272727272727,0.09090909090909091,0.18181818181818182,0.45454545454545453,0.18181818181818182,0.09090909090909091,[('l' 1)]\n",
      "360_8,RNN,genderless_test,All,0.5375,0.3625,0.025,0.3625,0.1625,0.3375,0.1125,[('l' 5) ('a' 2) ('h' 1) ('c' 1)]\n",
      "360_8,RNN,genderless_test,M,0.46153846153846156,0.3076923076923077,0.0,0.38461538461538464,0.1794871794871795,0.3076923076923077,0.1282051282051282,[('l' 3) ('a' 1) ('h' 1)]\n",
      "360_8,RNN,genderless_test,F,0.6666666666666666,0.5,0.0,0.4,0.13333333333333333,0.36666666666666664,0.1,[('l' 2) ('c' 1)]\n",
      "360_8,RNN,genderless_test,N,0.45454545454545453,0.18181818181818182,0.18181818181818182,0.18181818181818182,0.18181818181818182,0.36363636363636365,0.09090909090909091,[('a' 1)]\n",
      "360_9,RNN,regular_test,All,0.7375,0.5375,0.0,0.475,0.1125,0.4,0.0125,[('l' 1)]\n",
      "360_9,RNN,regular_test,M,0.7073170731707317,0.4878048780487805,0.0,0.7560975609756098,0.0975609756097561,0.12195121951219512,0.024390243902439025,[('l' 1)]\n",
      "360_9,RNN,regular_test,F,0.96,0.8,0.0,0.0,0.0,1.0,0.0,[]\n",
      "360_9,RNN,regular_test,N,0.42857142857142855,0.21428571428571427,0.0,0.5,0.35714285714285715,0.14285714285714285,0.0,[]\n",
      "360_9,RNN,genderless_test,All,0.475,0.4,0.0875,0.25,0.1,0.4875,0.075,[('c' 2) ('l' 2) ('a' 1) ('o' 1)]\n",
      "360_9,RNN,genderless_test,M,0.5121951219512195,0.36585365853658536,0.12195121951219512,0.3170731707317073,0.0975609756097561,0.3902439024390244,0.07317073170731707,[('l' 2) ('o' 1)]\n",
      "360_9,RNN,genderless_test,F,0.48,0.52,0.08,0.16,0.12,0.56,0.08,[('c' 1) ('a' 1)]\n",
      "360_9,RNN,genderless_test,N,0.35714285714285715,0.2857142857142857,0.0,0.21428571428571427,0.07142857142857142,0.6428571428571429,0.07142857142857142,[('c' 1)]\n",
      "360_10,RNN,regular_test,All,0.8125,0.6125,0.025,0.3375,0.175,0.425,0.0375,[('l' 3)]\n",
      "360_10,RNN,regular_test,M,0.7105263157894737,0.5789473684210527,0.0,0.5263157894736842,0.2894736842105263,0.13157894736842105,0.05263157894736842,[('l' 2)]\n",
      "360_10,RNN,regular_test,F,0.9,0.6666666666666666,0.06666666666666667,0.03333333333333333,0.0,0.9,0.0,[]\n",
      "360_10,RNN,regular_test,N,0.9166666666666666,0.5833333333333334,0.0,0.5,0.25,0.16666666666666666,0.08333333333333333,[('l' 1)]\n",
      "360_10,RNN,genderless_test,All,0.375,0.25,0.075,0.3625,0.325,0.1625,0.075,[('l' 5) ('f' 1)]\n",
      "360_10,RNN,genderless_test,M,0.39473684210526316,0.3157894736842105,0.07894736842105263,0.39473684210526316,0.2894736842105263,0.21052631578947367,0.02631578947368421,[('l' 1)]\n",
      "360_10,RNN,genderless_test,F,0.3333333333333333,0.16666666666666666,0.1,0.4,0.3,0.13333333333333333,0.06666666666666667,[('l' 1) ('f' 1)]\n",
      "360_10,RNN,genderless_test,N,0.4166666666666667,0.25,0.0,0.16666666666666666,0.5,0.08333333333333333,0.25,[('l' 3)]\n",
      "360_11,RNN,regular_test,All,0.825,0.575,0.0125,0.4375,0.0875,0.4,0.0625,[('l' 4) ('m' 1)]\n",
      "360_11,RNN,regular_test,M,0.75,0.45,0.025,0.7,0.15,0.075,0.05,[('l' 2)]\n",
      "360_11,RNN,regular_test,F,0.9310344827586207,0.7931034482758621,0.0,0.034482758620689655,0.0,0.9655172413793104,0.0,[]\n",
      "360_11,RNN,regular_test,N,0.8181818181818182,0.45454545454545453,0.0,0.5454545454545454,0.09090909090909091,0.09090909090909091,0.2727272727272727,[('l' 2) ('m' 1)]\n",
      "360_11,RNN,genderless_test,All,0.6,0.3125,0.0,0.3125,0.125,0.5,0.0625,[('l' 4) ('m' 1)]\n",
      "360_11,RNN,genderless_test,M,0.625,0.25,0.0,0.325,0.125,0.5,0.05,[('l' 2)]\n",
      "360_11,RNN,genderless_test,F,0.5172413793103449,0.3793103448275862,0.0,0.3448275862068966,0.10344827586206896,0.4482758620689655,0.10344827586206896,[('l' 2) ('m' 1)]\n",
      "360_11,RNN,genderless_test,N,0.7272727272727273,0.36363636363636365,0.0,0.18181818181818182,0.18181818181818182,0.6363636363636364,0.0,[]\n",
      "360_12,RNN,regular_test,All,0.725,0.625,0.0125,0.325,0.1375,0.5,0.025,[('u' 1) ('f' 1)]\n",
      "360_12,RNN,regular_test,M,0.6578947368421053,0.5526315789473685,0.0,0.5263157894736842,0.18421052631578946,0.2631578947368421,0.02631578947368421,[('f' 1)]\n",
      "360_12,RNN,regular_test,F,0.9642857142857143,0.8571428571428571,0.0,0.0,0.0,0.9642857142857143,0.03571428571428571,[('u' 1)]\n",
      "360_12,RNN,regular_test,N,0.42857142857142855,0.35714285714285715,0.07142857142857142,0.42857142857142855,0.2857142857142857,0.21428571428571427,0.0,[]\n",
      "360_12,RNN,genderless_test,All,0.475,0.425,0.025,0.125,0.25,0.5625,0.0375,[('l' 2) ('-' 1)]\n",
      "360_12,RNN,genderless_test,M,0.47368421052631576,0.39473684210526316,0.05263157894736842,0.10526315789473684,0.23684210526315788,0.6052631578947368,0.0,[]\n",
      "360_12,RNN,genderless_test,F,0.5,0.42857142857142855,0.0,0.14285714285714285,0.25,0.5357142857142857,0.07142857142857142,[('l' 2)]\n",
      "360_12,RNN,genderless_test,N,0.42857142857142855,0.5,0.0,0.14285714285714285,0.2857142857142857,0.5,0.07142857142857142,[('-' 1)]\n",
      "360_13,RNN,regular_test,All,0.8125,0.7125,0.0,0.35,0.0875,0.55,0.0125,[('l' 1)]\n",
      "360_13,RNN,regular_test,M,0.8181818181818182,0.6363636363636364,0.0,0.5757575757575758,0.21212121212121213,0.18181818181818182,0.030303030303030304,[('l' 1)]\n",
      "360_13,RNN,regular_test,F,0.9705882352941176,0.8235294117647058,0.0,0.029411764705882353,0.0,0.9705882352941176,0.0,[]\n",
      "360_13,RNN,regular_test,N,0.38461538461538464,0.6153846153846154,0.0,0.6153846153846154,0.0,0.38461538461538464,0.0,[]\n",
      "360_13,RNN,genderless_test,All,0.5125,0.425,0.0,0.2375,0.1875,0.5375,0.0375,[('l' 2) ('k' 1)]\n",
      "360_13,RNN,genderless_test,M,0.6666666666666666,0.42424242424242425,0.0,0.24242424242424243,0.12121212121212122,0.6363636363636364,0.0,[]\n",
      "360_13,RNN,genderless_test,F,0.35294117647058826,0.5,0.0,0.17647058823529413,0.23529411764705882,0.5,0.08823529411764706,[('l' 2) ('k' 1)]\n",
      "360_13,RNN,genderless_test,N,0.5384615384615384,0.23076923076923078,0.0,0.38461538461538464,0.23076923076923078,0.38461538461538464,0.0,[]\n",
      "360_14,RNN,regular_test,All,0.8125,0.6875,0.05,0.3125,0.1625,0.4625,0.0125,[('l' 1)]\n",
      "360_14,RNN,regular_test,M,0.8,0.6,0.05714285714285714,0.4857142857142857,0.2857142857142857,0.14285714285714285,0.02857142857142857,[('l' 1)]\n",
      "360_14,RNN,regular_test,F,0.9629629629629629,0.8888888888888888,0.0,0.0,0.037037037037037035,0.9629629629629629,0.0,[]\n",
      "360_14,RNN,regular_test,N,0.6111111111111112,0.5555555555555556,0.1111111111111111,0.4444444444444444,0.1111111111111111,0.3333333333333333,0.0,[]\n",
      "360_14,RNN,genderless_test,All,0.3875,0.25,0.05,0.2875,0.2125,0.3125,0.1375,[('l' 4) ('m' 2) ('f' 1) ('o' 1) ('a' 1)]\n",
      "360_14,RNN,genderless_test,M,0.3142857142857143,0.17142857142857143,0.02857142857142857,0.3142857142857143,0.2,0.2571428571428571,0.2,[('l' 2) ('m' 2) ('o' 1) ('a' 1) ('u' 1)]\n",
      "360_14,RNN,genderless_test,F,0.5555555555555556,0.37037037037037035,0.037037037037037035,0.2962962962962963,0.2222222222222222,0.4074074074074074,0.037037037037037035,[('c' 1)]\n",
      "360_14,RNN,genderless_test,N,0.2777777777777778,0.2222222222222222,0.1111111111111111,0.2222222222222222,0.2222222222222222,0.2777777777777778,0.16666666666666666,[('l' 2) ('f' 1)]\n",
      "360_15,RNN,regular_test,All,0.6875,0.6,0.025,0.35,0.15,0.425,0.05,[('l' 4)]\n",
      "360_15,RNN,regular_test,M,0.6363636363636364,0.5757575757575758,0.030303030303030304,0.5454545454545454,0.24242424242424243,0.12121212121212122,0.06060606060606061,[('l' 2)]\n",
      "360_15,RNN,regular_test,F,0.7647058823529411,0.6176470588235294,0.029411764705882353,0.11764705882352941,0.058823529411764705,0.7647058823529411,0.029411764705882353,[('l' 1)]\n",
      "360_15,RNN,regular_test,N,0.6153846153846154,0.6153846153846154,0.0,0.46153846153846156,0.15384615384615385,0.3076923076923077,0.07692307692307693,[('l' 1)]\n",
      "360_15,RNN,genderless_test,All,0.4,0.3375,0.05,0.3375,0.125,0.325,0.1625,[('l' 11) ('t' 1) ('i' 1)]\n",
      "360_15,RNN,genderless_test,M,0.3333333333333333,0.2727272727272727,0.09090909090909091,0.3333333333333333,0.030303030303030304,0.3333333333333333,0.21212121212121213,[('l' 6) ('i' 1)]\n",
      "360_15,RNN,genderless_test,F,0.5,0.38235294117647056,0.029411764705882353,0.35294117647058826,0.14705882352941177,0.35294117647058826,0.11764705882352941,[('l' 3) ('t' 1)]\n",
      "360_15,RNN,genderless_test,N,0.3076923076923077,0.38461538461538464,0.0,0.3076923076923077,0.3076923076923077,0.23076923076923078,0.15384615384615385,[('l' 2)]\n",
      "360_16,RNN,regular_test,All,0.775,0.425,0.0125,0.3625,0.175,0.4375,0.0125,[('l' 1)]\n",
      "360_16,RNN,regular_test,M,0.7368421052631579,0.5,0.02631578947368421,0.631578947368421,0.18421052631578946,0.13157894736842105,0.02631578947368421,[('l' 1)]\n",
      "360_16,RNN,regular_test,F,0.9032258064516129,0.3548387096774194,0.0,0.0,0.06451612903225806,0.9354838709677419,0.0,[]\n",
      "360_16,RNN,regular_test,N,0.5454545454545454,0.36363636363636365,0.0,0.45454545454545453,0.45454545454545453,0.09090909090909091,0.0,[]\n",
      "360_16,RNN,genderless_test,All,0.4,0.25,0.0375,0.075,0.35,0.5125,0.025,[('l' 2)]\n",
      "360_16,RNN,genderless_test,M,0.4473684210526316,0.23684210526315788,0.02631578947368421,0.07894736842105263,0.34210526315789475,0.5263157894736842,0.02631578947368421,[('l' 1)]\n",
      "360_16,RNN,genderless_test,F,0.2903225806451613,0.1935483870967742,0.06451612903225806,0.0967741935483871,0.3870967741935484,0.41935483870967744,0.03225806451612903,[('l' 1)]\n",
      "360_16,RNN,genderless_test,N,0.5454545454545454,0.45454545454545453,0.0,0.0,0.2727272727272727,0.7272727272727273,0.0,[]\n",
      "360_17,RNN,regular_test,All,0.6875,0.65,0.025,0.3,0.1375,0.5125,0.025,[('l' 2)]\n",
      "360_17,RNN,regular_test,M,0.6363636363636364,0.5454545454545454,0.0,0.5454545454545454,0.21212121212121213,0.24242424242424243,0.0,[]\n",
      "360_17,RNN,regular_test,F,0.9333333333333333,0.8333333333333334,0.0,0.06666666666666667,0.0,0.9333333333333333,0.0,[]\n",
      "360_17,RNN,regular_test,N,0.35294117647058826,0.5294117647058824,0.11764705882352941,0.23529411764705882,0.23529411764705882,0.29411764705882354,0.11764705882352941,[('l' 2)]\n",
      "360_17,RNN,genderless_test,All,0.525,0.45,0.05,0.1375,0.2125,0.5375,0.0625,[('g' 1) ('k' 1) ('i' 1) ('l' 1) ('t' 1)]\n",
      "360_17,RNN,genderless_test,M,0.48484848484848486,0.36363636363636365,0.06060606060606061,0.15151515151515152,0.21212121212121213,0.5151515151515151,0.06060606060606061,[('k' 1) ('l' 1)]\n",
      "360_17,RNN,genderless_test,F,0.5,0.6,0.03333333333333333,0.03333333333333333,0.26666666666666666,0.6,0.06666666666666667,[('g' 1) ('i' 1)]\n",
      "360_17,RNN,genderless_test,N,0.6470588235294118,0.35294117647058826,0.058823529411764705,0.29411764705882354,0.11764705882352941,0.47058823529411764,0.058823529411764705,[('t' 1)]\n",
      "360_18,RNN,regular_test,All,0.7375,0.6625,0.0,0.45,0.075,0.4375,0.0375,[('l' 3)]\n",
      "360_18,RNN,regular_test,M,0.6666666666666666,0.6944444444444444,0.0,0.6388888888888888,0.1111111111111111,0.16666666666666666,0.08333333333333333,[('l' 3)]\n",
      "360_18,RNN,regular_test,F,0.8125,0.75,0.0,0.15625,0.03125,0.8125,0.0,[]\n",
      "360_18,RNN,regular_test,N,0.75,0.3333333333333333,0.0,0.6666666666666666,0.08333333333333333,0.25,0.0,[]\n",
      "360_18,RNN,genderless_test,All,0.4375,0.35,0.0,0.1875,0.2,0.575,0.0375,[('m' 1) ('l' 1) ('t' 1)]\n",
      "360_18,RNN,genderless_test,M,0.4166666666666667,0.2777777777777778,0.0,0.2222222222222222,0.16666666666666666,0.5555555555555556,0.05555555555555555,[('m' 1) ('t' 1)]\n",
      "360_18,RNN,genderless_test,F,0.4375,0.46875,0.0,0.21875,0.21875,0.53125,0.03125,[('l' 1)]\n",
      "360_18,RNN,genderless_test,N,0.5,0.25,0.0,0.0,0.25,0.75,0.0,[]\n",
      "360_19,RNN,regular_test,All,0.8125,0.6375,0.025,0.4,0.1625,0.4,0.0125,[('m' 1)]\n",
      "360_19,RNN,regular_test,M,0.7317073170731707,0.5121951219512195,0.04878048780487805,0.6341463414634146,0.21951219512195122,0.07317073170731707,0.024390243902439025,[('m' 1)]\n",
      "360_19,RNN,regular_test,F,1.0,0.9166666666666666,0.0,0.0,0.0,1.0,0.0,[]\n",
      "360_19,RNN,regular_test,N,0.7333333333333333,0.5333333333333333,0.0,0.4,0.26666666666666666,0.3333333333333333,0.0,[]\n",
      "360_19,RNN,genderless_test,All,0.375,0.275,0.0,0.3625,0.3875,0.225,0.025,[('g' 1) ('m' 1)]\n",
      "360_19,RNN,genderless_test,M,0.34146341463414637,0.34146341463414637,0.0,0.3170731707317073,0.36585365853658536,0.2926829268292683,0.024390243902439025,[('g' 1)]\n",
      "360_19,RNN,genderless_test,F,0.4166666666666667,0.20833333333333334,0.0,0.375,0.4583333333333333,0.16666666666666666,0.0,[]\n",
      "360_19,RNN,genderless_test,N,0.4,0.2,0.0,0.4666666666666667,0.3333333333333333,0.13333333333333333,0.06666666666666667,[('m' 1)]\n",
      "360_20,RNN,regular_test,All,0.775,0.7125,0.0,0.3375,0.1125,0.5375,0.0125,[('l' 1)]\n",
      "360_20,RNN,regular_test,M,0.6410256410256411,0.6666666666666666,0.0,0.5128205128205128,0.1282051282051282,0.3333333333333333,0.02564102564102564,[('l' 1)]\n",
      "360_20,RNN,regular_test,F,1.0,0.8333333333333334,0.0,0.0,0.0,1.0,0.0,[]\n",
      "360_20,RNN,regular_test,N,0.6363636363636364,0.5454545454545454,0.0,0.6363636363636364,0.36363636363636365,0.0,0.0,[]\n",
      "360_20,RNN,genderless_test,All,0.475,0.2125,0.0125,0.3125,0.3125,0.3125,0.05,[('l' 2) ('h' 2)]\n",
      "360_20,RNN,genderless_test,M,0.48717948717948717,0.23076923076923078,0.02564102564102564,0.358974358974359,0.28205128205128205,0.3076923076923077,0.02564102564102564,[('l' 1)]\n",
      "360_20,RNN,genderless_test,F,0.43333333333333335,0.26666666666666666,0.0,0.23333333333333334,0.3333333333333333,0.3333333333333333,0.1,[('h' 2) ('l' 1)]\n",
      "360_20,RNN,genderless_test,N,0.5454545454545454,0.0,0.0,0.36363636363636365,0.36363636363636365,0.2727272727272727,0.0,[]\n",
      "360_21,RNN,regular_test,All,0.7125,0.6375,0.0125,0.425,0.125,0.3875,0.05,[('l' 2) ('m' 1) ('o' 1)]\n",
      "360_21,RNN,regular_test,M,0.7941176470588235,0.6470588235294118,0.029411764705882353,0.6176470588235294,0.14705882352941177,0.14705882352941177,0.058823529411764705,[('l' 2)]\n",
      "360_21,RNN,regular_test,F,0.88,0.76,0.0,0.04,0.04,0.92,0.0,[]\n",
      "360_21,RNN,regular_test,N,0.38095238095238093,0.47619047619047616,0.0,0.5714285714285714,0.19047619047619047,0.14285714285714285,0.09523809523809523,[('m' 1) ('o' 1)]\n",
      "360_21,RNN,genderless_test,All,0.35,0.25,0.025,0.4625,0.2875,0.1375,0.0875,[('l' 6) ('m' 1)]\n",
      "360_21,RNN,genderless_test,M,0.38235294117647056,0.23529411764705882,0.058823529411764705,0.5588235294117647,0.2647058823529412,0.08823529411764706,0.029411764705882353,[('l' 1)]\n",
      "360_21,RNN,genderless_test,F,0.36,0.16,0.0,0.4,0.32,0.16,0.12,[('l' 3)]\n",
      "360_21,RNN,genderless_test,N,0.2857142857142857,0.38095238095238093,0.0,0.38095238095238093,0.2857142857142857,0.19047619047619047,0.14285714285714285,[('l' 2) ('m' 1)]\n",
      "360_22,RNN,regular_test,All,0.75,0.5875,0.025,0.3875,0.1375,0.4,0.05,[('o' 1) ('g' 1) ('l' 1) ('k' 1)]\n",
      "360_22,RNN,regular_test,M,0.7428571428571429,0.37142857142857144,0.02857142857142857,0.6857142857142857,0.17142857142857143,0.05714285714285714,0.05714285714285714,[('o' 1) ('g' 1)]\n",
      "360_22,RNN,regular_test,F,0.8387096774193549,0.8064516129032258,0.03225806451612903,0.0,0.06451612903225806,0.9032258064516129,0.0,[]\n",
      "360_22,RNN,regular_test,N,0.5714285714285714,0.6428571428571429,0.0,0.5,0.21428571428571427,0.14285714285714285,0.14285714285714285,[('l' 1) ('k' 1)]\n",
      "360_22,RNN,genderless_test,All,0.425,0.3,0.025,0.4125,0.3125,0.25,0.0,[]\n",
      "360_22,RNN,genderless_test,M,0.45714285714285713,0.2571428571428571,0.0,0.5142857142857142,0.2857142857142857,0.2,0.0,[]\n",
      "360_22,RNN,genderless_test,F,0.4838709677419355,0.3870967741935484,0.06451612903225806,0.3225806451612903,0.2903225806451613,0.3225806451612903,0.0,[]\n",
      "360_22,RNN,genderless_test,N,0.21428571428571427,0.21428571428571427,0.0,0.35714285714285715,0.42857142857142855,0.21428571428571427,0.0,[]\n",
      "360_23,RNN,regular_test,All,0.8125,0.7125,0.0375,0.3875,0.0875,0.45,0.0375,[('l' 3)]\n",
      "360_23,RNN,regular_test,M,0.75,0.6666666666666666,0.08333333333333333,0.5277777777777778,0.19444444444444445,0.16666666666666666,0.027777777777777776,[('l' 1)]\n",
      "360_23,RNN,regular_test,F,0.9333333333333333,0.8333333333333334,0.0,0.06666666666666667,0.0,0.9333333333333333,0.0,[]\n",
      "360_23,RNN,regular_test,N,0.7142857142857143,0.5714285714285714,0.0,0.7142857142857143,0.0,0.14285714285714285,0.14285714285714285,[('l' 2)]\n",
      "360_23,RNN,genderless_test,All,0.5125,0.4625,0.0125,0.05,0.1125,0.825,0.0,[]\n",
      "360_23,RNN,genderless_test,M,0.4166666666666667,0.2222222222222222,0.0,0.05555555555555555,0.19444444444444445,0.75,0.0,[]\n",
      "360_23,RNN,genderless_test,F,0.5333333333333333,0.8333333333333334,0.03333333333333333,0.0,0.06666666666666667,0.9,0.0,[]\n",
      "360_23,RNN,genderless_test,N,0.7142857142857143,0.2857142857142857,0.0,0.14285714285714285,0.0,0.8571428571428571,0.0,[]\n",
      "360_24,RNN,regular_test,All,0.775,0.6375,0.0,0.375,0.0625,0.55,0.0125,[('l' 1)]\n",
      "360_24,RNN,regular_test,M,0.6829268292682927,0.7073170731707317,0.0,0.6097560975609756,0.07317073170731707,0.2926829268292683,0.024390243902439025,[('l' 1)]\n",
      "360_24,RNN,regular_test,F,0.9354838709677419,0.5806451612903226,0.0,0.0,0.06451612903225806,0.9354838709677419,0.0,[]\n",
      "360_24,RNN,regular_test,N,0.625,0.5,0.0,0.625,0.0,0.375,0.0,[]\n",
      "360_24,RNN,genderless_test,All,0.475,0.275,0.025,0.225,0.3875,0.3625,0.0,[]\n",
      "360_24,RNN,genderless_test,M,0.43902439024390244,0.3170731707317073,0.0,0.1951219512195122,0.43902439024390244,0.36585365853658536,0.0,[]\n",
      "360_24,RNN,genderless_test,F,0.5483870967741935,0.22580645161290322,0.06451612903225806,0.3225806451612903,0.2903225806451613,0.3225806451612903,0.0,[]\n",
      "360_24,RNN,genderless_test,N,0.375,0.25,0.0,0.0,0.5,0.5,0.0,[]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# print('Data Type:', data_type)\n",
    "\n",
    "def analyze_by_inflection(preds):\n",
    "  inflections = [p[-1] if len(p) > 1 else '-' for p in preds]\n",
    "  s_r = sum([t == 's' for t in inflections])/len(inflections)\n",
    "  print('-s', s_r)\n",
    "  e_r = sum([t == 'e' for t in inflections])/len(inflections)\n",
    "  print('-e', e_r)\n",
    "  r_r = sum([t == 'r' for t in inflections])/len(inflections)\n",
    "  print('-r', r_r) \n",
    "  n_r = sum([t == 'n' for t in inflections])/len(inflections)\n",
    "  print('-n', n_r)\n",
    "  other = [t for t in inflections if t not in ['n','e','r','s']]\n",
    "  o_r = len(other)/len(inflections)\n",
    "  other_most_common = Counter(other).most_common(5)\n",
    "  print('Other:', o_r, other_most_common)\n",
    "  return s_r, e_r, r_r, n_r, o_r, other_most_common\n",
    "\n",
    "def frequency_test(train_pairs, srclines, predlines, c=1):\n",
    "  # Frequency test\n",
    "  # For each of the verbs in the test set, get the original ending. Then, get the predictions for number of times that ending appears \n",
    "  nouns_with_inflections_matching_train = [] \n",
    "  for i,noun in enumerate(srclines):\n",
    "    noun = noun.replace(' ','')\n",
    "    ending = noun[-c:]\n",
    "    predicted_inflection = predlines[i].replace(' ','')[-c:] if len(predlines[i]) > 1 else '-'\n",
    "    train_pairs_with_same_ending = [(s,t) for s,t in train_pairs if s[-c:] == ending]\n",
    "    # get most frequent inflection for the train data verbs that share the ending\n",
    "    inflections = [t[-c:] for s,t in train_pairs_with_same_ending]\n",
    "    if not inflections: continue\n",
    "    popular_inflection = Counter(inflections).most_common(1)[0][0]\n",
    "    if predicted_inflection == popular_inflection:\n",
    "      nouns_with_inflections_matching_train.append((noun, predicted_inflection, popular_inflection))\n",
    "\n",
    "  print(f'% of inflections (len {c}) that match most popular in training:', len(nouns_with_inflections_matching_train)/len(srclines))\n",
    "  return len(nouns_with_inflections_matching_train)/len(srclines)\n",
    "\n",
    "results = []\n",
    "for i, data_type in enumerate(datasizes):\n",
    "  print(f'\\nData type: {data_type}')\n",
    "  datadir = f'drive/MyDrive/GermanToleranceBaselineCogSci/processed_data/{data_type}'\n",
    "  \n",
    "  train_tgt_lines = open(f'{datadir}/german-tgt-train.txt','r').read().splitlines()\n",
    "  train_src_lines = open(f'{datadir}/german-src-train.txt','r').read().splitlines()\n",
    "  train_pairs = list(zip([t.replace(' ','') for t in train_src_lines], \n",
    "                       [t.replace(' ','') for t in train_tgt_lines]))\n",
    "  \n",
    "  for model in ['rnn']:\n",
    "    srclines = open(f'{datadir}/german-src-test.txt','r').read().splitlines()\n",
    "    for condition, tgtlines, predlines in [('regular_test', open(f'{datadir}/german-tgt-test.txt','r').read().splitlines(), open(f'{outdir}/german-{model}-{data_type}-pred.txt','r').read().splitlines()),\n",
    "                                           ('genderless_test', open(f'{datadir}/german-tgt-test-genderless.txt','r').read().splitlines(), open(f'{outdir}/german-{model}-{data_type}-genderless-pred.txt','r').read().splitlines())]:\n",
    "      print(f'\\n\\n{model} {condition} results')\n",
    "      predlines = [p if len(p) > 1 else '-' for p in predlines]\n",
    "      tups = list(zip(srclines,tgtlines,predlines))\n",
    "\n",
    "      m = []\n",
    "      f = []\n",
    "      n = []\n",
    "      for src,tgt,pred in tups:\n",
    "        # get the learned inflection\n",
    "        src,tgt,pred = src.strip(), tgt.strip(), pred.strip()\n",
    "        gender = src.split()[0]\n",
    "        if gender == 'MAS': m.append((src,tgt,pred))\n",
    "        elif gender == 'FEM': f.append((src,tgt,pred))\n",
    "        elif gender == 'NTR': n.append((src,tgt,pred))\n",
    "\n",
    "      accuracy = sum([t[1][-1]==t[2][-1] for t in tups])/len(tups)\n",
    "      freq = frequency_test(train_pairs, srclines, predlines)\n",
    "      print('Test accuracy:', accuracy) \n",
    "      ans = analyze_by_inflection(predlines)\n",
    "      results.append((data_type, model.upper(), condition, 'All', accuracy, freq, *ans)) \n",
    "      \n",
    "      m_acc = sum([t[1][-1]==t[2][-1] for t in m])/len(m)\n",
    "      l1,_, l2 = zip(*m)\n",
    "      freq = frequency_test(train_pairs, l1, l2)\n",
    "      print('M Accuracy:',m_acc)\n",
    "      ans = analyze_by_inflection(t[2] for t in m)\n",
    "      results.append((data_type, model.upper(), condition, 'M', m_acc, freq, *ans))\n",
    "\n",
    "      f_acc = sum([t[1][-1]==t[2][-1] for t in f])/len(f)\n",
    "      l1,_, l2 = zip(*f)\n",
    "      freq = frequency_test(train_pairs, l1, l2)\n",
    "      print('F Accuracy:',f_acc)\n",
    "      ans = analyze_by_inflection(t[2] for t in f)\n",
    "      results.append((data_type, model.upper(), condition, 'F', f_acc, freq, *ans))\n",
    "\n",
    "      n_acc = sum([t[1][-1]==t[2][-1] for t in n])/len(n)\n",
    "      l1,_, l2 = zip(*n)\n",
    "      freq = frequency_test(train_pairs, l1, l2)\n",
    "      print('N Accuracy:',n_acc)\n",
    "      ans = analyze_by_inflection(t[2] for t in n)\n",
    "      results.append((data_type, model.upper(), condition, 'N', n_acc, freq, *ans))\n",
    "      \n",
    "print()\n",
    "print(f'data_type, {model.upper()}, condition, test_subset, acc, freq, s, e, r, n, others, others_most_common')\n",
    "for r in results:\n",
    "  print(','.join(str(x).replace(',','') for x in r))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "gLHSxL9lDr9J"
   ],
   "machine_shape": "hm",
   "name": "CogSci - German Tolerance Baseline",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
