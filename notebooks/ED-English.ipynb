{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tj_h6vayxLh5"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "paPA2HL5w3T1",
    "outputId": "d766fce0-281e-45cb-b5e5-9894bfc4a2ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Cloning into 'OpenNMT-py'...\n",
      "remote: Enumerating objects: 134, done.\u001b[K\n",
      "remote: Counting objects: 100% (134/134), done.\u001b[K\n",
      "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
      "remote: Total 17083 (delta 76), reused 46 (delta 19), pack-reused 16949\u001b[K\n",
      "Receiving objects: 100% (17083/17083), 273.24 MiB | 18.98 MiB/s, done.\n",
      "Resolving deltas: 100% (12297/12297), done.\n",
      "Collecting OpenNMT-py\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/62/2c50d622c24cdce54523ec64051511793661ec14d396e05875597befa00d/OpenNMT_py-2.0.1-py3-none-any.whl (207kB)\n",
      "\u001b[K     |████████████████████████████████| 215kB 4.1MB/s \n",
      "\u001b[?25hCollecting waitress==1.4.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/d1/5209fb8c764497a592363c47054436a515b47b8c3e4970ddd7184f088857/waitress-1.4.4-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 8.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: flask==1.1.2 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.1.2)\n",
      "Collecting torch==1.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/53/914885a93a44b96c0dd1c36f36ff10afe341f091230aad68f7228d61db1e/torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n",
      "\u001b[K     |████████████████████████████████| 748.8MB 22kB/s \n",
      "\u001b[?25hCollecting tqdm<5,>=4.51\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/02/8f8880a4fd6625461833abcf679d4c12a44c76f9925f92bf212bb6cefaad/tqdm-4.56.0-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 10.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorboard<3,>=2.3 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (2.4.1)\n",
      "Collecting torchtext==0.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 11.2MB/s \n",
      "\u001b[?25hCollecting pyonmttok<2,>=1.23; platform_system == \"Linux\" or platform_system == \"Darwin\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/02/45433f86ff7dbccae319f4aff32187fa9e9c9c3204c9f7f7b07b5c9aa9dc/pyonmttok-1.23.0-cp36-cp36m-manylinux1_x86_64.whl (2.5MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5MB 56.3MB/s \n",
      "\u001b[?25hCollecting configargparse<2,>=1.2.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/79/3045743bb26ca2e44a1d317c37395462bfed82dbbd38e69a3280b63696ce/ConfigArgParse-1.2.3.tar.gz (42kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 8.0MB/s \n",
      "\u001b[?25hCollecting pyyaml==5.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 44.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask==1.1.2->OpenNMT-py) (1.0.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask==1.1.2->OpenNMT-py) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask==1.1.2->OpenNMT-py) (2.11.2)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask==1.1.2->OpenNMT-py) (7.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->OpenNMT-py) (1.19.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->OpenNMT-py) (0.16.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (2.23.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (51.3.3)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (0.36.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (3.3.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (0.4.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (0.10.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.17.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (3.12.4)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.32.0)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 53.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask==1.1.2->OpenNMT-py) (1.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3->OpenNMT-py) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3->OpenNMT-py) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3->OpenNMT-py) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3->OpenNMT-py) (1.24.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py) (3.4.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3->OpenNMT-py) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (4.7)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (0.2.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py) (3.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3->OpenNMT-py) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (0.4.8)\n",
      "Building wheels for collected packages: configargparse, pyyaml\n",
      "  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for configargparse: filename=ConfigArgParse-1.2.3-cp36-none-any.whl size=19328 sha256=c7d7c147531a1204d1dabf3c7a4e07be9c46e92bee982ae8d55a894fa16a1631\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/d6/53/034032da9498bda2385cd50a51a289e88090b5da2d592b1fdf\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=31ff5ca48101e99ec24177e6e2c1674ee9c15cd4c9edd14f909ce81427003cc1\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
      "Successfully built configargparse pyyaml\n",
      "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.6.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: waitress, torch, tqdm, sentencepiece, torchtext, pyonmttok, configargparse, pyyaml, OpenNMT-py\n",
      "  Found existing installation: torch 1.7.0+cu101\n",
      "    Uninstalling torch-1.7.0+cu101:\n",
      "      Successfully uninstalled torch-1.7.0+cu101\n",
      "  Found existing installation: tqdm 4.41.1\n",
      "    Uninstalling tqdm-4.41.1:\n",
      "      Successfully uninstalled tqdm-4.41.1\n",
      "  Found existing installation: torchtext 0.3.1\n",
      "    Uninstalling torchtext-0.3.1:\n",
      "      Successfully uninstalled torchtext-0.3.1\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed OpenNMT-py-2.0.1 configargparse-1.2.3 pyonmttok-1.23.0 pyyaml-5.3.1 sentencepiece-0.1.95 torch-1.6.0 torchtext-0.5.0 tqdm-4.56.0 waitress-1.4.4\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.6.0+cu101\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.6.0%2Bcu101-cp36-cp36m-linux_x86_64.whl (708.0MB)\n",
      "\u001b[K     |████████████████████████████████| 708.0MB 25kB/s \n",
      "\u001b[?25hCollecting torchvision==0.7.0+cu101\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.7.0%2Bcu101-cp36-cp36m-linux_x86_64.whl (5.9MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9MB 27.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0+cu101) (1.19.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0+cu101) (0.16.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.7.0+cu101) (7.0.0)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Found existing installation: torch 1.6.0\n",
      "    Uninstalling torch-1.6.0:\n",
      "      Successfully uninstalled torch-1.6.0\n",
      "  Found existing installation: torchvision 0.8.1+cu101\n",
      "    Uninstalling torchvision-0.8.1+cu101:\n",
      "      Successfully uninstalled torchvision-0.8.1+cu101\n",
      "Successfully installed torch-1.6.0+cu101 torchvision-0.7.0+cu101\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!git clone -b legacy https://github.com/OpenNMT/OpenNMT-py\n",
    "!pip install OpenNMT-py\n",
    "import os\n",
    "outdir = 'drive/MyDrive/EnglishToleranceBaseline/output'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLdmZ2V07QvF"
   },
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEwbT4BS7Fwx"
   },
   "outputs": [],
   "source": [
    "datasizes = [f'{s}_{i}' for s in ['100','200','400','600','800','1000'] for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIv9YxYjxLHk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import operator\n",
    "random.seed(1)\n",
    "\n",
    "def preprocess_line(line):\n",
    "  tokens = line.split()\n",
    "  source = tokens[1].lower()\n",
    "  target = tokens[3].lower()\n",
    "  \n",
    "  return f'{\" \".join(source)}',  f'{\" \".join(target)}'\n",
    "\n",
    "for datasize in datasizes:\n",
    "  new_data_path = f'drive/MyDrive/EnglishToleranceBaseline/processed_data/{datasize}/'\n",
    "  if not os.path.exists(new_data_path):\n",
    "    os.makedirs(new_data_path)\n",
    "\n",
    "\n",
    "  with open(f'drive/MyDrive/EnglishToleranceBaseline/raw_data/unimorph_celex0_train{datasize}.txt','r') as raw_file:\n",
    "    with open(f'{new_data_path}english-src-train.txt','w') as src_file:\n",
    "      with open(f'{new_data_path}english-tgt-train.txt','w') as tgt_file:\n",
    "        lines = raw_file.readlines()\n",
    "        random.shuffle(lines)\n",
    "        for line in lines:\n",
    "          src, tgt = preprocess_line(line)\n",
    "          print(src, file=src_file)\n",
    "          print(tgt, file=tgt_file)\n",
    "           \n",
    "  with open(f'drive/MyDrive/EnglishToleranceBaseline/raw_data/unimorph_celex0_dev_{datasize.split(\"_\")[1]}.txt','r') as raw_file:\n",
    "    with open(f'{new_data_path}english-src-val.txt','w') as src_file:\n",
    "      with open(f'{new_data_path}english-tgt-val.txt','w') as tgt_file:\n",
    "        lines = raw_file.readlines()\n",
    "        random.shuffle(lines)\n",
    "        for line in lines:\n",
    "          src, tgt = preprocess_line(line)\n",
    "          print(src, file=src_file)\n",
    "          print(tgt, file=tgt_file)\n",
    "  \n",
    "  with open(f'drive/MyDrive/EnglishToleranceBaseline/raw_data/unimorph_celex0_test_{datasize.split(\"_\")[1]}.txt','r') as raw_file:\n",
    "    with open(f'{new_data_path}english-src-test.txt','w') as src_file:\n",
    "      with open(f'{new_data_path}english-tgt-test.txt','w') as tgt_file:\n",
    "        lines = raw_file.readlines()\n",
    "        random.shuffle(lines)\n",
    "        for line in lines:\n",
    "          src, tgt = preprocess_line(line)\n",
    "          print(src, file=src_file)\n",
    "          print(tgt, file=tgt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6kSE_Vp7OWx"
   },
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3TH9GN6L7NqW"
   },
   "outputs": [],
   "source": [
    "for datasize in datasizes:\n",
    "  datadir = f'drive/MyDrive/EnglishToleranceBaseline/processed_data/{datasize}'\n",
    "  !python OpenNMT-py/preprocess.py -train_src $datadir/english-src-train.txt -train_tgt $datadir/english-tgt-train.txt -valid_src $datadir/english-src-val.txt -valid_tgt $datadir/english-tgt-val.txt -save_data $datadir/processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwfIfXr58ZNF"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKknUyYV8YsQ",
    "outputId": "79099158-1f20-4afd-c716-c642dab3db91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "[2021-01-30 02:27:28,413 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:28,611 INFO] Step 850/ 4000; acc:  86.68; ppl:  1.63; xent: 0.49; lr: 1.00000; 5092/7274 tok/s;     17 sec\n",
      "[2021-01-30 02:27:29,197 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:29,204 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:29,600 INFO] Step 900/ 4000; acc:  89.32; ppl:  1.53; xent: 0.42; lr: 1.00000; 4771/6865 tok/s;     18 sec\n",
      "[2021-01-30 02:27:29,993 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:30,001 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:30,577 INFO] Step 950/ 4000; acc:  91.06; ppl:  1.41; xent: 0.34; lr: 1.00000; 5140/7292 tok/s;     19 sec\n",
      "[2021-01-30 02:27:30,751 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:30,759 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:31,528 INFO] Step 1000/ 4000; acc:  93.50; ppl:  1.30; xent: 0.26; lr: 1.00000; 4977/7181 tok/s;     20 sec\n",
      "[2021-01-30 02:27:31,528 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:31,535 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:32,302 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:32,309 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:32,512 INFO] Step 1050/ 4000; acc:  93.70; ppl:  1.27; xent: 0.24; lr: 1.00000; 5050/7214 tok/s;     21 sec\n",
      "[2021-01-30 02:27:33,087 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:33,116 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:33,511 INFO] Step 1100/ 4000; acc:  92.37; ppl:  1.37; xent: 0.31; lr: 1.00000; 4725/6799 tok/s;     22 sec\n",
      "[2021-01-30 02:27:33,897 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:33,904 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:34,492 INFO] Step 1150/ 4000; acc:  93.91; ppl:  1.26; xent: 0.23; lr: 1.00000; 5120/7264 tok/s;     23 sec\n",
      "[2021-01-30 02:27:34,663 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:34,670 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:35,475 INFO] Step 1200/ 4000; acc:  95.01; ppl:  1.22; xent: 0.20; lr: 1.00000; 4815/6948 tok/s;     24 sec\n",
      "[2021-01-30 02:27:35,475 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:35,482 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:36,236 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:36,243 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:36,437 INFO] Step 1250/ 4000; acc:  96.41; ppl:  1.15; xent: 0.14; lr: 1.00000; 5163/7375 tok/s;     25 sec\n",
      "[2021-01-30 02:27:37,003 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:37,010 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:37,389 INFO] Step 1300/ 4000; acc:  94.55; ppl:  1.25; xent: 0.22; lr: 1.00000; 4956/7132 tok/s;     26 sec\n",
      "[2021-01-30 02:27:37,782 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:37,789 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:38,383 INFO] Step 1350/ 4000; acc:  95.54; ppl:  1.18; xent: 0.16; lr: 1.00000; 5056/7173 tok/s;     27 sec\n",
      "[2021-01-30 02:27:38,555 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:38,562 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:39,355 INFO] Step 1400/ 4000; acc:  95.74; ppl:  1.18; xent: 0.17; lr: 1.00000; 4870/7027 tok/s;     28 sec\n",
      "[2021-01-30 02:27:39,355 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:39,362 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:40,171 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:40,179 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:40,376 INFO] Step 1450/ 4000; acc:  95.74; ppl:  1.17; xent: 0.16; lr: 1.00000; 4868/6954 tok/s;     29 sec\n",
      "[2021-01-30 02:27:40,930 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:40,938 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:41,336 INFO] Step 1500/ 4000; acc:  95.62; ppl:  1.19; xent: 0.17; lr: 1.00000; 4912/7069 tok/s;     30 sec\n",
      "[2021-01-30 02:27:41,712 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:41,719 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:42,337 INFO] Step 1550/ 4000; acc:  95.57; ppl:  1.21; xent: 0.19; lr: 1.00000; 5022/7126 tok/s;     31 sec\n",
      "[2021-01-30 02:27:42,511 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:42,518 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:43,293 INFO] Step 1600/ 4000; acc:  96.16; ppl:  1.16; xent: 0.15; lr: 1.00000; 4947/7139 tok/s;     32 sec\n",
      "[2021-01-30 02:27:43,293 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:43,300 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:44,055 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:44,062 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:44,262 INFO] Step 1650/ 4000; acc:  96.65; ppl:  1.13; xent: 0.12; lr: 1.00000; 5126/7323 tok/s;     33 sec\n",
      "[2021-01-30 02:27:44,813 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:44,819 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:45,198 INFO] Step 1700/ 4000; acc:  95.68; ppl:  1.21; xent: 0.19; lr: 1.00000; 5041/7255 tok/s;     34 sec\n",
      "[2021-01-30 02:27:45,583 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:45,591 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:46,227 INFO] Step 1750/ 4000; acc:  95.92; ppl:  1.19; xent: 0.18; lr: 1.00000; 4887/6933 tok/s;     35 sec\n",
      "[2021-01-30 02:27:46,414 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:46,421 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:47,174 INFO] Step 1800/ 4000; acc:  96.15; ppl:  1.17; xent: 0.15; lr: 1.00000; 4996/7209 tok/s;     36 sec\n",
      "[2021-01-30 02:27:47,174 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:47,181 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:47,931 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:47,957 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:48,164 INFO] Step 1850/ 4000; acc:  96.83; ppl:  1.16; xent: 0.15; lr: 1.00000; 5016/7166 tok/s;     37 sec\n",
      "[2021-01-30 02:27:48,712 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:48,718 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:49,127 INFO] Step 1900/ 4000; acc:  96.51; ppl:  1.15; xent: 0.14; lr: 1.00000; 4903/7056 tok/s;     38 sec\n",
      "[2021-01-30 02:27:49,516 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:49,523 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:50,141 INFO] Step 1950/ 4000; acc:  97.25; ppl:  1.11; xent: 0.11; lr: 1.00000; 4955/7030 tok/s;     39 sec\n",
      "[2021-01-30 02:27:50,315 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:50,322 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:51,089 INFO] Step 2000/ 4000; acc:  97.03; ppl:  1.13; xent: 0.12; lr: 1.00000; 4989/7200 tok/s;     40 sec\n",
      "[2021-01-30 02:27:51,089 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:51,096 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:51,851 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:51,858 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:52,060 INFO] Step 2050/ 4000; acc:  97.00; ppl:  1.14; xent: 0.13; lr: 1.00000; 5119/7312 tok/s;     41 sec\n",
      "[2021-01-30 02:27:52,633 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:52,640 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:53,014 INFO] Step 2100/ 4000; acc:  96.54; ppl:  1.16; xent: 0.15; lr: 1.00000; 4942/7112 tok/s;     41 sec\n",
      "[2021-01-30 02:27:53,405 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:53,413 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:54,015 INFO] Step 2150/ 4000; acc:  95.72; ppl:  1.20; xent: 0.18; lr: 1.00000; 5021/7124 tok/s;     42 sec\n",
      "[2021-01-30 02:27:54,192 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:54,199 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:54,967 INFO] Step 2200/ 4000; acc:  96.54; ppl:  1.16; xent: 0.15; lr: 1.00000; 4970/7173 tok/s;     43 sec\n",
      "[2021-01-30 02:27:54,967 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:54,974 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:55,736 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:55,743 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:55,944 INFO] Step 2250/ 4000; acc:  96.84; ppl:  1.15; xent: 0.14; lr: 1.00000; 5085/7264 tok/s;     44 sec\n",
      "[2021-01-30 02:27:56,552 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:56,558 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:56,934 INFO] Step 2300/ 4000; acc:  96.85; ppl:  1.15; xent: 0.14; lr: 1.00000; 4768/6861 tok/s;     45 sec\n",
      "[2021-01-30 02:27:57,325 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:57,332 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:57,951 INFO] Step 2350/ 4000; acc:  96.69; ppl:  1.15; xent: 0.14; lr: 1.00000; 4941/7011 tok/s;     46 sec\n",
      "[2021-01-30 02:27:58,122 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:58,128 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:58,903 INFO] Step 2400/ 4000; acc:  96.84; ppl:  1.15; xent: 0.14; lr: 1.00000; 4970/7172 tok/s;     47 sec\n",
      "[2021-01-30 02:27:58,903 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:58,910 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:59,703 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:27:59,710 INFO] number of examples: 800\n",
      "[2021-01-30 02:27:59,905 INFO] Step 2450/ 4000; acc:  97.20; ppl:  1.12; xent: 0.11; lr: 1.00000; 4957/7082 tok/s;     48 sec\n",
      "[2021-01-30 02:28:00,459 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:00,470 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:00,867 INFO] Step 2500/ 4000; acc:  96.45; ppl:  1.18; xent: 0.16; lr: 1.00000; 4903/7055 tok/s;     49 sec\n",
      "[2021-01-30 02:28:01,267 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:01,275 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:01,861 INFO] Step 2550/ 4000; acc:  96.68; ppl:  1.17; xent: 0.16; lr: 1.00000; 5055/7173 tok/s;     50 sec\n",
      "[2021-01-30 02:28:02,034 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:02,040 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:02,799 INFO] Step 2600/ 4000; acc:  96.91; ppl:  1.16; xent: 0.15; lr: 1.00000; 5049/7285 tok/s;     51 sec\n",
      "[2021-01-30 02:28:02,799 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:02,824 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:03,614 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:03,621 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:03,821 INFO] Step 2650/ 4000; acc:  96.46; ppl:  1.18; xent: 0.17; lr: 1.00000; 4862/6946 tok/s;     52 sec\n",
      "[2021-01-30 02:28:04,384 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:04,391 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:04,778 INFO] Step 2700/ 4000; acc:  96.35; ppl:  1.19; xent: 0.18; lr: 1.00000; 4929/7094 tok/s;     53 sec\n",
      "[2021-01-30 02:28:05,170 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:05,177 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:05,752 INFO] Step 2750/ 4000; acc:  96.66; ppl:  1.16; xent: 0.15; lr: 1.00000; 5159/7319 tok/s;     54 sec\n",
      "[2021-01-30 02:28:05,921 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:05,928 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:06,752 INFO] Step 2800/ 4000; acc:  96.92; ppl:  1.13; xent: 0.12; lr: 1.00000; 4731/6827 tok/s;     55 sec\n",
      "[2021-01-30 02:28:06,752 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:06,759 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:07,525 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:07,532 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:07,734 INFO] Step 2850/ 4000; acc:  96.96; ppl:  1.15; xent: 0.14; lr: 1.00000; 5058/7226 tok/s;     56 sec\n",
      "[2021-01-30 02:28:08,328 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:08,340 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:08,727 INFO] Step 2900/ 4000; acc:  96.33; ppl:  1.17; xent: 0.16; lr: 1.00000; 4755/6842 tok/s;     57 sec\n",
      "[2021-01-30 02:28:09,121 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:09,129 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:09,724 INFO] Step 2950/ 4000; acc:  96.91; ppl:  1.15; xent: 0.14; lr: 1.00000; 5039/7150 tok/s;     58 sec\n",
      "[2021-01-30 02:28:09,898 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:09,905 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:10,699 INFO] Step 3000/ 4000; acc:  97.10; ppl:  1.13; xent: 0.13; lr: 1.00000; 4853/7003 tok/s;     59 sec\n",
      "[2021-01-30 02:28:10,699 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:10,706 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:11,460 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:11,467 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:11,662 INFO] Step 3050/ 4000; acc:  96.97; ppl:  1.15; xent: 0.14; lr: 1.00000; 5160/7372 tok/s;     60 sec\n",
      "[2021-01-30 02:28:12,253 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:12,260 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:12,660 INFO] Step 3100/ 4000; acc:  96.39; ppl:  1.19; xent: 0.17; lr: 1.00000; 4727/6802 tok/s;     61 sec\n",
      "[2021-01-30 02:28:13,050 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:13,056 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:13,645 INFO] Step 3150/ 4000; acc:  96.62; ppl:  1.17; xent: 0.16; lr: 1.00000; 5098/7233 tok/s;     62 sec\n",
      "[2021-01-30 02:28:13,824 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:13,831 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:14,632 INFO] Step 3200/ 4000; acc:  96.35; ppl:  1.19; xent: 0.18; lr: 1.00000; 4795/6920 tok/s;     63 sec\n",
      "[2021-01-30 02:28:14,632 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:14,640 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:15,393 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:15,400 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:15,602 INFO] Step 3250/ 4000; acc:  97.03; ppl:  1.15; xent: 0.14; lr: 1.00000; 5122/7317 tok/s;     64 sec\n",
      "[2021-01-30 02:28:16,217 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:16,225 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:16,600 INFO] Step 3300/ 4000; acc:  96.63; ppl:  1.17; xent: 0.15; lr: 1.00000; 4729/6805 tok/s;     65 sec\n",
      "[2021-01-30 02:28:16,988 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:16,995 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:17,597 INFO] Step 3350/ 4000; acc:  96.83; ppl:  1.16; xent: 0.14; lr: 1.00000; 5039/7149 tok/s;     66 sec\n",
      "[2021-01-30 02:28:17,764 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:17,790 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:18,577 INFO] Step 3400/ 4000; acc:  97.25; ppl:  1.15; xent: 0.14; lr: 1.00000; 4832/6973 tok/s;     67 sec\n",
      "[2021-01-30 02:28:18,577 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:18,584 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:19,340 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:19,346 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:19,548 INFO] Step 3450/ 4000; acc:  96.86; ppl:  1.16; xent: 0.15; lr: 1.00000; 5114/7305 tok/s;     68 sec\n",
      "[2021-01-30 02:28:20,104 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:20,111 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:20,521 INFO] Step 3500/ 4000; acc:  96.66; ppl:  1.16; xent: 0.15; lr: 1.00000; 4850/6979 tok/s;     69 sec\n",
      "[2021-01-30 02:28:20,907 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:20,914 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:21,550 INFO] Step 3550/ 4000; acc:  97.11; ppl:  1.14; xent: 0.13; lr: 1.00000; 4882/6926 tok/s;     70 sec\n",
      "[2021-01-30 02:28:21,725 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:21,732 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:22,476 INFO] Step 3600/ 4000; acc:  96.95; ppl:  1.18; xent: 0.17; lr: 1.00000; 5113/7378 tok/s;     71 sec\n",
      "[2021-01-30 02:28:22,476 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:22,483 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:23,238 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:23,245 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:23,451 INFO] Step 3650/ 4000; acc:  96.93; ppl:  1.18; xent: 0.17; lr: 1.00000; 5098/7283 tok/s;     72 sec\n",
      "[2021-01-30 02:28:24,018 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:24,026 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:24,397 INFO] Step 3700/ 4000; acc:  96.63; ppl:  1.16; xent: 0.15; lr: 1.00000; 4983/7170 tok/s;     73 sec\n",
      "[2021-01-30 02:28:24,804 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:24,814 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:25,428 INFO] Step 3750/ 4000; acc:  97.00; ppl:  1.14; xent: 0.13; lr: 1.00000; 4875/6916 tok/s;     74 sec\n",
      "[2021-01-30 02:28:25,602 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:25,609 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:26,361 INFO] Step 3800/ 4000; acc:  97.33; ppl:  1.14; xent: 0.13; lr: 1.00000; 5075/7323 tok/s;     75 sec\n",
      "[2021-01-30 02:28:26,361 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:26,368 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:27,128 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:27,136 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:27,332 INFO] Step 3850/ 4000; acc:  97.77; ppl:  1.10; xent: 0.09; lr: 1.00000; 5118/7311 tok/s;     76 sec\n",
      "[2021-01-30 02:28:27,921 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:27,929 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:28,299 INFO] Step 3900/ 4000; acc:  97.44; ppl:  1.14; xent: 0.13; lr: 1.00000; 4875/7016 tok/s;     77 sec\n",
      "[2021-01-30 02:28:28,689 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:28,696 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:29,310 INFO] Step 3950/ 4000; acc:  97.77; ppl:  1.12; xent: 0.11; lr: 1.00000; 4973/7056 tok/s;     78 sec\n",
      "[2021-01-30 02:28:29,486 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_5/processed.train.0.pt\n",
      "[2021-01-30 02:28:29,494 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:30,274 INFO] Step 4000/ 4000; acc:  97.51; ppl:  1.13; xent: 0.12; lr: 1.00000; 4908/7083 tok/s;     79 sec\n",
      "[2021-01-30 02:28:30,275 INFO] Saving checkpoint drive/MyDrive/EnglishToleranceBaseline/output/english_rnn_model_800_5_step_4000.pt\n",
      "[2021-01-30 02:28:31,913 INFO]  * src vocab size = 44\n",
      "[2021-01-30 02:28:31,913 INFO]  * tgt vocab size = 46\n",
      "[2021-01-30 02:28:31,913 INFO] Building model...\n",
      "[2021-01-30 02:28:36,305 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(44, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(46, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=46, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-01-30 02:28:36,305 INFO] encoder: 254800\n",
      "[2021-01-30 02:28:36,305 INFO] decoder: 330046\n",
      "[2021-01-30 02:28:36,305 INFO] * number of parameters: 584846\n",
      "[2021-01-30 02:28:36,308 INFO] Starting training on GPU: [0]\n",
      "[2021-01-30 02:28:36,308 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-01-30 02:28:36,308 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:36,961 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:37,762 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:37,769 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:37,954 INFO] Step 50/ 4000; acc:  13.58; ppl: 35.97; xent: 3.58; lr: 1.00000; 2940/4205 tok/s;      2 sec\n",
      "[2021-01-30 02:28:38,532 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:38,539 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:38,930 INFO] Step 100/ 4000; acc:  21.42; ppl: 18.50; xent: 2.92; lr: 1.00000; 4992/7150 tok/s;      3 sec\n",
      "[2021-01-30 02:28:39,313 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:39,320 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:39,930 INFO] Step 150/ 4000; acc:  25.58; ppl: 14.32; xent: 2.66; lr: 1.00000; 4964/7097 tok/s;      4 sec\n",
      "[2021-01-30 02:28:40,104 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:40,111 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:40,907 INFO] Step 200/ 4000; acc:  30.58; ppl: 12.16; xent: 2.50; lr: 1.00000; 4828/6983 tok/s;      5 sec\n",
      "[2021-01-30 02:28:40,908 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:40,915 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:41,671 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:41,678 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:41,875 INFO] Step 250/ 4000; acc:  33.17; ppl: 10.30; xent: 2.33; lr: 1.00000; 5003/7154 tok/s;      6 sec\n",
      "[2021-01-30 02:28:42,458 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:42,465 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:42,852 INFO] Step 300/ 4000; acc:  38.89; ppl:  8.40; xent: 2.13; lr: 1.00000; 4985/7140 tok/s;      7 sec\n",
      "[2021-01-30 02:28:43,229 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:43,256 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:43,838 INFO] Step 350/ 4000; acc:  42.94; ppl:  7.08; xent: 1.96; lr: 1.00000; 5036/7200 tok/s;      8 sec\n",
      "[2021-01-30 02:28:44,005 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:44,012 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:44,781 INFO] Step 400/ 4000; acc:  45.73; ppl:  6.35; xent: 1.85; lr: 1.00000; 5002/7235 tok/s;      8 sec\n",
      "[2021-01-30 02:28:44,781 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:44,788 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:45,539 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:45,545 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:45,729 INFO] Step 450/ 4000; acc:  49.14; ppl:  5.62; xent: 1.73; lr: 1.00000; 5106/7301 tok/s;      9 sec\n",
      "[2021-01-30 02:28:46,293 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:46,300 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:46,694 INFO] Step 500/ 4000; acc:  52.95; ppl:  4.98; xent: 1.60; lr: 1.00000; 5045/7226 tok/s;     10 sec\n",
      "[2021-01-30 02:28:47,078 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:47,086 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:47,663 INFO] Step 550/ 4000; acc:  57.16; ppl:  4.27; xent: 1.45; lr: 1.00000; 5127/7330 tok/s;     11 sec\n",
      "[2021-01-30 02:28:47,836 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:47,843 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:48,595 INFO] Step 600/ 4000; acc:  63.23; ppl:  3.39; xent: 1.22; lr: 1.00000; 5060/7319 tok/s;     12 sec\n",
      "[2021-01-30 02:28:48,595 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:48,603 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:49,378 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:49,385 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:49,571 INFO] Step 650/ 4000; acc:  67.17; ppl:  3.02; xent: 1.11; lr: 1.00000; 4960/7093 tok/s;     13 sec\n",
      "[2021-01-30 02:28:50,171 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:50,181 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:50,579 INFO] Step 700/ 4000; acc:  71.45; ppl:  2.63; xent: 0.97; lr: 1.00000; 4832/6920 tok/s;     14 sec\n",
      "[2021-01-30 02:28:50,953 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:50,959 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:51,547 INFO] Step 750/ 4000; acc:  76.35; ppl:  2.24; xent: 0.81; lr: 1.00000; 5130/7335 tok/s;     15 sec\n",
      "[2021-01-30 02:28:51,716 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:51,723 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:52,512 INFO] Step 800/ 4000; acc:  82.99; ppl:  1.82; xent: 0.60; lr: 1.00000; 4888/7070 tok/s;     16 sec\n",
      "[2021-01-30 02:28:52,512 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:52,519 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:53,297 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:53,303 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:53,484 INFO] Step 850/ 4000; acc:  85.61; ppl:  1.70; xent: 0.53; lr: 1.00000; 4980/7121 tok/s;     17 sec\n",
      "[2021-01-30 02:28:54,050 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:54,057 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:54,445 INFO] Step 900/ 4000; acc:  87.68; ppl:  1.55; xent: 0.44; lr: 1.00000; 5066/7257 tok/s;     18 sec\n",
      "[2021-01-30 02:28:54,812 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:54,819 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:55,406 INFO] Step 950/ 4000; acc:  90.32; ppl:  1.41; xent: 0.34; lr: 1.00000; 5167/7388 tok/s;     19 sec\n",
      "[2021-01-30 02:28:55,575 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:55,582 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:56,388 INFO] Step 1000/ 4000; acc:  91.92; ppl:  1.35; xent: 0.30; lr: 1.00000; 4805/6950 tok/s;     20 sec\n",
      "[2021-01-30 02:28:56,388 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:56,395 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:57,138 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:57,145 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:57,326 INFO] Step 1050/ 4000; acc:  91.65; ppl:  1.40; xent: 0.34; lr: 1.00000; 5162/7381 tok/s;     21 sec\n",
      "[2021-01-30 02:28:57,903 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:57,929 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:58,316 INFO] Step 1100/ 4000; acc:  92.24; ppl:  1.32; xent: 0.28; lr: 1.00000; 4920/7047 tok/s;     22 sec\n",
      "[2021-01-30 02:28:58,678 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:58,685 INFO] number of examples: 800\n",
      "[2021-01-30 02:28:59,279 INFO] Step 1150/ 4000; acc:  93.20; ppl:  1.28; xent: 0.25; lr: 1.00000; 5155/7370 tok/s;     23 sec\n",
      "[2021-01-30 02:28:59,446 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:28:59,453 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:00,218 INFO] Step 1200/ 4000; acc:  94.69; ppl:  1.22; xent: 0.20; lr: 1.00000; 5021/7262 tok/s;     24 sec\n",
      "[2021-01-30 02:29:00,219 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:00,225 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:01,021 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:01,029 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:01,214 INFO] Step 1250/ 4000; acc:  94.29; ppl:  1.25; xent: 0.22; lr: 1.00000; 4862/6953 tok/s;     25 sec\n",
      "[2021-01-30 02:29:01,783 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:01,791 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:02,177 INFO] Step 1300/ 4000; acc:  95.43; ppl:  1.18; xent: 0.17; lr: 1.00000; 5057/7243 tok/s;     26 sec\n",
      "[2021-01-30 02:29:02,552 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:02,558 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:03,159 INFO] Step 1350/ 4000; acc:  95.96; ppl:  1.17; xent: 0.15; lr: 1.00000; 5055/7228 tok/s;     27 sec\n",
      "[2021-01-30 02:29:03,333 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:03,340 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:04,107 INFO] Step 1400/ 4000; acc:  95.51; ppl:  1.20; xent: 0.18; lr: 1.00000; 4980/7203 tok/s;     28 sec\n",
      "[2021-01-30 02:29:04,107 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:04,114 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:04,879 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:04,886 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:05,072 INFO] Step 1450/ 4000; acc:  94.28; ppl:  1.24; xent: 0.22; lr: 1.00000; 5013/7169 tok/s;     29 sec\n",
      "[2021-01-30 02:29:05,649 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:05,657 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:06,046 INFO] Step 1500/ 4000; acc:  95.02; ppl:  1.22; xent: 0.20; lr: 1.00000; 5004/7167 tok/s;     30 sec\n",
      "[2021-01-30 02:29:06,424 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:06,431 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:07,032 INFO] Step 1550/ 4000; acc:  95.62; ppl:  1.17; xent: 0.16; lr: 1.00000; 5034/7197 tok/s;     31 sec\n",
      "[2021-01-30 02:29:07,211 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:07,218 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:07,975 INFO] Step 1600/ 4000; acc:  95.73; ppl:  1.18; xent: 0.16; lr: 1.00000; 5000/7232 tok/s;     32 sec\n",
      "[2021-01-30 02:29:07,976 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:07,982 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:08,735 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:08,743 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:08,926 INFO] Step 1650/ 4000; acc:  95.43; ppl:  1.19; xent: 0.17; lr: 1.00000; 5092/7282 tok/s;     33 sec\n",
      "[2021-01-30 02:29:09,495 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:09,502 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:09,900 INFO] Step 1700/ 4000; acc:  96.06; ppl:  1.16; xent: 0.15; lr: 1.00000; 4999/7161 tok/s;     34 sec\n",
      "[2021-01-30 02:29:10,274 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:10,282 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:10,907 INFO] Step 1750/ 4000; acc:  96.48; ppl:  1.15; xent: 0.14; lr: 1.00000; 4931/7051 tok/s;     35 sec\n",
      "[2021-01-30 02:29:11,085 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:11,092 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:11,868 INFO] Step 1800/ 4000; acc:  96.19; ppl:  1.15; xent: 0.14; lr: 1.00000; 4911/7103 tok/s;     36 sec\n",
      "[2021-01-30 02:29:11,868 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:11,875 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:12,633 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:12,659 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:12,845 INFO] Step 1850/ 4000; acc:  95.87; ppl:  1.17; xent: 0.16; lr: 1.00000; 4951/7079 tok/s;     37 sec\n",
      "[2021-01-30 02:29:13,440 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:13,447 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:13,829 INFO] Step 1900/ 4000; acc:  96.00; ppl:  1.17; xent: 0.16; lr: 1.00000; 4951/7091 tok/s;     38 sec\n",
      "[2021-01-30 02:29:14,212 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:14,219 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:14,825 INFO] Step 1950/ 4000; acc:  96.06; ppl:  1.17; xent: 0.16; lr: 1.00000; 4985/7128 tok/s;     39 sec\n",
      "[2021-01-30 02:29:14,994 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:15,001 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:15,767 INFO] Step 2000/ 4000; acc:  96.77; ppl:  1.15; xent: 0.14; lr: 1.00000; 5009/7245 tok/s;     39 sec\n",
      "[2021-01-30 02:29:15,767 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:15,774 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:16,539 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:16,545 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:16,729 INFO] Step 2050/ 4000; acc:  96.63; ppl:  1.15; xent: 0.14; lr: 1.00000; 5032/7196 tok/s;     40 sec\n",
      "[2021-01-30 02:29:17,323 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:17,333 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:17,730 INFO] Step 2100/ 4000; acc:  96.77; ppl:  1.14; xent: 0.13; lr: 1.00000; 4867/6971 tok/s;     41 sec\n",
      "[2021-01-30 02:29:18,117 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:18,125 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:18,748 INFO] Step 2150/ 4000; acc:  97.15; ppl:  1.13; xent: 0.12; lr: 1.00000; 4874/6968 tok/s;     42 sec\n",
      "[2021-01-30 02:29:18,918 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:18,925 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:19,679 INFO] Step 2200/ 4000; acc:  96.69; ppl:  1.14; xent: 0.13; lr: 1.00000; 5066/7328 tok/s;     43 sec\n",
      "[2021-01-30 02:29:19,680 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:19,686 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:20,470 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:20,477 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:20,661 INFO] Step 2250/ 4000; acc:  96.11; ppl:  1.17; xent: 0.16; lr: 1.00000; 4930/7049 tok/s;     44 sec\n",
      "[2021-01-30 02:29:21,238 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:21,245 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:21,645 INFO] Step 2300/ 4000; acc:  96.50; ppl:  1.16; xent: 0.15; lr: 1.00000; 4950/7090 tok/s;     45 sec\n",
      "[2021-01-30 02:29:22,026 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:22,034 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:22,663 INFO] Step 2350/ 4000; acc:  97.08; ppl:  1.14; xent: 0.13; lr: 1.00000; 4879/6975 tok/s;     46 sec\n",
      "[2021-01-30 02:29:22,841 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:22,848 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:23,681 INFO] Step 2400/ 4000; acc:  96.64; ppl:  1.16; xent: 0.15; lr: 1.00000; 4636/6705 tok/s;     47 sec\n",
      "[2021-01-30 02:29:23,681 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:23,688 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:24,462 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:24,470 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:24,655 INFO] Step 2450/ 4000; acc:  96.66; ppl:  1.16; xent: 0.14; lr: 1.00000; 4968/7103 tok/s;     48 sec\n",
      "[2021-01-30 02:29:25,239 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:25,246 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:25,639 INFO] Step 2500/ 4000; acc:  96.62; ppl:  1.16; xent: 0.15; lr: 1.00000; 4948/7087 tok/s;     49 sec\n",
      "[2021-01-30 02:29:26,027 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:26,036 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:26,636 INFO] Step 2550/ 4000; acc:  96.56; ppl:  1.18; xent: 0.16; lr: 1.00000; 4983/7125 tok/s;     50 sec\n",
      "[2021-01-30 02:29:26,809 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:26,817 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:27,647 INFO] Step 2600/ 4000; acc:  96.58; ppl:  1.17; xent: 0.15; lr: 1.00000; 4665/6747 tok/s;     51 sec\n",
      "[2021-01-30 02:29:27,647 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:27,673 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:28,491 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:28,498 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:28,692 INFO] Step 2650/ 4000; acc:  96.29; ppl:  1.17; xent: 0.16; lr: 1.00000; 4633/6625 tok/s;     52 sec\n",
      "[2021-01-30 02:29:29,264 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:29,271 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:29,689 INFO] Step 2700/ 4000; acc:  96.23; ppl:  1.18; xent: 0.17; lr: 1.00000; 4884/6995 tok/s;     53 sec\n",
      "[2021-01-30 02:29:30,091 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:30,098 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:30,688 INFO] Step 2750/ 4000; acc:  96.80; ppl:  1.16; xent: 0.15; lr: 1.00000; 4970/7105 tok/s;     54 sec\n",
      "[2021-01-30 02:29:30,860 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:30,867 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:31,636 INFO] Step 2800/ 4000; acc:  95.94; ppl:  1.20; xent: 0.18; lr: 1.00000; 4978/7201 tok/s;     55 sec\n",
      "[2021-01-30 02:29:31,636 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:31,643 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:32,442 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:32,448 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:32,638 INFO] Step 2850/ 4000; acc:  97.02; ppl:  1.14; xent: 0.13; lr: 1.00000; 4829/6905 tok/s;     56 sec\n",
      "[2021-01-30 02:29:33,214 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:33,222 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:33,622 INFO] Step 2900/ 4000; acc:  96.64; ppl:  1.17; xent: 0.15; lr: 1.00000; 4949/7088 tok/s;     57 sec\n",
      "[2021-01-30 02:29:33,997 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:34,004 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:34,620 INFO] Step 2950/ 4000; acc:  96.39; ppl:  1.19; xent: 0.18; lr: 1.00000; 4976/7115 tok/s;     58 sec\n",
      "[2021-01-30 02:29:34,791 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:34,799 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:35,565 INFO] Step 3000/ 4000; acc:  96.73; ppl:  1.17; xent: 0.16; lr: 1.00000; 4995/7224 tok/s;     59 sec\n",
      "[2021-01-30 02:29:35,565 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:35,572 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:36,329 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:36,336 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:36,539 INFO] Step 3050/ 4000; acc:  95.61; ppl:  1.23; xent: 0.21; lr: 1.00000; 4970/7107 tok/s;     60 sec\n",
      "[2021-01-30 02:29:37,111 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:37,119 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:37,514 INFO] Step 3100/ 4000; acc:  96.66; ppl:  1.17; xent: 0.15; lr: 1.00000; 4993/7151 tok/s;     61 sec\n",
      "[2021-01-30 02:29:37,896 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:37,903 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:38,487 INFO] Step 3150/ 4000; acc:  96.65; ppl:  1.17; xent: 0.16; lr: 1.00000; 5103/7297 tok/s;     62 sec\n",
      "[2021-01-30 02:29:38,662 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:38,669 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:39,487 INFO] Step 3200/ 4000; acc:  96.72; ppl:  1.17; xent: 0.16; lr: 1.00000; 4716/6821 tok/s;     63 sec\n",
      "[2021-01-30 02:29:39,487 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:39,495 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:40,283 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:40,292 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:40,483 INFO] Step 3250/ 4000; acc:  96.78; ppl:  1.16; xent: 0.15; lr: 1.00000; 4862/6952 tok/s;     64 sec\n",
      "[2021-01-30 02:29:41,058 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:41,066 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:41,458 INFO] Step 3300/ 4000; acc:  96.63; ppl:  1.17; xent: 0.16; lr: 1.00000; 4994/7153 tok/s;     65 sec\n",
      "[2021-01-30 02:29:41,838 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:41,845 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:42,440 INFO] Step 3350/ 4000; acc:  96.72; ppl:  1.20; xent: 0.18; lr: 1.00000; 5057/7231 tok/s;     66 sec\n",
      "[2021-01-30 02:29:42,619 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:42,645 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:43,422 INFO] Step 3400/ 4000; acc:  96.85; ppl:  1.17; xent: 0.16; lr: 1.00000; 4803/6946 tok/s;     67 sec\n",
      "[2021-01-30 02:29:43,422 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:43,429 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:44,201 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:44,208 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:44,391 INFO] Step 3450/ 4000; acc:  96.40; ppl:  1.19; xent: 0.17; lr: 1.00000; 4997/7145 tok/s;     68 sec\n",
      "[2021-01-30 02:29:44,966 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:44,974 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:45,366 INFO] Step 3500/ 4000; acc:  97.19; ppl:  1.15; xent: 0.14; lr: 1.00000; 4994/7154 tok/s;     69 sec\n",
      "[2021-01-30 02:29:45,757 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:45,763 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:46,341 INFO] Step 3550/ 4000; acc:  96.89; ppl:  1.16; xent: 0.15; lr: 1.00000; 5096/7286 tok/s;     70 sec\n",
      "[2021-01-30 02:29:46,507 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:46,514 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:47,286 INFO] Step 3600/ 4000; acc:  97.29; ppl:  1.14; xent: 0.13; lr: 1.00000; 4987/7213 tok/s;     71 sec\n",
      "[2021-01-30 02:29:47,287 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:47,293 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:48,043 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:48,050 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:48,233 INFO] Step 3650/ 4000; acc:  96.60; ppl:  1.19; xent: 0.17; lr: 1.00000; 5115/7315 tok/s;     72 sec\n",
      "[2021-01-30 02:29:48,785 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:48,792 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:49,170 INFO] Step 3700/ 4000; acc:  96.31; ppl:  1.21; xent: 0.19; lr: 1.00000; 5198/7445 tok/s;     73 sec\n",
      "[2021-01-30 02:29:49,554 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:49,562 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:50,171 INFO] Step 3750/ 4000; acc:  96.77; ppl:  1.17; xent: 0.16; lr: 1.00000; 4957/7088 tok/s;     74 sec\n",
      "[2021-01-30 02:29:50,343 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:50,351 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:51,162 INFO] Step 3800/ 4000; acc:  97.02; ppl:  1.17; xent: 0.15; lr: 1.00000; 4763/6890 tok/s;     75 sec\n",
      "[2021-01-30 02:29:51,162 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:51,169 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:51,943 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:51,950 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:52,137 INFO] Step 3850/ 4000; acc:  97.20; ppl:  1.14; xent: 0.13; lr: 1.00000; 4961/7095 tok/s;     76 sec\n",
      "[2021-01-30 02:29:52,708 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:52,715 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:53,104 INFO] Step 3900/ 4000; acc:  96.70; ppl:  1.20; xent: 0.18; lr: 1.00000; 5040/7219 tok/s;     77 sec\n",
      "[2021-01-30 02:29:53,480 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:53,487 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:54,079 INFO] Step 3950/ 4000; acc:  97.07; ppl:  1.17; xent: 0.15; lr: 1.00000; 5089/7276 tok/s;     78 sec\n",
      "[2021-01-30 02:29:54,250 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_6/processed.train.0.pt\n",
      "[2021-01-30 02:29:54,258 INFO] number of examples: 800\n",
      "[2021-01-30 02:29:55,049 INFO] Step 4000/ 4000; acc:  97.29; ppl:  1.13; xent: 0.12; lr: 1.00000; 4862/7032 tok/s;     79 sec\n",
      "[2021-01-30 02:29:55,051 INFO] Saving checkpoint drive/MyDrive/EnglishToleranceBaseline/output/english_rnn_model_800_6_step_4000.pt\n",
      "[2021-01-30 02:29:56,681 INFO]  * src vocab size = 43\n",
      "[2021-01-30 02:29:56,681 INFO]  * tgt vocab size = 45\n",
      "[2021-01-30 02:29:56,681 INFO] Building model...\n",
      "[2021-01-30 02:30:01,050 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(43, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(45, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=45, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-01-30 02:30:01,050 INFO] encoder: 254500\n",
      "[2021-01-30 02:30:01,051 INFO] decoder: 329645\n",
      "[2021-01-30 02:30:01,051 INFO] * number of parameters: 584145\n",
      "[2021-01-30 02:30:01,053 INFO] Starting training on GPU: [0]\n",
      "[2021-01-30 02:30:01,053 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-01-30 02:30:01,054 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:01,671 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:02,473 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:02,482 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:02,670 INFO] Step 50/ 4000; acc:  12.96; ppl: 34.92; xent: 3.55; lr: 1.00000; 2963/4261 tok/s;      2 sec\n",
      "[2021-01-30 02:30:03,283 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:03,290 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:03,668 INFO] Step 100/ 4000; acc:  23.50; ppl: 18.70; xent: 2.93; lr: 1.00000; 4750/6870 tok/s;      3 sec\n",
      "[2021-01-30 02:30:04,080 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:04,087 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:04,692 INFO] Step 150/ 4000; acc:  25.13; ppl: 15.78; xent: 2.76; lr: 1.00000; 4846/6885 tok/s;      4 sec\n",
      "[2021-01-30 02:30:04,878 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:04,885 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:05,665 INFO] Step 200/ 4000; acc:  29.30; ppl: 12.84; xent: 2.55; lr: 1.00000; 4904/7033 tok/s;      5 sec\n",
      "[2021-01-30 02:30:05,665 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:05,672 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:06,453 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:06,460 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:06,649 INFO] Step 250/ 4000; acc:  34.42; ppl: 10.01; xent: 2.30; lr: 1.00000; 4864/6994 tok/s;      6 sec\n",
      "[2021-01-30 02:30:07,243 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:07,251 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:07,648 INFO] Step 300/ 4000; acc:  37.53; ppl:  8.89; xent: 2.18; lr: 1.00000; 4749/6868 tok/s;      7 sec\n",
      "[2021-01-30 02:30:08,073 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:08,099 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:08,681 INFO] Step 350/ 4000; acc:  39.82; ppl:  7.93; xent: 2.07; lr: 1.00000; 4805/6827 tok/s;      8 sec\n",
      "[2021-01-30 02:30:08,859 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:08,866 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:09,638 INFO] Step 400/ 4000; acc:  42.21; ppl:  7.39; xent: 2.00; lr: 1.00000; 4982/7144 tok/s;      9 sec\n",
      "[2021-01-30 02:30:09,638 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:09,645 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:10,383 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:10,390 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:10,569 INFO] Step 450/ 4000; acc:  44.82; ppl:  6.57; xent: 1.88; lr: 1.00000; 5146/7399 tok/s;     10 sec\n",
      "[2021-01-30 02:30:11,127 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:11,135 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:11,518 INFO] Step 500/ 4000; acc:  47.54; ppl:  5.84; xent: 1.76; lr: 1.00000; 4996/7226 tok/s;     10 sec\n",
      "[2021-01-30 02:30:11,923 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:11,937 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:12,502 INFO] Step 550/ 4000; acc:  48.56; ppl:  5.65; xent: 1.73; lr: 1.00000; 5042/7163 tok/s;     11 sec\n",
      "[2021-01-30 02:30:12,675 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:12,682 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:13,472 INFO] Step 600/ 4000; acc:  52.80; ppl:  4.83; xent: 1.58; lr: 1.00000; 4918/7053 tok/s;     12 sec\n",
      "[2021-01-30 02:30:13,472 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:13,479 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:14,267 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:14,273 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:14,457 INFO] Step 650/ 4000; acc:  57.36; ppl:  4.00; xent: 1.39; lr: 1.00000; 4863/6993 tok/s;     13 sec\n",
      "[2021-01-30 02:30:15,013 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:15,020 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:15,374 INFO] Step 700/ 4000; acc:  61.18; ppl:  3.62; xent: 1.29; lr: 1.00000; 5168/7473 tok/s;     14 sec\n",
      "[2021-01-30 02:30:15,773 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:15,780 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:16,346 INFO] Step 750/ 4000; acc:  63.52; ppl:  3.31; xent: 1.20; lr: 1.00000; 5106/7254 tok/s;     15 sec\n",
      "[2021-01-30 02:30:16,517 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:16,524 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:17,286 INFO] Step 800/ 4000; acc:  69.98; ppl:  2.70; xent: 0.99; lr: 1.00000; 5076/7279 tok/s;     16 sec\n",
      "[2021-01-30 02:30:17,286 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:17,294 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:18,040 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:18,046 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:18,227 INFO] Step 850/ 4000; acc:  74.00; ppl:  2.35; xent: 0.85; lr: 1.00000; 5088/7316 tok/s;     17 sec\n",
      "[2021-01-30 02:30:18,810 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:18,817 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:19,188 INFO] Step 900/ 4000; acc:  78.02; ppl:  2.13; xent: 0.76; lr: 1.00000; 4934/7135 tok/s;     18 sec\n",
      "[2021-01-30 02:30:19,573 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:19,580 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:20,192 INFO] Step 950/ 4000; acc:  80.60; ppl:  1.94; xent: 0.66; lr: 1.00000; 4947/7028 tok/s;     19 sec\n",
      "[2021-01-30 02:30:20,367 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:20,375 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:21,137 INFO] Step 1000/ 4000; acc:  84.98; ppl:  1.70; xent: 0.53; lr: 1.00000; 5044/7234 tok/s;     20 sec\n",
      "[2021-01-30 02:30:21,138 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:21,145 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:21,887 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:21,893 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:22,078 INFO] Step 1050/ 4000; acc:  85.84; ppl:  1.63; xent: 0.49; lr: 1.00000; 5089/7318 tok/s;     21 sec\n",
      "[2021-01-30 02:30:22,686 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:22,714 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:23,113 INFO] Step 1100/ 4000; acc:  88.02; ppl:  1.51; xent: 0.41; lr: 1.00000; 4581/6625 tok/s;     22 sec\n",
      "[2021-01-30 02:30:23,551 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:23,559 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:24,151 INFO] Step 1150/ 4000; acc:  87.48; ppl:  1.55; xent: 0.44; lr: 1.00000; 4784/6796 tok/s;     23 sec\n",
      "[2021-01-30 02:30:24,342 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:24,350 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:25,100 INFO] Step 1200/ 4000; acc:  90.25; ppl:  1.41; xent: 0.34; lr: 1.00000; 5028/7210 tok/s;     24 sec\n",
      "[2021-01-30 02:30:25,100 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:25,110 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:25,874 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:25,882 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:26,069 INFO] Step 1250/ 4000; acc:  91.23; ppl:  1.36; xent: 0.31; lr: 1.00000; 4942/7106 tok/s;     25 sec\n",
      "[2021-01-30 02:30:26,635 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:26,642 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:27,030 INFO] Step 1300/ 4000; acc:  92.12; ppl:  1.33; xent: 0.28; lr: 1.00000; 4932/7133 tok/s;     26 sec\n",
      "[2021-01-30 02:30:27,428 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:27,435 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:28,018 INFO] Step 1350/ 4000; acc:  91.76; ppl:  1.35; xent: 0.30; lr: 1.00000; 5021/7133 tok/s;     27 sec\n",
      "[2021-01-30 02:30:28,199 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:28,207 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:28,982 INFO] Step 1400/ 4000; acc:  93.29; ppl:  1.27; xent: 0.24; lr: 1.00000; 4948/7096 tok/s;     28 sec\n",
      "[2021-01-30 02:30:28,983 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:28,989 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:29,753 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:29,760 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:29,944 INFO] Step 1450/ 4000; acc:  93.94; ppl:  1.23; xent: 0.21; lr: 1.00000; 4981/7163 tok/s;     29 sec\n",
      "[2021-01-30 02:30:30,606 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:30,614 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:30,997 INFO] Step 1500/ 4000; acc:  93.81; ppl:  1.24; xent: 0.21; lr: 1.00000; 4502/6511 tok/s;     30 sec\n",
      "[2021-01-30 02:30:31,401 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:31,408 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:32,006 INFO] Step 1550/ 4000; acc:  92.82; ppl:  1.30; xent: 0.26; lr: 1.00000; 4920/6990 tok/s;     31 sec\n",
      "[2021-01-30 02:30:32,181 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:32,188 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:32,947 INFO] Step 1600/ 4000; acc:  94.06; ppl:  1.24; xent: 0.22; lr: 1.00000; 5067/7266 tok/s;     32 sec\n",
      "[2021-01-30 02:30:32,947 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:32,954 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:33,705 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:33,713 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:33,898 INFO] Step 1650/ 4000; acc:  94.41; ppl:  1.21; xent: 0.19; lr: 1.00000; 5038/7244 tok/s;     33 sec\n",
      "[2021-01-30 02:30:34,513 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:34,520 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:34,882 INFO] Step 1700/ 4000; acc:  93.70; ppl:  1.26; xent: 0.23; lr: 1.00000; 4815/6963 tok/s;     34 sec\n",
      "[2021-01-30 02:30:35,278 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:35,285 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:35,877 INFO] Step 1750/ 4000; acc:  94.38; ppl:  1.24; xent: 0.22; lr: 1.00000; 4991/7091 tok/s;     35 sec\n",
      "[2021-01-30 02:30:36,057 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:36,065 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:36,880 INFO] Step 1800/ 4000; acc:  95.38; ppl:  1.17; xent: 0.16; lr: 1.00000; 4753/6817 tok/s;     36 sec\n",
      "[2021-01-30 02:30:36,881 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:36,887 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:37,661 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:37,687 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:37,870 INFO] Step 1850/ 4000; acc:  95.61; ppl:  1.18; xent: 0.16; lr: 1.00000; 4840/6960 tok/s;     37 sec\n",
      "[2021-01-30 02:30:38,438 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:38,444 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:38,819 INFO] Step 1900/ 4000; acc:  95.65; ppl:  1.17; xent: 0.16; lr: 1.00000; 4997/7226 tok/s;     38 sec\n",
      "[2021-01-30 02:30:39,217 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:39,224 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:39,798 INFO] Step 1950/ 4000; acc:  94.94; ppl:  1.21; xent: 0.19; lr: 1.00000; 5067/7198 tok/s;     39 sec\n",
      "[2021-01-30 02:30:39,989 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:40,000 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:40,761 INFO] Step 2000/ 4000; acc:  95.92; ppl:  1.17; xent: 0.15; lr: 1.00000; 4955/7106 tok/s;     40 sec\n",
      "[2021-01-30 02:30:40,761 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:40,768 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:41,542 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:41,550 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:41,735 INFO] Step 2050/ 4000; acc:  96.01; ppl:  1.16; xent: 0.15; lr: 1.00000; 4918/7072 tok/s;     41 sec\n",
      "[2021-01-30 02:30:42,336 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:42,350 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:42,723 INFO] Step 2100/ 4000; acc:  96.05; ppl:  1.18; xent: 0.16; lr: 1.00000; 4796/6936 tok/s;     42 sec\n",
      "[2021-01-30 02:30:43,109 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:43,116 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:43,687 INFO] Step 2150/ 4000; acc:  95.60; ppl:  1.17; xent: 0.16; lr: 1.00000; 5149/7316 tok/s;     43 sec\n",
      "[2021-01-30 02:30:43,864 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:43,871 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:44,630 INFO] Step 2200/ 4000; acc:  95.98; ppl:  1.18; xent: 0.17; lr: 1.00000; 5059/7255 tok/s;     44 sec\n",
      "[2021-01-30 02:30:44,630 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:44,637 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:45,412 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:45,419 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:45,597 INFO] Step 2250/ 4000; acc:  95.72; ppl:  1.18; xent: 0.16; lr: 1.00000; 4954/7124 tok/s;     45 sec\n",
      "[2021-01-30 02:30:46,161 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:46,167 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:46,537 INFO] Step 2300/ 4000; acc:  95.35; ppl:  1.19; xent: 0.18; lr: 1.00000; 5040/7289 tok/s;     45 sec\n",
      "[2021-01-30 02:30:46,912 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:46,919 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:47,509 INFO] Step 2350/ 4000; acc:  96.41; ppl:  1.14; xent: 0.13; lr: 1.00000; 5109/7258 tok/s;     46 sec\n",
      "[2021-01-30 02:30:47,680 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:47,686 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:48,465 INFO] Step 2400/ 4000; acc:  96.02; ppl:  1.18; xent: 0.17; lr: 1.00000; 4986/7151 tok/s;     47 sec\n",
      "[2021-01-30 02:30:48,466 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:48,473 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:49,204 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:49,211 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:49,390 INFO] Step 2450/ 4000; acc:  95.92; ppl:  1.18; xent: 0.17; lr: 1.00000; 5180/7449 tok/s;     48 sec\n",
      "[2021-01-30 02:30:50,007 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:50,014 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:50,386 INFO] Step 2500/ 4000; acc:  95.96; ppl:  1.16; xent: 0.15; lr: 1.00000; 4757/6880 tok/s;     49 sec\n",
      "[2021-01-30 02:30:50,765 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:50,772 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:51,369 INFO] Step 2550/ 4000; acc:  95.96; ppl:  1.18; xent: 0.16; lr: 1.00000; 5053/7179 tok/s;     50 sec\n",
      "[2021-01-30 02:30:51,550 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:51,557 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:52,295 INFO] Step 2600/ 4000; acc:  96.05; ppl:  1.19; xent: 0.17; lr: 1.00000; 5150/7385 tok/s;     51 sec\n",
      "[2021-01-30 02:30:52,295 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:52,322 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:53,059 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:53,065 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:53,241 INFO] Step 2650/ 4000; acc:  96.49; ppl:  1.14; xent: 0.14; lr: 1.00000; 5060/7276 tok/s;     52 sec\n",
      "[2021-01-30 02:30:53,797 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:53,804 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:54,160 INFO] Step 2700/ 4000; acc:  96.67; ppl:  1.13; xent: 0.12; lr: 1.00000; 5160/7462 tok/s;     53 sec\n",
      "[2021-01-30 02:30:54,540 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:54,548 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:55,138 INFO] Step 2750/ 4000; acc:  96.43; ppl:  1.17; xent: 0.16; lr: 1.00000; 5078/7215 tok/s;     54 sec\n",
      "[2021-01-30 02:30:55,330 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:55,337 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:56,096 INFO] Step 2800/ 4000; acc:  96.23; ppl:  1.17; xent: 0.16; lr: 1.00000; 4978/7139 tok/s;     55 sec\n",
      "[2021-01-30 02:30:56,096 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:56,103 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:56,862 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:56,868 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:57,046 INFO] Step 2850/ 4000; acc:  96.50; ppl:  1.16; xent: 0.15; lr: 1.00000; 5038/7245 tok/s;     56 sec\n",
      "[2021-01-30 02:30:57,646 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:57,655 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:58,012 INFO] Step 2900/ 4000; acc:  96.98; ppl:  1.13; xent: 0.12; lr: 1.00000; 4908/7099 tok/s;     57 sec\n",
      "[2021-01-30 02:30:58,419 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:58,426 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:59,032 INFO] Step 2950/ 4000; acc:  95.80; ppl:  1.21; xent: 0.19; lr: 1.00000; 4868/6915 tok/s;     58 sec\n",
      "[2021-01-30 02:30:59,210 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:59,217 INFO] number of examples: 800\n",
      "[2021-01-30 02:30:59,976 INFO] Step 3000/ 4000; acc:  96.01; ppl:  1.18; xent: 0.16; lr: 1.00000; 5054/7248 tok/s;     59 sec\n",
      "[2021-01-30 02:30:59,976 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:30:59,984 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:00,733 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:00,740 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:00,924 INFO] Step 3050/ 4000; acc:  96.63; ppl:  1.14; xent: 0.13; lr: 1.00000; 5053/7266 tok/s;     60 sec\n",
      "[2021-01-30 02:31:01,514 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:01,521 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:01,888 INFO] Step 3100/ 4000; acc:  96.46; ppl:  1.16; xent: 0.15; lr: 1.00000; 4917/7111 tok/s;     61 sec\n",
      "[2021-01-30 02:31:02,293 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:02,300 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:02,890 INFO] Step 3150/ 4000; acc:  96.23; ppl:  1.18; xent: 0.17; lr: 1.00000; 4954/7038 tok/s;     62 sec\n",
      "[2021-01-30 02:31:03,070 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:03,077 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:03,836 INFO] Step 3200/ 4000; acc:  96.49; ppl:  1.15; xent: 0.14; lr: 1.00000; 5041/7229 tok/s;     63 sec\n",
      "[2021-01-30 02:31:03,836 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:03,843 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:04,615 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:04,622 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:04,813 INFO] Step 3250/ 4000; acc:  96.91; ppl:  1.14; xent: 0.13; lr: 1.00000; 4901/7048 tok/s;     64 sec\n",
      "[2021-01-30 02:31:05,373 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:05,380 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:05,758 INFO] Step 3300/ 4000; acc:  96.31; ppl:  1.18; xent: 0.16; lr: 1.00000; 5017/7255 tok/s;     65 sec\n",
      "[2021-01-30 02:31:06,174 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:06,181 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:06,754 INFO] Step 3350/ 4000; acc:  96.75; ppl:  1.18; xent: 0.16; lr: 1.00000; 4985/7083 tok/s;     66 sec\n",
      "[2021-01-30 02:31:06,931 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:06,957 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:07,700 INFO] Step 3400/ 4000; acc:  96.53; ppl:  1.18; xent: 0.16; lr: 1.00000; 5041/7229 tok/s;     67 sec\n",
      "[2021-01-30 02:31:07,700 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:07,707 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:08,471 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:08,477 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:08,654 INFO] Step 3450/ 4000; acc:  96.22; ppl:  1.17; xent: 0.16; lr: 1.00000; 5023/7222 tok/s;     68 sec\n",
      "[2021-01-30 02:31:09,207 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:09,214 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:09,608 INFO] Step 3500/ 4000; acc:  95.13; ppl:  1.25; xent: 0.23; lr: 1.00000; 4968/7184 tok/s;     69 sec\n",
      "[2021-01-30 02:31:10,011 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:10,018 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:10,576 INFO] Step 3550/ 4000; acc:  95.29; ppl:  1.22; xent: 0.20; lr: 1.00000; 5128/7286 tok/s;     70 sec\n",
      "[2021-01-30 02:31:10,756 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:10,763 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:11,496 INFO] Step 3600/ 4000; acc:  95.73; ppl:  1.20; xent: 0.18; lr: 1.00000; 5184/7434 tok/s;     70 sec\n",
      "[2021-01-30 02:31:11,496 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:11,503 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:12,240 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:12,246 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:12,429 INFO] Step 3650/ 4000; acc:  95.73; ppl:  1.21; xent: 0.19; lr: 1.00000; 5130/7377 tok/s;     71 sec\n",
      "[2021-01-30 02:31:13,040 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:13,048 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:13,419 INFO] Step 3700/ 4000; acc:  96.44; ppl:  1.17; xent: 0.15; lr: 1.00000; 4792/6930 tok/s;     72 sec\n",
      "[2021-01-30 02:31:13,810 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:13,818 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:14,397 INFO] Step 3750/ 4000; acc:  96.36; ppl:  1.18; xent: 0.17; lr: 1.00000; 5074/7209 tok/s;     73 sec\n",
      "[2021-01-30 02:31:14,573 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:14,580 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:15,332 INFO] Step 3800/ 4000; acc:  96.52; ppl:  1.15; xent: 0.14; lr: 1.00000; 5103/7318 tok/s;     74 sec\n",
      "[2021-01-30 02:31:15,332 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:15,339 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:16,113 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:16,120 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:16,303 INFO] Step 3850/ 4000; acc:  96.70; ppl:  1.15; xent: 0.14; lr: 1.00000; 4932/7092 tok/s;     75 sec\n",
      "[2021-01-30 02:31:16,872 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:16,879 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:17,266 INFO] Step 3900/ 4000; acc:  96.56; ppl:  1.18; xent: 0.17; lr: 1.00000; 4923/7120 tok/s;     76 sec\n",
      "[2021-01-30 02:31:17,684 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:17,691 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:18,278 INFO] Step 3950/ 4000; acc:  95.80; ppl:  1.23; xent: 0.21; lr: 1.00000; 4905/6968 tok/s;     77 sec\n",
      "[2021-01-30 02:31:18,459 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_7/processed.train.0.pt\n",
      "[2021-01-30 02:31:18,466 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:19,269 INFO] Step 4000/ 4000; acc:  96.08; ppl:  1.19; xent: 0.18; lr: 1.00000; 4810/6898 tok/s;     78 sec\n",
      "[2021-01-30 02:31:19,271 INFO] Saving checkpoint drive/MyDrive/EnglishToleranceBaseline/output/english_rnn_model_800_7_step_4000.pt\n",
      "[2021-01-30 02:31:20,995 INFO]  * src vocab size = 43\n",
      "[2021-01-30 02:31:20,995 INFO]  * tgt vocab size = 45\n",
      "[2021-01-30 02:31:20,995 INFO] Building model...\n",
      "[2021-01-30 02:31:25,326 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(43, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(45, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=45, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-01-30 02:31:25,327 INFO] encoder: 254500\n",
      "[2021-01-30 02:31:25,327 INFO] decoder: 329645\n",
      "[2021-01-30 02:31:25,327 INFO] * number of parameters: 584145\n",
      "[2021-01-30 02:31:25,329 INFO] Starting training on GPU: [0]\n",
      "[2021-01-30 02:31:25,329 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-01-30 02:31:25,330 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:25,933 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:26,737 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:26,745 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:26,935 INFO] Step 50/ 4000; acc:  12.81; ppl: 34.35; xent: 3.54; lr: 1.00000; 2999/4315 tok/s;      2 sec\n",
      "[2021-01-30 02:31:27,496 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:27,502 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:27,869 INFO] Step 100/ 4000; acc:  20.79; ppl: 18.97; xent: 2.94; lr: 1.00000; 5246/7509 tok/s;      3 sec\n",
      "[2021-01-30 02:31:28,242 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:28,249 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:28,843 INFO] Step 150/ 4000; acc:  22.67; ppl: 16.76; xent: 2.82; lr: 1.00000; 4967/7125 tok/s;      4 sec\n",
      "[2021-01-30 02:31:29,036 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:29,043 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:29,807 INFO] Step 200/ 4000; acc:  25.30; ppl: 14.61; xent: 2.68; lr: 1.00000; 5006/7194 tok/s;      4 sec\n",
      "[2021-01-30 02:31:29,807 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:29,815 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:30,608 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:30,615 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:30,792 INFO] Step 250/ 4000; acc:  29.76; ppl: 12.04; xent: 2.49; lr: 1.00000; 4889/7032 tok/s;      5 sec\n",
      "[2021-01-30 02:31:31,361 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:31,370 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:31,762 INFO] Step 300/ 4000; acc:  33.01; ppl: 10.29; xent: 2.33; lr: 1.00000; 5053/7233 tok/s;      6 sec\n",
      "[2021-01-30 02:31:32,139 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:32,165 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:32,732 INFO] Step 350/ 4000; acc:  37.87; ppl:  8.57; xent: 2.15; lr: 1.00000; 4985/7150 tok/s;      7 sec\n",
      "[2021-01-30 02:31:32,915 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:32,921 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:33,661 INFO] Step 400/ 4000; acc:  41.32; ppl:  7.41; xent: 2.00; lr: 1.00000; 5194/7464 tok/s;      8 sec\n",
      "[2021-01-30 02:31:33,662 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:33,669 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:34,427 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:34,434 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:34,619 INFO] Step 450/ 4000; acc:  44.34; ppl:  6.73; xent: 1.91; lr: 1.00000; 5031/7237 tok/s;      9 sec\n",
      "[2021-01-30 02:31:35,224 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:35,232 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:35,604 INFO] Step 500/ 4000; acc:  47.97; ppl:  6.10; xent: 1.81; lr: 1.00000; 4973/7118 tok/s;     10 sec\n",
      "[2021-01-30 02:31:35,985 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:35,992 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:36,565 INFO] Step 550/ 4000; acc:  51.16; ppl:  5.21; xent: 1.65; lr: 1.00000; 5033/7218 tok/s;     11 sec\n",
      "[2021-01-30 02:31:36,752 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:36,758 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:37,549 INFO] Step 600/ 4000; acc:  55.65; ppl:  4.49; xent: 1.50; lr: 1.00000; 4907/7052 tok/s;     12 sec\n",
      "[2021-01-30 02:31:37,549 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:37,556 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:38,294 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:38,301 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:38,486 INFO] Step 650/ 4000; acc:  60.47; ppl:  3.82; xent: 1.34; lr: 1.00000; 5135/7387 tok/s;     13 sec\n",
      "[2021-01-30 02:31:39,052 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:39,063 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:39,442 INFO] Step 700/ 4000; acc:  66.05; ppl:  3.12; xent: 1.14; lr: 1.00000; 5130/7343 tok/s;     14 sec\n",
      "[2021-01-30 02:31:39,818 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:39,824 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:40,421 INFO] Step 750/ 4000; acc:  70.80; ppl:  2.63; xent: 0.97; lr: 1.00000; 4936/7080 tok/s;     15 sec\n",
      "[2021-01-30 02:31:40,605 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:40,611 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:41,358 INFO] Step 800/ 4000; acc:  75.17; ppl:  2.28; xent: 0.82; lr: 1.00000; 5153/7406 tok/s;     16 sec\n",
      "[2021-01-30 02:31:41,358 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:41,365 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:42,138 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:42,144 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:42,329 INFO] Step 850/ 4000; acc:  79.18; ppl:  2.11; xent: 0.75; lr: 1.00000; 4959/7133 tok/s;     17 sec\n",
      "[2021-01-30 02:31:42,922 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:42,929 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:43,310 INFO] Step 900/ 4000; acc:  83.40; ppl:  1.77; xent: 0.57; lr: 1.00000; 4997/7153 tok/s;     18 sec\n",
      "[2021-01-30 02:31:43,684 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:43,691 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:44,274 INFO] Step 950/ 4000; acc:  86.46; ppl:  1.64; xent: 0.50; lr: 1.00000; 5016/7195 tok/s;     19 sec\n",
      "[2021-01-30 02:31:44,461 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:44,468 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:45,203 INFO] Step 1000/ 4000; acc:  88.16; ppl:  1.56; xent: 0.44; lr: 1.00000; 5194/7465 tok/s;     20 sec\n",
      "[2021-01-30 02:31:45,203 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:45,210 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:45,939 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:45,946 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:46,126 INFO] Step 1050/ 4000; acc:  90.01; ppl:  1.43; xent: 0.36; lr: 1.00000; 5219/7507 tok/s;     21 sec\n",
      "[2021-01-30 02:31:46,687 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:46,712 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:47,101 INFO] Step 1100/ 4000; acc:  91.19; ppl:  1.39; xent: 0.33; lr: 1.00000; 5023/7190 tok/s;     22 sec\n",
      "[2021-01-30 02:31:47,483 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:47,490 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:48,056 INFO] Step 1150/ 4000; acc:  91.94; ppl:  1.33; xent: 0.28; lr: 1.00000; 5065/7265 tok/s;     23 sec\n",
      "[2021-01-30 02:31:48,251 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:48,259 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:48,993 INFO] Step 1200/ 4000; acc:  92.63; ppl:  1.31; xent: 0.27; lr: 1.00000; 5153/7406 tok/s;     24 sec\n",
      "[2021-01-30 02:31:48,993 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:49,000 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:49,768 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:49,775 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:49,960 INFO] Step 1250/ 4000; acc:  92.81; ppl:  1.31; xent: 0.27; lr: 1.00000; 4980/7163 tok/s;     25 sec\n",
      "[2021-01-30 02:31:50,538 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:50,545 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:50,928 INFO] Step 1300/ 4000; acc:  93.04; ppl:  1.29; xent: 0.25; lr: 1.00000; 5064/7249 tok/s;     26 sec\n",
      "[2021-01-30 02:31:51,322 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:51,333 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:51,907 INFO] Step 1350/ 4000; acc:  93.87; ppl:  1.24; xent: 0.22; lr: 1.00000; 4939/7085 tok/s;     27 sec\n",
      "[2021-01-30 02:31:52,103 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:52,111 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:52,919 INFO] Step 1400/ 4000; acc:  94.80; ppl:  1.21; xent: 0.19; lr: 1.00000; 4769/6854 tok/s;     28 sec\n",
      "[2021-01-30 02:31:52,919 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:52,926 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:53,673 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:53,680 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:53,864 INFO] Step 1450/ 4000; acc:  94.07; ppl:  1.25; xent: 0.22; lr: 1.00000; 5096/7331 tok/s;     29 sec\n",
      "[2021-01-30 02:31:54,440 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:54,447 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:54,831 INFO] Step 1500/ 4000; acc:  95.19; ppl:  1.20; xent: 0.18; lr: 1.00000; 5067/7253 tok/s;     30 sec\n",
      "[2021-01-30 02:31:55,198 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:55,205 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:55,774 INFO] Step 1550/ 4000; acc:  95.62; ppl:  1.19; xent: 0.17; lr: 1.00000; 5128/7355 tok/s;     30 sec\n",
      "[2021-01-30 02:31:55,965 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:55,972 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:56,790 INFO] Step 1600/ 4000; acc:  95.54; ppl:  1.18; xent: 0.16; lr: 1.00000; 4752/6830 tok/s;     31 sec\n",
      "[2021-01-30 02:31:56,790 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:56,797 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:57,554 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:57,561 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:57,741 INFO] Step 1650/ 4000; acc:  95.57; ppl:  1.18; xent: 0.17; lr: 1.00000; 5064/7285 tok/s;     32 sec\n",
      "[2021-01-30 02:31:58,349 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:58,356 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:58,734 INFO] Step 1700/ 4000; acc:  94.92; ppl:  1.23; xent: 0.21; lr: 1.00000; 4934/7064 tok/s;     33 sec\n",
      "[2021-01-30 02:31:59,108 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:59,115 INFO] number of examples: 800\n",
      "[2021-01-30 02:31:59,687 INFO] Step 1750/ 4000; acc:  96.02; ppl:  1.17; xent: 0.16; lr: 1.00000; 5071/7274 tok/s;     34 sec\n",
      "[2021-01-30 02:31:59,874 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:31:59,881 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:00,644 INFO] Step 1800/ 4000; acc:  95.99; ppl:  1.16; xent: 0.15; lr: 1.00000; 5047/7253 tok/s;     35 sec\n",
      "[2021-01-30 02:32:00,644 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:00,651 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:01,422 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:01,448 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:01,630 INFO] Step 1850/ 4000; acc:  96.00; ppl:  1.16; xent: 0.15; lr: 1.00000; 4883/7023 tok/s;     36 sec\n",
      "[2021-01-30 02:32:02,216 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:02,223 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:02,604 INFO] Step 1900/ 4000; acc:  95.79; ppl:  1.18; xent: 0.16; lr: 1.00000; 5031/7203 tok/s;     37 sec\n",
      "[2021-01-30 02:32:02,989 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:02,997 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:03,588 INFO] Step 1950/ 4000; acc:  95.82; ppl:  1.16; xent: 0.15; lr: 1.00000; 4913/7046 tok/s;     38 sec\n",
      "[2021-01-30 02:32:03,775 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:03,782 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:04,521 INFO] Step 2000/ 4000; acc:  95.39; ppl:  1.21; xent: 0.19; lr: 1.00000; 5173/7435 tok/s;     39 sec\n",
      "[2021-01-30 02:32:04,521 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:04,528 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:05,267 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:05,274 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:05,467 INFO] Step 2050/ 4000; acc:  96.25; ppl:  1.15; xent: 0.14; lr: 1.00000; 5089/7321 tok/s;     40 sec\n",
      "[2021-01-30 02:32:06,029 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:06,036 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:06,421 INFO] Step 2100/ 4000; acc:  95.85; ppl:  1.19; xent: 0.18; lr: 1.00000; 5141/7360 tok/s;     41 sec\n",
      "[2021-01-30 02:32:06,805 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:06,812 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:07,391 INFO] Step 2150/ 4000; acc:  96.14; ppl:  1.18; xent: 0.16; lr: 1.00000; 4982/7147 tok/s;     42 sec\n",
      "[2021-01-30 02:32:07,588 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:07,595 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:08,379 INFO] Step 2200/ 4000; acc:  95.56; ppl:  1.20; xent: 0.18; lr: 1.00000; 4885/7020 tok/s;     43 sec\n",
      "[2021-01-30 02:32:08,380 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:08,386 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:09,129 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:09,136 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:09,315 INFO] Step 2250/ 4000; acc:  96.72; ppl:  1.13; xent: 0.12; lr: 1.00000; 5147/7404 tok/s;     44 sec\n",
      "[2021-01-30 02:32:09,900 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:09,907 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:10,298 INFO] Step 2300/ 4000; acc:  96.49; ppl:  1.16; xent: 0.15; lr: 1.00000; 4986/7137 tok/s;     45 sec\n",
      "[2021-01-30 02:32:10,693 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:10,700 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:11,268 INFO] Step 2350/ 4000; acc:  96.47; ppl:  1.16; xent: 0.15; lr: 1.00000; 4983/7147 tok/s;     46 sec\n",
      "[2021-01-30 02:32:11,467 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:11,474 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:12,288 INFO] Step 2400/ 4000; acc:  96.16; ppl:  1.18; xent: 0.17; lr: 1.00000; 4734/6804 tok/s;     47 sec\n",
      "[2021-01-30 02:32:12,288 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:12,295 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:13,059 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:13,066 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:13,245 INFO] Step 2450/ 4000; acc:  96.77; ppl:  1.14; xent: 0.13; lr: 1.00000; 5031/7237 tok/s;     48 sec\n",
      "[2021-01-30 02:32:13,797 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:13,804 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:14,188 INFO] Step 2500/ 4000; acc:  96.92; ppl:  1.15; xent: 0.14; lr: 1.00000; 5194/7435 tok/s;     49 sec\n",
      "[2021-01-30 02:32:14,568 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:14,576 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:15,126 INFO] Step 2550/ 4000; acc:  96.52; ppl:  1.16; xent: 0.15; lr: 1.00000; 5159/7400 tok/s;     50 sec\n",
      "[2021-01-30 02:32:15,325 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:15,333 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:16,102 INFO] Step 2600/ 4000; acc:  96.60; ppl:  1.15; xent: 0.14; lr: 1.00000; 4945/7107 tok/s;     51 sec\n",
      "[2021-01-30 02:32:16,102 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:16,128 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:16,896 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:16,902 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:17,083 INFO] Step 2650/ 4000; acc:  95.99; ppl:  1.18; xent: 0.17; lr: 1.00000; 4908/7060 tok/s;     52 sec\n",
      "[2021-01-30 02:32:17,645 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:17,652 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:18,020 INFO] Step 2700/ 4000; acc:  96.28; ppl:  1.18; xent: 0.16; lr: 1.00000; 5227/7483 tok/s;     53 sec\n",
      "[2021-01-30 02:32:18,398 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:18,406 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:18,984 INFO] Step 2750/ 4000; acc:  96.61; ppl:  1.16; xent: 0.15; lr: 1.00000; 5018/7197 tok/s;     54 sec\n",
      "[2021-01-30 02:32:19,172 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:19,179 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:19,975 INFO] Step 2800/ 4000; acc:  96.18; ppl:  1.19; xent: 0.17; lr: 1.00000; 4873/7004 tok/s;     55 sec\n",
      "[2021-01-30 02:32:19,975 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:19,982 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:20,731 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:20,739 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:20,920 INFO] Step 2850/ 4000; acc:  96.52; ppl:  1.17; xent: 0.15; lr: 1.00000; 5093/7326 tok/s;     56 sec\n",
      "[2021-01-30 02:32:21,511 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:21,519 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:21,903 INFO] Step 2900/ 4000; acc:  96.08; ppl:  1.20; xent: 0.18; lr: 1.00000; 4985/7135 tok/s;     57 sec\n",
      "[2021-01-30 02:32:22,301 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:22,308 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:22,870 INFO] Step 2950/ 4000; acc:  96.65; ppl:  1.17; xent: 0.15; lr: 1.00000; 5001/7173 tok/s;     58 sec\n",
      "[2021-01-30 02:32:23,057 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:23,064 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:23,838 INFO] Step 3000/ 4000; acc:  97.15; ppl:  1.13; xent: 0.12; lr: 1.00000; 4987/7167 tok/s;     59 sec\n",
      "[2021-01-30 02:32:23,839 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:23,846 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:24,598 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:24,604 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:24,794 INFO] Step 3050/ 4000; acc:  96.30; ppl:  1.18; xent: 0.16; lr: 1.00000; 5041/7252 tok/s;     59 sec\n",
      "[2021-01-30 02:32:25,354 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:25,361 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:25,739 INFO] Step 3100/ 4000; acc:  96.39; ppl:  1.20; xent: 0.18; lr: 1.00000; 5181/7417 tok/s;     60 sec\n",
      "[2021-01-30 02:32:26,129 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:26,137 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:26,721 INFO] Step 3150/ 4000; acc:  95.88; ppl:  1.21; xent: 0.19; lr: 1.00000; 4925/7065 tok/s;     61 sec\n",
      "[2021-01-30 02:32:26,910 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:26,917 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:27,703 INFO] Step 3200/ 4000; acc:  96.35; ppl:  1.17; xent: 0.16; lr: 1.00000; 4917/7067 tok/s;     62 sec\n",
      "[2021-01-30 02:32:27,703 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:27,711 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:28,489 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:28,497 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:28,680 INFO] Step 3250/ 4000; acc:  96.36; ppl:  1.19; xent: 0.17; lr: 1.00000; 4930/7091 tok/s;     63 sec\n",
      "[2021-01-30 02:32:29,252 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:29,260 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:29,646 INFO] Step 3300/ 4000; acc:  96.72; ppl:  1.16; xent: 0.15; lr: 1.00000; 5071/7259 tok/s;     64 sec\n",
      "[2021-01-30 02:32:30,031 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:30,038 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:30,605 INFO] Step 3350/ 4000; acc:  96.74; ppl:  1.18; xent: 0.17; lr: 1.00000; 5045/7236 tok/s;     65 sec\n",
      "[2021-01-30 02:32:30,798 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:30,825 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:31,592 INFO] Step 3400/ 4000; acc:  96.41; ppl:  1.20; xent: 0.18; lr: 1.00000; 4890/7027 tok/s;     66 sec\n",
      "[2021-01-30 02:32:31,592 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:31,606 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:32,405 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:32,412 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:32,596 INFO] Step 3450/ 4000; acc:  96.52; ppl:  1.16; xent: 0.15; lr: 1.00000; 4795/6897 tok/s;     67 sec\n",
      "[2021-01-30 02:32:33,191 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:33,198 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:33,579 INFO] Step 3500/ 4000; acc:  96.63; ppl:  1.17; xent: 0.16; lr: 1.00000; 4985/7137 tok/s;     68 sec\n",
      "[2021-01-30 02:32:33,985 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:33,993 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:34,561 INFO] Step 3550/ 4000; acc:  96.01; ppl:  1.21; xent: 0.19; lr: 1.00000; 4926/7065 tok/s;     69 sec\n",
      "[2021-01-30 02:32:34,747 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:34,755 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:35,536 INFO] Step 3600/ 4000; acc:  96.09; ppl:  1.20; xent: 0.19; lr: 1.00000; 4953/7118 tok/s;     70 sec\n",
      "[2021-01-30 02:32:35,536 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:35,542 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:36,352 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:36,359 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:36,540 INFO] Step 3650/ 4000; acc:  96.56; ppl:  1.18; xent: 0.17; lr: 1.00000; 4794/6895 tok/s;     71 sec\n",
      "[2021-01-30 02:32:37,106 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:37,113 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:37,492 INFO] Step 3700/ 4000; acc:  96.29; ppl:  1.19; xent: 0.17; lr: 1.00000; 5148/7369 tok/s;     72 sec\n",
      "[2021-01-30 02:32:37,872 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:37,879 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:38,440 INFO] Step 3750/ 4000; acc:  96.32; ppl:  1.19; xent: 0.17; lr: 1.00000; 5103/7320 tok/s;     73 sec\n",
      "[2021-01-30 02:32:38,626 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:38,633 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:39,382 INFO] Step 3800/ 4000; acc:  96.54; ppl:  1.17; xent: 0.16; lr: 1.00000; 5120/7359 tok/s;     74 sec\n",
      "[2021-01-30 02:32:39,383 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:39,389 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:40,141 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:40,148 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:40,334 INFO] Step 3850/ 4000; acc:  96.75; ppl:  1.15; xent: 0.14; lr: 1.00000; 5060/7279 tok/s;     75 sec\n",
      "[2021-01-30 02:32:40,951 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:40,958 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:41,335 INFO] Step 3900/ 4000; acc:  96.75; ppl:  1.15; xent: 0.14; lr: 1.00000; 4896/7009 tok/s;     76 sec\n",
      "[2021-01-30 02:32:41,717 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:41,724 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:42,316 INFO] Step 3950/ 4000; acc:  97.39; ppl:  1.15; xent: 0.14; lr: 1.00000; 4927/7067 tok/s;     77 sec\n",
      "[2021-01-30 02:32:42,502 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_8/processed.train.0.pt\n",
      "[2021-01-30 02:32:42,509 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:43,249 INFO] Step 4000/ 4000; acc:  97.04; ppl:  1.15; xent: 0.14; lr: 1.00000; 5174/7437 tok/s;     78 sec\n",
      "[2021-01-30 02:32:43,250 INFO] Saving checkpoint drive/MyDrive/EnglishToleranceBaseline/output/english_rnn_model_800_8_step_4000.pt\n",
      "[2021-01-30 02:32:44,984 INFO]  * src vocab size = 43\n",
      "[2021-01-30 02:32:44,984 INFO]  * tgt vocab size = 45\n",
      "[2021-01-30 02:32:44,984 INFO] Building model...\n",
      "[2021-01-30 02:32:49,232 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(43, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(45, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=45, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-01-30 02:32:49,233 INFO] encoder: 254500\n",
      "[2021-01-30 02:32:49,233 INFO] decoder: 329645\n",
      "[2021-01-30 02:32:49,233 INFO] * number of parameters: 584145\n",
      "[2021-01-30 02:32:49,235 INFO] Starting training on GPU: [0]\n",
      "[2021-01-30 02:32:49,235 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-01-30 02:32:49,236 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:32:49,874 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:50,662 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:32:50,669 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:50,848 INFO] Step 50/ 4000; acc:  13.57; ppl: 37.21; xent: 3.62; lr: 1.00000; 2965/4268 tok/s;      2 sec\n",
      "[2021-01-30 02:32:51,478 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:32:51,485 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:51,871 INFO] Step 100/ 4000; acc:  21.82; ppl: 18.76; xent: 2.93; lr: 1.00000; 4718/6806 tok/s;      3 sec\n",
      "[2021-01-30 02:32:52,262 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:32:52,269 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:52,815 INFO] Step 150/ 4000; acc:  24.24; ppl: 16.17; xent: 2.78; lr: 1.00000; 5023/7245 tok/s;      4 sec\n",
      "[2021-01-30 02:32:53,007 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:32:53,014 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:53,773 INFO] Step 200/ 4000; acc:  27.89; ppl: 14.05; xent: 2.64; lr: 1.00000; 5240/7413 tok/s;      5 sec\n",
      "[2021-01-30 02:32:53,773 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:32:53,780 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:54,547 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:32:54,554 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:54,736 INFO] Step 250/ 4000; acc:  30.22; ppl: 11.94; xent: 2.48; lr: 1.00000; 4965/7147 tok/s;      6 sec\n",
      "[2021-01-30 02:32:55,329 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:32:55,336 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:55,728 INFO] Step 300/ 4000; acc:  33.85; ppl: 10.48; xent: 2.35; lr: 1.00000; 4861/7014 tok/s;      6 sec\n",
      "[2021-01-30 02:32:56,123 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:32:56,148 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:56,730 INFO] Step 350/ 4000; acc:  40.21; ppl:  8.21; xent: 2.11; lr: 1.00000; 4734/6828 tok/s;      7 sec\n",
      "[2021-01-30 02:32:56,934 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:32:56,941 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:57,699 INFO] Step 400/ 4000; acc:  42.27; ppl:  7.45; xent: 2.01; lr: 1.00000; 5184/7333 tok/s;      8 sec\n",
      "[2021-01-30 02:32:57,699 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:32:57,706 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:58,475 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:32:58,481 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:58,669 INFO] Step 450/ 4000; acc:  46.35; ppl:  6.37; xent: 1.85; lr: 1.00000; 4930/7096 tok/s;      9 sec\n",
      "[2021-01-30 02:32:59,266 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:32:59,273 INFO] number of examples: 800\n",
      "[2021-01-30 02:32:59,652 INFO] Step 500/ 4000; acc:  47.98; ppl:  5.97; xent: 1.79; lr: 1.00000; 4906/7078 tok/s;     10 sec\n",
      "[2021-01-30 02:33:00,032 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:00,039 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:00,633 INFO] Step 550/ 4000; acc:  53.11; ppl:  4.92; xent: 1.59; lr: 1.00000; 4837/6976 tok/s;     11 sec\n",
      "[2021-01-30 02:33:00,831 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:00,838 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:01,605 INFO] Step 600/ 4000; acc:  56.47; ppl:  4.29; xent: 1.46; lr: 1.00000; 5164/7305 tok/s;     12 sec\n",
      "[2021-01-30 02:33:01,605 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:01,612 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:02,397 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:02,403 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:02,585 INFO] Step 650/ 4000; acc:  61.91; ppl:  3.55; xent: 1.27; lr: 1.00000; 4877/7020 tok/s;     13 sec\n",
      "[2021-01-30 02:33:03,164 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:03,171 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:03,538 INFO] Step 700/ 4000; acc:  66.34; ppl:  3.15; xent: 1.15; lr: 1.00000; 5061/7302 tok/s;     14 sec\n",
      "[2021-01-30 02:33:03,912 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:03,920 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:04,493 INFO] Step 750/ 4000; acc:  71.76; ppl:  2.63; xent: 0.97; lr: 1.00000; 4970/7168 tok/s;     15 sec\n",
      "[2021-01-30 02:33:04,689 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:04,696 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:05,481 INFO] Step 800/ 4000; acc:  74.95; ppl:  2.36; xent: 0.86; lr: 1.00000; 5078/7184 tok/s;     16 sec\n",
      "[2021-01-30 02:33:05,481 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:05,489 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:06,303 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:06,310 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:06,499 INFO] Step 850/ 4000; acc:  81.17; ppl:  1.96; xent: 0.67; lr: 1.00000; 4701/6767 tok/s;     17 sec\n",
      "[2021-01-30 02:33:07,109 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:07,117 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:07,513 INFO] Step 900/ 4000; acc:  83.48; ppl:  1.78; xent: 0.58; lr: 1.00000; 4755/6861 tok/s;     18 sec\n",
      "[2021-01-30 02:33:07,887 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:07,894 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:08,455 INFO] Step 950/ 4000; acc:  86.93; ppl:  1.60; xent: 0.47; lr: 1.00000; 5033/7259 tok/s;     19 sec\n",
      "[2021-01-30 02:33:08,647 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:08,654 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:09,462 INFO] Step 1000/ 4000; acc:  89.70; ppl:  1.45; xent: 0.37; lr: 1.00000; 4985/7053 tok/s;     20 sec\n",
      "[2021-01-30 02:33:09,462 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:09,469 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:10,216 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:10,222 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:10,408 INFO] Step 1050/ 4000; acc:  90.93; ppl:  1.42; xent: 0.35; lr: 1.00000; 5056/7278 tok/s;     21 sec\n",
      "[2021-01-30 02:33:10,979 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:11,004 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:11,388 INFO] Step 1100/ 4000; acc:  90.64; ppl:  1.40; xent: 0.34; lr: 1.00000; 4921/7100 tok/s;     22 sec\n",
      "[2021-01-30 02:33:11,765 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:11,771 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:12,342 INFO] Step 1150/ 4000; acc:  92.18; ppl:  1.34; xent: 0.29; lr: 1.00000; 4973/7173 tok/s;     23 sec\n",
      "[2021-01-30 02:33:12,549 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:12,557 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:13,340 INFO] Step 1200/ 4000; acc:  92.87; ppl:  1.31; xent: 0.27; lr: 1.00000; 5027/7112 tok/s;     24 sec\n",
      "[2021-01-30 02:33:13,340 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:13,347 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:14,094 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:14,103 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:14,286 INFO] Step 1250/ 4000; acc:  94.33; ppl:  1.26; xent: 0.23; lr: 1.00000; 5057/7279 tok/s;     25 sec\n",
      "[2021-01-30 02:33:14,893 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:14,900 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:15,277 INFO] Step 1300/ 4000; acc:  94.70; ppl:  1.22; xent: 0.20; lr: 1.00000; 4869/7025 tok/s;     26 sec\n",
      "[2021-01-30 02:33:15,655 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:15,662 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:16,214 INFO] Step 1350/ 4000; acc:  95.10; ppl:  1.20; xent: 0.18; lr: 1.00000; 5058/7295 tok/s;     27 sec\n",
      "[2021-01-30 02:33:16,411 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:16,418 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:17,191 INFO] Step 1400/ 4000; acc:  95.38; ppl:  1.20; xent: 0.18; lr: 1.00000; 5141/7273 tok/s;     28 sec\n",
      "[2021-01-30 02:33:17,191 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:17,198 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:17,972 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:17,979 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:18,164 INFO] Step 1450/ 4000; acc:  95.26; ppl:  1.20; xent: 0.18; lr: 1.00000; 4917/7077 tok/s;     29 sec\n",
      "[2021-01-30 02:33:18,751 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:18,758 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:19,154 INFO] Step 1500/ 4000; acc:  95.76; ppl:  1.17; xent: 0.16; lr: 1.00000; 4868/7023 tok/s;     30 sec\n",
      "[2021-01-30 02:33:19,553 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:19,560 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:20,177 INFO] Step 1550/ 4000; acc:  95.57; ppl:  1.18; xent: 0.16; lr: 1.00000; 4637/6688 tok/s;     31 sec\n",
      "[2021-01-30 02:33:20,390 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:20,403 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:21,196 INFO] Step 1600/ 4000; acc:  94.87; ppl:  1.21; xent: 0.19; lr: 1.00000; 4929/6973 tok/s;     32 sec\n",
      "[2021-01-30 02:33:21,196 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:21,202 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:21,975 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:21,983 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:22,171 INFO] Step 1650/ 4000; acc:  96.19; ppl:  1.15; xent: 0.14; lr: 1.00000; 4906/7061 tok/s;     33 sec\n",
      "[2021-01-30 02:33:22,750 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:22,756 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:23,148 INFO] Step 1700/ 4000; acc:  95.10; ppl:  1.21; xent: 0.19; lr: 1.00000; 4932/7116 tok/s;     34 sec\n",
      "[2021-01-30 02:33:23,548 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:23,555 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:24,112 INFO] Step 1750/ 4000; acc:  95.73; ppl:  1.17; xent: 0.16; lr: 1.00000; 4925/7104 tok/s;     35 sec\n",
      "[2021-01-30 02:33:24,308 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:24,315 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:25,124 INFO] Step 1800/ 4000; acc:  96.21; ppl:  1.16; xent: 0.15; lr: 1.00000; 4958/7014 tok/s;     36 sec\n",
      "[2021-01-30 02:33:25,124 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:25,131 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:25,894 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:25,921 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:26,108 INFO] Step 1850/ 4000; acc:  95.86; ppl:  1.16; xent: 0.15; lr: 1.00000; 4863/6999 tok/s;     37 sec\n",
      "[2021-01-30 02:33:26,719 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:26,726 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:27,117 INFO] Step 1900/ 4000; acc:  95.33; ppl:  1.21; xent: 0.19; lr: 1.00000; 4778/6893 tok/s;     38 sec\n",
      "[2021-01-30 02:33:27,514 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:27,522 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:28,077 INFO] Step 1950/ 4000; acc:  95.53; ppl:  1.21; xent: 0.19; lr: 1.00000; 4944/7131 tok/s;     39 sec\n",
      "[2021-01-30 02:33:28,271 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:28,278 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:29,066 INFO] Step 2000/ 4000; acc:  96.00; ppl:  1.18; xent: 0.16; lr: 1.00000; 5072/7176 tok/s;     40 sec\n",
      "[2021-01-30 02:33:29,066 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:29,073 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:29,845 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:29,853 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:30,036 INFO] Step 2050/ 4000; acc:  95.58; ppl:  1.19; xent: 0.18; lr: 1.00000; 4929/7095 tok/s;     41 sec\n",
      "[2021-01-30 02:33:30,626 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:30,633 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:31,034 INFO] Step 2100/ 4000; acc:  95.92; ppl:  1.18; xent: 0.17; lr: 1.00000; 4832/6972 tok/s;     42 sec\n",
      "[2021-01-30 02:33:31,441 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:31,448 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:32,027 INFO] Step 2150/ 4000; acc:  96.01; ppl:  1.16; xent: 0.15; lr: 1.00000; 4780/6894 tok/s;     43 sec\n",
      "[2021-01-30 02:33:32,234 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:32,242 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:33,016 INFO] Step 2200/ 4000; acc:  96.30; ppl:  1.18; xent: 0.16; lr: 1.00000; 5074/7178 tok/s;     44 sec\n",
      "[2021-01-30 02:33:33,016 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:33,023 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:33,796 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:33,804 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:33,987 INFO] Step 2250/ 4000; acc:  96.21; ppl:  1.16; xent: 0.15; lr: 1.00000; 4925/7088 tok/s;     45 sec\n",
      "[2021-01-30 02:33:34,561 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:34,567 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:34,997 INFO] Step 2300/ 4000; acc:  96.36; ppl:  1.15; xent: 0.14; lr: 1.00000; 4777/6893 tok/s;     46 sec\n",
      "[2021-01-30 02:33:35,408 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:35,415 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:35,985 INFO] Step 2350/ 4000; acc:  96.37; ppl:  1.16; xent: 0.15; lr: 1.00000; 4799/6922 tok/s;     47 sec\n",
      "[2021-01-30 02:33:36,188 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:36,199 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:36,991 INFO] Step 2400/ 4000; acc:  96.62; ppl:  1.15; xent: 0.14; lr: 1.00000; 4990/7059 tok/s;     48 sec\n",
      "[2021-01-30 02:33:36,991 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:36,998 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:37,775 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:37,782 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:37,968 INFO] Step 2450/ 4000; acc:  96.31; ppl:  1.17; xent: 0.16; lr: 1.00000; 4895/7046 tok/s;     49 sec\n",
      "[2021-01-30 02:33:38,545 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:38,552 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:38,971 INFO] Step 2500/ 4000; acc:  96.49; ppl:  1.16; xent: 0.15; lr: 1.00000; 4811/6941 tok/s;     50 sec\n",
      "[2021-01-30 02:33:39,345 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:39,352 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:39,909 INFO] Step 2550/ 4000; acc:  96.43; ppl:  1.16; xent: 0.15; lr: 1.00000; 5054/7290 tok/s;     51 sec\n",
      "[2021-01-30 02:33:40,104 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:40,111 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:40,869 INFO] Step 2600/ 4000; acc:  96.62; ppl:  1.16; xent: 0.15; lr: 1.00000; 5227/7395 tok/s;     52 sec\n",
      "[2021-01-30 02:33:40,869 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:40,895 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:41,637 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:41,644 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:41,831 INFO] Step 2650/ 4000; acc:  97.08; ppl:  1.14; xent: 0.13; lr: 1.00000; 4972/7156 tok/s;     53 sec\n",
      "[2021-01-30 02:33:42,429 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:42,436 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:42,823 INFO] Step 2700/ 4000; acc:  97.43; ppl:  1.11; xent: 0.11; lr: 1.00000; 4865/7020 tok/s;     54 sec\n",
      "[2021-01-30 02:33:43,196 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:43,205 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:43,806 INFO] Step 2750/ 4000; acc:  97.41; ppl:  1.12; xent: 0.12; lr: 1.00000; 4824/6958 tok/s;     55 sec\n",
      "[2021-01-30 02:33:44,002 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:44,009 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:44,758 INFO] Step 2800/ 4000; acc:  97.55; ppl:  1.12; xent: 0.11; lr: 1.00000; 5273/7459 tok/s;     56 sec\n",
      "[2021-01-30 02:33:44,758 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:44,765 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:45,504 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:45,510 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:45,691 INFO] Step 2850/ 4000; acc:  97.84; ppl:  1.09; xent: 0.09; lr: 1.00000; 5125/7376 tok/s;     56 sec\n",
      "[2021-01-30 02:33:46,272 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:46,279 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:46,662 INFO] Step 2900/ 4000; acc:  96.98; ppl:  1.14; xent: 0.13; lr: 1.00000; 4967/7166 tok/s;     57 sec\n",
      "[2021-01-30 02:33:47,070 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:47,079 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:47,658 INFO] Step 2950/ 4000; acc:  97.05; ppl:  1.15; xent: 0.14; lr: 1.00000; 4763/6870 tok/s;     58 sec\n",
      "[2021-01-30 02:33:47,863 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:47,869 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:48,673 INFO] Step 3000/ 4000; acc:  96.84; ppl:  1.13; xent: 0.12; lr: 1.00000; 4946/6997 tok/s;     59 sec\n",
      "[2021-01-30 02:33:48,673 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:48,680 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:49,434 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:49,440 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:49,633 INFO] Step 3050/ 4000; acc:  97.12; ppl:  1.12; xent: 0.12; lr: 1.00000; 4982/7171 tok/s;     60 sec\n",
      "[2021-01-30 02:33:50,238 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:50,245 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:50,630 INFO] Step 3100/ 4000; acc:  96.68; ppl:  1.17; xent: 0.16; lr: 1.00000; 4838/6979 tok/s;     61 sec\n",
      "[2021-01-30 02:33:51,009 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:51,015 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:51,586 INFO] Step 3150/ 4000; acc:  96.86; ppl:  1.14; xent: 0.13; lr: 1.00000; 4961/7155 tok/s;     62 sec\n",
      "[2021-01-30 02:33:51,787 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:51,794 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:52,590 INFO] Step 3200/ 4000; acc:  96.94; ppl:  1.15; xent: 0.14; lr: 1.00000; 5001/7075 tok/s;     63 sec\n",
      "[2021-01-30 02:33:52,590 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:52,597 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:53,342 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:53,349 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:53,524 INFO] Step 3250/ 4000; acc:  96.75; ppl:  1.15; xent: 0.14; lr: 1.00000; 5118/7367 tok/s;     64 sec\n",
      "[2021-01-30 02:33:54,101 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:54,109 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:54,487 INFO] Step 3300/ 4000; acc:  96.62; ppl:  1.17; xent: 0.16; lr: 1.00000; 5006/7223 tok/s;     65 sec\n",
      "[2021-01-30 02:33:54,885 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:54,892 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:55,439 INFO] Step 3350/ 4000; acc:  96.68; ppl:  1.19; xent: 0.18; lr: 1.00000; 4985/7189 tok/s;     66 sec\n",
      "[2021-01-30 02:33:55,636 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:55,661 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:56,423 INFO] Step 3400/ 4000; acc:  96.48; ppl:  1.16; xent: 0.15; lr: 1.00000; 5099/7213 tok/s;     67 sec\n",
      "[2021-01-30 02:33:56,424 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:56,431 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:57,181 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:57,187 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:57,367 INFO] Step 3450/ 4000; acc:  96.76; ppl:  1.15; xent: 0.14; lr: 1.00000; 5071/7299 tok/s;     68 sec\n",
      "[2021-01-30 02:33:57,938 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:57,946 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:58,338 INFO] Step 3500/ 4000; acc:  97.01; ppl:  1.14; xent: 0.13; lr: 1.00000; 4964/7162 tok/s;     69 sec\n",
      "[2021-01-30 02:33:58,724 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:58,731 INFO] number of examples: 800\n",
      "[2021-01-30 02:33:59,308 INFO] Step 3550/ 4000; acc:  97.13; ppl:  1.13; xent: 0.13; lr: 1.00000; 4892/7056 tok/s;     70 sec\n",
      "[2021-01-30 02:33:59,511 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:33:59,519 INFO] number of examples: 800\n",
      "[2021-01-30 02:34:00,321 INFO] Step 3600/ 4000; acc:  96.82; ppl:  1.16; xent: 0.15; lr: 1.00000; 4955/7010 tok/s;     71 sec\n",
      "[2021-01-30 02:34:00,321 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:34:00,328 INFO] number of examples: 800\n",
      "[2021-01-30 02:34:01,142 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:34:01,149 INFO] number of examples: 800\n",
      "[2021-01-30 02:34:01,347 INFO] Step 3650/ 4000; acc:  97.08; ppl:  1.14; xent: 0.13; lr: 1.00000; 4658/6705 tok/s;     72 sec\n",
      "[2021-01-30 02:34:01,956 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:34:01,964 INFO] number of examples: 800\n",
      "[2021-01-30 02:34:02,375 INFO] Step 3700/ 4000; acc:  96.98; ppl:  1.13; xent: 0.12; lr: 1.00000; 4693/6771 tok/s;     73 sec\n",
      "[2021-01-30 02:34:02,770 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:34:02,777 INFO] number of examples: 800\n",
      "[2021-01-30 02:34:03,329 INFO] Step 3750/ 4000; acc:  96.90; ppl:  1.15; xent: 0.14; lr: 1.00000; 4973/7172 tok/s;     74 sec\n",
      "[2021-01-30 02:34:03,524 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:34:03,531 INFO] number of examples: 800\n",
      "[2021-01-30 02:34:04,277 INFO] Step 3800/ 4000; acc:  97.30; ppl:  1.13; xent: 0.12; lr: 1.00000; 5298/7495 tok/s;     75 sec\n",
      "[2021-01-30 02:34:04,277 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:34:04,283 INFO] number of examples: 800\n",
      "[2021-01-30 02:34:05,067 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:34:05,075 INFO] number of examples: 800\n",
      "[2021-01-30 02:34:05,262 INFO] Step 3850/ 4000; acc:  97.24; ppl:  1.15; xent: 0.14; lr: 1.00000; 4852/6984 tok/s;     76 sec\n",
      "[2021-01-30 02:34:05,842 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:34:05,850 INFO] number of examples: 800\n",
      "[2021-01-30 02:34:06,233 INFO] Step 3900/ 4000; acc:  97.07; ppl:  1.16; xent: 0.15; lr: 1.00000; 4970/7171 tok/s;     77 sec\n",
      "[2021-01-30 02:34:06,616 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:34:06,623 INFO] number of examples: 800\n",
      "[2021-01-30 02:34:07,218 INFO] Step 3950/ 4000; acc:  97.38; ppl:  1.12; xent: 0.12; lr: 1.00000; 4814/6943 tok/s;     78 sec\n",
      "[2021-01-30 02:34:07,414 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/800_9/processed.train.0.pt\n",
      "[2021-01-30 02:34:07,422 INFO] number of examples: 800\n",
      "[2021-01-30 02:34:08,179 INFO] Step 4000/ 4000; acc:  97.49; ppl:  1.12; xent: 0.11; lr: 1.00000; 5222/7387 tok/s;     79 sec\n",
      "[2021-01-30 02:34:08,181 INFO] Saving checkpoint drive/MyDrive/EnglishToleranceBaseline/output/english_rnn_model_800_9_step_4000.pt\n",
      "[2021-01-30 02:34:09,879 INFO]  * src vocab size = 44\n",
      "[2021-01-30 02:34:09,879 INFO]  * tgt vocab size = 46\n",
      "[2021-01-30 02:34:09,879 INFO] Building model...\n",
      "[2021-01-30 02:34:14,167 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(44, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(46, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=46, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-01-30 02:34:14,167 INFO] encoder: 254800\n",
      "[2021-01-30 02:34:14,167 INFO] decoder: 330046\n",
      "[2021-01-30 02:34:14,167 INFO] * number of parameters: 584846\n",
      "[2021-01-30 02:34:14,170 INFO] Starting training on GPU: [0]\n",
      "[2021-01-30 02:34:14,170 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-01-30 02:34:14,170 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:14,804 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:15,756 INFO] Step 50/ 5000; acc:  13.10; ppl: 31.60; xent: 3.45; lr: 1.00000; 3038/4373 tok/s;      2 sec\n",
      "[2021-01-30 02:34:15,756 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:15,764 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:16,707 INFO] Step 100/ 5000; acc:  18.68; ppl: 21.98; xent: 3.09; lr: 1.00000; 5063/7287 tok/s;      3 sec\n",
      "[2021-01-30 02:34:16,707 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:16,716 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:17,635 INFO] Step 150/ 5000; acc:  24.02; ppl: 16.39; xent: 2.80; lr: 1.00000; 5191/7471 tok/s;      3 sec\n",
      "[2021-01-30 02:34:17,635 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:17,643 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:18,564 INFO] Step 200/ 5000; acc:  25.75; ppl: 14.78; xent: 2.69; lr: 1.00000; 5189/7468 tok/s;      4 sec\n",
      "[2021-01-30 02:34:18,564 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:18,572 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:19,533 INFO] Step 250/ 5000; acc:  31.07; ppl: 11.96; xent: 2.48; lr: 1.00000; 4969/7152 tok/s;      5 sec\n",
      "[2021-01-30 02:34:19,534 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:19,542 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:20,456 INFO] Step 300/ 5000; acc:  35.17; ppl:  9.56; xent: 2.26; lr: 1.00000; 5222/7516 tok/s;      6 sec\n",
      "[2021-01-30 02:34:20,456 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:20,483 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:21,427 INFO] Step 350/ 5000; acc:  39.58; ppl:  8.35; xent: 2.12; lr: 1.00000; 4962/7141 tok/s;      7 sec\n",
      "[2021-01-30 02:34:21,427 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:21,436 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:22,357 INFO] Step 400/ 5000; acc:  41.79; ppl:  7.50; xent: 2.02; lr: 1.00000; 5183/7459 tok/s;      8 sec\n",
      "[2021-01-30 02:34:22,357 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:22,365 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:23,275 INFO] Step 450/ 5000; acc:  44.71; ppl:  6.89; xent: 1.93; lr: 1.00000; 5247/7552 tok/s;      9 sec\n",
      "[2021-01-30 02:34:23,275 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:23,283 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:24,218 INFO] Step 500/ 5000; acc:  47.05; ppl:  6.28; xent: 1.84; lr: 1.00000; 5106/7349 tok/s;     10 sec\n",
      "[2021-01-30 02:34:24,219 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:24,231 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:25,125 INFO] Step 550/ 5000; acc:  50.15; ppl:  5.54; xent: 1.71; lr: 1.00000; 5313/7647 tok/s;     11 sec\n",
      "[2021-01-30 02:34:25,125 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:25,134 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:26,074 INFO] Step 600/ 5000; acc:  53.71; ppl:  4.83; xent: 1.58; lr: 1.00000; 5076/7306 tok/s;     12 sec\n",
      "[2021-01-30 02:34:26,075 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:26,083 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:26,989 INFO] Step 650/ 5000; acc:  57.68; ppl:  4.27; xent: 1.45; lr: 1.00000; 5268/7582 tok/s;     13 sec\n",
      "[2021-01-30 02:34:26,989 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:26,998 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:27,969 INFO] Step 700/ 5000; acc:  62.15; ppl:  3.54; xent: 1.26; lr: 1.00000; 4914/7073 tok/s;     14 sec\n",
      "[2021-01-30 02:34:27,969 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:27,978 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:28,909 INFO] Step 750/ 5000; acc:  67.68; ppl:  2.94; xent: 1.08; lr: 1.00000; 5126/7377 tok/s;     15 sec\n",
      "[2021-01-30 02:34:28,909 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:28,917 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:29,834 INFO] Step 800/ 5000; acc:  71.64; ppl:  2.64; xent: 0.97; lr: 1.00000; 5208/7496 tok/s;     16 sec\n",
      "[2021-01-30 02:34:29,835 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:29,843 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:30,823 INFO] Step 850/ 5000; acc:  78.75; ppl:  2.11; xent: 0.75; lr: 1.00000; 4875/7016 tok/s;     17 sec\n",
      "[2021-01-30 02:34:30,823 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:30,832 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:31,772 INFO] Step 900/ 5000; acc:  83.35; ppl:  1.85; xent: 0.61; lr: 1.00000; 5073/7302 tok/s;     18 sec\n",
      "[2021-01-30 02:34:31,773 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:31,781 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:32,718 INFO] Step 950/ 5000; acc:  86.04; ppl:  1.68; xent: 0.52; lr: 1.00000; 5093/7330 tok/s;     19 sec\n",
      "[2021-01-30 02:34:32,719 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:32,727 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:33,661 INFO] Step 1000/ 5000; acc:  87.28; ppl:  1.60; xent: 0.47; lr: 1.00000; 5111/7356 tok/s;     19 sec\n",
      "[2021-01-30 02:34:33,661 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:33,670 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:34,571 INFO] Step 1050/ 5000; acc:  89.01; ppl:  1.50; xent: 0.41; lr: 1.00000; 5295/7622 tok/s;     20 sec\n",
      "[2021-01-30 02:34:34,571 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:34,599 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:35,530 INFO] Step 1100/ 5000; acc:  90.29; ppl:  1.43; xent: 0.36; lr: 1.00000; 5024/7231 tok/s;     21 sec\n",
      "[2021-01-30 02:34:35,530 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:35,539 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:36,464 INFO] Step 1150/ 5000; acc:  91.16; ppl:  1.39; xent: 0.33; lr: 1.00000; 5158/7424 tok/s;     22 sec\n",
      "[2021-01-30 02:34:36,464 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:36,473 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:37,403 INFO] Step 1200/ 5000; acc:  92.77; ppl:  1.32; xent: 0.28; lr: 1.00000; 5132/7386 tok/s;     23 sec\n",
      "[2021-01-30 02:34:37,403 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:37,412 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:38,347 INFO] Step 1250/ 5000; acc:  93.11; ppl:  1.31; xent: 0.27; lr: 1.00000; 5100/7340 tok/s;     24 sec\n",
      "[2021-01-30 02:34:38,348 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:38,356 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:39,294 INFO] Step 1300/ 5000; acc:  93.57; ppl:  1.27; xent: 0.24; lr: 1.00000; 5090/7326 tok/s;     25 sec\n",
      "[2021-01-30 02:34:39,294 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:39,303 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:40,243 INFO] Step 1350/ 5000; acc:  93.37; ppl:  1.28; xent: 0.25; lr: 1.00000; 5077/7307 tok/s;     26 sec\n",
      "[2021-01-30 02:34:40,243 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:40,251 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:41,174 INFO] Step 1400/ 5000; acc:  94.24; ppl:  1.25; xent: 0.23; lr: 1.00000; 5174/7447 tok/s;     27 sec\n",
      "[2021-01-30 02:34:41,174 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:41,184 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:42,121 INFO] Step 1450/ 5000; acc:  94.78; ppl:  1.23; xent: 0.21; lr: 1.00000; 5088/7323 tok/s;     28 sec\n",
      "[2021-01-30 02:34:42,121 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:42,130 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:43,050 INFO] Step 1500/ 5000; acc:  95.31; ppl:  1.20; xent: 0.18; lr: 1.00000; 5184/7462 tok/s;     29 sec\n",
      "[2021-01-30 02:34:43,051 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:43,059 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:44,005 INFO] Step 1550/ 5000; acc:  95.33; ppl:  1.20; xent: 0.18; lr: 1.00000; 5047/7264 tok/s;     30 sec\n",
      "[2021-01-30 02:34:44,005 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:44,014 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:44,926 INFO] Step 1600/ 5000; acc:  96.03; ppl:  1.17; xent: 0.15; lr: 1.00000; 5230/7527 tok/s;     31 sec\n",
      "[2021-01-30 02:34:44,926 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:44,935 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:45,827 INFO] Step 1650/ 5000; acc:  96.50; ppl:  1.14; xent: 0.13; lr: 1.00000; 5346/7695 tok/s;     32 sec\n",
      "[2021-01-30 02:34:45,828 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:45,836 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:46,745 INFO] Step 1700/ 5000; acc:  95.96; ppl:  1.15; xent: 0.14; lr: 1.00000; 5248/7553 tok/s;     33 sec\n",
      "[2021-01-30 02:34:46,746 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:46,754 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:47,670 INFO] Step 1750/ 5000; acc:  95.72; ppl:  1.17; xent: 0.16; lr: 1.00000; 5212/7502 tok/s;     33 sec\n",
      "[2021-01-30 02:34:47,670 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:47,679 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:48,593 INFO] Step 1800/ 5000; acc:  95.04; ppl:  1.23; xent: 0.21; lr: 1.00000; 5219/7511 tok/s;     34 sec\n",
      "[2021-01-30 02:34:48,593 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:48,620 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:49,596 INFO] Step 1850/ 5000; acc:  95.79; ppl:  1.17; xent: 0.15; lr: 1.00000; 4802/6912 tok/s;     35 sec\n",
      "[2021-01-30 02:34:49,596 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:49,605 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:50,513 INFO] Step 1900/ 5000; acc:  96.13; ppl:  1.17; xent: 0.15; lr: 1.00000; 5258/7568 tok/s;     36 sec\n",
      "[2021-01-30 02:34:50,513 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:50,521 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:51,445 INFO] Step 1950/ 5000; acc:  96.94; ppl:  1.13; xent: 0.13; lr: 1.00000; 5167/7437 tok/s;     37 sec\n",
      "[2021-01-30 02:34:51,445 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:51,453 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:52,369 INFO] Step 2000/ 5000; acc:  96.09; ppl:  1.17; xent: 0.15; lr: 1.00000; 5214/7505 tok/s;     38 sec\n",
      "[2021-01-30 02:34:52,369 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:52,377 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:53,300 INFO] Step 2050/ 5000; acc:  97.10; ppl:  1.12; xent: 0.11; lr: 1.00000; 5175/7448 tok/s;     39 sec\n",
      "[2021-01-30 02:34:53,300 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:53,308 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:54,248 INFO] Step 2100/ 5000; acc:  96.97; ppl:  1.12; xent: 0.11; lr: 1.00000; 5083/7317 tok/s;     40 sec\n",
      "[2021-01-30 02:34:54,248 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:54,256 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:55,204 INFO] Step 2150/ 5000; acc:  96.80; ppl:  1.14; xent: 0.13; lr: 1.00000; 5038/7251 tok/s;     41 sec\n",
      "[2021-01-30 02:34:55,204 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:55,213 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:56,164 INFO] Step 2200/ 5000; acc:  97.14; ppl:  1.12; xent: 0.12; lr: 1.00000; 5018/7223 tok/s;     42 sec\n",
      "[2021-01-30 02:34:56,164 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:56,174 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:57,115 INFO] Step 2250/ 5000; acc:  96.60; ppl:  1.15; xent: 0.14; lr: 1.00000; 5065/7290 tok/s;     43 sec\n",
      "[2021-01-30 02:34:57,115 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:57,124 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:58,049 INFO] Step 2300/ 5000; acc:  96.26; ppl:  1.17; xent: 0.15; lr: 1.00000; 5159/7425 tok/s;     44 sec\n",
      "[2021-01-30 02:34:58,049 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:58,059 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:58,964 INFO] Step 2350/ 5000; acc:  96.73; ppl:  1.15; xent: 0.14; lr: 1.00000; 5264/7576 tok/s;     45 sec\n",
      "[2021-01-30 02:34:58,965 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:58,973 INFO] number of examples: 1000\n",
      "[2021-01-30 02:34:59,875 INFO] Step 2400/ 5000; acc:  96.42; ppl:  1.15; xent: 0.14; lr: 1.00000; 5292/7616 tok/s;     46 sec\n",
      "[2021-01-30 02:34:59,875 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:34:59,884 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:00,780 INFO] Step 2450/ 5000; acc:  96.64; ppl:  1.15; xent: 0.14; lr: 1.00000; 5323/7661 tok/s;     47 sec\n",
      "[2021-01-30 02:35:00,780 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:00,789 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:01,742 INFO] Step 2500/ 5000; acc:  97.12; ppl:  1.12; xent: 0.11; lr: 1.00000; 5011/7212 tok/s;     48 sec\n",
      "[2021-01-30 02:35:01,742 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:01,770 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:02,697 INFO] Step 2550/ 5000; acc:  96.45; ppl:  1.17; xent: 0.16; lr: 1.00000; 5045/7261 tok/s;     49 sec\n",
      "[2021-01-30 02:35:02,697 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:02,706 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:03,627 INFO] Step 2600/ 5000; acc:  96.18; ppl:  1.18; xent: 0.17; lr: 1.00000; 5178/7453 tok/s;     49 sec\n",
      "[2021-01-30 02:35:03,627 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:03,636 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:04,552 INFO] Step 2650/ 5000; acc:  97.23; ppl:  1.12; xent: 0.11; lr: 1.00000; 5211/7500 tok/s;     50 sec\n",
      "[2021-01-30 02:35:04,552 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:04,560 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:05,473 INFO] Step 2700/ 5000; acc:  97.07; ppl:  1.14; xent: 0.13; lr: 1.00000; 5231/7529 tok/s;     51 sec\n",
      "[2021-01-30 02:35:05,473 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:05,482 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:06,405 INFO] Step 2750/ 5000; acc:  97.45; ppl:  1.10; xent: 0.10; lr: 1.00000; 5166/7435 tok/s;     52 sec\n",
      "[2021-01-30 02:35:06,406 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:06,414 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:07,324 INFO] Step 2800/ 5000; acc:  97.36; ppl:  1.12; xent: 0.11; lr: 1.00000; 5247/7552 tok/s;     53 sec\n",
      "[2021-01-30 02:35:07,324 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:07,332 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:08,272 INFO] Step 2850/ 5000; acc:  96.48; ppl:  1.16; xent: 0.15; lr: 1.00000; 5078/7309 tok/s;     54 sec\n",
      "[2021-01-30 02:35:08,273 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:08,281 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:09,254 INFO] Step 2900/ 5000; acc:  96.70; ppl:  1.16; xent: 0.14; lr: 1.00000; 4911/7068 tok/s;     55 sec\n",
      "[2021-01-30 02:35:09,254 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:09,263 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:10,280 INFO] Step 2950/ 5000; acc:  97.17; ppl:  1.12; xent: 0.12; lr: 1.00000; 4694/6756 tok/s;     56 sec\n",
      "[2021-01-30 02:35:10,280 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:10,289 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:11,250 INFO] Step 3000/ 5000; acc:  96.84; ppl:  1.15; xent: 0.14; lr: 1.00000; 4967/7149 tok/s;     57 sec\n",
      "[2021-01-30 02:35:11,250 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:11,258 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:12,228 INFO] Step 3050/ 5000; acc:  96.61; ppl:  1.16; xent: 0.14; lr: 1.00000; 4927/7092 tok/s;     58 sec\n",
      "[2021-01-30 02:35:12,228 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:12,237 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:13,194 INFO] Step 3100/ 5000; acc:  96.65; ppl:  1.15; xent: 0.14; lr: 1.00000; 4985/7174 tok/s;     59 sec\n",
      "[2021-01-30 02:35:13,194 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:13,203 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:14,141 INFO] Step 3150/ 5000; acc:  96.62; ppl:  1.15; xent: 0.14; lr: 1.00000; 5088/7323 tok/s;     60 sec\n",
      "[2021-01-30 02:35:14,141 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:14,150 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:15,081 INFO] Step 3200/ 5000; acc:  96.52; ppl:  1.16; xent: 0.15; lr: 1.00000; 5128/7380 tok/s;     61 sec\n",
      "[2021-01-30 02:35:15,081 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:15,090 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:16,049 INFO] Step 3250/ 5000; acc:  96.08; ppl:  1.20; xent: 0.19; lr: 1.00000; 4977/7164 tok/s;     62 sec\n",
      "[2021-01-30 02:35:16,049 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:16,076 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:17,008 INFO] Step 3300/ 5000; acc:  96.52; ppl:  1.17; xent: 0.16; lr: 1.00000; 5021/7226 tok/s;     63 sec\n",
      "[2021-01-30 02:35:17,008 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:17,017 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:17,982 INFO] Step 3350/ 5000; acc:  96.70; ppl:  1.15; xent: 0.14; lr: 1.00000; 4946/7119 tok/s;     64 sec\n",
      "[2021-01-30 02:35:17,982 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:17,991 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:18,932 INFO] Step 3400/ 5000; acc:  96.60; ppl:  1.16; xent: 0.15; lr: 1.00000; 5075/7304 tok/s;     65 sec\n",
      "[2021-01-30 02:35:18,932 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:18,941 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:19,870 INFO] Step 3450/ 5000; acc:  96.64; ppl:  1.17; xent: 0.16; lr: 1.00000; 5135/7391 tok/s;     66 sec\n",
      "[2021-01-30 02:35:19,870 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:19,879 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:20,810 INFO] Step 3500/ 5000; acc:  96.99; ppl:  1.15; xent: 0.14; lr: 1.00000; 5123/7373 tok/s;     67 sec\n",
      "[2021-01-30 02:35:20,811 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:20,819 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:21,766 INFO] Step 3550/ 5000; acc:  97.25; ppl:  1.13; xent: 0.12; lr: 1.00000; 5040/7254 tok/s;     68 sec\n",
      "[2021-01-30 02:35:21,767 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:21,775 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:22,685 INFO] Step 3600/ 5000; acc:  96.97; ppl:  1.16; xent: 0.15; lr: 1.00000; 5244/7547 tok/s;     69 sec\n",
      "[2021-01-30 02:35:22,685 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:22,694 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:23,612 INFO] Step 3650/ 5000; acc:  96.94; ppl:  1.15; xent: 0.14; lr: 1.00000; 5199/7482 tok/s;     69 sec\n",
      "[2021-01-30 02:35:23,612 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:23,621 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:24,594 INFO] Step 3700/ 5000; acc:  97.01; ppl:  1.14; xent: 0.13; lr: 1.00000; 4903/7057 tok/s;     70 sec\n",
      "[2021-01-30 02:35:24,595 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:24,603 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:25,532 INFO] Step 3750/ 5000; acc:  97.32; ppl:  1.13; xent: 0.12; lr: 1.00000; 5138/7395 tok/s;     71 sec\n",
      "[2021-01-30 02:35:25,532 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:25,540 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:26,469 INFO] Step 3800/ 5000; acc:  96.86; ppl:  1.16; xent: 0.15; lr: 1.00000; 5140/7398 tok/s;     72 sec\n",
      "[2021-01-30 02:35:26,470 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:26,478 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:27,429 INFO] Step 3850/ 5000; acc:  96.90; ppl:  1.15; xent: 0.14; lr: 1.00000; 5020/7225 tok/s;     73 sec\n",
      "[2021-01-30 02:35:27,429 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:27,438 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:28,375 INFO] Step 3900/ 5000; acc:  96.81; ppl:  1.15; xent: 0.14; lr: 1.00000; 5093/7330 tok/s;     74 sec\n",
      "[2021-01-30 02:35:28,375 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:28,384 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:29,343 INFO] Step 3950/ 5000; acc:  97.19; ppl:  1.15; xent: 0.14; lr: 1.00000; 4977/7163 tok/s;     75 sec\n",
      "[2021-01-30 02:35:29,343 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:29,352 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:30,294 INFO] Step 4000/ 5000; acc:  96.44; ppl:  1.19; xent: 0.17; lr: 1.00000; 5065/7290 tok/s;     76 sec\n",
      "[2021-01-30 02:35:30,295 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:30,322 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:31,242 INFO] Step 4050/ 5000; acc:  96.97; ppl:  1.17; xent: 0.16; lr: 1.00000; 5086/7320 tok/s;     77 sec\n",
      "[2021-01-30 02:35:31,242 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:31,251 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:32,174 INFO] Step 4100/ 5000; acc:  96.77; ppl:  1.15; xent: 0.14; lr: 1.00000; 5168/7439 tok/s;     78 sec\n",
      "[2021-01-30 02:35:32,174 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:32,182 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:33,136 INFO] Step 4150/ 5000; acc:  96.78; ppl:  1.16; xent: 0.15; lr: 1.00000; 5007/7207 tok/s;     79 sec\n",
      "[2021-01-30 02:35:33,136 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:33,144 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:34,056 INFO] Step 4200/ 5000; acc:  96.64; ppl:  1.17; xent: 0.16; lr: 1.00000; 5236/7535 tok/s;     80 sec\n",
      "[2021-01-30 02:35:34,057 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:34,065 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:34,997 INFO] Step 4250/ 5000; acc:  96.55; ppl:  1.19; xent: 0.18; lr: 1.00000; 5121/7371 tok/s;     81 sec\n",
      "[2021-01-30 02:35:34,997 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:35,006 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:35,918 INFO] Step 4300/ 5000; acc:  96.86; ppl:  1.17; xent: 0.15; lr: 1.00000; 5233/7532 tok/s;     82 sec\n",
      "[2021-01-30 02:35:35,918 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:35,926 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:36,854 INFO] Step 4350/ 5000; acc:  97.07; ppl:  1.15; xent: 0.14; lr: 1.00000; 5148/7409 tok/s;     83 sec\n",
      "[2021-01-30 02:35:36,854 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:36,862 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:37,780 INFO] Step 4400/ 5000; acc:  97.12; ppl:  1.14; xent: 0.13; lr: 1.00000; 5201/7486 tok/s;     84 sec\n",
      "[2021-01-30 02:35:37,780 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:37,789 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:38,695 INFO] Step 4450/ 5000; acc:  97.14; ppl:  1.14; xent: 0.13; lr: 1.00000; 5265/7577 tok/s;     85 sec\n",
      "[2021-01-30 02:35:38,695 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:38,704 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:39,661 INFO] Step 4500/ 5000; acc:  96.60; ppl:  1.18; xent: 0.16; lr: 1.00000; 4990/7182 tok/s;     85 sec\n",
      "[2021-01-30 02:35:39,661 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:39,670 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:40,594 INFO] Step 4550/ 5000; acc:  97.12; ppl:  1.14; xent: 0.13; lr: 1.00000; 5159/7426 tok/s;     86 sec\n",
      "[2021-01-30 02:35:40,595 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:40,603 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:41,540 INFO] Step 4600/ 5000; acc:  96.91; ppl:  1.17; xent: 0.16; lr: 1.00000; 5096/7334 tok/s;     87 sec\n",
      "[2021-01-30 02:35:41,540 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:41,549 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:42,512 INFO] Step 4650/ 5000; acc:  96.52; ppl:  1.19; xent: 0.17; lr: 1.00000; 4954/7130 tok/s;     88 sec\n",
      "[2021-01-30 02:35:42,513 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:42,521 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:43,446 INFO] Step 4700/ 5000; acc:  97.03; ppl:  1.17; xent: 0.15; lr: 1.00000; 5159/7425 tok/s;     89 sec\n",
      "[2021-01-30 02:35:43,447 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:43,474 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:44,380 INFO] Step 4750/ 5000; acc:  96.28; ppl:  1.20; xent: 0.18; lr: 1.00000; 5163/7431 tok/s;     90 sec\n",
      "[2021-01-30 02:35:44,380 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:44,387 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:45,311 INFO] Step 4800/ 5000; acc:  97.36; ppl:  1.13; xent: 0.12; lr: 1.00000; 5171/7443 tok/s;     91 sec\n",
      "[2021-01-30 02:35:45,311 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:45,320 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:46,236 INFO] Step 4850/ 5000; acc:  97.48; ppl:  1.13; xent: 0.13; lr: 1.00000; 5213/7503 tok/s;     92 sec\n",
      "[2021-01-30 02:35:46,236 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:46,244 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:47,177 INFO] Step 4900/ 5000; acc:  96.08; ppl:  1.20; xent: 0.19; lr: 1.00000; 5118/7367 tok/s;     93 sec\n",
      "[2021-01-30 02:35:47,177 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:47,186 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:48,134 INFO] Step 4950/ 5000; acc:  96.74; ppl:  1.18; xent: 0.17; lr: 1.00000; 5033/7244 tok/s;     94 sec\n",
      "[2021-01-30 02:35:48,134 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_0/processed.train.0.pt\n",
      "[2021-01-30 02:35:48,142 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:49,089 INFO] Step 5000/ 5000; acc:  96.97; ppl:  1.15; xent: 0.14; lr: 1.00000; 5046/7263 tok/s;     95 sec\n",
      "[2021-01-30 02:35:49,090 INFO] Saving checkpoint drive/MyDrive/EnglishToleranceBaseline/output/english_rnn_model_1000_0_step_5000.pt\n",
      "[2021-01-30 02:35:50,819 INFO]  * src vocab size = 44\n",
      "[2021-01-30 02:35:50,819 INFO]  * tgt vocab size = 46\n",
      "[2021-01-30 02:35:50,819 INFO] Building model...\n",
      "[2021-01-30 02:35:55,140 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(44, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(46, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=46, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-01-30 02:35:55,141 INFO] encoder: 254800\n",
      "[2021-01-30 02:35:55,141 INFO] decoder: 330046\n",
      "[2021-01-30 02:35:55,141 INFO] * number of parameters: 584846\n",
      "[2021-01-30 02:35:55,143 INFO] Starting training on GPU: [0]\n",
      "[2021-01-30 02:35:55,143 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-01-30 02:35:55,144 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:35:56,032 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:57,010 INFO] Step 50/ 5000; acc:  14.03; ppl: 35.33; xent: 3.56; lr: 1.00000; 2588/3714 tok/s;      2 sec\n",
      "[2021-01-30 02:35:57,011 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:35:57,019 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:57,938 INFO] Step 100/ 5000; acc:  21.66; ppl: 19.51; xent: 2.97; lr: 1.00000; 5208/7474 tok/s;      3 sec\n",
      "[2021-01-30 02:35:57,938 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:35:57,946 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:58,873 INFO] Step 150/ 5000; acc:  24.41; ppl: 15.89; xent: 2.77; lr: 1.00000; 5168/7417 tok/s;      4 sec\n",
      "[2021-01-30 02:35:58,873 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:35:58,881 INFO] number of examples: 1000\n",
      "[2021-01-30 02:35:59,852 INFO] Step 200/ 5000; acc:  24.20; ppl: 14.92; xent: 2.70; lr: 1.00000; 4935/7083 tok/s;      5 sec\n",
      "[2021-01-30 02:35:59,853 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:35:59,863 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:00,792 INFO] Step 250/ 5000; acc:  28.43; ppl: 12.32; xent: 2.51; lr: 1.00000; 5139/7376 tok/s;      6 sec\n",
      "[2021-01-30 02:36:00,793 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:00,802 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:01,747 INFO] Step 300/ 5000; acc:  32.31; ppl: 10.55; xent: 2.36; lr: 1.00000; 5064/7267 tok/s;      7 sec\n",
      "[2021-01-30 02:36:01,747 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:01,775 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:02,723 INFO] Step 350/ 5000; acc:  39.30; ppl:  8.37; xent: 2.12; lr: 1.00000; 4952/7106 tok/s;      8 sec\n",
      "[2021-01-30 02:36:02,723 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:02,731 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:03,703 INFO] Step 400/ 5000; acc:  42.35; ppl:  7.37; xent: 2.00; lr: 1.00000; 4929/7073 tok/s;      9 sec\n",
      "[2021-01-30 02:36:03,703 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:03,711 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:04,642 INFO] Step 450/ 5000; acc:  45.88; ppl:  6.66; xent: 1.90; lr: 1.00000; 5144/7382 tok/s;      9 sec\n",
      "[2021-01-30 02:36:04,642 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:04,651 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:05,575 INFO] Step 500/ 5000; acc:  47.48; ppl:  6.04; xent: 1.80; lr: 1.00000; 5179/7433 tok/s;     10 sec\n",
      "[2021-01-30 02:36:05,575 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:05,584 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:06,503 INFO] Step 550/ 5000; acc:  53.14; ppl:  5.04; xent: 1.62; lr: 1.00000; 5206/7471 tok/s;     11 sec\n",
      "[2021-01-30 02:36:06,504 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:06,511 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:07,457 INFO] Step 600/ 5000; acc:  55.70; ppl:  4.49; xent: 1.50; lr: 1.00000; 5064/7268 tok/s;     12 sec\n",
      "[2021-01-30 02:36:07,458 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:07,466 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:08,391 INFO] Step 650/ 5000; acc:  59.97; ppl:  3.86; xent: 1.35; lr: 1.00000; 5176/7428 tok/s;     13 sec\n",
      "[2021-01-30 02:36:08,391 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:08,400 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:09,350 INFO] Step 700/ 5000; acc:  64.82; ppl:  3.27; xent: 1.18; lr: 1.00000; 5041/7234 tok/s;     14 sec\n",
      "[2021-01-30 02:36:09,350 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:09,358 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:10,321 INFO] Step 750/ 5000; acc:  69.02; ppl:  2.86; xent: 1.05; lr: 1.00000; 4976/7141 tok/s;     15 sec\n",
      "[2021-01-30 02:36:10,321 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:10,329 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:11,256 INFO] Step 800/ 5000; acc:  74.64; ppl:  2.45; xent: 0.89; lr: 1.00000; 5165/7412 tok/s;     16 sec\n",
      "[2021-01-30 02:36:11,256 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:11,265 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:12,224 INFO] Step 850/ 5000; acc:  77.63; ppl:  2.17; xent: 0.78; lr: 1.00000; 4991/7163 tok/s;     17 sec\n",
      "[2021-01-30 02:36:12,225 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:12,234 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:13,176 INFO] Step 900/ 5000; acc:  81.81; ppl:  1.89; xent: 0.64; lr: 1.00000; 5076/7285 tok/s;     18 sec\n",
      "[2021-01-30 02:36:13,176 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:13,185 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:14,127 INFO] Step 950/ 5000; acc:  85.30; ppl:  1.74; xent: 0.56; lr: 1.00000; 5084/7296 tok/s;     19 sec\n",
      "[2021-01-30 02:36:14,127 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:14,135 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:15,115 INFO] Step 1000/ 5000; acc:  87.03; ppl:  1.60; xent: 0.47; lr: 1.00000; 4888/7015 tok/s;     20 sec\n",
      "[2021-01-30 02:36:15,115 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:15,124 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:16,040 INFO] Step 1050/ 5000; acc:  89.46; ppl:  1.48; xent: 0.39; lr: 1.00000; 5225/7498 tok/s;     21 sec\n",
      "[2021-01-30 02:36:16,040 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:16,068 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:16,985 INFO] Step 1100/ 5000; acc:  90.64; ppl:  1.45; xent: 0.37; lr: 1.00000; 5112/7336 tok/s;     22 sec\n",
      "[2021-01-30 02:36:16,985 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:16,994 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:17,919 INFO] Step 1150/ 5000; acc:  92.41; ppl:  1.33; xent: 0.29; lr: 1.00000; 5172/7422 tok/s;     23 sec\n",
      "[2021-01-30 02:36:17,920 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:17,927 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:18,894 INFO] Step 1200/ 5000; acc:  92.12; ppl:  1.34; xent: 0.29; lr: 1.00000; 4956/7112 tok/s;     24 sec\n",
      "[2021-01-30 02:36:18,895 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:18,904 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:19,803 INFO] Step 1250/ 5000; acc:  92.54; ppl:  1.32; xent: 0.28; lr: 1.00000; 5317/7631 tok/s;     25 sec\n",
      "[2021-01-30 02:36:19,803 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:19,812 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:20,773 INFO] Step 1300/ 5000; acc:  92.66; ppl:  1.31; xent: 0.27; lr: 1.00000; 4980/7147 tok/s;     26 sec\n",
      "[2021-01-30 02:36:20,774 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:20,782 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:21,709 INFO] Step 1350/ 5000; acc:  93.09; ppl:  1.30; xent: 0.26; lr: 1.00000; 5165/7412 tok/s;     27 sec\n",
      "[2021-01-30 02:36:21,709 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:21,717 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:22,645 INFO] Step 1400/ 5000; acc:  94.62; ppl:  1.23; xent: 0.21; lr: 1.00000; 5163/7410 tok/s;     28 sec\n",
      "[2021-01-30 02:36:22,645 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:22,653 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:23,571 INFO] Step 1450/ 5000; acc:  94.95; ppl:  1.22; xent: 0.20; lr: 1.00000; 5217/7487 tok/s;     28 sec\n",
      "[2021-01-30 02:36:23,571 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:23,579 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:24,502 INFO] Step 1500/ 5000; acc:  95.37; ppl:  1.19; xent: 0.17; lr: 1.00000; 5189/7446 tok/s;     29 sec\n",
      "[2021-01-30 02:36:24,502 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:24,511 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:25,434 INFO] Step 1550/ 5000; acc:  95.18; ppl:  1.19; xent: 0.17; lr: 1.00000; 5184/7440 tok/s;     30 sec\n",
      "[2021-01-30 02:36:25,435 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:25,443 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:26,402 INFO] Step 1600/ 5000; acc:  95.43; ppl:  1.19; xent: 0.17; lr: 1.00000; 4996/7170 tok/s;     31 sec\n",
      "[2021-01-30 02:36:26,402 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:26,410 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:27,340 INFO] Step 1650/ 5000; acc:  95.50; ppl:  1.18; xent: 0.17; lr: 1.00000; 5147/7387 tok/s;     32 sec\n",
      "[2021-01-30 02:36:27,340 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:27,349 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:28,275 INFO] Step 1700/ 5000; acc:  95.93; ppl:  1.17; xent: 0.16; lr: 1.00000; 5168/7417 tok/s;     33 sec\n",
      "[2021-01-30 02:36:28,275 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:28,284 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:29,196 INFO] Step 1750/ 5000; acc:  95.50; ppl:  1.18; xent: 0.17; lr: 1.00000; 5246/7529 tok/s;     34 sec\n",
      "[2021-01-30 02:36:29,197 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:29,204 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:30,146 INFO] Step 1800/ 5000; acc:  96.09; ppl:  1.16; xent: 0.15; lr: 1.00000; 5090/7305 tok/s;     35 sec\n",
      "[2021-01-30 02:36:30,146 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:30,173 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:31,108 INFO] Step 1850/ 5000; acc:  95.86; ppl:  1.17; xent: 0.16; lr: 1.00000; 5020/7204 tok/s;     36 sec\n",
      "[2021-01-30 02:36:31,108 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:31,117 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:32,082 INFO] Step 1900/ 5000; acc:  96.09; ppl:  1.17; xent: 0.15; lr: 1.00000; 4961/7119 tok/s;     37 sec\n",
      "[2021-01-30 02:36:32,082 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:32,091 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:33,058 INFO] Step 1950/ 5000; acc:  96.52; ppl:  1.15; xent: 0.14; lr: 1.00000; 4952/7107 tok/s;     38 sec\n",
      "[2021-01-30 02:36:33,058 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:33,066 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:34,016 INFO] Step 2000/ 5000; acc:  96.28; ppl:  1.15; xent: 0.14; lr: 1.00000; 5045/7240 tok/s;     39 sec\n",
      "[2021-01-30 02:36:34,016 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:34,025 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:34,946 INFO] Step 2050/ 5000; acc:  96.41; ppl:  1.15; xent: 0.14; lr: 1.00000; 5192/7451 tok/s;     40 sec\n",
      "[2021-01-30 02:36:34,947 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:34,956 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:35,892 INFO] Step 2100/ 5000; acc:  96.77; ppl:  1.14; xent: 0.13; lr: 1.00000; 5111/7335 tok/s;     41 sec\n",
      "[2021-01-30 02:36:35,892 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:35,900 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:36,845 INFO] Step 2150/ 5000; acc:  95.96; ppl:  1.17; xent: 0.16; lr: 1.00000; 5070/7275 tok/s;     42 sec\n",
      "[2021-01-30 02:36:36,845 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:36,853 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:37,836 INFO] Step 2200/ 5000; acc:  96.47; ppl:  1.15; xent: 0.14; lr: 1.00000; 4877/6999 tok/s;     43 sec\n",
      "[2021-01-30 02:36:37,836 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:37,845 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:38,808 INFO] Step 2250/ 5000; acc:  96.65; ppl:  1.13; xent: 0.12; lr: 1.00000; 4971/7134 tok/s;     44 sec\n",
      "[2021-01-30 02:36:38,808 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:38,816 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:39,828 INFO] Step 2300/ 5000; acc:  96.44; ppl:  1.15; xent: 0.14; lr: 1.00000; 4735/6795 tok/s;     45 sec\n",
      "[2021-01-30 02:36:39,828 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:39,837 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:40,807 INFO] Step 2350/ 5000; acc:  97.07; ppl:  1.12; xent: 0.12; lr: 1.00000; 4935/7082 tok/s;     46 sec\n",
      "[2021-01-30 02:36:40,808 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:40,815 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:41,756 INFO] Step 2400/ 5000; acc:  96.26; ppl:  1.16; xent: 0.15; lr: 1.00000; 5094/7310 tok/s;     47 sec\n",
      "[2021-01-30 02:36:41,756 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:41,765 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:42,745 INFO] Step 2450/ 5000; acc:  96.65; ppl:  1.15; xent: 0.14; lr: 1.00000; 4885/7010 tok/s;     48 sec\n",
      "[2021-01-30 02:36:42,745 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:42,755 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:43,727 INFO] Step 2500/ 5000; acc:  96.55; ppl:  1.16; xent: 0.15; lr: 1.00000; 4920/7061 tok/s;     49 sec\n",
      "[2021-01-30 02:36:43,727 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:43,761 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:44,751 INFO] Step 2550/ 5000; acc:  96.34; ppl:  1.16; xent: 0.15; lr: 1.00000; 4719/6772 tok/s;     50 sec\n",
      "[2021-01-30 02:36:44,751 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:44,760 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:45,793 INFO] Step 2600/ 5000; acc:  96.77; ppl:  1.15; xent: 0.14; lr: 1.00000; 4637/6655 tok/s;     51 sec\n",
      "[2021-01-30 02:36:45,793 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:45,802 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:46,752 INFO] Step 2650/ 5000; acc:  96.68; ppl:  1.14; xent: 0.13; lr: 1.00000; 5039/7232 tok/s;     52 sec\n",
      "[2021-01-30 02:36:46,752 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:46,760 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:47,740 INFO] Step 2700/ 5000; acc:  96.50; ppl:  1.16; xent: 0.15; lr: 1.00000; 4891/7018 tok/s;     53 sec\n",
      "[2021-01-30 02:36:47,740 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:47,749 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:48,714 INFO] Step 2750/ 5000; acc:  96.64; ppl:  1.15; xent: 0.14; lr: 1.00000; 4960/7119 tok/s;     54 sec\n",
      "[2021-01-30 02:36:48,714 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:48,722 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:49,658 INFO] Step 2800/ 5000; acc:  96.93; ppl:  1.15; xent: 0.14; lr: 1.00000; 5118/7344 tok/s;     55 sec\n",
      "[2021-01-30 02:36:49,658 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:49,667 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:50,594 INFO] Step 2850/ 5000; acc:  96.96; ppl:  1.14; xent: 0.13; lr: 1.00000; 5162/7408 tok/s;     55 sec\n",
      "[2021-01-30 02:36:50,595 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:50,603 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:51,556 INFO] Step 2900/ 5000; acc:  97.10; ppl:  1.13; xent: 0.12; lr: 1.00000; 5025/7211 tok/s;     56 sec\n",
      "[2021-01-30 02:36:51,556 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:51,565 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:52,497 INFO] Step 2950/ 5000; acc:  96.64; ppl:  1.15; xent: 0.14; lr: 1.00000; 5136/7371 tok/s;     57 sec\n",
      "[2021-01-30 02:36:52,497 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:52,505 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:53,439 INFO] Step 3000/ 5000; acc:  96.70; ppl:  1.15; xent: 0.14; lr: 1.00000; 5130/7361 tok/s;     58 sec\n",
      "[2021-01-30 02:36:53,439 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:53,447 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:54,408 INFO] Step 3050/ 5000; acc:  97.09; ppl:  1.15; xent: 0.14; lr: 1.00000; 4984/7153 tok/s;     59 sec\n",
      "[2021-01-30 02:36:54,408 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:54,417 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:55,331 INFO] Step 3100/ 5000; acc:  97.20; ppl:  1.13; xent: 0.13; lr: 1.00000; 5233/7510 tok/s;     60 sec\n",
      "[2021-01-30 02:36:55,332 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:55,341 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:56,279 INFO] Step 3150/ 5000; acc:  97.25; ppl:  1.13; xent: 0.12; lr: 1.00000; 5099/7318 tok/s;     61 sec\n",
      "[2021-01-30 02:36:56,279 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:56,287 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:57,202 INFO] Step 3200/ 5000; acc:  96.67; ppl:  1.17; xent: 0.15; lr: 1.00000; 5235/7512 tok/s;     62 sec\n",
      "[2021-01-30 02:36:57,202 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:57,211 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:58,145 INFO] Step 3250/ 5000; acc:  96.00; ppl:  1.20; xent: 0.18; lr: 1.00000; 5126/7356 tok/s;     63 sec\n",
      "[2021-01-30 02:36:58,145 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:58,172 INFO] number of examples: 1000\n",
      "[2021-01-30 02:36:59,094 INFO] Step 3300/ 5000; acc:  96.80; ppl:  1.14; xent: 0.14; lr: 1.00000; 5088/7302 tok/s;     64 sec\n",
      "[2021-01-30 02:36:59,095 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:36:59,103 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:00,074 INFO] Step 3350/ 5000; acc:  97.48; ppl:  1.11; xent: 0.10; lr: 1.00000; 4932/7078 tok/s;     65 sec\n",
      "[2021-01-30 02:37:00,074 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:00,082 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:01,015 INFO] Step 3400/ 5000; acc:  97.43; ppl:  1.12; xent: 0.11; lr: 1.00000; 5137/7372 tok/s;     66 sec\n",
      "[2021-01-30 02:37:01,015 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:01,023 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:02,011 INFO] Step 3450/ 5000; acc:  97.16; ppl:  1.14; xent: 0.13; lr: 1.00000; 4849/6959 tok/s;     67 sec\n",
      "[2021-01-30 02:37:02,011 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:02,020 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:02,956 INFO] Step 3500/ 5000; acc:  96.57; ppl:  1.17; xent: 0.16; lr: 1.00000; 5116/7342 tok/s;     68 sec\n",
      "[2021-01-30 02:37:02,956 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:02,964 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:03,911 INFO] Step 3550/ 5000; acc:  96.93; ppl:  1.14; xent: 0.13; lr: 1.00000; 5057/7258 tok/s;     69 sec\n",
      "[2021-01-30 02:37:03,911 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:03,919 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:04,835 INFO] Step 3600/ 5000; acc:  96.84; ppl:  1.14; xent: 0.13; lr: 1.00000; 5227/7502 tok/s;     70 sec\n",
      "[2021-01-30 02:37:04,836 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:04,844 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:05,774 INFO] Step 3650/ 5000; acc:  96.61; ppl:  1.15; xent: 0.14; lr: 1.00000; 5148/7388 tok/s;     71 sec\n",
      "[2021-01-30 02:37:05,774 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:05,782 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:06,772 INFO] Step 3700/ 5000; acc:  96.61; ppl:  1.18; xent: 0.17; lr: 1.00000; 4839/6945 tok/s;     72 sec\n",
      "[2021-01-30 02:37:06,773 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:06,781 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:07,701 INFO] Step 3750/ 5000; acc:  96.48; ppl:  1.18; xent: 0.16; lr: 1.00000; 5201/7465 tok/s;     73 sec\n",
      "[2021-01-30 02:37:07,702 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:07,710 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:08,611 INFO] Step 3800/ 5000; acc:  96.67; ppl:  1.17; xent: 0.16; lr: 1.00000; 5313/7625 tok/s;     73 sec\n",
      "[2021-01-30 02:37:08,611 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:08,619 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:09,537 INFO] Step 3850/ 5000; acc:  96.55; ppl:  1.17; xent: 0.16; lr: 1.00000; 5215/7484 tok/s;     74 sec\n",
      "[2021-01-30 02:37:09,538 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:09,546 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:10,499 INFO] Step 3900/ 5000; acc:  96.61; ppl:  1.17; xent: 0.16; lr: 1.00000; 5026/7213 tok/s;     75 sec\n",
      "[2021-01-30 02:37:10,499 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:10,507 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:11,467 INFO] Step 3950/ 5000; acc:  96.42; ppl:  1.21; xent: 0.19; lr: 1.00000; 4991/7163 tok/s;     76 sec\n",
      "[2021-01-30 02:37:11,467 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:11,476 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:12,415 INFO] Step 4000/ 5000; acc:  96.80; ppl:  1.17; xent: 0.16; lr: 1.00000; 5095/7312 tok/s;     77 sec\n",
      "[2021-01-30 02:37:12,415 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:12,442 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:13,378 INFO] Step 4050/ 5000; acc:  96.93; ppl:  1.16; xent: 0.15; lr: 1.00000; 5016/7198 tok/s;     78 sec\n",
      "[2021-01-30 02:37:13,379 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:13,388 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:14,317 INFO] Step 4100/ 5000; acc:  96.86; ppl:  1.15; xent: 0.14; lr: 1.00000; 5151/7392 tok/s;     79 sec\n",
      "[2021-01-30 02:37:14,317 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:14,325 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:15,237 INFO] Step 4150/ 5000; acc:  97.16; ppl:  1.13; xent: 0.13; lr: 1.00000; 5247/7530 tok/s;     80 sec\n",
      "[2021-01-30 02:37:15,238 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:15,245 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:16,174 INFO] Step 4200/ 5000; acc:  97.07; ppl:  1.15; xent: 0.14; lr: 1.00000; 5161/7407 tok/s;     81 sec\n",
      "[2021-01-30 02:37:16,174 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:16,182 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:17,141 INFO] Step 4250/ 5000; acc:  96.86; ppl:  1.16; xent: 0.15; lr: 1.00000; 4994/7167 tok/s;     82 sec\n",
      "[2021-01-30 02:37:17,141 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:17,150 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:18,061 INFO] Step 4300/ 5000; acc:  96.71; ppl:  1.17; xent: 0.16; lr: 1.00000; 5251/7536 tok/s;     83 sec\n",
      "[2021-01-30 02:37:18,062 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:18,070 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:18,987 INFO] Step 4350/ 5000; acc:  96.75; ppl:  1.17; xent: 0.16; lr: 1.00000; 5218/7489 tok/s;     84 sec\n",
      "[2021-01-30 02:37:18,988 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:18,995 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:19,923 INFO] Step 4400/ 5000; acc:  96.31; ppl:  1.20; xent: 0.18; lr: 1.00000; 5164/7411 tok/s;     85 sec\n",
      "[2021-01-30 02:37:19,923 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:19,933 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:20,869 INFO] Step 4450/ 5000; acc:  96.58; ppl:  1.18; xent: 0.17; lr: 1.00000; 5106/7327 tok/s;     86 sec\n",
      "[2021-01-30 02:37:20,870 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:20,878 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:21,813 INFO] Step 4500/ 5000; acc:  96.70; ppl:  1.17; xent: 0.16; lr: 1.00000; 5120/7348 tok/s;     87 sec\n",
      "[2021-01-30 02:37:21,813 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:21,822 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:22,783 INFO] Step 4550/ 5000; acc:  96.38; ppl:  1.21; xent: 0.19; lr: 1.00000; 4981/7148 tok/s;     88 sec\n",
      "[2021-01-30 02:37:22,783 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:22,791 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:23,721 INFO] Step 4600/ 5000; acc:  96.71; ppl:  1.18; xent: 0.17; lr: 1.00000; 5154/7397 tok/s;     89 sec\n",
      "[2021-01-30 02:37:23,721 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:23,730 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:24,676 INFO] Step 4650/ 5000; acc:  97.22; ppl:  1.13; xent: 0.12; lr: 1.00000; 5058/7259 tok/s;     90 sec\n",
      "[2021-01-30 02:37:24,676 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:24,685 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:25,602 INFO] Step 4700/ 5000; acc:  96.87; ppl:  1.17; xent: 0.16; lr: 1.00000; 5218/7488 tok/s;     90 sec\n",
      "[2021-01-30 02:37:25,602 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:25,629 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:26,587 INFO] Step 4750/ 5000; acc:  96.94; ppl:  1.17; xent: 0.16; lr: 1.00000; 4904/7037 tok/s;     91 sec\n",
      "[2021-01-30 02:37:26,588 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:26,596 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:27,531 INFO] Step 4800/ 5000; acc:  97.16; ppl:  1.15; xent: 0.14; lr: 1.00000; 5123/7352 tok/s;     92 sec\n",
      "[2021-01-30 02:37:27,531 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:27,540 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:28,494 INFO] Step 4850/ 5000; acc:  96.94; ppl:  1.16; xent: 0.14; lr: 1.00000; 5015/7197 tok/s;     93 sec\n",
      "[2021-01-30 02:37:28,494 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:28,503 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:29,461 INFO] Step 4900/ 5000; acc:  96.99; ppl:  1.16; xent: 0.15; lr: 1.00000; 4999/7174 tok/s;     94 sec\n",
      "[2021-01-30 02:37:29,461 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:29,469 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:30,393 INFO] Step 4950/ 5000; acc:  96.99; ppl:  1.15; xent: 0.14; lr: 1.00000; 5183/7439 tok/s;     95 sec\n",
      "[2021-01-30 02:37:30,393 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_1/processed.train.0.pt\n",
      "[2021-01-30 02:37:30,401 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:31,356 INFO] Step 5000/ 5000; acc:  97.03; ppl:  1.15; xent: 0.14; lr: 1.00000; 5018/7201 tok/s;     96 sec\n",
      "[2021-01-30 02:37:31,357 INFO] Saving checkpoint drive/MyDrive/EnglishToleranceBaseline/output/english_rnn_model_1000_1_step_5000.pt\n",
      "[2021-01-30 02:37:33,031 INFO]  * src vocab size = 43\n",
      "[2021-01-30 02:37:33,032 INFO]  * tgt vocab size = 45\n",
      "[2021-01-30 02:37:33,032 INFO] Building model...\n",
      "[2021-01-30 02:37:37,331 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(43, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(45, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=45, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-01-30 02:37:37,331 INFO] encoder: 254500\n",
      "[2021-01-30 02:37:37,332 INFO] decoder: 329645\n",
      "[2021-01-30 02:37:37,332 INFO] * number of parameters: 584145\n",
      "[2021-01-30 02:37:37,334 INFO] Starting training on GPU: [0]\n",
      "[2021-01-30 02:37:37,334 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-01-30 02:37:37,335 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:37,958 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:38,938 INFO] Step 50/ 5000; acc:  11.37; ppl: 34.32; xent: 3.54; lr: 1.00000; 3016/4335 tok/s;      2 sec\n",
      "[2021-01-30 02:37:38,938 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:38,947 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:39,883 INFO] Step 100/ 5000; acc:  16.99; ppl: 22.60; xent: 3.12; lr: 1.00000; 5117/7354 tok/s;      3 sec\n",
      "[2021-01-30 02:37:39,884 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:39,892 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:40,859 INFO] Step 150/ 5000; acc:  22.58; ppl: 18.30; xent: 2.91; lr: 1.00000; 4956/7122 tok/s;      4 sec\n",
      "[2021-01-30 02:37:40,860 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:40,867 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:41,805 INFO] Step 200/ 5000; acc:  24.55; ppl: 15.24; xent: 2.72; lr: 1.00000; 5115/7350 tok/s;      4 sec\n",
      "[2021-01-30 02:37:41,805 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:41,814 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:42,788 INFO] Step 250/ 5000; acc:  27.57; ppl: 13.28; xent: 2.59; lr: 1.00000; 4923/7075 tok/s;      5 sec\n",
      "[2021-01-30 02:37:42,788 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:42,796 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:43,711 INFO] Step 300/ 5000; acc:  31.42; ppl: 11.78; xent: 2.47; lr: 1.00000; 5235/7524 tok/s;      6 sec\n",
      "[2021-01-30 02:37:43,712 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:43,739 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:44,679 INFO] Step 350/ 5000; acc:  36.42; ppl:  9.67; xent: 2.27; lr: 1.00000; 5000/7185 tok/s;      7 sec\n",
      "[2021-01-30 02:37:44,679 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:44,687 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:45,615 INFO] Step 400/ 5000; acc:  40.47; ppl:  8.09; xent: 2.09; lr: 1.00000; 5164/7422 tok/s;      8 sec\n",
      "[2021-01-30 02:37:45,616 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:45,624 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:46,587 INFO] Step 450/ 5000; acc:  43.63; ppl:  7.23; xent: 1.98; lr: 1.00000; 4977/7153 tok/s;      9 sec\n",
      "[2021-01-30 02:37:46,588 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:46,596 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:47,529 INFO] Step 500/ 5000; acc:  47.05; ppl:  6.22; xent: 1.83; lr: 1.00000; 5138/7384 tok/s;     10 sec\n",
      "[2021-01-30 02:37:47,529 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:47,537 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:48,450 INFO] Step 550/ 5000; acc:  51.29; ppl:  5.41; xent: 1.69; lr: 1.00000; 5252/7548 tok/s;     11 sec\n",
      "[2021-01-30 02:37:48,450 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:48,458 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:49,427 INFO] Step 600/ 5000; acc:  58.47; ppl:  4.15; xent: 1.42; lr: 1.00000; 4949/7112 tok/s;     12 sec\n",
      "[2021-01-30 02:37:49,427 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:49,436 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:50,382 INFO] Step 650/ 5000; acc:  66.01; ppl:  3.23; xent: 1.17; lr: 1.00000; 5066/7281 tok/s;     13 sec\n",
      "[2021-01-30 02:37:50,382 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:50,391 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:51,352 INFO] Step 700/ 5000; acc:  71.76; ppl:  2.72; xent: 1.00; lr: 1.00000; 4984/7163 tok/s;     14 sec\n",
      "[2021-01-30 02:37:51,353 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:51,362 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:52,336 INFO] Step 750/ 5000; acc:  78.35; ppl:  2.16; xent: 0.77; lr: 1.00000; 4917/7067 tok/s;     15 sec\n",
      "[2021-01-30 02:37:52,336 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:52,344 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:53,322 INFO] Step 800/ 5000; acc:  83.94; ppl:  1.82; xent: 0.60; lr: 1.00000; 4905/7050 tok/s;     16 sec\n",
      "[2021-01-30 02:37:53,322 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:53,332 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:54,418 INFO] Step 850/ 5000; acc:  87.08; ppl:  1.64; xent: 0.50; lr: 1.00000; 4414/6343 tok/s;     17 sec\n",
      "[2021-01-30 02:37:54,418 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:54,427 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:55,463 INFO] Step 900/ 5000; acc:  90.63; ppl:  1.45; xent: 0.37; lr: 1.00000; 4629/6653 tok/s;     18 sec\n",
      "[2021-01-30 02:37:55,463 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:55,472 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:56,457 INFO] Step 950/ 5000; acc:  90.66; ppl:  1.46; xent: 0.38; lr: 1.00000; 4867/6995 tok/s;     19 sec\n",
      "[2021-01-30 02:37:56,457 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:56,465 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:57,431 INFO] Step 1000/ 5000; acc:  92.47; ppl:  1.38; xent: 0.32; lr: 1.00000; 4965/7135 tok/s;     20 sec\n",
      "[2021-01-30 02:37:57,431 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:57,440 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:58,411 INFO] Step 1050/ 5000; acc:  93.25; ppl:  1.32; xent: 0.28; lr: 1.00000; 4934/7091 tok/s;     21 sec\n",
      "[2021-01-30 02:37:58,412 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:58,442 INFO] number of examples: 1000\n",
      "[2021-01-30 02:37:59,393 INFO] Step 1100/ 5000; acc:  93.37; ppl:  1.30; xent: 0.26; lr: 1.00000; 4926/7079 tok/s;     22 sec\n",
      "[2021-01-30 02:37:59,394 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:37:59,402 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:00,350 INFO] Step 1150/ 5000; acc:  93.86; ppl:  1.27; xent: 0.24; lr: 1.00000; 5054/7263 tok/s;     23 sec\n",
      "[2021-01-30 02:38:00,351 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:00,359 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:01,313 INFO] Step 1200/ 5000; acc:  94.42; ppl:  1.26; xent: 0.23; lr: 1.00000; 5026/7223 tok/s;     24 sec\n",
      "[2021-01-30 02:38:01,313 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:01,322 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:02,310 INFO] Step 1250/ 5000; acc:  95.17; ppl:  1.22; xent: 0.20; lr: 1.00000; 4853/6974 tok/s;     25 sec\n",
      "[2021-01-30 02:38:02,310 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:02,318 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:03,274 INFO] Step 1300/ 5000; acc:  94.63; ppl:  1.24; xent: 0.21; lr: 1.00000; 5018/7211 tok/s;     26 sec\n",
      "[2021-01-30 02:38:03,274 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:03,282 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:04,251 INFO] Step 1350/ 5000; acc:  95.45; ppl:  1.20; xent: 0.18; lr: 1.00000; 4951/7115 tok/s;     27 sec\n",
      "[2021-01-30 02:38:04,251 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:04,259 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:05,212 INFO] Step 1400/ 5000; acc:  95.86; ppl:  1.19; xent: 0.17; lr: 1.00000; 5033/7233 tok/s;     28 sec\n",
      "[2021-01-30 02:38:05,212 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:05,221 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:06,239 INFO] Step 1450/ 5000; acc:  95.05; ppl:  1.21; xent: 0.19; lr: 1.00000; 4707/6765 tok/s;     29 sec\n",
      "[2021-01-30 02:38:06,239 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:06,248 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:07,220 INFO] Step 1500/ 5000; acc:  95.73; ppl:  1.19; xent: 0.17; lr: 1.00000; 4932/7088 tok/s;     30 sec\n",
      "[2021-01-30 02:38:07,220 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:07,229 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:08,188 INFO] Step 1550/ 5000; acc:  96.36; ppl:  1.15; xent: 0.14; lr: 1.00000; 4998/7182 tok/s;     31 sec\n",
      "[2021-01-30 02:38:08,188 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:08,196 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:09,150 INFO] Step 1600/ 5000; acc:  96.50; ppl:  1.15; xent: 0.14; lr: 1.00000; 5026/7222 tok/s;     32 sec\n",
      "[2021-01-30 02:38:09,150 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:09,159 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:10,114 INFO] Step 1650/ 5000; acc:  96.91; ppl:  1.15; xent: 0.14; lr: 1.00000; 5018/7211 tok/s;     33 sec\n",
      "[2021-01-30 02:38:10,114 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:10,125 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:11,053 INFO] Step 1700/ 5000; acc:  95.73; ppl:  1.19; xent: 0.17; lr: 1.00000; 5152/7404 tok/s;     34 sec\n",
      "[2021-01-30 02:38:11,053 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:11,062 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:12,055 INFO] Step 1750/ 5000; acc:  96.46; ppl:  1.15; xent: 0.14; lr: 1.00000; 4828/6938 tok/s;     35 sec\n",
      "[2021-01-30 02:38:12,055 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:12,064 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:13,034 INFO] Step 1800/ 5000; acc:  95.81; ppl:  1.19; xent: 0.17; lr: 1.00000; 4941/7100 tok/s;     36 sec\n",
      "[2021-01-30 02:38:13,034 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:13,061 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:13,989 INFO] Step 1850/ 5000; acc:  96.62; ppl:  1.15; xent: 0.14; lr: 1.00000; 5064/7277 tok/s;     37 sec\n",
      "[2021-01-30 02:38:13,990 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:13,998 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:14,970 INFO] Step 1900/ 5000; acc:  96.95; ppl:  1.13; xent: 0.12; lr: 1.00000; 4934/7091 tok/s;     38 sec\n",
      "[2021-01-30 02:38:14,970 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:14,978 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:15,900 INFO] Step 1950/ 5000; acc:  96.81; ppl:  1.13; xent: 0.12; lr: 1.00000; 5199/7472 tok/s;     39 sec\n",
      "[2021-01-30 02:38:15,900 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:15,908 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:16,865 INFO] Step 2000/ 5000; acc:  95.71; ppl:  1.20; xent: 0.18; lr: 1.00000; 5011/7201 tok/s;     40 sec\n",
      "[2021-01-30 02:38:16,866 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:16,874 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:17,798 INFO] Step 2050/ 5000; acc:  96.82; ppl:  1.14; xent: 0.13; lr: 1.00000; 5184/7451 tok/s;     40 sec\n",
      "[2021-01-30 02:38:17,799 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:17,807 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:18,761 INFO] Step 2100/ 5000; acc:  96.23; ppl:  1.19; xent: 0.17; lr: 1.00000; 5025/7222 tok/s;     41 sec\n",
      "[2021-01-30 02:38:18,761 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:18,770 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:19,715 INFO] Step 2150/ 5000; acc:  95.87; ppl:  1.19; xent: 0.17; lr: 1.00000; 5069/7285 tok/s;     42 sec\n",
      "[2021-01-30 02:38:19,715 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:19,723 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:20,650 INFO] Step 2200/ 5000; acc:  96.47; ppl:  1.17; xent: 0.15; lr: 1.00000; 5173/7434 tok/s;     43 sec\n",
      "[2021-01-30 02:38:20,650 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:20,659 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:21,574 INFO] Step 2250/ 5000; acc:  96.66; ppl:  1.15; xent: 0.14; lr: 1.00000; 5238/7528 tok/s;     44 sec\n",
      "[2021-01-30 02:38:21,574 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:21,583 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:22,590 INFO] Step 2300/ 5000; acc:  96.65; ppl:  1.16; xent: 0.15; lr: 1.00000; 4758/6838 tok/s;     45 sec\n",
      "[2021-01-30 02:38:22,590 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:22,599 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:23,556 INFO] Step 2350/ 5000; acc:  97.08; ppl:  1.14; xent: 0.13; lr: 1.00000; 5008/7197 tok/s;     46 sec\n",
      "[2021-01-30 02:38:23,556 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:23,564 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:24,523 INFO] Step 2400/ 5000; acc:  96.98; ppl:  1.14; xent: 0.13; lr: 1.00000; 5004/7191 tok/s;     47 sec\n",
      "[2021-01-30 02:38:24,523 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:24,531 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:25,496 INFO] Step 2450/ 5000; acc:  97.05; ppl:  1.14; xent: 0.13; lr: 1.00000; 4967/7138 tok/s;     48 sec\n",
      "[2021-01-30 02:38:25,497 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:25,505 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:26,492 INFO] Step 2500/ 5000; acc:  97.29; ppl:  1.12; xent: 0.11; lr: 1.00000; 4859/6983 tok/s;     49 sec\n",
      "[2021-01-30 02:38:26,492 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:26,520 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:27,469 INFO] Step 2550/ 5000; acc:  97.27; ppl:  1.13; xent: 0.12; lr: 1.00000; 4953/7117 tok/s;     50 sec\n",
      "[2021-01-30 02:38:27,469 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:27,477 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:28,433 INFO] Step 2600/ 5000; acc:  96.99; ppl:  1.13; xent: 0.12; lr: 1.00000; 5016/7208 tok/s;     51 sec\n",
      "[2021-01-30 02:38:28,433 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:28,441 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:29,399 INFO] Step 2650/ 5000; acc:  97.12; ppl:  1.14; xent: 0.13; lr: 1.00000; 5008/7197 tok/s;     52 sec\n",
      "[2021-01-30 02:38:29,399 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:29,409 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:30,347 INFO] Step 2700/ 5000; acc:  97.51; ppl:  1.11; xent: 0.11; lr: 1.00000; 5099/7328 tok/s;     53 sec\n",
      "[2021-01-30 02:38:30,348 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:30,356 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:31,310 INFO] Step 2750/ 5000; acc:  96.72; ppl:  1.15; xent: 0.14; lr: 1.00000; 5025/7222 tok/s;     54 sec\n",
      "[2021-01-30 02:38:31,310 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:31,318 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:32,270 INFO] Step 2800/ 5000; acc:  97.09; ppl:  1.15; xent: 0.14; lr: 1.00000; 5038/7240 tok/s;     55 sec\n",
      "[2021-01-30 02:38:32,270 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:32,280 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:33,237 INFO] Step 2850/ 5000; acc:  97.37; ppl:  1.12; xent: 0.12; lr: 1.00000; 5002/7188 tok/s;     56 sec\n",
      "[2021-01-30 02:38:33,237 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:33,246 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:34,195 INFO] Step 2900/ 5000; acc:  96.82; ppl:  1.15; xent: 0.14; lr: 1.00000; 5048/7255 tok/s;     57 sec\n",
      "[2021-01-30 02:38:34,196 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:34,204 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:35,200 INFO] Step 2950/ 5000; acc:  95.96; ppl:  1.20; xent: 0.19; lr: 1.00000; 4815/6920 tok/s;     58 sec\n",
      "[2021-01-30 02:38:35,200 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:35,208 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:36,151 INFO] Step 3000/ 5000; acc:  96.06; ppl:  1.21; xent: 0.19; lr: 1.00000; 5086/7309 tok/s;     59 sec\n",
      "[2021-01-30 02:38:36,151 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:36,160 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:37,158 INFO] Step 3050/ 5000; acc:  96.26; ppl:  1.19; xent: 0.18; lr: 1.00000; 4804/6904 tok/s;     60 sec\n",
      "[2021-01-30 02:38:37,158 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:37,166 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:38,124 INFO] Step 3100/ 5000; acc:  96.81; ppl:  1.17; xent: 0.16; lr: 1.00000; 5004/7192 tok/s;     61 sec\n",
      "[2021-01-30 02:38:38,125 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:38,133 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:39,115 INFO] Step 3150/ 5000; acc:  96.30; ppl:  1.19; xent: 0.17; lr: 1.00000; 4883/7017 tok/s;     62 sec\n",
      "[2021-01-30 02:38:39,115 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:39,123 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:40,081 INFO] Step 3200/ 5000; acc:  96.47; ppl:  1.18; xent: 0.17; lr: 1.00000; 5004/7192 tok/s;     63 sec\n",
      "[2021-01-30 02:38:40,082 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:40,090 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:41,026 INFO] Step 3250/ 5000; acc:  96.86; ppl:  1.15; xent: 0.14; lr: 1.00000; 5122/7361 tok/s;     64 sec\n",
      "[2021-01-30 02:38:41,026 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:41,055 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:42,017 INFO] Step 3300/ 5000; acc:  97.06; ppl:  1.15; xent: 0.14; lr: 1.00000; 4879/7012 tok/s;     65 sec\n",
      "[2021-01-30 02:38:42,017 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:42,026 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:42,993 INFO] Step 3350/ 5000; acc:  96.20; ppl:  1.22; xent: 0.20; lr: 1.00000; 4956/7122 tok/s;     66 sec\n",
      "[2021-01-30 02:38:42,993 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:43,001 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:43,945 INFO] Step 3400/ 5000; acc:  96.60; ppl:  1.17; xent: 0.16; lr: 1.00000; 5081/7303 tok/s;     67 sec\n",
      "[2021-01-30 02:38:43,945 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:43,954 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:44,905 INFO] Step 3450/ 5000; acc:  96.73; ppl:  1.17; xent: 0.16; lr: 1.00000; 5040/7243 tok/s;     68 sec\n",
      "[2021-01-30 02:38:44,905 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:44,914 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:45,856 INFO] Step 3500/ 5000; acc:  97.31; ppl:  1.14; xent: 0.14; lr: 1.00000; 5087/7311 tok/s;     69 sec\n",
      "[2021-01-30 02:38:45,856 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:45,864 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:46,773 INFO] Step 3550/ 5000; acc:  97.45; ppl:  1.14; xent: 0.13; lr: 1.00000; 5270/7574 tok/s;     69 sec\n",
      "[2021-01-30 02:38:46,774 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:46,782 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:47,721 INFO] Step 3600/ 5000; acc:  96.91; ppl:  1.18; xent: 0.17; lr: 1.00000; 5104/7335 tok/s;     70 sec\n",
      "[2021-01-30 02:38:47,721 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:47,730 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:48,672 INFO] Step 3650/ 5000; acc:  96.96; ppl:  1.18; xent: 0.16; lr: 1.00000; 5086/7309 tok/s;     71 sec\n",
      "[2021-01-30 02:38:48,672 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:48,681 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:49,595 INFO] Step 3700/ 5000; acc:  97.14; ppl:  1.14; xent: 0.13; lr: 1.00000; 5243/7535 tok/s;     72 sec\n",
      "[2021-01-30 02:38:49,595 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:49,603 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:50,531 INFO] Step 3750/ 5000; acc:  97.08; ppl:  1.14; xent: 0.13; lr: 1.00000; 5167/7426 tok/s;     73 sec\n",
      "[2021-01-30 02:38:50,531 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:50,539 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:51,455 INFO] Step 3800/ 5000; acc:  97.40; ppl:  1.16; xent: 0.14; lr: 1.00000; 5236/7525 tok/s;     74 sec\n",
      "[2021-01-30 02:38:51,455 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:51,463 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:52,406 INFO] Step 3850/ 5000; acc:  96.89; ppl:  1.17; xent: 0.16; lr: 1.00000; 5082/7304 tok/s;     75 sec\n",
      "[2021-01-30 02:38:52,407 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:52,415 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:53,385 INFO] Step 3900/ 5000; acc:  96.68; ppl:  1.18; xent: 0.17; lr: 1.00000; 4942/7103 tok/s;     76 sec\n",
      "[2021-01-30 02:38:53,385 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:53,394 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:54,332 INFO] Step 3950/ 5000; acc:  96.73; ppl:  1.17; xent: 0.16; lr: 1.00000; 5108/7341 tok/s;     77 sec\n",
      "[2021-01-30 02:38:54,332 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:54,340 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:55,314 INFO] Step 4000/ 5000; acc:  97.44; ppl:  1.14; xent: 0.13; lr: 1.00000; 4925/7077 tok/s;     78 sec\n",
      "[2021-01-30 02:38:55,314 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:55,342 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:56,288 INFO] Step 4050/ 5000; acc:  97.29; ppl:  1.15; xent: 0.14; lr: 1.00000; 4967/7138 tok/s;     79 sec\n",
      "[2021-01-30 02:38:56,288 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:56,296 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:57,220 INFO] Step 4100/ 5000; acc:  97.15; ppl:  1.15; xent: 0.14; lr: 1.00000; 5189/7458 tok/s;     80 sec\n",
      "[2021-01-30 02:38:57,220 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:57,229 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:58,187 INFO] Step 4150/ 5000; acc:  97.44; ppl:  1.12; xent: 0.12; lr: 1.00000; 5005/7192 tok/s;     81 sec\n",
      "[2021-01-30 02:38:58,187 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:58,194 INFO] number of examples: 1000\n",
      "[2021-01-30 02:38:59,152 INFO] Step 4200/ 5000; acc:  97.55; ppl:  1.12; xent: 0.12; lr: 1.00000; 5008/7197 tok/s;     82 sec\n",
      "[2021-01-30 02:38:59,153 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:38:59,161 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:00,101 INFO] Step 4250/ 5000; acc:  97.55; ppl:  1.14; xent: 0.13; lr: 1.00000; 5098/7327 tok/s;     83 sec\n",
      "[2021-01-30 02:39:00,101 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:39:00,110 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:01,054 INFO] Step 4300/ 5000; acc:  97.06; ppl:  1.13; xent: 0.12; lr: 1.00000; 5077/7296 tok/s;     84 sec\n",
      "[2021-01-30 02:39:01,054 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:39:01,063 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:02,070 INFO] Step 4350/ 5000; acc:  96.98; ppl:  1.16; xent: 0.15; lr: 1.00000; 4758/6838 tok/s;     85 sec\n",
      "[2021-01-30 02:39:02,071 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:39:02,079 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:03,040 INFO] Step 4400/ 5000; acc:  97.28; ppl:  1.15; xent: 0.14; lr: 1.00000; 4991/7173 tok/s;     86 sec\n",
      "[2021-01-30 02:39:03,040 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:39:03,049 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:04,031 INFO] Step 4450/ 5000; acc:  96.92; ppl:  1.16; xent: 0.15; lr: 1.00000; 4876/7008 tok/s;     87 sec\n",
      "[2021-01-30 02:39:04,032 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:39:04,040 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:04,987 INFO] Step 4500/ 5000; acc:  97.21; ppl:  1.17; xent: 0.16; lr: 1.00000; 5060/7272 tok/s;     88 sec\n",
      "[2021-01-30 02:39:04,988 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:39:04,996 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:05,940 INFO] Step 4550/ 5000; acc:  96.82; ppl:  1.15; xent: 0.14; lr: 1.00000; 5079/7300 tok/s;     89 sec\n",
      "[2021-01-30 02:39:05,940 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:39:05,948 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:06,887 INFO] Step 4600/ 5000; acc:  97.12; ppl:  1.16; xent: 0.15; lr: 1.00000; 5108/7341 tok/s;     90 sec\n",
      "[2021-01-30 02:39:06,887 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:39:06,899 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:07,877 INFO] Step 4650/ 5000; acc:  97.44; ppl:  1.12; xent: 0.11; lr: 1.00000; 4882/7016 tok/s;     91 sec\n",
      "[2021-01-30 02:39:07,878 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:39:07,886 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:08,816 INFO] Step 4700/ 5000; acc:  97.17; ppl:  1.16; xent: 0.15; lr: 1.00000; 5156/7409 tok/s;     91 sec\n",
      "[2021-01-30 02:39:08,816 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:39:08,842 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:09,771 INFO] Step 4750/ 5000; acc:  96.89; ppl:  1.19; xent: 0.18; lr: 1.00000; 5061/7273 tok/s;     92 sec\n",
      "[2021-01-30 02:39:09,772 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:39:09,780 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:10,755 INFO] Step 4800/ 5000; acc:  96.76; ppl:  1.22; xent: 0.20; lr: 1.00000; 4918/7068 tok/s;     93 sec\n",
      "[2021-01-30 02:39:10,755 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:39:10,764 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:11,715 INFO] Step 4850/ 5000; acc:  96.32; ppl:  1.23; xent: 0.20; lr: 1.00000; 5035/7236 tok/s;     94 sec\n",
      "[2021-01-30 02:39:11,716 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:39:11,724 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:12,653 INFO] Step 4900/ 5000; acc:  97.08; ppl:  1.17; xent: 0.16; lr: 1.00000; 5159/7414 tok/s;     95 sec\n",
      "[2021-01-30 02:39:12,653 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:39:12,662 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:13,591 INFO] Step 4950/ 5000; acc:  96.66; ppl:  1.18; xent: 0.17; lr: 1.00000; 5158/7413 tok/s;     96 sec\n",
      "[2021-01-30 02:39:13,591 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_2/processed.train.0.pt\n",
      "[2021-01-30 02:39:13,599 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:14,535 INFO] Step 5000/ 5000; acc:  97.27; ppl:  1.15; xent: 0.14; lr: 1.00000; 5122/7361 tok/s;     97 sec\n",
      "[2021-01-30 02:39:14,536 INFO] Saving checkpoint drive/MyDrive/EnglishToleranceBaseline/output/english_rnn_model_1000_2_step_5000.pt\n",
      "[2021-01-30 02:39:16,256 INFO]  * src vocab size = 44\n",
      "[2021-01-30 02:39:16,257 INFO]  * tgt vocab size = 46\n",
      "[2021-01-30 02:39:16,257 INFO] Building model...\n",
      "[2021-01-30 02:39:20,529 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(44, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(46, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=46, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-01-30 02:39:20,530 INFO] encoder: 254800\n",
      "[2021-01-30 02:39:20,530 INFO] decoder: 330046\n",
      "[2021-01-30 02:39:20,530 INFO] * number of parameters: 584846\n",
      "[2021-01-30 02:39:20,532 INFO] Starting training on GPU: [0]\n",
      "[2021-01-30 02:39:20,533 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-01-30 02:39:20,533 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:21,157 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:22,140 INFO] Step 50/ 5000; acc:  12.13; ppl: 32.88; xent: 3.49; lr: 1.00000; 2997/4315 tok/s;      2 sec\n",
      "[2021-01-30 02:39:22,140 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:22,149 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:23,072 INFO] Step 100/ 5000; acc:  21.99; ppl: 19.42; xent: 2.97; lr: 1.00000; 5169/7441 tok/s;      3 sec\n",
      "[2021-01-30 02:39:23,072 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:23,081 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:23,995 INFO] Step 150/ 5000; acc:  24.44; ppl: 15.61; xent: 2.75; lr: 1.00000; 5219/7513 tok/s;      3 sec\n",
      "[2021-01-30 02:39:23,996 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:24,004 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:24,919 INFO] Step 200/ 5000; acc:  27.07; ppl: 13.57; xent: 2.61; lr: 1.00000; 5219/7513 tok/s;      4 sec\n",
      "[2021-01-30 02:39:24,919 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:24,927 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:25,851 INFO] Step 250/ 5000; acc:  31.19; ppl: 11.29; xent: 2.42; lr: 1.00000; 5166/7438 tok/s;      5 sec\n",
      "[2021-01-30 02:39:25,851 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:25,859 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:26,806 INFO] Step 300/ 5000; acc:  35.69; ppl:  9.59; xent: 2.26; lr: 1.00000; 5047/7266 tok/s;      6 sec\n",
      "[2021-01-30 02:39:26,806 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:26,833 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:27,770 INFO] Step 350/ 5000; acc:  38.93; ppl:  8.44; xent: 2.13; lr: 1.00000; 4995/7191 tok/s;      7 sec\n",
      "[2021-01-30 02:39:27,771 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:27,779 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:28,758 INFO] Step 400/ 5000; acc:  43.88; ppl:  7.22; xent: 1.98; lr: 1.00000; 4877/7021 tok/s;      8 sec\n",
      "[2021-01-30 02:39:28,758 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:28,768 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:29,712 INFO] Step 450/ 5000; acc:  46.21; ppl:  6.41; xent: 1.86; lr: 1.00000; 5051/7271 tok/s;      9 sec\n",
      "[2021-01-30 02:39:29,712 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:29,721 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:30,662 INFO] Step 500/ 5000; acc:  49.23; ppl:  5.74; xent: 1.75; lr: 1.00000; 5074/7305 tok/s;     10 sec\n",
      "[2021-01-30 02:39:30,662 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:30,671 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:31,631 INFO] Step 550/ 5000; acc:  51.05; ppl:  5.38; xent: 1.68; lr: 1.00000; 4971/7157 tok/s;     11 sec\n",
      "[2021-01-30 02:39:31,631 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:31,639 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:32,581 INFO] Step 600/ 5000; acc:  56.63; ppl:  4.32; xent: 1.46; lr: 1.00000; 5069/7298 tok/s;     12 sec\n",
      "[2021-01-30 02:39:32,582 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:32,590 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:33,587 INFO] Step 650/ 5000; acc:  62.13; ppl:  3.60; xent: 1.28; lr: 1.00000; 4793/6901 tok/s;     13 sec\n",
      "[2021-01-30 02:39:33,587 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:33,595 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:34,547 INFO] Step 700/ 5000; acc:  67.12; ppl:  3.01; xent: 1.10; lr: 1.00000; 5015/7220 tok/s;     14 sec\n",
      "[2021-01-30 02:39:34,547 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:34,556 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:35,519 INFO] Step 750/ 5000; acc:  74.20; ppl:  2.44; xent: 0.89; lr: 1.00000; 4960/7140 tok/s;     15 sec\n",
      "[2021-01-30 02:39:35,519 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:35,528 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:36,463 INFO] Step 800/ 5000; acc:  78.50; ppl:  2.16; xent: 0.77; lr: 1.00000; 5104/7348 tok/s;     16 sec\n",
      "[2021-01-30 02:39:36,463 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:36,471 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:37,416 INFO] Step 850/ 5000; acc:  82.41; ppl:  1.87; xent: 0.63; lr: 1.00000; 5055/7277 tok/s;     17 sec\n",
      "[2021-01-30 02:39:37,416 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:37,425 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:38,397 INFO] Step 900/ 5000; acc:  85.58; ppl:  1.71; xent: 0.53; lr: 1.00000; 4910/7069 tok/s;     18 sec\n",
      "[2021-01-30 02:39:38,397 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:38,405 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:39,324 INFO] Step 950/ 5000; acc:  88.55; ppl:  1.51; xent: 0.41; lr: 1.00000; 5199/7485 tok/s;     19 sec\n",
      "[2021-01-30 02:39:39,324 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:39,332 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:40,269 INFO] Step 1000/ 5000; acc:  90.48; ppl:  1.45; xent: 0.37; lr: 1.00000; 5094/7334 tok/s;     20 sec\n",
      "[2021-01-30 02:39:40,270 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:40,278 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:41,193 INFO] Step 1050/ 5000; acc:  91.29; ppl:  1.41; xent: 0.34; lr: 1.00000; 5216/7509 tok/s;     21 sec\n",
      "[2021-01-30 02:39:41,193 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:41,220 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:42,149 INFO] Step 1100/ 5000; acc:  92.92; ppl:  1.32; xent: 0.28; lr: 1.00000; 5041/7258 tok/s;     22 sec\n",
      "[2021-01-30 02:39:42,149 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:42,159 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:43,100 INFO] Step 1150/ 5000; acc:  92.96; ppl:  1.31; xent: 0.27; lr: 1.00000; 5065/7292 tok/s;     23 sec\n",
      "[2021-01-30 02:39:43,100 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:43,108 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:44,055 INFO] Step 1200/ 5000; acc:  94.69; ppl:  1.23; xent: 0.21; lr: 1.00000; 5043/7260 tok/s;     24 sec\n",
      "[2021-01-30 02:39:44,056 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:44,064 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:45,008 INFO] Step 1250/ 5000; acc:  94.98; ppl:  1.22; xent: 0.20; lr: 1.00000; 5056/7280 tok/s;     24 sec\n",
      "[2021-01-30 02:39:45,008 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:45,017 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:45,978 INFO] Step 1300/ 5000; acc:  95.49; ppl:  1.20; xent: 0.18; lr: 1.00000; 4969/7154 tok/s;     25 sec\n",
      "[2021-01-30 02:39:45,978 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:45,987 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:46,911 INFO] Step 1350/ 5000; acc:  94.90; ppl:  1.21; xent: 0.19; lr: 1.00000; 5160/7429 tok/s;     26 sec\n",
      "[2021-01-30 02:39:46,912 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:46,920 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:47,833 INFO] Step 1400/ 5000; acc:  95.04; ppl:  1.22; xent: 0.20; lr: 1.00000; 5227/7525 tok/s;     27 sec\n",
      "[2021-01-30 02:39:47,833 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:47,842 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:48,780 INFO] Step 1450/ 5000; acc:  95.65; ppl:  1.20; xent: 0.18; lr: 1.00000; 5090/7328 tok/s;     28 sec\n",
      "[2021-01-30 02:39:48,780 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:48,789 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:49,712 INFO] Step 1500/ 5000; acc:  96.08; ppl:  1.15; xent: 0.14; lr: 1.00000; 5166/7437 tok/s;     29 sec\n",
      "[2021-01-30 02:39:49,713 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:49,721 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:50,651 INFO] Step 1550/ 5000; acc:  96.32; ppl:  1.15; xent: 0.14; lr: 1.00000; 5131/7387 tok/s;     30 sec\n",
      "[2021-01-30 02:39:50,652 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:50,659 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:51,576 INFO] Step 1600/ 5000; acc:  96.12; ppl:  1.15; xent: 0.14; lr: 1.00000; 5209/7499 tok/s;     31 sec\n",
      "[2021-01-30 02:39:51,577 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:51,585 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:52,490 INFO] Step 1650/ 5000; acc:  96.64; ppl:  1.13; xent: 0.12; lr: 1.00000; 5273/7591 tok/s;     32 sec\n",
      "[2021-01-30 02:39:52,490 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:52,499 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:53,407 INFO] Step 1700/ 5000; acc:  95.21; ppl:  1.22; xent: 0.20; lr: 1.00000; 5256/7567 tok/s;     33 sec\n",
      "[2021-01-30 02:39:53,407 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:53,415 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:54,341 INFO] Step 1750/ 5000; acc:  95.86; ppl:  1.18; xent: 0.17; lr: 1.00000; 5157/7424 tok/s;     34 sec\n",
      "[2021-01-30 02:39:54,341 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:54,349 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:55,354 INFO] Step 1800/ 5000; acc:  96.28; ppl:  1.16; xent: 0.15; lr: 1.00000; 4754/6845 tok/s;     35 sec\n",
      "[2021-01-30 02:39:55,355 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:55,383 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:56,314 INFO] Step 1850/ 5000; acc:  96.68; ppl:  1.14; xent: 0.13; lr: 1.00000; 5019/7226 tok/s;     36 sec\n",
      "[2021-01-30 02:39:56,314 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:56,324 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:57,257 INFO] Step 1900/ 5000; acc:  97.20; ppl:  1.12; xent: 0.12; lr: 1.00000; 5111/7358 tok/s;     37 sec\n",
      "[2021-01-30 02:39:57,257 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:57,266 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:58,207 INFO] Step 1950/ 5000; acc:  97.42; ppl:  1.10; xent: 0.10; lr: 1.00000; 5073/7304 tok/s;     38 sec\n",
      "[2021-01-30 02:39:58,207 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:58,215 INFO] number of examples: 1000\n",
      "[2021-01-30 02:39:59,133 INFO] Step 2000/ 5000; acc:  97.29; ppl:  1.12; xent: 0.11; lr: 1.00000; 5204/7492 tok/s;     39 sec\n",
      "[2021-01-30 02:39:59,133 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:39:59,142 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:00,089 INFO] Step 2050/ 5000; acc:  96.97; ppl:  1.12; xent: 0.11; lr: 1.00000; 5038/7253 tok/s;     40 sec\n",
      "[2021-01-30 02:40:00,089 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:00,098 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:01,038 INFO] Step 2100/ 5000; acc:  96.83; ppl:  1.13; xent: 0.12; lr: 1.00000; 5074/7306 tok/s;     41 sec\n",
      "[2021-01-30 02:40:01,038 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:01,047 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:02,022 INFO] Step 2150/ 5000; acc:  96.24; ppl:  1.18; xent: 0.17; lr: 1.00000; 4897/7051 tok/s;     41 sec\n",
      "[2021-01-30 02:40:02,022 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:02,031 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:03,038 INFO] Step 2200/ 5000; acc:  96.80; ppl:  1.13; xent: 0.12; lr: 1.00000; 4744/6830 tok/s;     43 sec\n",
      "[2021-01-30 02:40:03,038 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:03,047 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:04,002 INFO] Step 2250/ 5000; acc:  97.00; ppl:  1.14; xent: 0.13; lr: 1.00000; 4994/7190 tok/s;     43 sec\n",
      "[2021-01-30 02:40:04,003 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:04,012 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:04,959 INFO] Step 2300/ 5000; acc:  96.45; ppl:  1.16; xent: 0.15; lr: 1.00000; 5034/7248 tok/s;     44 sec\n",
      "[2021-01-30 02:40:04,960 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:04,968 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:05,923 INFO] Step 2350/ 5000; acc:  97.22; ppl:  1.13; xent: 0.12; lr: 1.00000; 5000/7199 tok/s;     45 sec\n",
      "[2021-01-30 02:40:05,923 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:05,931 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:06,842 INFO] Step 2400/ 5000; acc:  97.46; ppl:  1.11; xent: 0.10; lr: 1.00000; 5241/7546 tok/s;     46 sec\n",
      "[2021-01-30 02:40:06,842 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:06,851 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:07,806 INFO] Step 2450/ 5000; acc:  96.90; ppl:  1.13; xent: 0.12; lr: 1.00000; 5000/7198 tok/s;     47 sec\n",
      "[2021-01-30 02:40:07,806 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:07,815 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:08,805 INFO] Step 2500/ 5000; acc:  97.07; ppl:  1.12; xent: 0.12; lr: 1.00000; 4820/6939 tok/s;     48 sec\n",
      "[2021-01-30 02:40:08,806 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:08,834 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:09,827 INFO] Step 2550/ 5000; acc:  96.94; ppl:  1.13; xent: 0.12; lr: 1.00000; 4717/6792 tok/s;     49 sec\n",
      "[2021-01-30 02:40:09,827 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:09,835 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:10,808 INFO] Step 2600/ 5000; acc:  96.91; ppl:  1.15; xent: 0.14; lr: 1.00000; 4908/7066 tok/s;     50 sec\n",
      "[2021-01-30 02:40:10,808 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:10,817 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:11,767 INFO] Step 2650/ 5000; acc:  96.97; ppl:  1.14; xent: 0.13; lr: 1.00000; 5023/7232 tok/s;     51 sec\n",
      "[2021-01-30 02:40:11,768 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:11,776 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:12,715 INFO] Step 2700/ 5000; acc:  97.29; ppl:  1.12; xent: 0.11; lr: 1.00000; 5086/7322 tok/s;     52 sec\n",
      "[2021-01-30 02:40:12,715 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:12,724 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:13,686 INFO] Step 2750/ 5000; acc:  97.58; ppl:  1.10; xent: 0.09; lr: 1.00000; 4963/7145 tok/s;     53 sec\n",
      "[2021-01-30 02:40:13,686 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:13,694 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:14,643 INFO] Step 2800/ 5000; acc:  97.51; ppl:  1.10; xent: 0.10; lr: 1.00000; 5031/7243 tok/s;     54 sec\n",
      "[2021-01-30 02:40:14,643 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:14,652 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:15,589 INFO] Step 2850/ 5000; acc:  97.00; ppl:  1.14; xent: 0.14; lr: 1.00000; 5094/7334 tok/s;     55 sec\n",
      "[2021-01-30 02:40:15,589 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:15,598 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:16,564 INFO] Step 2900/ 5000; acc:  97.06; ppl:  1.13; xent: 0.12; lr: 1.00000; 4939/7111 tok/s;     56 sec\n",
      "[2021-01-30 02:40:16,565 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:16,574 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:17,588 INFO] Step 2950/ 5000; acc:  96.89; ppl:  1.13; xent: 0.13; lr: 1.00000; 4705/6774 tok/s;     57 sec\n",
      "[2021-01-30 02:40:17,589 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:17,597 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:18,574 INFO] Step 3000/ 5000; acc:  97.09; ppl:  1.13; xent: 0.13; lr: 1.00000; 4888/7037 tok/s;     58 sec\n",
      "[2021-01-30 02:40:18,574 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:18,583 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:19,571 INFO] Step 3050/ 5000; acc:  97.17; ppl:  1.13; xent: 0.12; lr: 1.00000; 4834/6960 tok/s;     59 sec\n",
      "[2021-01-30 02:40:19,571 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:19,580 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:20,550 INFO] Step 3100/ 5000; acc:  96.99; ppl:  1.16; xent: 0.15; lr: 1.00000; 4920/7083 tok/s;     60 sec\n",
      "[2021-01-30 02:40:20,550 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:20,559 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:21,548 INFO] Step 3150/ 5000; acc:  97.42; ppl:  1.13; xent: 0.12; lr: 1.00000; 4825/6947 tok/s;     61 sec\n",
      "[2021-01-30 02:40:21,549 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:21,557 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:22,581 INFO] Step 3200/ 5000; acc:  97.65; ppl:  1.11; xent: 0.11; lr: 1.00000; 4666/6717 tok/s;     62 sec\n",
      "[2021-01-30 02:40:22,581 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:22,590 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:23,599 INFO] Step 3250/ 5000; acc:  97.26; ppl:  1.13; xent: 0.12; lr: 1.00000; 4731/6811 tok/s;     63 sec\n",
      "[2021-01-30 02:40:23,600 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:23,628 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:24,630 INFO] Step 3300/ 5000; acc:  97.16; ppl:  1.15; xent: 0.14; lr: 1.00000; 4676/6732 tok/s;     64 sec\n",
      "[2021-01-30 02:40:24,630 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:24,638 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:25,604 INFO] Step 3350/ 5000; acc:  97.16; ppl:  1.14; xent: 0.13; lr: 1.00000; 4946/7121 tok/s;     65 sec\n",
      "[2021-01-30 02:40:25,604 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:25,612 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:26,572 INFO] Step 3400/ 5000; acc:  96.81; ppl:  1.14; xent: 0.13; lr: 1.00000; 4976/7164 tok/s;     66 sec\n",
      "[2021-01-30 02:40:26,572 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:26,581 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:27,531 INFO] Step 3450/ 5000; acc:  97.36; ppl:  1.12; xent: 0.11; lr: 1.00000; 5022/7230 tok/s;     67 sec\n",
      "[2021-01-30 02:40:27,532 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:27,540 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:28,514 INFO] Step 3500/ 5000; acc:  97.59; ppl:  1.11; xent: 0.11; lr: 1.00000; 4904/7060 tok/s;     68 sec\n",
      "[2021-01-30 02:40:28,514 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:28,523 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:29,507 INFO] Step 3550/ 5000; acc:  97.61; ppl:  1.12; xent: 0.12; lr: 1.00000; 4853/6987 tok/s;     69 sec\n",
      "[2021-01-30 02:40:29,507 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:29,516 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:30,519 INFO] Step 3600/ 5000; acc:  97.26; ppl:  1.13; xent: 0.13; lr: 1.00000; 4758/6850 tok/s;     70 sec\n",
      "[2021-01-30 02:40:30,519 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:30,529 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:31,535 INFO] Step 3650/ 5000; acc:  97.52; ppl:  1.11; xent: 0.11; lr: 1.00000; 4742/6827 tok/s;     71 sec\n",
      "[2021-01-30 02:40:31,536 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:31,545 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:32,509 INFO] Step 3700/ 5000; acc:  97.03; ppl:  1.17; xent: 0.15; lr: 1.00000; 4946/7121 tok/s;     72 sec\n",
      "[2021-01-30 02:40:32,510 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:32,518 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:33,506 INFO] Step 3750/ 5000; acc:  97.43; ppl:  1.13; xent: 0.12; lr: 1.00000; 4835/6961 tok/s;     73 sec\n",
      "[2021-01-30 02:40:33,506 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:33,514 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:34,473 INFO] Step 3800/ 5000; acc:  97.35; ppl:  1.13; xent: 0.12; lr: 1.00000; 4982/7172 tok/s;     74 sec\n",
      "[2021-01-30 02:40:34,473 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:34,482 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:35,433 INFO] Step 3850/ 5000; acc:  97.20; ppl:  1.13; xent: 0.12; lr: 1.00000; 5019/7227 tok/s;     75 sec\n",
      "[2021-01-30 02:40:35,433 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:35,441 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:36,484 INFO] Step 3900/ 5000; acc:  97.09; ppl:  1.15; xent: 0.14; lr: 1.00000; 4581/6595 tok/s;     76 sec\n",
      "[2021-01-30 02:40:36,485 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:36,494 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:37,628 INFO] Step 3950/ 5000; acc:  97.68; ppl:  1.11; xent: 0.10; lr: 1.00000; 4213/6066 tok/s;     77 sec\n",
      "[2021-01-30 02:40:37,628 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:37,637 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:38,634 INFO] Step 4000/ 5000; acc:  96.96; ppl:  1.16; xent: 0.15; lr: 1.00000; 4789/6895 tok/s;     78 sec\n",
      "[2021-01-30 02:40:38,634 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:38,662 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:39,675 INFO] Step 4050/ 5000; acc:  96.97; ppl:  1.17; xent: 0.16; lr: 1.00000; 4626/6660 tok/s;     79 sec\n",
      "[2021-01-30 02:40:39,676 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:39,684 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:40,637 INFO] Step 4100/ 5000; acc:  97.07; ppl:  1.16; xent: 0.15; lr: 1.00000; 5010/7212 tok/s;     80 sec\n",
      "[2021-01-30 02:40:40,637 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:40,646 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:41,616 INFO] Step 4150/ 5000; acc:  97.29; ppl:  1.16; xent: 0.14; lr: 1.00000; 4920/7083 tok/s;     81 sec\n",
      "[2021-01-30 02:40:41,617 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:41,625 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:42,598 INFO] Step 4200/ 5000; acc:  97.48; ppl:  1.14; xent: 0.13; lr: 1.00000; 4908/7066 tok/s;     82 sec\n",
      "[2021-01-30 02:40:42,598 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:42,607 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:43,583 INFO] Step 4250/ 5000; acc:  97.52; ppl:  1.12; xent: 0.12; lr: 1.00000; 4890/7040 tok/s;     83 sec\n",
      "[2021-01-30 02:40:43,584 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:43,592 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:44,536 INFO] Step 4300/ 5000; acc:  97.36; ppl:  1.12; xent: 0.12; lr: 1.00000; 5058/7282 tok/s;     84 sec\n",
      "[2021-01-30 02:40:44,536 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:44,545 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:45,513 INFO] Step 4350/ 5000; acc:  96.51; ppl:  1.20; xent: 0.19; lr: 1.00000; 4933/7102 tok/s;     85 sec\n",
      "[2021-01-30 02:40:45,513 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:45,521 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:46,490 INFO] Step 4400/ 5000; acc:  96.91; ppl:  1.17; xent: 0.15; lr: 1.00000; 4931/7100 tok/s;     86 sec\n",
      "[2021-01-30 02:40:46,490 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:46,498 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:47,429 INFO] Step 4450/ 5000; acc:  97.25; ppl:  1.13; xent: 0.13; lr: 1.00000; 5129/7384 tok/s;     87 sec\n",
      "[2021-01-30 02:40:47,429 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:47,438 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:48,419 INFO] Step 4500/ 5000; acc:  97.32; ppl:  1.14; xent: 0.13; lr: 1.00000; 4865/7003 tok/s;     88 sec\n",
      "[2021-01-30 02:40:48,420 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:48,428 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:49,382 INFO] Step 4550/ 5000; acc:  97.89; ppl:  1.11; xent: 0.11; lr: 1.00000; 5005/7206 tok/s;     89 sec\n",
      "[2021-01-30 02:40:49,382 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:49,390 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:50,406 INFO] Step 4600/ 5000; acc:  97.62; ppl:  1.14; xent: 0.13; lr: 1.00000; 4703/6772 tok/s;     90 sec\n",
      "[2021-01-30 02:40:50,407 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:50,415 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:51,378 INFO] Step 4650/ 5000; acc:  97.40; ppl:  1.12; xent: 0.12; lr: 1.00000; 4960/7140 tok/s;     91 sec\n",
      "[2021-01-30 02:40:51,378 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:51,387 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:52,339 INFO] Step 4700/ 5000; acc:  97.42; ppl:  1.13; xent: 0.12; lr: 1.00000; 5013/7218 tok/s;     92 sec\n",
      "[2021-01-30 02:40:52,339 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:52,366 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:53,371 INFO] Step 4750/ 5000; acc:  96.84; ppl:  1.18; xent: 0.16; lr: 1.00000; 4670/6723 tok/s;     93 sec\n",
      "[2021-01-30 02:40:53,371 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:53,379 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:54,332 INFO] Step 4800/ 5000; acc:  97.55; ppl:  1.14; xent: 0.13; lr: 1.00000; 5012/7216 tok/s;     94 sec\n",
      "[2021-01-30 02:40:54,332 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:54,341 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:55,320 INFO] Step 4850/ 5000; acc:  97.36; ppl:  1.14; xent: 0.13; lr: 1.00000; 4875/7018 tok/s;     95 sec\n",
      "[2021-01-30 02:40:55,320 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:55,331 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:56,323 INFO] Step 4900/ 5000; acc:  97.40; ppl:  1.15; xent: 0.14; lr: 1.00000; 4802/6914 tok/s;     96 sec\n",
      "[2021-01-30 02:40:56,324 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:56,340 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:57,329 INFO] Step 4950/ 5000; acc:  97.51; ppl:  1.14; xent: 0.13; lr: 1.00000; 4792/6899 tok/s;     97 sec\n",
      "[2021-01-30 02:40:57,329 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_3/processed.train.0.pt\n",
      "[2021-01-30 02:40:57,341 INFO] number of examples: 1000\n",
      "[2021-01-30 02:40:58,381 INFO] Step 5000/ 5000; acc:  96.52; ppl:  1.23; xent: 0.21; lr: 1.00000; 4578/6591 tok/s;     98 sec\n",
      "[2021-01-30 02:40:58,383 INFO] Saving checkpoint drive/MyDrive/EnglishToleranceBaseline/output/english_rnn_model_1000_3_step_5000.pt\n",
      "[2021-01-30 02:41:00,164 INFO]  * src vocab size = 44\n",
      "[2021-01-30 02:41:00,164 INFO]  * tgt vocab size = 46\n",
      "[2021-01-30 02:41:00,164 INFO] Building model...\n",
      "[2021-01-30 02:41:04,560 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(44, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(46, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=46, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-01-30 02:41:04,560 INFO] encoder: 254800\n",
      "[2021-01-30 02:41:04,560 INFO] decoder: 330046\n",
      "[2021-01-30 02:41:04,560 INFO] * number of parameters: 584846\n",
      "[2021-01-30 02:41:04,563 INFO] Starting training on GPU: [0]\n",
      "[2021-01-30 02:41:04,563 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-01-30 02:41:04,563 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:05,186 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:06,226 INFO] Step 50/ 5000; acc:  12.25; ppl: 33.89; xent: 3.52; lr: 1.00000; 2891/4166 tok/s;      2 sec\n",
      "[2021-01-30 02:41:06,226 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:06,235 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:07,203 INFO] Step 100/ 5000; acc:  20.31; ppl: 21.22; xent: 3.06; lr: 1.00000; 4918/7085 tok/s;      3 sec\n",
      "[2021-01-30 02:41:07,203 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:07,212 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:08,174 INFO] Step 150/ 5000; acc:  24.25; ppl: 16.58; xent: 2.81; lr: 1.00000; 4951/7132 tok/s;      4 sec\n",
      "[2021-01-30 02:41:08,174 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:08,182 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:09,130 INFO] Step 200/ 5000; acc:  28.13; ppl: 13.72; xent: 2.62; lr: 1.00000; 5027/7242 tok/s;      5 sec\n",
      "[2021-01-30 02:41:09,131 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:09,139 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:10,076 INFO] Step 250/ 5000; acc:  32.22; ppl: 10.79; xent: 2.38; lr: 1.00000; 5083/7323 tok/s;      6 sec\n",
      "[2021-01-30 02:41:10,076 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:10,085 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:11,018 INFO] Step 300/ 5000; acc:  37.59; ppl:  8.61; xent: 2.15; lr: 1.00000; 5102/7350 tok/s;      6 sec\n",
      "[2021-01-30 02:41:11,018 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:11,046 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:12,031 INFO] Step 350/ 5000; acc:  42.66; ppl:  7.42; xent: 2.00; lr: 1.00000; 4745/6836 tok/s;      7 sec\n",
      "[2021-01-30 02:41:12,032 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:12,040 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:12,964 INFO] Step 400/ 5000; acc:  43.73; ppl:  6.98; xent: 1.94; lr: 1.00000; 5156/7428 tok/s;      8 sec\n",
      "[2021-01-30 02:41:12,964 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:12,972 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:13,889 INFO] Step 450/ 5000; acc:  46.53; ppl:  6.10; xent: 1.81; lr: 1.00000; 5196/7486 tok/s;      9 sec\n",
      "[2021-01-30 02:41:13,889 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:13,897 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:14,808 INFO] Step 500/ 5000; acc:  48.96; ppl:  5.44; xent: 1.69; lr: 1.00000; 5226/7530 tok/s;     10 sec\n",
      "[2021-01-30 02:41:14,809 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:14,818 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:15,730 INFO] Step 550/ 5000; acc:  51.83; ppl:  4.87; xent: 1.58; lr: 1.00000; 5219/7519 tok/s;     11 sec\n",
      "[2021-01-30 02:41:15,730 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:15,738 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:16,664 INFO] Step 600/ 5000; acc:  56.46; ppl:  4.18; xent: 1.43; lr: 1.00000; 5145/7412 tok/s;     12 sec\n",
      "[2021-01-30 02:41:16,664 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:16,672 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:17,640 INFO] Step 650/ 5000; acc:  60.17; ppl:  3.75; xent: 1.32; lr: 1.00000; 4925/7095 tok/s;     13 sec\n",
      "[2021-01-30 02:41:17,640 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:17,648 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:18,625 INFO] Step 700/ 5000; acc:  67.55; ppl:  3.03; xent: 1.11; lr: 1.00000; 4880/7031 tok/s;     14 sec\n",
      "[2021-01-30 02:41:18,625 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:18,638 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:19,587 INFO] Step 750/ 5000; acc:  73.30; ppl:  2.52; xent: 0.93; lr: 1.00000; 4995/7197 tok/s;     15 sec\n",
      "[2021-01-30 02:41:19,587 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:19,595 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:20,524 INFO] Step 800/ 5000; acc:  77.72; ppl:  2.20; xent: 0.79; lr: 1.00000; 5130/7392 tok/s;     16 sec\n",
      "[2021-01-30 02:41:20,524 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:20,532 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:21,461 INFO] Step 850/ 5000; acc:  82.58; ppl:  1.91; xent: 0.65; lr: 1.00000; 5129/7390 tok/s;     17 sec\n",
      "[2021-01-30 02:41:21,462 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:21,470 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:22,442 INFO] Step 900/ 5000; acc:  86.63; ppl:  1.64; xent: 0.50; lr: 1.00000; 4899/7058 tok/s;     18 sec\n",
      "[2021-01-30 02:41:22,443 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:22,451 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:23,389 INFO] Step 950/ 5000; acc:  88.17; ppl:  1.59; xent: 0.46; lr: 1.00000; 5078/7316 tok/s;     19 sec\n",
      "[2021-01-30 02:41:23,389 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:23,397 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:24,346 INFO] Step 1000/ 5000; acc:  90.18; ppl:  1.46; xent: 0.38; lr: 1.00000; 5025/7239 tok/s;     20 sec\n",
      "[2021-01-30 02:41:24,346 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:24,354 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:25,299 INFO] Step 1050/ 5000; acc:  91.13; ppl:  1.42; xent: 0.35; lr: 1.00000; 5043/7265 tok/s;     21 sec\n",
      "[2021-01-30 02:41:25,299 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:25,327 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:26,253 INFO] Step 1100/ 5000; acc:  93.05; ppl:  1.30; xent: 0.26; lr: 1.00000; 5041/7262 tok/s;     22 sec\n",
      "[2021-01-30 02:41:26,253 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:26,261 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:27,184 INFO] Step 1150/ 5000; acc:  93.53; ppl:  1.27; xent: 0.24; lr: 1.00000; 5160/7435 tok/s;     23 sec\n",
      "[2021-01-30 02:41:27,184 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:27,192 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:28,147 INFO] Step 1200/ 5000; acc:  93.83; ppl:  1.29; xent: 0.25; lr: 1.00000; 4993/7194 tok/s;     24 sec\n",
      "[2021-01-30 02:41:28,147 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:28,155 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:29,172 INFO] Step 1250/ 5000; acc:  94.47; ppl:  1.25; xent: 0.23; lr: 1.00000; 4687/6753 tok/s;     25 sec\n",
      "[2021-01-30 02:41:29,172 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:29,181 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:30,230 INFO] Step 1300/ 5000; acc:  95.23; ppl:  1.21; xent: 0.19; lr: 1.00000; 4545/6548 tok/s;     26 sec\n",
      "[2021-01-30 02:41:30,230 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:30,239 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:31,224 INFO] Step 1350/ 5000; acc:  95.35; ppl:  1.19; xent: 0.18; lr: 1.00000; 4834/6964 tok/s;     27 sec\n",
      "[2021-01-30 02:41:31,224 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:31,233 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:32,217 INFO] Step 1400/ 5000; acc:  96.04; ppl:  1.16; xent: 0.15; lr: 1.00000; 4842/6976 tok/s;     28 sec\n",
      "[2021-01-30 02:41:32,217 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:32,227 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:33,227 INFO] Step 1450/ 5000; acc:  96.06; ppl:  1.17; xent: 0.15; lr: 1.00000; 4761/6859 tok/s;     29 sec\n",
      "[2021-01-30 02:41:33,227 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:33,237 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:34,218 INFO] Step 1500/ 5000; acc:  95.74; ppl:  1.19; xent: 0.18; lr: 1.00000; 4847/6983 tok/s;     30 sec\n",
      "[2021-01-30 02:41:34,219 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:34,227 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:35,202 INFO] Step 1550/ 5000; acc:  95.58; ppl:  1.17; xent: 0.16; lr: 1.00000; 4886/7040 tok/s;     31 sec\n",
      "[2021-01-30 02:41:35,202 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:35,210 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:36,133 INFO] Step 1600/ 5000; acc:  95.67; ppl:  1.20; xent: 0.18; lr: 1.00000; 5165/7441 tok/s;     32 sec\n",
      "[2021-01-30 02:41:36,133 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:36,142 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:37,090 INFO] Step 1650/ 5000; acc:  95.74; ppl:  1.19; xent: 0.17; lr: 1.00000; 5021/7234 tok/s;     33 sec\n",
      "[2021-01-30 02:41:37,090 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:37,099 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:38,027 INFO] Step 1700/ 5000; acc:  96.14; ppl:  1.16; xent: 0.15; lr: 1.00000; 5131/7392 tok/s;     33 sec\n",
      "[2021-01-30 02:41:38,027 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:38,036 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:38,961 INFO] Step 1750/ 5000; acc:  95.13; ppl:  1.23; xent: 0.21; lr: 1.00000; 5146/7414 tok/s;     34 sec\n",
      "[2021-01-30 02:41:38,961 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:38,969 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:39,947 INFO] Step 1800/ 5000; acc:  95.74; ppl:  1.17; xent: 0.16; lr: 1.00000; 4877/7026 tok/s;     35 sec\n",
      "[2021-01-30 02:41:39,947 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:39,974 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:40,903 INFO] Step 1850/ 5000; acc:  95.94; ppl:  1.17; xent: 0.15; lr: 1.00000; 5027/7242 tok/s;     36 sec\n",
      "[2021-01-30 02:41:40,903 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:40,912 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:41,837 INFO] Step 1900/ 5000; acc:  95.60; ppl:  1.19; xent: 0.17; lr: 1.00000; 5149/7418 tok/s;     37 sec\n",
      "[2021-01-30 02:41:41,837 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:41,845 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:42,785 INFO] Step 1950/ 5000; acc:  95.73; ppl:  1.21; xent: 0.19; lr: 1.00000; 5067/7300 tok/s;     38 sec\n",
      "[2021-01-30 02:41:42,786 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:42,793 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:43,706 INFO] Step 2000/ 5000; acc:  96.65; ppl:  1.15; xent: 0.14; lr: 1.00000; 5222/7524 tok/s;     39 sec\n",
      "[2021-01-30 02:41:43,706 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:43,714 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:44,630 INFO] Step 2050/ 5000; acc:  97.10; ppl:  1.13; xent: 0.12; lr: 1.00000; 5203/7495 tok/s;     40 sec\n",
      "[2021-01-30 02:41:44,630 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:44,638 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:45,588 INFO] Step 2100/ 5000; acc:  96.40; ppl:  1.16; xent: 0.15; lr: 1.00000; 5018/7229 tok/s;     41 sec\n",
      "[2021-01-30 02:41:45,588 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:45,596 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:46,531 INFO] Step 2150/ 5000; acc:  96.76; ppl:  1.14; xent: 0.13; lr: 1.00000; 5097/7343 tok/s;     42 sec\n",
      "[2021-01-30 02:41:46,531 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:46,539 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:47,489 INFO] Step 2200/ 5000; acc:  96.45; ppl:  1.15; xent: 0.14; lr: 1.00000; 5019/7231 tok/s;     43 sec\n",
      "[2021-01-30 02:41:47,489 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:47,497 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:48,435 INFO] Step 2250/ 5000; acc:  96.56; ppl:  1.15; xent: 0.14; lr: 1.00000; 5078/7316 tok/s;     44 sec\n",
      "[2021-01-30 02:41:48,435 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:48,444 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:49,370 INFO] Step 2300/ 5000; acc:  96.78; ppl:  1.15; xent: 0.14; lr: 1.00000; 5141/7406 tok/s;     45 sec\n",
      "[2021-01-30 02:41:49,370 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:49,379 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:50,335 INFO] Step 2350/ 5000; acc:  96.50; ppl:  1.13; xent: 0.13; lr: 1.00000; 4984/7180 tok/s;     46 sec\n",
      "[2021-01-30 02:41:50,335 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:50,343 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:51,268 INFO] Step 2400/ 5000; acc:  97.27; ppl:  1.13; xent: 0.12; lr: 1.00000; 5150/7419 tok/s;     47 sec\n",
      "[2021-01-30 02:41:51,268 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:51,277 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:52,203 INFO] Step 2450/ 5000; acc:  96.49; ppl:  1.15; xent: 0.14; lr: 1.00000; 5143/7409 tok/s;     48 sec\n",
      "[2021-01-30 02:41:52,203 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:52,212 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:53,164 INFO] Step 2500/ 5000; acc:  96.56; ppl:  1.16; xent: 0.15; lr: 1.00000; 5002/7206 tok/s;     49 sec\n",
      "[2021-01-30 02:41:53,164 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:53,191 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:54,129 INFO] Step 2550/ 5000; acc:  96.88; ppl:  1.14; xent: 0.13; lr: 1.00000; 4982/7177 tok/s;     50 sec\n",
      "[2021-01-30 02:41:54,129 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:54,137 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:55,061 INFO] Step 2600/ 5000; acc:  96.27; ppl:  1.18; xent: 0.16; lr: 1.00000; 5159/7432 tok/s;     50 sec\n",
      "[2021-01-30 02:41:55,061 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:55,069 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:55,992 INFO] Step 2650/ 5000; acc:  96.69; ppl:  1.15; xent: 0.14; lr: 1.00000; 5163/7439 tok/s;     51 sec\n",
      "[2021-01-30 02:41:55,992 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:56,000 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:56,952 INFO] Step 2700/ 5000; acc:  97.05; ppl:  1.13; xent: 0.12; lr: 1.00000; 5004/7209 tok/s;     52 sec\n",
      "[2021-01-30 02:41:56,952 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:56,961 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:57,940 INFO] Step 2750/ 5000; acc:  96.74; ppl:  1.17; xent: 0.16; lr: 1.00000; 4867/7011 tok/s;     53 sec\n",
      "[2021-01-30 02:41:57,940 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:57,948 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:58,878 INFO] Step 2800/ 5000; acc:  96.78; ppl:  1.16; xent: 0.15; lr: 1.00000; 5124/7382 tok/s;     54 sec\n",
      "[2021-01-30 02:41:58,878 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:58,887 INFO] number of examples: 1000\n",
      "[2021-01-30 02:41:59,814 INFO] Step 2850/ 5000; acc:  97.02; ppl:  1.14; xent: 0.13; lr: 1.00000; 5138/7403 tok/s;     55 sec\n",
      "[2021-01-30 02:41:59,814 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:41:59,822 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:00,759 INFO] Step 2900/ 5000; acc:  96.89; ppl:  1.16; xent: 0.14; lr: 1.00000; 5085/7327 tok/s;     56 sec\n",
      "[2021-01-30 02:42:00,759 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:00,767 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:01,724 INFO] Step 2950/ 5000; acc:  97.43; ppl:  1.13; xent: 0.12; lr: 1.00000; 4981/7176 tok/s;     57 sec\n",
      "[2021-01-30 02:42:01,724 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:01,732 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:02,646 INFO] Step 3000/ 5000; acc:  97.13; ppl:  1.14; xent: 0.13; lr: 1.00000; 5213/7510 tok/s;     58 sec\n",
      "[2021-01-30 02:42:02,646 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:02,655 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:03,593 INFO] Step 3050/ 5000; acc:  97.15; ppl:  1.13; xent: 0.13; lr: 1.00000; 5073/7309 tok/s;     59 sec\n",
      "[2021-01-30 02:42:03,594 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:03,602 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:04,554 INFO] Step 3100/ 5000; acc:  97.37; ppl:  1.14; xent: 0.13; lr: 1.00000; 5003/7208 tok/s;     60 sec\n",
      "[2021-01-30 02:42:04,555 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:04,564 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:05,567 INFO] Step 3150/ 5000; acc:  97.07; ppl:  1.14; xent: 0.13; lr: 1.00000; 4747/6839 tok/s;     61 sec\n",
      "[2021-01-30 02:42:05,567 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:05,575 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:06,595 INFO] Step 3200/ 5000; acc:  97.18; ppl:  1.13; xent: 0.12; lr: 1.00000; 4677/6738 tok/s;     62 sec\n",
      "[2021-01-30 02:42:06,595 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:06,604 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:07,592 INFO] Step 3250/ 5000; acc:  97.37; ppl:  1.12; xent: 0.11; lr: 1.00000; 4821/6946 tok/s;     63 sec\n",
      "[2021-01-30 02:42:07,592 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:07,619 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:08,597 INFO] Step 3300/ 5000; acc:  97.43; ppl:  1.13; xent: 0.12; lr: 1.00000; 4780/6886 tok/s;     64 sec\n",
      "[2021-01-30 02:42:08,598 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:08,607 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:09,646 INFO] Step 3350/ 5000; acc:  97.46; ppl:  1.11; xent: 0.10; lr: 1.00000; 4586/6607 tok/s;     65 sec\n",
      "[2021-01-30 02:42:09,646 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:09,654 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:10,609 INFO] Step 3400/ 5000; acc:  97.60; ppl:  1.12; xent: 0.11; lr: 1.00000; 4988/7187 tok/s;     66 sec\n",
      "[2021-01-30 02:42:10,610 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:10,618 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:11,587 INFO] Step 3450/ 5000; acc:  97.31; ppl:  1.15; xent: 0.14; lr: 1.00000; 4916/7083 tok/s;     67 sec\n",
      "[2021-01-30 02:42:11,587 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:11,596 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:12,558 INFO] Step 3500/ 5000; acc:  97.53; ppl:  1.12; xent: 0.11; lr: 1.00000; 4950/7132 tok/s;     68 sec\n",
      "[2021-01-30 02:42:12,558 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:12,567 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:13,591 INFO] Step 3550/ 5000; acc:  97.52; ppl:  1.13; xent: 0.12; lr: 1.00000; 4655/6706 tok/s;     69 sec\n",
      "[2021-01-30 02:42:13,591 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:13,599 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:14,552 INFO] Step 3600/ 5000; acc:  97.47; ppl:  1.13; xent: 0.12; lr: 1.00000; 5003/7208 tok/s;     70 sec\n",
      "[2021-01-30 02:42:14,552 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:14,560 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:15,529 INFO] Step 3650/ 5000; acc:  97.60; ppl:  1.11; xent: 0.11; lr: 1.00000; 4916/7082 tok/s;     71 sec\n",
      "[2021-01-30 02:42:15,530 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:15,538 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:16,505 INFO] Step 3700/ 5000; acc:  97.23; ppl:  1.14; xent: 0.13; lr: 1.00000; 4929/7101 tok/s;     72 sec\n",
      "[2021-01-30 02:42:16,505 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:16,514 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:17,502 INFO] Step 3750/ 5000; acc:  97.44; ppl:  1.12; xent: 0.11; lr: 1.00000; 4817/6940 tok/s;     73 sec\n",
      "[2021-01-30 02:42:17,503 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:17,511 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:18,504 INFO] Step 3800/ 5000; acc:  97.05; ppl:  1.17; xent: 0.15; lr: 1.00000; 4801/6917 tok/s;     74 sec\n",
      "[2021-01-30 02:42:18,504 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:18,513 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:19,455 INFO] Step 3850/ 5000; acc:  96.52; ppl:  1.19; xent: 0.17; lr: 1.00000; 5053/7280 tok/s;     75 sec\n",
      "[2021-01-30 02:42:19,455 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:19,463 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:20,424 INFO] Step 3900/ 5000; acc:  96.88; ppl:  1.17; xent: 0.16; lr: 1.00000; 4960/7146 tok/s;     76 sec\n",
      "[2021-01-30 02:42:20,424 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:20,433 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:21,410 INFO] Step 3950/ 5000; acc:  96.52; ppl:  1.19; xent: 0.17; lr: 1.00000; 4873/7020 tok/s;     77 sec\n",
      "[2021-01-30 02:42:21,411 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:21,420 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:22,420 INFO] Step 4000/ 5000; acc:  97.14; ppl:  1.16; xent: 0.15; lr: 1.00000; 4760/6857 tok/s;     78 sec\n",
      "[2021-01-30 02:42:22,421 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:22,450 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:23,425 INFO] Step 4050/ 5000; acc:  97.27; ppl:  1.16; xent: 0.15; lr: 1.00000; 4787/6896 tok/s;     79 sec\n",
      "[2021-01-30 02:42:23,425 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:23,433 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:24,405 INFO] Step 4100/ 5000; acc:  97.60; ppl:  1.15; xent: 0.14; lr: 1.00000; 4903/7063 tok/s;     80 sec\n",
      "[2021-01-30 02:42:24,405 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:24,414 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:25,358 INFO] Step 4150/ 5000; acc:  96.69; ppl:  1.17; xent: 0.16; lr: 1.00000; 5045/7269 tok/s;     81 sec\n",
      "[2021-01-30 02:42:25,358 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:25,366 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:26,303 INFO] Step 4200/ 5000; acc:  96.43; ppl:  1.19; xent: 0.18; lr: 1.00000; 5085/7326 tok/s;     82 sec\n",
      "[2021-01-30 02:42:26,303 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:26,312 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:27,288 INFO] Step 4250/ 5000; acc:  96.79; ppl:  1.17; xent: 0.16; lr: 1.00000; 4881/7032 tok/s;     83 sec\n",
      "[2021-01-30 02:42:27,288 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:27,297 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:28,263 INFO] Step 4300/ 5000; acc:  97.53; ppl:  1.13; xent: 0.12; lr: 1.00000; 4929/7101 tok/s;     84 sec\n",
      "[2021-01-30 02:42:28,264 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:28,272 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:29,235 INFO] Step 4350/ 5000; acc:  97.60; ppl:  1.13; xent: 0.12; lr: 1.00000; 4945/7124 tok/s;     85 sec\n",
      "[2021-01-30 02:42:29,236 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:29,244 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:30,213 INFO] Step 4400/ 5000; acc:  97.18; ppl:  1.15; xent: 0.14; lr: 1.00000; 4918/7086 tok/s;     86 sec\n",
      "[2021-01-30 02:42:30,213 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:30,222 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:31,165 INFO] Step 4450/ 5000; acc:  96.91; ppl:  1.17; xent: 0.16; lr: 1.00000; 5047/7271 tok/s;     87 sec\n",
      "[2021-01-30 02:42:31,166 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:31,175 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:32,159 INFO] Step 4500/ 5000; acc:  97.46; ppl:  1.14; xent: 0.13; lr: 1.00000; 4840/6973 tok/s;     88 sec\n",
      "[2021-01-30 02:42:32,159 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:32,167 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:33,132 INFO] Step 4550/ 5000; acc:  96.89; ppl:  1.17; xent: 0.16; lr: 1.00000; 4938/7115 tok/s;     89 sec\n",
      "[2021-01-30 02:42:33,132 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:33,140 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:34,108 INFO] Step 4600/ 5000; acc:  97.10; ppl:  1.18; xent: 0.16; lr: 1.00000; 4925/7095 tok/s;     90 sec\n",
      "[2021-01-30 02:42:34,108 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:34,117 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:35,049 INFO] Step 4650/ 5000; acc:  97.31; ppl:  1.14; xent: 0.13; lr: 1.00000; 5108/7358 tok/s;     90 sec\n",
      "[2021-01-30 02:42:35,049 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:35,058 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:36,022 INFO] Step 4700/ 5000; acc:  97.11; ppl:  1.16; xent: 0.15; lr: 1.00000; 4943/7122 tok/s;     91 sec\n",
      "[2021-01-30 02:42:36,022 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:36,049 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:36,982 INFO] Step 4750/ 5000; acc:  97.05; ppl:  1.16; xent: 0.15; lr: 1.00000; 5002/7207 tok/s;     92 sec\n",
      "[2021-01-30 02:42:36,983 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:36,990 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:37,923 INFO] Step 4800/ 5000; acc:  96.58; ppl:  1.20; xent: 0.18; lr: 1.00000; 5111/7363 tok/s;     93 sec\n",
      "[2021-01-30 02:42:37,923 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:37,932 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:38,887 INFO] Step 4850/ 5000; acc:  96.85; ppl:  1.16; xent: 0.15; lr: 1.00000; 4986/7184 tok/s;     94 sec\n",
      "[2021-01-30 02:42:38,887 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:38,896 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:39,878 INFO] Step 4900/ 5000; acc:  96.66; ppl:  1.18; xent: 0.17; lr: 1.00000; 4851/6989 tok/s;     95 sec\n",
      "[2021-01-30 02:42:39,878 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:39,886 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:40,828 INFO] Step 4950/ 5000; acc:  96.61; ppl:  1.21; xent: 0.19; lr: 1.00000; 5062/7292 tok/s;     96 sec\n",
      "[2021-01-30 02:42:40,828 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_4/processed.train.0.pt\n",
      "[2021-01-30 02:42:40,835 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:41,763 INFO] Step 5000/ 5000; acc:  97.18; ppl:  1.14; xent: 0.13; lr: 1.00000; 5138/7402 tok/s;     97 sec\n",
      "[2021-01-30 02:42:41,764 INFO] Saving checkpoint drive/MyDrive/EnglishToleranceBaseline/output/english_rnn_model_1000_4_step_5000.pt\n",
      "[2021-01-30 02:42:43,447 INFO]  * src vocab size = 44\n",
      "[2021-01-30 02:42:43,447 INFO]  * tgt vocab size = 46\n",
      "[2021-01-30 02:42:43,447 INFO] Building model...\n",
      "[2021-01-30 02:42:47,876 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(44, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(46, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=46, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-01-30 02:42:47,877 INFO] encoder: 254800\n",
      "[2021-01-30 02:42:47,877 INFO] decoder: 330046\n",
      "[2021-01-30 02:42:47,877 INFO] * number of parameters: 584846\n",
      "[2021-01-30 02:42:47,879 INFO] Starting training on GPU: [0]\n",
      "[2021-01-30 02:42:47,879 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-01-30 02:42:47,880 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:42:49,342 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:50,332 INFO] Step 50/ 5000; acc:  12.25; ppl: 33.76; xent: 3.52; lr: 1.00000; 1981/2843 tok/s;      2 sec\n",
      "[2021-01-30 02:42:50,332 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:42:50,342 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:51,293 INFO] Step 100/ 5000; acc:  16.75; ppl: 24.25; xent: 3.19; lr: 1.00000; 5056/7255 tok/s;      3 sec\n",
      "[2021-01-30 02:42:51,294 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:42:51,302 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:52,276 INFO] Step 150/ 5000; acc:  22.52; ppl: 17.58; xent: 2.87; lr: 1.00000; 4944/7094 tok/s;      4 sec\n",
      "[2021-01-30 02:42:52,276 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:42:52,284 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:53,277 INFO] Step 200/ 5000; acc:  24.24; ppl: 15.76; xent: 2.76; lr: 1.00000; 4857/6968 tok/s;      5 sec\n",
      "[2021-01-30 02:42:53,277 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:42:53,286 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:54,253 INFO] Step 250/ 5000; acc:  27.90; ppl: 13.22; xent: 2.58; lr: 1.00000; 4977/7141 tok/s;      6 sec\n",
      "[2021-01-30 02:42:54,254 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:42:54,262 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:55,211 INFO] Step 300/ 5000; acc:  30.38; ppl: 10.92; xent: 2.39; lr: 1.00000; 5077/7284 tok/s;      7 sec\n",
      "[2021-01-30 02:42:55,211 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:42:55,238 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:56,207 INFO] Step 350/ 5000; acc:  36.10; ppl:  9.16; xent: 2.21; lr: 1.00000; 4877/6997 tok/s;      8 sec\n",
      "[2021-01-30 02:42:56,208 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:42:56,215 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:57,177 INFO] Step 400/ 5000; acc:  39.92; ppl:  8.01; xent: 2.08; lr: 1.00000; 5010/7189 tok/s;      9 sec\n",
      "[2021-01-30 02:42:57,178 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:42:57,186 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:58,138 INFO] Step 450/ 5000; acc:  43.63; ppl:  7.17; xent: 1.97; lr: 1.00000; 5062/7263 tok/s;     10 sec\n",
      "[2021-01-30 02:42:58,138 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:42:58,146 INFO] number of examples: 1000\n",
      "[2021-01-30 02:42:59,121 INFO] Step 500/ 5000; acc:  46.18; ppl:  6.55; xent: 1.88; lr: 1.00000; 4939/7087 tok/s;     11 sec\n",
      "[2021-01-30 02:42:59,122 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:42:59,130 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:00,070 INFO] Step 550/ 5000; acc:  47.58; ppl:  6.16; xent: 1.82; lr: 1.00000; 5124/7352 tok/s;     12 sec\n",
      "[2021-01-30 02:43:00,070 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:00,078 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:01,063 INFO] Step 600/ 5000; acc:  50.24; ppl:  5.50; xent: 1.70; lr: 1.00000; 4893/7021 tok/s;     13 sec\n",
      "[2021-01-30 02:43:01,064 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:01,073 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:02,038 INFO] Step 650/ 5000; acc:  55.46; ppl:  4.69; xent: 1.55; lr: 1.00000; 4984/7151 tok/s;     14 sec\n",
      "[2021-01-30 02:43:02,039 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:02,047 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:03,030 INFO] Step 700/ 5000; acc:  60.67; ppl:  3.94; xent: 1.37; lr: 1.00000; 4899/7029 tok/s;     15 sec\n",
      "[2021-01-30 02:43:03,031 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:03,039 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:03,973 INFO] Step 750/ 5000; acc:  64.04; ppl:  3.51; xent: 1.26; lr: 1.00000; 5156/7399 tok/s;     16 sec\n",
      "[2021-01-30 02:43:03,973 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:03,981 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:04,977 INFO] Step 800/ 5000; acc:  70.20; ppl:  2.74; xent: 1.01; lr: 1.00000; 4838/6942 tok/s;     17 sec\n",
      "[2021-01-30 02:43:04,978 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:04,987 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:05,977 INFO] Step 850/ 5000; acc:  76.18; ppl:  2.28; xent: 0.82; lr: 1.00000; 4861/6976 tok/s;     18 sec\n",
      "[2021-01-30 02:43:05,977 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:05,986 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:07,016 INFO] Step 900/ 5000; acc:  81.67; ppl:  1.92; xent: 0.65; lr: 1.00000; 4676/6709 tok/s;     19 sec\n",
      "[2021-01-30 02:43:07,017 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:07,026 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:08,047 INFO] Step 950/ 5000; acc:  86.09; ppl:  1.72; xent: 0.54; lr: 1.00000; 4717/6768 tok/s;     20 sec\n",
      "[2021-01-30 02:43:08,047 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:08,057 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:09,072 INFO] Step 1000/ 5000; acc:  87.23; ppl:  1.66; xent: 0.51; lr: 1.00000; 4739/6800 tok/s;     21 sec\n",
      "[2021-01-30 02:43:09,072 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:09,081 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:10,104 INFO] Step 1050/ 5000; acc:  90.15; ppl:  1.49; xent: 0.40; lr: 1.00000; 4711/6760 tok/s;     22 sec\n",
      "[2021-01-30 02:43:10,104 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:10,138 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:11,095 INFO] Step 1100/ 5000; acc:  90.83; ppl:  1.44; xent: 0.36; lr: 1.00000; 4902/7034 tok/s;     23 sec\n",
      "[2021-01-30 02:43:11,095 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:11,104 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:12,081 INFO] Step 1150/ 5000; acc:  93.07; ppl:  1.32; xent: 0.27; lr: 1.00000; 4927/7070 tok/s;     24 sec\n",
      "[2021-01-30 02:43:12,082 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:12,090 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:13,091 INFO] Step 1200/ 5000; acc:  93.32; ppl:  1.30; xent: 0.26; lr: 1.00000; 4814/6908 tok/s;     25 sec\n",
      "[2021-01-30 02:43:13,091 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:13,100 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:14,070 INFO] Step 1250/ 5000; acc:  94.36; ppl:  1.24; xent: 0.21; lr: 1.00000; 4963/7122 tok/s;     26 sec\n",
      "[2021-01-30 02:43:14,070 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:14,079 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:15,090 INFO] Step 1300/ 5000; acc:  93.49; ppl:  1.31; xent: 0.27; lr: 1.00000; 4764/6836 tok/s;     27 sec\n",
      "[2021-01-30 02:43:15,090 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:15,099 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:16,104 INFO] Step 1350/ 5000; acc:  94.98; ppl:  1.22; xent: 0.20; lr: 1.00000; 4793/6878 tok/s;     28 sec\n",
      "[2021-01-30 02:43:16,104 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:16,113 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:17,103 INFO] Step 1400/ 5000; acc:  95.67; ppl:  1.18; xent: 0.17; lr: 1.00000; 4864/6980 tok/s;     29 sec\n",
      "[2021-01-30 02:43:17,103 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:17,112 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:18,077 INFO] Step 1450/ 5000; acc:  95.31; ppl:  1.19; xent: 0.18; lr: 1.00000; 4991/7161 tok/s;     30 sec\n",
      "[2021-01-30 02:43:18,077 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:18,086 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:19,144 INFO] Step 1500/ 5000; acc:  95.71; ppl:  1.18; xent: 0.17; lr: 1.00000; 4556/6537 tok/s;     31 sec\n",
      "[2021-01-30 02:43:19,144 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:19,154 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:20,151 INFO] Step 1550/ 5000; acc:  95.65; ppl:  1.19; xent: 0.18; lr: 1.00000; 4827/6925 tok/s;     32 sec\n",
      "[2021-01-30 02:43:20,151 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:20,159 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:21,117 INFO] Step 1600/ 5000; acc:  95.71; ppl:  1.20; xent: 0.18; lr: 1.00000; 5028/7214 tok/s;     33 sec\n",
      "[2021-01-30 02:43:21,117 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:21,126 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:22,096 INFO] Step 1650/ 5000; acc:  96.37; ppl:  1.16; xent: 0.15; lr: 1.00000; 4967/7127 tok/s;     34 sec\n",
      "[2021-01-30 02:43:22,096 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:22,105 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:23,083 INFO] Step 1700/ 5000; acc:  96.40; ppl:  1.15; xent: 0.14; lr: 1.00000; 4922/7063 tok/s;     35 sec\n",
      "[2021-01-30 02:43:23,083 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:23,092 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:24,043 INFO] Step 1750/ 5000; acc:  96.31; ppl:  1.16; xent: 0.14; lr: 1.00000; 5064/7266 tok/s;     36 sec\n",
      "[2021-01-30 02:43:24,043 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:24,051 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:25,031 INFO] Step 1800/ 5000; acc:  97.16; ppl:  1.12; xent: 0.11; lr: 1.00000; 4920/7059 tok/s;     37 sec\n",
      "[2021-01-30 02:43:25,031 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:25,059 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:26,088 INFO] Step 1850/ 5000; acc:  95.87; ppl:  1.20; xent: 0.18; lr: 1.00000; 4597/6596 tok/s;     38 sec\n",
      "[2021-01-30 02:43:26,088 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:26,097 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:27,059 INFO] Step 1900/ 5000; acc:  96.08; ppl:  1.16; xent: 0.15; lr: 1.00000; 5003/7179 tok/s;     39 sec\n",
      "[2021-01-30 02:43:27,060 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:27,068 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:28,037 INFO] Step 1950/ 5000; acc:  96.33; ppl:  1.15; xent: 0.14; lr: 1.00000; 4968/7129 tok/s;     40 sec\n",
      "[2021-01-30 02:43:28,038 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:28,046 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:29,019 INFO] Step 2000/ 5000; acc:  96.73; ppl:  1.14; xent: 0.13; lr: 1.00000; 4950/7102 tok/s;     41 sec\n",
      "[2021-01-30 02:43:29,020 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:29,029 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:30,014 INFO] Step 2050/ 5000; acc:  97.15; ppl:  1.11; xent: 0.11; lr: 1.00000; 4884/7008 tok/s;     42 sec\n",
      "[2021-01-30 02:43:30,015 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:30,023 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:31,062 INFO] Step 2100/ 5000; acc:  97.00; ppl:  1.12; xent: 0.12; lr: 1.00000; 4638/6655 tok/s;     43 sec\n",
      "[2021-01-30 02:43:31,062 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:31,071 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:32,067 INFO] Step 2150/ 5000; acc:  97.22; ppl:  1.12; xent: 0.11; lr: 1.00000; 4838/6942 tok/s;     44 sec\n",
      "[2021-01-30 02:43:32,067 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:32,075 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:33,066 INFO] Step 2200/ 5000; acc:  97.03; ppl:  1.11; xent: 0.11; lr: 1.00000; 4862/6976 tok/s;     45 sec\n",
      "[2021-01-30 02:43:33,067 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:33,075 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:34,039 INFO] Step 2250/ 5000; acc:  97.09; ppl:  1.12; xent: 0.12; lr: 1.00000; 4996/7169 tok/s;     46 sec\n",
      "[2021-01-30 02:43:34,039 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:34,049 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:35,022 INFO] Step 2300/ 5000; acc:  97.05; ppl:  1.13; xent: 0.12; lr: 1.00000; 4944/7095 tok/s;     47 sec\n",
      "[2021-01-30 02:43:35,022 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:35,031 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:36,037 INFO] Step 2350/ 5000; acc:  96.92; ppl:  1.12; xent: 0.12; lr: 1.00000; 4787/6868 tok/s;     48 sec\n",
      "[2021-01-30 02:43:36,038 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:36,046 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:37,011 INFO] Step 2400/ 5000; acc:  96.30; ppl:  1.16; xent: 0.15; lr: 1.00000; 4993/7164 tok/s;     49 sec\n",
      "[2021-01-30 02:43:37,011 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:37,026 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:37,990 INFO] Step 2450/ 5000; acc:  97.02; ppl:  1.14; xent: 0.13; lr: 1.00000; 4961/7118 tok/s;     50 sec\n",
      "[2021-01-30 02:43:37,991 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:38,000 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:38,982 INFO] Step 2500/ 5000; acc:  96.86; ppl:  1.12; xent: 0.11; lr: 1.00000; 4904/7036 tok/s;     51 sec\n",
      "[2021-01-30 02:43:38,982 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:39,010 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:39,971 INFO] Step 2550/ 5000; acc:  97.23; ppl:  1.13; xent: 0.13; lr: 1.00000; 4910/7045 tok/s;     52 sec\n",
      "[2021-01-30 02:43:39,972 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:39,980 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:41,032 INFO] Step 2600/ 5000; acc:  96.97; ppl:  1.14; xent: 0.13; lr: 1.00000; 4584/6577 tok/s;     53 sec\n",
      "[2021-01-30 02:43:41,032 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:41,041 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:42,009 INFO] Step 2650/ 5000; acc:  97.46; ppl:  1.12; xent: 0.11; lr: 1.00000; 4970/7132 tok/s;     54 sec\n",
      "[2021-01-30 02:43:42,010 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:42,019 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:43,003 INFO] Step 2700/ 5000; acc:  97.75; ppl:  1.10; xent: 0.09; lr: 1.00000; 4890/7016 tok/s;     55 sec\n",
      "[2021-01-30 02:43:43,003 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:43,012 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:43,964 INFO] Step 2750/ 5000; acc:  97.65; ppl:  1.10; xent: 0.10; lr: 1.00000; 5058/7258 tok/s;     56 sec\n",
      "[2021-01-30 02:43:43,964 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:43,973 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:44,963 INFO] Step 2800/ 5000; acc:  97.49; ppl:  1.11; xent: 0.10; lr: 1.00000; 4867/6984 tok/s;     57 sec\n",
      "[2021-01-30 02:43:44,963 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:44,971 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:45,971 INFO] Step 2850/ 5000; acc:  96.83; ppl:  1.16; xent: 0.15; lr: 1.00000; 4820/6916 tok/s;     58 sec\n",
      "[2021-01-30 02:43:45,971 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:45,980 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:46,929 INFO] Step 2900/ 5000; acc:  97.30; ppl:  1.12; xent: 0.11; lr: 1.00000; 5070/7275 tok/s;     59 sec\n",
      "[2021-01-30 02:43:46,930 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:46,939 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:47,891 INFO] Step 2950/ 5000; acc:  96.86; ppl:  1.15; xent: 0.14; lr: 1.00000; 5053/7250 tok/s;     60 sec\n",
      "[2021-01-30 02:43:47,892 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:47,900 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:48,872 INFO] Step 3000/ 5000; acc:  97.43; ppl:  1.12; xent: 0.11; lr: 1.00000; 4957/7113 tok/s;     61 sec\n",
      "[2021-01-30 02:43:48,872 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:48,881 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:49,846 INFO] Step 3050/ 5000; acc:  97.52; ppl:  1.11; xent: 0.11; lr: 1.00000; 4988/7157 tok/s;     62 sec\n",
      "[2021-01-30 02:43:49,846 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:49,856 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:50,849 INFO] Step 3100/ 5000; acc:  97.75; ppl:  1.10; xent: 0.10; lr: 1.00000; 4846/6953 tok/s;     63 sec\n",
      "[2021-01-30 02:43:50,849 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:50,858 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:51,826 INFO] Step 3150/ 5000; acc:  97.58; ppl:  1.11; xent: 0.10; lr: 1.00000; 4977/7141 tok/s;     64 sec\n",
      "[2021-01-30 02:43:51,826 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:51,834 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:52,786 INFO] Step 3200/ 5000; acc:  97.38; ppl:  1.12; xent: 0.11; lr: 1.00000; 5058/7257 tok/s;     65 sec\n",
      "[2021-01-30 02:43:52,787 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:52,795 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:53,792 INFO] Step 3250/ 5000; acc:  97.22; ppl:  1.13; xent: 0.12; lr: 1.00000; 4831/6931 tok/s;     66 sec\n",
      "[2021-01-30 02:43:53,793 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:53,820 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:54,804 INFO] Step 3300/ 5000; acc:  96.97; ppl:  1.15; xent: 0.14; lr: 1.00000; 4804/6893 tok/s;     67 sec\n",
      "[2021-01-30 02:43:54,804 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:54,813 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:55,821 INFO] Step 3350/ 5000; acc:  97.82; ppl:  1.11; xent: 0.10; lr: 1.00000; 4781/6860 tok/s;     68 sec\n",
      "[2021-01-30 02:43:55,821 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:55,829 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:56,800 INFO] Step 3400/ 5000; acc:  97.25; ppl:  1.14; xent: 0.13; lr: 1.00000; 4962/7120 tok/s;     69 sec\n",
      "[2021-01-30 02:43:56,800 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:56,809 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:57,755 INFO] Step 3450/ 5000; acc:  97.27; ppl:  1.13; xent: 0.12; lr: 1.00000; 5086/7297 tok/s;     70 sec\n",
      "[2021-01-30 02:43:57,756 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:57,768 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:58,713 INFO] Step 3500/ 5000; acc:  97.65; ppl:  1.10; xent: 0.09; lr: 1.00000; 5075/7283 tok/s;     71 sec\n",
      "[2021-01-30 02:43:58,713 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:58,722 INFO] number of examples: 1000\n",
      "[2021-01-30 02:43:59,703 INFO] Step 3550/ 5000; acc:  97.45; ppl:  1.11; xent: 0.10; lr: 1.00000; 4909/7044 tok/s;     72 sec\n",
      "[2021-01-30 02:43:59,703 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:43:59,711 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:00,676 INFO] Step 3600/ 5000; acc:  97.39; ppl:  1.13; xent: 0.12; lr: 1.00000; 4993/7164 tok/s;     73 sec\n",
      "[2021-01-30 02:44:00,677 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:00,685 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:01,695 INFO] Step 3650/ 5000; acc:  98.12; ppl:  1.08; xent: 0.08; lr: 1.00000; 4770/6844 tok/s;     74 sec\n",
      "[2021-01-30 02:44:01,696 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:01,705 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:02,726 INFO] Step 3700/ 5000; acc:  97.71; ppl:  1.11; xent: 0.10; lr: 1.00000; 4716/6766 tok/s;     75 sec\n",
      "[2021-01-30 02:44:02,726 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:02,735 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:03,749 INFO] Step 3750/ 5000; acc:  98.12; ppl:  1.09; xent: 0.09; lr: 1.00000; 4752/6818 tok/s;     76 sec\n",
      "[2021-01-30 02:44:03,749 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:03,757 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:04,715 INFO] Step 3800/ 5000; acc:  97.33; ppl:  1.14; xent: 0.13; lr: 1.00000; 5030/7218 tok/s;     77 sec\n",
      "[2021-01-30 02:44:04,715 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:04,724 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:05,677 INFO] Step 3850/ 5000; acc:  97.22; ppl:  1.12; xent: 0.11; lr: 1.00000; 5050/7247 tok/s;     78 sec\n",
      "[2021-01-30 02:44:05,677 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:05,686 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:06,641 INFO] Step 3900/ 5000; acc:  97.48; ppl:  1.12; xent: 0.11; lr: 1.00000; 5044/7238 tok/s;     79 sec\n",
      "[2021-01-30 02:44:06,641 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:06,649 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:07,596 INFO] Step 3950/ 5000; acc:  97.50; ppl:  1.13; xent: 0.12; lr: 1.00000; 5085/7297 tok/s;     80 sec\n",
      "[2021-01-30 02:44:07,597 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:07,604 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:08,559 INFO] Step 4000/ 5000; acc:  97.39; ppl:  1.13; xent: 0.13; lr: 1.00000; 5048/7243 tok/s;     81 sec\n",
      "[2021-01-30 02:44:08,559 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:08,587 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:09,585 INFO] Step 4050/ 5000; acc:  96.94; ppl:  1.15; xent: 0.14; lr: 1.00000; 4738/6798 tok/s;     82 sec\n",
      "[2021-01-30 02:44:09,585 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:09,595 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:10,613 INFO] Step 4100/ 5000; acc:  97.56; ppl:  1.12; xent: 0.12; lr: 1.00000; 4725/6780 tok/s;     83 sec\n",
      "[2021-01-30 02:44:10,614 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:10,622 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:11,587 INFO] Step 4150/ 5000; acc:  96.94; ppl:  1.16; xent: 0.15; lr: 1.00000; 4994/7166 tok/s;     84 sec\n",
      "[2021-01-30 02:44:11,587 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:11,595 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:12,546 INFO] Step 4200/ 5000; acc:  97.58; ppl:  1.12; xent: 0.11; lr: 1.00000; 5064/7266 tok/s;     85 sec\n",
      "[2021-01-30 02:44:12,546 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:12,555 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:13,541 INFO] Step 4250/ 5000; acc:  97.20; ppl:  1.13; xent: 0.12; lr: 1.00000; 4886/7011 tok/s;     86 sec\n",
      "[2021-01-30 02:44:13,541 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:13,550 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:14,502 INFO] Step 4300/ 5000; acc:  97.42; ppl:  1.13; xent: 0.12; lr: 1.00000; 5056/7255 tok/s;     87 sec\n",
      "[2021-01-30 02:44:14,502 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:14,511 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:15,466 INFO] Step 4350/ 5000; acc:  97.43; ppl:  1.12; xent: 0.12; lr: 1.00000; 5041/7233 tok/s;     88 sec\n",
      "[2021-01-30 02:44:15,466 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:15,475 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:16,417 INFO] Step 4400/ 5000; acc:  97.50; ppl:  1.13; xent: 0.12; lr: 1.00000; 5112/7335 tok/s;     89 sec\n",
      "[2021-01-30 02:44:16,417 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:16,426 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:17,442 INFO] Step 4450/ 5000; acc:  97.52; ppl:  1.13; xent: 0.12; lr: 1.00000; 4740/6801 tok/s;     90 sec\n",
      "[2021-01-30 02:44:17,442 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:17,452 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:18,422 INFO] Step 4500/ 5000; acc:  97.27; ppl:  1.13; xent: 0.12; lr: 1.00000; 4961/7119 tok/s;     91 sec\n",
      "[2021-01-30 02:44:18,422 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:18,430 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:19,407 INFO] Step 4550/ 5000; acc:  97.69; ppl:  1.12; xent: 0.11; lr: 1.00000; 4933/7078 tok/s;     92 sec\n",
      "[2021-01-30 02:44:19,407 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:19,415 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:20,386 INFO] Step 4600/ 5000; acc:  97.46; ppl:  1.14; xent: 0.13; lr: 1.00000; 4963/7122 tok/s;     93 sec\n",
      "[2021-01-30 02:44:20,386 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:20,395 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:21,343 INFO] Step 4650/ 5000; acc:  97.09; ppl:  1.14; xent: 0.13; lr: 1.00000; 5077/7285 tok/s;     93 sec\n",
      "[2021-01-30 02:44:21,344 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:21,352 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:22,300 INFO] Step 4700/ 5000; acc:  97.62; ppl:  1.11; xent: 0.11; lr: 1.00000; 5080/7289 tok/s;     94 sec\n",
      "[2021-01-30 02:44:22,300 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:22,329 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:23,286 INFO] Step 4750/ 5000; acc:  97.83; ppl:  1.10; xent: 0.10; lr: 1.00000; 4930/7074 tok/s;     95 sec\n",
      "[2021-01-30 02:44:23,286 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:23,294 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:24,299 INFO] Step 4800/ 5000; acc:  97.85; ppl:  1.11; xent: 0.10; lr: 1.00000; 4798/6884 tok/s;     96 sec\n",
      "[2021-01-30 02:44:24,299 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:24,308 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:25,272 INFO] Step 4850/ 5000; acc:  97.98; ppl:  1.09; xent: 0.09; lr: 1.00000; 4996/7168 tok/s;     97 sec\n",
      "[2021-01-30 02:44:25,272 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:25,281 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:26,243 INFO] Step 4900/ 5000; acc:  97.68; ppl:  1.11; xent: 0.10; lr: 1.00000; 5003/7178 tok/s;     98 sec\n",
      "[2021-01-30 02:44:26,243 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:26,252 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:27,230 INFO] Step 4950/ 5000; acc:  97.65; ppl:  1.12; xent: 0.11; lr: 1.00000; 4925/7066 tok/s;     99 sec\n",
      "[2021-01-30 02:44:27,230 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_5/processed.train.0.pt\n",
      "[2021-01-30 02:44:27,238 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:28,185 INFO] Step 5000/ 5000; acc:  97.86; ppl:  1.11; xent: 0.10; lr: 1.00000; 5090/7303 tok/s;    100 sec\n",
      "[2021-01-30 02:44:28,187 INFO] Saving checkpoint drive/MyDrive/EnglishToleranceBaseline/output/english_rnn_model_1000_5_step_5000.pt\n",
      "[2021-01-30 02:44:29,953 INFO]  * src vocab size = 44\n",
      "[2021-01-30 02:44:29,954 INFO]  * tgt vocab size = 46\n",
      "[2021-01-30 02:44:29,954 INFO] Building model...\n",
      "[2021-01-30 02:44:34,374 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(44, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(46, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=46, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-01-30 02:44:34,375 INFO] encoder: 254800\n",
      "[2021-01-30 02:44:34,375 INFO] decoder: 330046\n",
      "[2021-01-30 02:44:34,375 INFO] * number of parameters: 584846\n",
      "[2021-01-30 02:44:34,377 INFO] Starting training on GPU: [0]\n",
      "[2021-01-30 02:44:34,378 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-01-30 02:44:34,378 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:35,072 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:36,075 INFO] Step 50/ 5000; acc:  11.68; ppl: 35.68; xent: 3.57; lr: 1.00000; 2860/4111 tok/s;      2 sec\n",
      "[2021-01-30 02:44:36,075 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:36,084 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:37,054 INFO] Step 100/ 5000; acc:  20.48; ppl: 19.41; xent: 2.97; lr: 1.00000; 4957/7127 tok/s;      3 sec\n",
      "[2021-01-30 02:44:37,054 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:37,063 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:38,022 INFO] Step 150/ 5000; acc:  23.81; ppl: 16.01; xent: 2.77; lr: 1.00000; 5014/7209 tok/s;      4 sec\n",
      "[2021-01-30 02:44:38,022 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:38,031 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:39,049 INFO] Step 200/ 5000; acc:  27.12; ppl: 12.96; xent: 2.56; lr: 1.00000; 4725/6792 tok/s;      5 sec\n",
      "[2021-01-30 02:44:39,050 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:39,059 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:40,103 INFO] Step 250/ 5000; acc:  31.15; ppl: 10.98; xent: 2.40; lr: 1.00000; 4607/6623 tok/s;      6 sec\n",
      "[2021-01-30 02:44:40,103 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:40,112 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:41,124 INFO] Step 300/ 5000; acc:  36.02; ppl:  9.23; xent: 2.22; lr: 1.00000; 4752/6832 tok/s;      7 sec\n",
      "[2021-01-30 02:44:41,125 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:41,153 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:42,133 INFO] Step 350/ 5000; acc:  38.66; ppl:  8.48; xent: 2.14; lr: 1.00000; 4811/6917 tok/s;      8 sec\n",
      "[2021-01-30 02:44:42,134 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:42,142 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:43,108 INFO] Step 400/ 5000; acc:  41.94; ppl:  7.33; xent: 1.99; lr: 1.00000; 4979/7159 tok/s;      9 sec\n",
      "[2021-01-30 02:44:43,108 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:43,118 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:44,130 INFO] Step 450/ 5000; acc:  44.60; ppl:  6.66; xent: 1.90; lr: 1.00000; 4751/6831 tok/s;     10 sec\n",
      "[2021-01-30 02:44:44,130 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:44,139 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:45,105 INFO] Step 500/ 5000; acc:  46.54; ppl:  6.12; xent: 1.81; lr: 1.00000; 4977/7156 tok/s;     11 sec\n",
      "[2021-01-30 02:44:45,105 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:45,114 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:46,078 INFO] Step 550/ 5000; acc:  49.63; ppl:  5.38; xent: 1.68; lr: 1.00000; 4985/7167 tok/s;     12 sec\n",
      "[2021-01-30 02:44:46,079 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:46,087 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:47,068 INFO] Step 600/ 5000; acc:  52.89; ppl:  4.83; xent: 1.58; lr: 1.00000; 4904/7051 tok/s;     13 sec\n",
      "[2021-01-30 02:44:47,068 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:47,077 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:48,052 INFO] Step 650/ 5000; acc:  57.16; ppl:  4.25; xent: 1.45; lr: 1.00000; 4933/7092 tok/s;     14 sec\n",
      "[2021-01-30 02:44:48,052 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:48,061 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:49,005 INFO] Step 700/ 5000; acc:  60.58; ppl:  3.68; xent: 1.30; lr: 1.00000; 5094/7324 tok/s;     15 sec\n",
      "[2021-01-30 02:44:49,005 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:49,014 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:49,982 INFO] Step 750/ 5000; acc:  64.84; ppl:  3.23; xent: 1.17; lr: 1.00000; 4970/7145 tok/s;     16 sec\n",
      "[2021-01-30 02:44:49,982 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:49,990 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:50,980 INFO] Step 800/ 5000; acc:  70.03; ppl:  2.75; xent: 1.01; lr: 1.00000; 4862/6990 tok/s;     17 sec\n",
      "[2021-01-30 02:44:50,980 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:50,989 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:51,962 INFO] Step 850/ 5000; acc:  75.02; ppl:  2.32; xent: 0.84; lr: 1.00000; 4942/7104 tok/s;     18 sec\n",
      "[2021-01-30 02:44:51,962 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:51,971 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:52,937 INFO] Step 900/ 5000; acc:  79.40; ppl:  2.04; xent: 0.71; lr: 1.00000; 4980/7159 tok/s;     19 sec\n",
      "[2021-01-30 02:44:52,937 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:52,946 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:53,904 INFO] Step 950/ 5000; acc:  83.39; ppl:  1.82; xent: 0.60; lr: 1.00000; 5020/7216 tok/s;     20 sec\n",
      "[2021-01-30 02:44:53,904 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:53,913 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:54,874 INFO] Step 1000/ 5000; acc:  85.85; ppl:  1.66; xent: 0.51; lr: 1.00000; 5005/7196 tok/s;     20 sec\n",
      "[2021-01-30 02:44:54,874 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:54,882 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:55,831 INFO] Step 1050/ 5000; acc:  89.02; ppl:  1.50; xent: 0.41; lr: 1.00000; 5071/7290 tok/s;     21 sec\n",
      "[2021-01-30 02:44:55,831 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:55,859 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:56,869 INFO] Step 1100/ 5000; acc:  90.34; ppl:  1.42; xent: 0.35; lr: 1.00000; 4678/6725 tok/s;     22 sec\n",
      "[2021-01-30 02:44:56,869 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:56,878 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:57,875 INFO] Step 1150/ 5000; acc:  91.24; ppl:  1.41; xent: 0.34; lr: 1.00000; 4822/6932 tok/s;     23 sec\n",
      "[2021-01-30 02:44:57,875 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:57,884 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:58,878 INFO] Step 1200/ 5000; acc:  91.31; ppl:  1.43; xent: 0.35; lr: 1.00000; 4842/6961 tok/s;     24 sec\n",
      "[2021-01-30 02:44:58,878 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:58,887 INFO] number of examples: 1000\n",
      "[2021-01-30 02:44:59,868 INFO] Step 1250/ 5000; acc:  93.77; ppl:  1.27; xent: 0.24; lr: 1.00000; 4903/7048 tok/s;     25 sec\n",
      "[2021-01-30 02:44:59,868 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:44:59,876 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:00,863 INFO] Step 1300/ 5000; acc:  94.58; ppl:  1.25; xent: 0.22; lr: 1.00000; 4877/7012 tok/s;     26 sec\n",
      "[2021-01-30 02:45:00,863 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:00,872 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:01,840 INFO] Step 1350/ 5000; acc:  94.55; ppl:  1.23; xent: 0.21; lr: 1.00000; 4969/7143 tok/s;     27 sec\n",
      "[2021-01-30 02:45:01,840 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:01,849 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:02,814 INFO] Step 1400/ 5000; acc:  95.67; ppl:  1.18; xent: 0.16; lr: 1.00000; 4981/7161 tok/s;     28 sec\n",
      "[2021-01-30 02:45:02,815 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:02,824 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:03,784 INFO] Step 1450/ 5000; acc:  95.80; ppl:  1.18; xent: 0.16; lr: 1.00000; 5005/7195 tok/s;     29 sec\n",
      "[2021-01-30 02:45:03,785 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:03,794 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:04,816 INFO] Step 1500/ 5000; acc:  95.18; ppl:  1.20; xent: 0.19; lr: 1.00000; 4707/6767 tok/s;     30 sec\n",
      "[2021-01-30 02:45:04,816 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:04,824 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:05,809 INFO] Step 1550/ 5000; acc:  94.95; ppl:  1.21; xent: 0.19; lr: 1.00000; 4886/7024 tok/s;     31 sec\n",
      "[2021-01-30 02:45:05,809 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:05,817 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:06,796 INFO] Step 1600/ 5000; acc:  95.36; ppl:  1.20; xent: 0.18; lr: 1.00000; 4916/7068 tok/s;     32 sec\n",
      "[2021-01-30 02:45:06,797 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:06,806 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:07,793 INFO] Step 1650/ 5000; acc:  95.96; ppl:  1.16; xent: 0.15; lr: 1.00000; 4872/7005 tok/s;     33 sec\n",
      "[2021-01-30 02:45:07,793 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:07,802 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:08,784 INFO] Step 1700/ 5000; acc:  95.60; ppl:  1.19; xent: 0.17; lr: 1.00000; 4898/7041 tok/s;     34 sec\n",
      "[2021-01-30 02:45:08,784 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:08,793 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:09,769 INFO] Step 1750/ 5000; acc:  95.89; ppl:  1.18; xent: 0.17; lr: 1.00000; 4928/7085 tok/s;     35 sec\n",
      "[2021-01-30 02:45:09,769 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:09,781 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:10,747 INFO] Step 1800/ 5000; acc:  95.87; ppl:  1.17; xent: 0.16; lr: 1.00000; 4964/7136 tok/s;     36 sec\n",
      "[2021-01-30 02:45:10,747 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:10,775 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:11,812 INFO] Step 1850/ 5000; acc:  96.45; ppl:  1.14; xent: 0.13; lr: 1.00000; 4555/6549 tok/s;     37 sec\n",
      "[2021-01-30 02:45:11,812 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:11,821 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:12,795 INFO] Step 1900/ 5000; acc:  96.70; ppl:  1.14; xent: 0.13; lr: 1.00000; 4939/7100 tok/s;     38 sec\n",
      "[2021-01-30 02:45:12,795 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:12,804 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:13,794 INFO] Step 1950/ 5000; acc:  96.42; ppl:  1.15; xent: 0.14; lr: 1.00000; 4857/6983 tok/s;     39 sec\n",
      "[2021-01-30 02:45:13,795 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:13,803 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:14,779 INFO] Step 2000/ 5000; acc:  96.89; ppl:  1.13; xent: 0.13; lr: 1.00000; 4930/7087 tok/s;     40 sec\n",
      "[2021-01-30 02:45:14,779 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:14,788 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:15,747 INFO] Step 2050/ 5000; acc:  95.83; ppl:  1.19; xent: 0.17; lr: 1.00000; 5015/7210 tok/s;     41 sec\n",
      "[2021-01-30 02:45:15,747 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:15,756 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:16,736 INFO] Step 2100/ 5000; acc:  96.24; ppl:  1.16; xent: 0.15; lr: 1.00000; 4905/7052 tok/s;     42 sec\n",
      "[2021-01-30 02:45:16,737 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:16,746 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:17,711 INFO] Step 2150/ 5000; acc:  96.62; ppl:  1.15; xent: 0.14; lr: 1.00000; 4981/7161 tok/s;     43 sec\n",
      "[2021-01-30 02:45:17,711 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:17,720 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:18,698 INFO] Step 2200/ 5000; acc:  96.83; ppl:  1.14; xent: 0.13; lr: 1.00000; 4917/7069 tok/s;     44 sec\n",
      "[2021-01-30 02:45:18,698 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:18,708 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:19,701 INFO] Step 2250/ 5000; acc:  96.65; ppl:  1.15; xent: 0.14; lr: 1.00000; 4840/6958 tok/s;     45 sec\n",
      "[2021-01-30 02:45:19,701 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:19,711 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:20,683 INFO] Step 2300/ 5000; acc:  97.06; ppl:  1.12; xent: 0.12; lr: 1.00000; 4944/7108 tok/s;     46 sec\n",
      "[2021-01-30 02:45:20,683 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:20,692 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:21,694 INFO] Step 2350/ 5000; acc:  96.75; ppl:  1.14; xent: 0.13; lr: 1.00000; 4799/6899 tok/s;     47 sec\n",
      "[2021-01-30 02:45:21,695 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:21,703 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:22,669 INFO] Step 2400/ 5000; acc:  96.89; ppl:  1.14; xent: 0.13; lr: 1.00000; 4978/7157 tok/s;     48 sec\n",
      "[2021-01-30 02:45:22,670 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:22,678 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:23,658 INFO] Step 2450/ 5000; acc:  96.52; ppl:  1.16; xent: 0.15; lr: 1.00000; 4913/7063 tok/s;     49 sec\n",
      "[2021-01-30 02:45:23,658 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:23,667 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:24,689 INFO] Step 2500/ 5000; acc:  96.36; ppl:  1.16; xent: 0.14; lr: 1.00000; 4704/6763 tok/s;     50 sec\n",
      "[2021-01-30 02:45:24,690 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:24,718 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:25,721 INFO] Step 2550/ 5000; acc:  96.14; ppl:  1.18; xent: 0.17; lr: 1.00000; 4704/6762 tok/s;     51 sec\n",
      "[2021-01-30 02:45:25,721 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:25,730 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:26,701 INFO] Step 2600/ 5000; acc:  96.12; ppl:  1.19; xent: 0.17; lr: 1.00000; 4956/7126 tok/s;     52 sec\n",
      "[2021-01-30 02:45:26,701 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:26,710 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:27,662 INFO] Step 2650/ 5000; acc:  96.86; ppl:  1.15; xent: 0.14; lr: 1.00000; 5049/7259 tok/s;     53 sec\n",
      "[2021-01-30 02:45:27,662 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:27,671 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:28,636 INFO] Step 2700/ 5000; acc:  97.08; ppl:  1.14; xent: 0.13; lr: 1.00000; 4982/7162 tok/s;     54 sec\n",
      "[2021-01-30 02:45:28,636 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:28,645 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:29,623 INFO] Step 2750/ 5000; acc:  96.78; ppl:  1.16; xent: 0.14; lr: 1.00000; 4919/7072 tok/s;     55 sec\n",
      "[2021-01-30 02:45:29,623 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:29,632 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:30,602 INFO] Step 2800/ 5000; acc:  97.00; ppl:  1.14; xent: 0.13; lr: 1.00000; 4956/7125 tok/s;     56 sec\n",
      "[2021-01-30 02:45:30,603 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:30,612 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:31,595 INFO] Step 2850/ 5000; acc:  97.23; ppl:  1.13; xent: 0.12; lr: 1.00000; 4889/7029 tok/s;     57 sec\n",
      "[2021-01-30 02:45:31,595 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:31,605 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:32,623 INFO] Step 2900/ 5000; acc:  97.19; ppl:  1.12; xent: 0.12; lr: 1.00000; 4725/6793 tok/s;     58 sec\n",
      "[2021-01-30 02:45:32,623 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:32,632 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:33,599 INFO] Step 2950/ 5000; acc:  97.05; ppl:  1.14; xent: 0.13; lr: 1.00000; 4969/7144 tok/s;     59 sec\n",
      "[2021-01-30 02:45:33,600 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:33,608 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:34,623 INFO] Step 3000/ 5000; acc:  96.53; ppl:  1.16; xent: 0.15; lr: 1.00000; 4742/6817 tok/s;     60 sec\n",
      "[2021-01-30 02:45:34,623 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:34,632 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:35,621 INFO] Step 3050/ 5000; acc:  96.78; ppl:  1.15; xent: 0.14; lr: 1.00000; 4864/6992 tok/s;     61 sec\n",
      "[2021-01-30 02:45:35,621 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:35,630 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:36,634 INFO] Step 3100/ 5000; acc:  97.05; ppl:  1.14; xent: 0.13; lr: 1.00000; 4794/6892 tok/s;     62 sec\n",
      "[2021-01-30 02:45:36,634 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:36,643 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:37,641 INFO] Step 3150/ 5000; acc:  97.31; ppl:  1.12; xent: 0.12; lr: 1.00000; 4816/6923 tok/s;     63 sec\n",
      "[2021-01-30 02:45:37,642 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:37,650 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:38,677 INFO] Step 3200/ 5000; acc:  96.43; ppl:  1.18; xent: 0.16; lr: 1.00000; 4689/6741 tok/s;     64 sec\n",
      "[2021-01-30 02:45:38,677 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:38,687 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:39,690 INFO] Step 3250/ 5000; acc:  97.13; ppl:  1.13; xent: 0.12; lr: 1.00000; 4789/6886 tok/s;     65 sec\n",
      "[2021-01-30 02:45:39,690 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:39,718 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:40,719 INFO] Step 3300/ 5000; acc:  97.19; ppl:  1.14; xent: 0.13; lr: 1.00000; 4718/6783 tok/s;     66 sec\n",
      "[2021-01-30 02:45:40,719 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:40,728 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:41,724 INFO] Step 3350/ 5000; acc:  97.12; ppl:  1.14; xent: 0.13; lr: 1.00000; 4830/6944 tok/s;     67 sec\n",
      "[2021-01-30 02:45:41,724 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:41,732 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:42,681 INFO] Step 3400/ 5000; acc:  97.29; ppl:  1.13; xent: 0.12; lr: 1.00000; 5072/7291 tok/s;     68 sec\n",
      "[2021-01-30 02:45:42,681 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:42,689 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:43,658 INFO] Step 3450/ 5000; acc:  96.90; ppl:  1.17; xent: 0.15; lr: 1.00000; 4968/7142 tok/s;     69 sec\n",
      "[2021-01-30 02:45:43,658 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:43,667 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:44,635 INFO] Step 3500/ 5000; acc:  97.31; ppl:  1.13; xent: 0.13; lr: 1.00000; 4968/7143 tok/s;     70 sec\n",
      "[2021-01-30 02:45:44,635 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:44,644 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:45,654 INFO] Step 3550/ 5000; acc:  97.22; ppl:  1.15; xent: 0.14; lr: 1.00000; 4765/6850 tok/s;     71 sec\n",
      "[2021-01-30 02:45:45,654 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:45,662 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:46,612 INFO] Step 3600/ 5000; acc:  97.33; ppl:  1.12; xent: 0.11; lr: 1.00000; 5067/7284 tok/s;     72 sec\n",
      "[2021-01-30 02:45:46,612 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:46,621 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:47,605 INFO] Step 3650/ 5000; acc:  97.08; ppl:  1.14; xent: 0.13; lr: 1.00000; 4885/7022 tok/s;     73 sec\n",
      "[2021-01-30 02:45:47,606 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:47,614 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:48,557 INFO] Step 3700/ 5000; acc:  97.03; ppl:  1.14; xent: 0.13; lr: 1.00000; 5102/7335 tok/s;     74 sec\n",
      "[2021-01-30 02:45:48,557 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:48,565 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:49,517 INFO] Step 3750/ 5000; acc:  97.58; ppl:  1.12; xent: 0.12; lr: 1.00000; 5055/7268 tok/s;     75 sec\n",
      "[2021-01-30 02:45:49,517 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:49,525 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:50,498 INFO] Step 3800/ 5000; acc:  97.21; ppl:  1.14; xent: 0.13; lr: 1.00000; 4949/7115 tok/s;     76 sec\n",
      "[2021-01-30 02:45:50,498 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:50,506 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:51,480 INFO] Step 3850/ 5000; acc:  96.90; ppl:  1.17; xent: 0.16; lr: 1.00000; 4940/7102 tok/s;     77 sec\n",
      "[2021-01-30 02:45:51,480 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:51,490 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:52,510 INFO] Step 3900/ 5000; acc:  96.50; ppl:  1.20; xent: 0.19; lr: 1.00000; 4711/6774 tok/s;     78 sec\n",
      "[2021-01-30 02:45:52,511 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:52,520 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:53,519 INFO] Step 3950/ 5000; acc:  96.85; ppl:  1.17; xent: 0.15; lr: 1.00000; 4812/6919 tok/s;     79 sec\n",
      "[2021-01-30 02:45:53,519 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:53,528 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:54,476 INFO] Step 4000/ 5000; acc:  97.33; ppl:  1.14; xent: 0.13; lr: 1.00000; 5074/7294 tok/s;     80 sec\n",
      "[2021-01-30 02:45:54,476 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:54,503 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:55,455 INFO] Step 4050/ 5000; acc:  97.12; ppl:  1.14; xent: 0.13; lr: 1.00000; 4956/7125 tok/s;     81 sec\n",
      "[2021-01-30 02:45:55,455 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:55,464 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:56,453 INFO] Step 4100/ 5000; acc:  96.93; ppl:  1.15; xent: 0.14; lr: 1.00000; 4864/6993 tok/s;     82 sec\n",
      "[2021-01-30 02:45:56,453 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:56,462 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:57,446 INFO] Step 4150/ 5000; acc:  97.48; ppl:  1.14; xent: 0.13; lr: 1.00000; 4888/7028 tok/s;     83 sec\n",
      "[2021-01-30 02:45:57,446 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:57,454 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:58,409 INFO] Step 4200/ 5000; acc:  96.92; ppl:  1.16; xent: 0.15; lr: 1.00000; 5042/7249 tok/s;     84 sec\n",
      "[2021-01-30 02:45:58,409 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:58,418 INFO] number of examples: 1000\n",
      "[2021-01-30 02:45:59,381 INFO] Step 4250/ 5000; acc:  96.88; ppl:  1.17; xent: 0.16; lr: 1.00000; 4995/7181 tok/s;     85 sec\n",
      "[2021-01-30 02:45:59,381 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:45:59,389 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:00,357 INFO] Step 4300/ 5000; acc:  96.83; ppl:  1.17; xent: 0.16; lr: 1.00000; 4969/7144 tok/s;     86 sec\n",
      "[2021-01-30 02:46:00,358 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:46:00,366 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:01,339 INFO] Step 4350/ 5000; acc:  97.39; ppl:  1.15; xent: 0.14; lr: 1.00000; 4943/7106 tok/s;     87 sec\n",
      "[2021-01-30 02:46:01,340 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:46:01,348 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:02,347 INFO] Step 4400/ 5000; acc:  96.89; ppl:  1.18; xent: 0.17; lr: 1.00000; 4817/6926 tok/s;     88 sec\n",
      "[2021-01-30 02:46:02,347 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:46:02,356 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:03,307 INFO] Step 4450/ 5000; acc:  96.59; ppl:  1.19; xent: 0.17; lr: 1.00000; 5056/7270 tok/s;     89 sec\n",
      "[2021-01-30 02:46:03,307 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:46:03,316 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:04,293 INFO] Step 4500/ 5000; acc:  96.98; ppl:  1.17; xent: 0.16; lr: 1.00000; 4921/7074 tok/s;     90 sec\n",
      "[2021-01-30 02:46:04,294 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:46:04,303 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:05,310 INFO] Step 4550/ 5000; acc:  96.17; ppl:  1.21; xent: 0.19; lr: 1.00000; 4773/6862 tok/s;     91 sec\n",
      "[2021-01-30 02:46:05,310 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:46:05,319 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:06,283 INFO] Step 4600/ 5000; acc:  96.67; ppl:  1.19; xent: 0.17; lr: 1.00000; 4992/7177 tok/s;     92 sec\n",
      "[2021-01-30 02:46:06,283 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:46:06,291 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:07,264 INFO] Step 4650/ 5000; acc:  96.92; ppl:  1.16; xent: 0.15; lr: 1.00000; 4944/7108 tok/s;     93 sec\n",
      "[2021-01-30 02:46:07,265 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:46:07,274 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:08,274 INFO] Step 4700/ 5000; acc:  97.19; ppl:  1.15; xent: 0.14; lr: 1.00000; 4807/6911 tok/s;     94 sec\n",
      "[2021-01-30 02:46:08,274 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:46:08,303 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:09,317 INFO] Step 4750/ 5000; acc:  97.48; ppl:  1.13; xent: 0.12; lr: 1.00000; 4654/6690 tok/s;     95 sec\n",
      "[2021-01-30 02:46:09,317 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:46:09,327 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:10,286 INFO] Step 4800/ 5000; acc:  97.22; ppl:  1.14; xent: 0.13; lr: 1.00000; 5009/7201 tok/s;     96 sec\n",
      "[2021-01-30 02:46:10,286 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:46:10,295 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:11,268 INFO] Step 4850/ 5000; acc:  97.08; ppl:  1.16; xent: 0.14; lr: 1.00000; 4943/7106 tok/s;     97 sec\n",
      "[2021-01-30 02:46:11,269 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:46:11,277 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:12,239 INFO] Step 4900/ 5000; acc:  97.21; ppl:  1.16; xent: 0.14; lr: 1.00000; 5000/7189 tok/s;     98 sec\n",
      "[2021-01-30 02:46:12,239 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:46:12,248 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:13,222 INFO] Step 4950/ 5000; acc:  96.80; ppl:  1.17; xent: 0.16; lr: 1.00000; 4940/7102 tok/s;     99 sec\n",
      "[2021-01-30 02:46:13,222 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_6/processed.train.0.pt\n",
      "[2021-01-30 02:46:13,230 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:14,176 INFO] Step 5000/ 5000; acc:  96.79; ppl:  1.20; xent: 0.18; lr: 1.00000; 5088/7315 tok/s;    100 sec\n",
      "[2021-01-30 02:46:14,177 INFO] Saving checkpoint drive/MyDrive/EnglishToleranceBaseline/output/english_rnn_model_1000_6_step_5000.pt\n",
      "[2021-01-30 02:46:15,928 INFO]  * src vocab size = 43\n",
      "[2021-01-30 02:46:15,928 INFO]  * tgt vocab size = 45\n",
      "[2021-01-30 02:46:15,928 INFO] Building model...\n",
      "[2021-01-30 02:46:20,305 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(43, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(45, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=45, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-01-30 02:46:20,305 INFO] encoder: 254500\n",
      "[2021-01-30 02:46:20,305 INFO] decoder: 329645\n",
      "[2021-01-30 02:46:20,305 INFO] * number of parameters: 584145\n",
      "[2021-01-30 02:46:20,308 INFO] Starting training on GPU: [0]\n",
      "[2021-01-30 02:46:20,308 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-01-30 02:46:20,308 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:20,974 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:21,954 INFO] Step 50/ 5000; acc:  12.97; ppl: 41.58; xent: 3.73; lr: 1.00000; 2936/4216 tok/s;      2 sec\n",
      "[2021-01-30 02:46:21,955 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:21,964 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:22,971 INFO] Step 100/ 5000; acc:  21.24; ppl: 19.32; xent: 2.96; lr: 1.00000; 4757/6830 tok/s;      3 sec\n",
      "[2021-01-30 02:46:22,971 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:22,979 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:23,941 INFO] Step 150/ 5000; acc:  24.34; ppl: 16.18; xent: 2.78; lr: 1.00000; 4983/7154 tok/s;      4 sec\n",
      "[2021-01-30 02:46:23,941 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:23,949 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:24,964 INFO] Step 200/ 5000; acc:  27.73; ppl: 13.68; xent: 2.62; lr: 1.00000; 4724/6782 tok/s;      5 sec\n",
      "[2021-01-30 02:46:24,964 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:24,973 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:25,923 INFO] Step 250/ 5000; acc:  30.80; ppl: 11.91; xent: 2.48; lr: 1.00000; 5039/7235 tok/s;      6 sec\n",
      "[2021-01-30 02:46:25,923 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:25,932 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:26,906 INFO] Step 300/ 5000; acc:  35.75; ppl:  9.89; xent: 2.29; lr: 1.00000; 4917/7060 tok/s;      7 sec\n",
      "[2021-01-30 02:46:26,906 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:26,934 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:27,889 INFO] Step 350/ 5000; acc:  38.64; ppl:  8.38; xent: 2.13; lr: 1.00000; 4917/7059 tok/s;      8 sec\n",
      "[2021-01-30 02:46:27,890 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:27,897 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:28,860 INFO] Step 400/ 5000; acc:  41.91; ppl:  7.36; xent: 2.00; lr: 1.00000; 4980/7150 tok/s;      9 sec\n",
      "[2021-01-30 02:46:28,860 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:28,869 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:29,831 INFO] Step 450/ 5000; acc:  44.62; ppl:  6.73; xent: 1.91; lr: 1.00000; 4981/7151 tok/s;     10 sec\n",
      "[2021-01-30 02:46:29,831 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:29,843 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:30,803 INFO] Step 500/ 5000; acc:  48.08; ppl:  5.79; xent: 1.76; lr: 1.00000; 4971/7137 tok/s;     10 sec\n",
      "[2021-01-30 02:46:30,803 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:30,812 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:31,799 INFO] Step 550/ 5000; acc:  52.17; ppl:  5.09; xent: 1.63; lr: 1.00000; 4855/6970 tok/s;     11 sec\n",
      "[2021-01-30 02:46:31,799 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:31,807 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:32,779 INFO] Step 600/ 5000; acc:  57.43; ppl:  4.32; xent: 1.46; lr: 1.00000; 4931/7079 tok/s;     12 sec\n",
      "[2021-01-30 02:46:32,779 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:32,788 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:33,764 INFO] Step 650/ 5000; acc:  64.74; ppl:  3.33; xent: 1.20; lr: 1.00000; 4910/7049 tok/s;     13 sec\n",
      "[2021-01-30 02:46:33,764 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:33,772 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:34,726 INFO] Step 700/ 5000; acc:  71.19; ppl:  2.73; xent: 1.00; lr: 1.00000; 5024/7213 tok/s;     14 sec\n",
      "[2021-01-30 02:46:34,726 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:34,735 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:35,741 INFO] Step 750/ 5000; acc:  76.78; ppl:  2.30; xent: 0.83; lr: 1.00000; 4762/6836 tok/s;     15 sec\n",
      "[2021-01-30 02:46:35,741 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:35,749 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:36,730 INFO] Step 800/ 5000; acc:  81.76; ppl:  1.97; xent: 0.68; lr: 1.00000; 4890/7021 tok/s;     16 sec\n",
      "[2021-01-30 02:46:36,730 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:36,739 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:37,722 INFO] Step 850/ 5000; acc:  85.21; ppl:  1.74; xent: 0.55; lr: 1.00000; 4869/6990 tok/s;     17 sec\n",
      "[2021-01-30 02:46:37,723 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:37,731 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:38,729 INFO] Step 900/ 5000; acc:  88.47; ppl:  1.56; xent: 0.44; lr: 1.00000; 4805/6898 tok/s;     18 sec\n",
      "[2021-01-30 02:46:38,729 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:38,738 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:39,699 INFO] Step 950/ 5000; acc:  89.02; ppl:  1.53; xent: 0.43; lr: 1.00000; 4981/7152 tok/s;     19 sec\n",
      "[2021-01-30 02:46:39,699 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:39,707 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:40,691 INFO] Step 1000/ 5000; acc:  91.66; ppl:  1.39; xent: 0.33; lr: 1.00000; 4874/6999 tok/s;     20 sec\n",
      "[2021-01-30 02:46:40,691 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:40,700 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:41,673 INFO] Step 1050/ 5000; acc:  91.51; ppl:  1.39; xent: 0.33; lr: 1.00000; 4923/7068 tok/s;     21 sec\n",
      "[2021-01-30 02:46:41,673 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:41,701 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:42,690 INFO] Step 1100/ 5000; acc:  93.59; ppl:  1.28; xent: 0.25; lr: 1.00000; 4750/6820 tok/s;     22 sec\n",
      "[2021-01-30 02:46:42,690 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:42,699 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:43,689 INFO] Step 1150/ 5000; acc:  93.96; ppl:  1.27; xent: 0.24; lr: 1.00000; 4837/6945 tok/s;     23 sec\n",
      "[2021-01-30 02:46:43,690 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:43,698 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:44,700 INFO] Step 1200/ 5000; acc:  94.44; ppl:  1.26; xent: 0.23; lr: 1.00000; 4782/6866 tok/s;     24 sec\n",
      "[2021-01-30 02:46:44,701 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:44,709 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:45,698 INFO] Step 1250/ 5000; acc:  95.16; ppl:  1.22; xent: 0.20; lr: 1.00000; 4845/6956 tok/s;     25 sec\n",
      "[2021-01-30 02:46:45,698 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:45,707 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:46,743 INFO] Step 1300/ 5000; acc:  94.94; ppl:  1.21; xent: 0.19; lr: 1.00000; 4625/6640 tok/s;     26 sec\n",
      "[2021-01-30 02:46:46,743 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:46,753 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:47,741 INFO] Step 1350/ 5000; acc:  95.19; ppl:  1.21; xent: 0.19; lr: 1.00000; 4843/6953 tok/s;     27 sec\n",
      "[2021-01-30 02:46:47,742 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:47,750 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:48,748 INFO] Step 1400/ 5000; acc:  95.53; ppl:  1.20; xent: 0.18; lr: 1.00000; 4804/6897 tok/s;     28 sec\n",
      "[2021-01-30 02:46:48,748 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:48,756 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:49,729 INFO] Step 1450/ 5000; acc:  96.04; ppl:  1.17; xent: 0.16; lr: 1.00000; 4927/7074 tok/s;     29 sec\n",
      "[2021-01-30 02:46:49,729 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:49,738 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:50,757 INFO] Step 1500/ 5000; acc:  95.86; ppl:  1.17; xent: 0.16; lr: 1.00000; 4700/6748 tok/s;     30 sec\n",
      "[2021-01-30 02:46:50,757 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:50,766 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:51,722 INFO] Step 1550/ 5000; acc:  95.85; ppl:  1.18; xent: 0.16; lr: 1.00000; 5012/7195 tok/s;     31 sec\n",
      "[2021-01-30 02:46:51,722 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:51,731 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:52,699 INFO] Step 1600/ 5000; acc:  96.08; ppl:  1.16; xent: 0.15; lr: 1.00000; 4944/7099 tok/s;     32 sec\n",
      "[2021-01-30 02:46:52,700 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:52,708 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:53,667 INFO] Step 1650/ 5000; acc:  95.99; ppl:  1.18; xent: 0.16; lr: 1.00000; 4996/7173 tok/s;     33 sec\n",
      "[2021-01-30 02:46:53,667 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:53,676 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:54,688 INFO] Step 1700/ 5000; acc:  95.56; ppl:  1.19; xent: 0.17; lr: 1.00000; 4735/6798 tok/s;     34 sec\n",
      "[2021-01-30 02:46:54,688 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:54,696 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:55,655 INFO] Step 1750/ 5000; acc:  96.04; ppl:  1.17; xent: 0.16; lr: 1.00000; 4996/7173 tok/s;     35 sec\n",
      "[2021-01-30 02:46:55,656 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:55,664 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:56,646 INFO] Step 1800/ 5000; acc:  96.40; ppl:  1.16; xent: 0.15; lr: 1.00000; 4882/7009 tok/s;     36 sec\n",
      "[2021-01-30 02:46:56,646 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:56,674 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:57,642 INFO] Step 1850/ 5000; acc:  96.08; ppl:  1.17; xent: 0.16; lr: 1.00000; 4850/6963 tok/s;     37 sec\n",
      "[2021-01-30 02:46:57,643 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:57,652 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:58,650 INFO] Step 1900/ 5000; acc:  96.24; ppl:  1.17; xent: 0.15; lr: 1.00000; 4798/6889 tok/s;     38 sec\n",
      "[2021-01-30 02:46:58,650 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:58,659 INFO] number of examples: 1000\n",
      "[2021-01-30 02:46:59,616 INFO] Step 1950/ 5000; acc:  96.25; ppl:  1.17; xent: 0.15; lr: 1.00000; 5004/7185 tok/s;     39 sec\n",
      "[2021-01-30 02:46:59,616 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:46:59,624 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:00,605 INFO] Step 2000/ 5000; acc:  96.45; ppl:  1.15; xent: 0.14; lr: 1.00000; 4885/7013 tok/s;     40 sec\n",
      "[2021-01-30 02:47:00,606 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:00,614 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:01,592 INFO] Step 2050/ 5000; acc:  95.98; ppl:  1.17; xent: 0.16; lr: 1.00000; 4902/7037 tok/s;     41 sec\n",
      "[2021-01-30 02:47:01,592 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:01,600 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:02,571 INFO] Step 2100/ 5000; acc:  96.01; ppl:  1.18; xent: 0.16; lr: 1.00000; 4934/7084 tok/s;     42 sec\n",
      "[2021-01-30 02:47:02,572 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:02,581 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:03,565 INFO] Step 2150/ 5000; acc:  96.15; ppl:  1.18; xent: 0.16; lr: 1.00000; 4866/6986 tok/s;     43 sec\n",
      "[2021-01-30 02:47:03,565 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:03,573 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:04,559 INFO] Step 2200/ 5000; acc:  96.21; ppl:  1.18; xent: 0.16; lr: 1.00000; 4863/6982 tok/s;     44 sec\n",
      "[2021-01-30 02:47:04,559 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:04,567 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:05,542 INFO] Step 2250/ 5000; acc:  96.41; ppl:  1.15; xent: 0.14; lr: 1.00000; 4914/7056 tok/s;     45 sec\n",
      "[2021-01-30 02:47:05,543 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:05,551 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:06,537 INFO] Step 2300/ 5000; acc:  96.14; ppl:  1.17; xent: 0.16; lr: 1.00000; 4862/6981 tok/s;     46 sec\n",
      "[2021-01-30 02:47:06,537 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:06,545 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:07,490 INFO] Step 2350/ 5000; acc:  96.47; ppl:  1.16; xent: 0.15; lr: 1.00000; 5071/7281 tok/s;     47 sec\n",
      "[2021-01-30 02:47:07,490 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:07,498 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:08,475 INFO] Step 2400/ 5000; acc:  96.90; ppl:  1.15; xent: 0.14; lr: 1.00000; 4908/7046 tok/s;     48 sec\n",
      "[2021-01-30 02:47:08,475 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:08,483 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:09,437 INFO] Step 2450/ 5000; acc:  96.79; ppl:  1.14; xent: 0.13; lr: 1.00000; 5023/7212 tok/s;     49 sec\n",
      "[2021-01-30 02:47:09,437 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:09,446 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:10,420 INFO] Step 2500/ 5000; acc:  96.82; ppl:  1.15; xent: 0.14; lr: 1.00000; 4919/7062 tok/s;     50 sec\n",
      "[2021-01-30 02:47:10,420 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:10,448 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:11,442 INFO] Step 2550/ 5000; acc:  96.60; ppl:  1.15; xent: 0.14; lr: 1.00000; 4731/6793 tok/s;     51 sec\n",
      "[2021-01-30 02:47:11,442 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:11,450 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:12,405 INFO] Step 2600/ 5000; acc:  96.90; ppl:  1.13; xent: 0.12; lr: 1.00000; 5018/7204 tok/s;     52 sec\n",
      "[2021-01-30 02:47:12,405 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:12,414 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:13,370 INFO] Step 2650/ 5000; acc:  96.66; ppl:  1.16; xent: 0.15; lr: 1.00000; 5011/7195 tok/s;     53 sec\n",
      "[2021-01-30 02:47:13,370 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:13,378 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:14,333 INFO] Step 2700/ 5000; acc:  96.40; ppl:  1.17; xent: 0.16; lr: 1.00000; 5019/7206 tok/s;     54 sec\n",
      "[2021-01-30 02:47:14,333 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:14,341 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:15,298 INFO] Step 2750/ 5000; acc:  97.28; ppl:  1.14; xent: 0.13; lr: 1.00000; 5010/7193 tok/s;     55 sec\n",
      "[2021-01-30 02:47:15,298 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:15,306 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:16,282 INFO] Step 2800/ 5000; acc:  96.50; ppl:  1.16; xent: 0.15; lr: 1.00000; 4910/7050 tok/s;     56 sec\n",
      "[2021-01-30 02:47:16,282 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:16,292 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:17,285 INFO] Step 2850/ 5000; acc:  96.37; ppl:  1.18; xent: 0.16; lr: 1.00000; 4819/6919 tok/s;     57 sec\n",
      "[2021-01-30 02:47:17,285 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:17,294 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:18,250 INFO] Step 2900/ 5000; acc:  96.99; ppl:  1.14; xent: 0.13; lr: 1.00000; 5007/7189 tok/s;     58 sec\n",
      "[2021-01-30 02:47:18,251 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:18,260 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:19,216 INFO] Step 2950/ 5000; acc:  96.69; ppl:  1.16; xent: 0.15; lr: 1.00000; 5008/7190 tok/s;     59 sec\n",
      "[2021-01-30 02:47:19,216 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:19,224 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:20,194 INFO] Step 3000/ 5000; acc:  96.93; ppl:  1.17; xent: 0.15; lr: 1.00000; 4941/7095 tok/s;     60 sec\n",
      "[2021-01-30 02:47:20,194 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:20,203 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:21,177 INFO] Step 3050/ 5000; acc:  96.89; ppl:  1.15; xent: 0.14; lr: 1.00000; 4918/7061 tok/s;     61 sec\n",
      "[2021-01-30 02:47:21,177 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:21,186 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:22,167 INFO] Step 3100/ 5000; acc:  97.25; ppl:  1.13; xent: 0.12; lr: 1.00000; 4882/7009 tok/s;     62 sec\n",
      "[2021-01-30 02:47:22,167 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:22,176 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:23,162 INFO] Step 3150/ 5000; acc:  97.02; ppl:  1.16; xent: 0.15; lr: 1.00000; 4857/6974 tok/s;     63 sec\n",
      "[2021-01-30 02:47:23,163 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:23,170 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:24,130 INFO] Step 3200/ 5000; acc:  96.57; ppl:  1.18; xent: 0.16; lr: 1.00000; 4995/7172 tok/s;     64 sec\n",
      "[2021-01-30 02:47:24,130 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:24,139 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:25,116 INFO] Step 3250/ 5000; acc:  96.76; ppl:  1.16; xent: 0.15; lr: 1.00000; 4901/7037 tok/s;     65 sec\n",
      "[2021-01-30 02:47:25,116 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:25,144 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:26,121 INFO] Step 3300/ 5000; acc:  96.74; ppl:  1.17; xent: 0.16; lr: 1.00000; 4812/6909 tok/s;     66 sec\n",
      "[2021-01-30 02:47:26,121 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:26,129 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:27,116 INFO] Step 3350/ 5000; acc:  96.61; ppl:  1.18; xent: 0.16; lr: 1.00000; 4858/6974 tok/s;     67 sec\n",
      "[2021-01-30 02:47:27,116 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:27,126 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:28,102 INFO] Step 3400/ 5000; acc:  96.47; ppl:  1.19; xent: 0.18; lr: 1.00000; 4904/7042 tok/s;     68 sec\n",
      "[2021-01-30 02:47:28,102 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:28,110 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:29,077 INFO] Step 3450/ 5000; acc:  96.58; ppl:  1.18; xent: 0.17; lr: 1.00000; 4958/7118 tok/s;     69 sec\n",
      "[2021-01-30 02:47:29,077 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:29,086 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:30,031 INFO] Step 3500/ 5000; acc:  96.31; ppl:  1.20; xent: 0.19; lr: 1.00000; 5062/7268 tok/s;     70 sec\n",
      "[2021-01-30 02:47:30,032 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:30,040 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:30,994 INFO] Step 3550/ 5000; acc:  96.38; ppl:  1.21; xent: 0.19; lr: 1.00000; 5021/7209 tok/s;     71 sec\n",
      "[2021-01-30 02:47:30,994 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:31,003 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:31,999 INFO] Step 3600/ 5000; acc:  96.08; ppl:  1.21; xent: 0.19; lr: 1.00000; 4812/6909 tok/s;     72 sec\n",
      "[2021-01-30 02:47:31,999 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:32,008 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:33,003 INFO] Step 3650/ 5000; acc:  96.43; ppl:  1.17; xent: 0.16; lr: 1.00000; 4814/6911 tok/s;     73 sec\n",
      "[2021-01-30 02:47:33,003 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:33,012 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:34,000 INFO] Step 3700/ 5000; acc:  96.37; ppl:  1.19; xent: 0.18; lr: 1.00000; 4849/6963 tok/s;     74 sec\n",
      "[2021-01-30 02:47:34,000 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:34,008 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:34,971 INFO] Step 3750/ 5000; acc:  96.18; ppl:  1.22; xent: 0.20; lr: 1.00000; 4977/7146 tok/s;     75 sec\n",
      "[2021-01-30 02:47:34,971 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:34,980 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:35,931 INFO] Step 3800/ 5000; acc:  96.45; ppl:  1.20; xent: 0.18; lr: 1.00000; 5034/7227 tok/s;     76 sec\n",
      "[2021-01-30 02:47:35,931 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:35,940 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:36,915 INFO] Step 3850/ 5000; acc:  96.67; ppl:  1.17; xent: 0.15; lr: 1.00000; 4911/7051 tok/s;     77 sec\n",
      "[2021-01-30 02:47:36,916 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:36,924 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:37,882 INFO] Step 3900/ 5000; acc:  96.69; ppl:  1.17; xent: 0.15; lr: 1.00000; 4999/7177 tok/s;     78 sec\n",
      "[2021-01-30 02:47:37,883 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:37,891 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:38,856 INFO] Step 3950/ 5000; acc:  97.09; ppl:  1.16; xent: 0.15; lr: 1.00000; 4967/7132 tok/s;     79 sec\n",
      "[2021-01-30 02:47:38,856 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:38,864 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:39,826 INFO] Step 4000/ 5000; acc:  96.40; ppl:  1.20; xent: 0.18; lr: 1.00000; 4979/7149 tok/s;     80 sec\n",
      "[2021-01-30 02:47:39,827 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:39,854 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:40,839 INFO] Step 4050/ 5000; acc:  96.56; ppl:  1.20; xent: 0.18; lr: 1.00000; 4771/6850 tok/s;     81 sec\n",
      "[2021-01-30 02:47:40,840 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:40,849 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:41,849 INFO] Step 4100/ 5000; acc:  96.50; ppl:  1.18; xent: 0.16; lr: 1.00000; 4788/6875 tok/s;     82 sec\n",
      "[2021-01-30 02:47:41,849 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:41,858 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:42,822 INFO] Step 4150/ 5000; acc:  96.27; ppl:  1.20; xent: 0.19; lr: 1.00000; 4968/7133 tok/s;     83 sec\n",
      "[2021-01-30 02:47:42,822 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:42,830 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:43,796 INFO] Step 4200/ 5000; acc:  96.18; ppl:  1.21; xent: 0.19; lr: 1.00000; 4965/7128 tok/s;     83 sec\n",
      "[2021-01-30 02:47:43,796 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:43,805 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:44,775 INFO] Step 4250/ 5000; acc:  96.54; ppl:  1.20; xent: 0.18; lr: 1.00000; 4934/7084 tok/s;     84 sec\n",
      "[2021-01-30 02:47:44,776 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:44,784 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:45,733 INFO] Step 4300/ 5000; acc:  96.17; ppl:  1.21; xent: 0.19; lr: 1.00000; 5050/7250 tok/s;     85 sec\n",
      "[2021-01-30 02:47:45,733 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:45,742 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:46,708 INFO] Step 4350/ 5000; acc:  96.70; ppl:  1.17; xent: 0.16; lr: 1.00000; 4955/7114 tok/s;     86 sec\n",
      "[2021-01-30 02:47:46,708 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:46,716 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:47,684 INFO] Step 4400/ 5000; acc:  96.45; ppl:  1.18; xent: 0.17; lr: 1.00000; 4953/7111 tok/s;     87 sec\n",
      "[2021-01-30 02:47:47,684 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:47,693 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:48,652 INFO] Step 4450/ 5000; acc:  96.37; ppl:  1.22; xent: 0.20; lr: 1.00000; 4993/7168 tok/s;     88 sec\n",
      "[2021-01-30 02:47:48,652 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:48,661 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:49,646 INFO] Step 4500/ 5000; acc:  96.71; ppl:  1.18; xent: 0.17; lr: 1.00000; 4863/6982 tok/s;     89 sec\n",
      "[2021-01-30 02:47:49,647 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:49,655 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:50,624 INFO] Step 4550/ 5000; acc:  96.74; ppl:  1.16; xent: 0.15; lr: 1.00000; 4942/7095 tok/s;     90 sec\n",
      "[2021-01-30 02:47:50,625 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:50,633 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:51,589 INFO] Step 4600/ 5000; acc:  97.12; ppl:  1.16; xent: 0.15; lr: 1.00000; 5011/7194 tok/s;     91 sec\n",
      "[2021-01-30 02:47:51,589 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:51,598 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:52,572 INFO] Step 4650/ 5000; acc:  96.86; ppl:  1.16; xent: 0.15; lr: 1.00000; 4920/7063 tok/s;     92 sec\n",
      "[2021-01-30 02:47:52,572 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:52,581 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:53,608 INFO] Step 4700/ 5000; acc:  96.33; ppl:  1.21; xent: 0.19; lr: 1.00000; 4666/6699 tok/s;     93 sec\n",
      "[2021-01-30 02:47:53,608 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:53,636 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:54,610 INFO] Step 4750/ 5000; acc:  95.62; ppl:  1.29; xent: 0.26; lr: 1.00000; 4822/6924 tok/s;     94 sec\n",
      "[2021-01-30 02:47:54,610 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:54,618 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:55,592 INFO] Step 4800/ 5000; acc:  96.21; ppl:  1.23; xent: 0.21; lr: 1.00000; 4924/7069 tok/s;     95 sec\n",
      "[2021-01-30 02:47:55,592 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:55,601 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:56,579 INFO] Step 4850/ 5000; acc:  96.34; ppl:  1.21; xent: 0.19; lr: 1.00000; 4898/7032 tok/s;     96 sec\n",
      "[2021-01-30 02:47:56,579 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:56,589 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:57,565 INFO] Step 4900/ 5000; acc:  96.11; ppl:  1.23; xent: 0.20; lr: 1.00000; 4904/7040 tok/s;     97 sec\n",
      "[2021-01-30 02:47:57,565 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:57,574 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:58,540 INFO] Step 4950/ 5000; acc:  96.80; ppl:  1.17; xent: 0.16; lr: 1.00000; 4958/7118 tok/s;     98 sec\n",
      "[2021-01-30 02:47:58,540 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_7/processed.train.0.pt\n",
      "[2021-01-30 02:47:58,548 INFO] number of examples: 1000\n",
      "[2021-01-30 02:47:59,523 INFO] Step 5000/ 5000; acc:  96.58; ppl:  1.20; xent: 0.18; lr: 1.00000; 4917/7059 tok/s;     99 sec\n",
      "[2021-01-30 02:47:59,524 INFO] Saving checkpoint drive/MyDrive/EnglishToleranceBaseline/output/english_rnn_model_1000_7_step_5000.pt\n",
      "[2021-01-30 02:48:01,253 INFO]  * src vocab size = 43\n",
      "[2021-01-30 02:48:01,253 INFO]  * tgt vocab size = 45\n",
      "[2021-01-30 02:48:01,253 INFO] Building model...\n",
      "[2021-01-30 02:48:05,639 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(43, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(45, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=45, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-01-30 02:48:05,640 INFO] encoder: 254500\n",
      "[2021-01-30 02:48:05,640 INFO] decoder: 329645\n",
      "[2021-01-30 02:48:05,640 INFO] * number of parameters: 584145\n",
      "[2021-01-30 02:48:05,643 INFO] Starting training on GPU: [0]\n",
      "[2021-01-30 02:48:05,643 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-01-30 02:48:05,643 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:06,296 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:07,268 INFO] Step 50/ 5000; acc:  12.64; ppl: 36.25; xent: 3.59; lr: 1.00000; 2978/4286 tok/s;      2 sec\n",
      "[2021-01-30 02:48:07,268 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:07,277 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:08,218 INFO] Step 100/ 5000; acc:  21.74; ppl: 19.77; xent: 2.98; lr: 1.00000; 5096/7336 tok/s;      3 sec\n",
      "[2021-01-30 02:48:08,218 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:08,226 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:09,174 INFO] Step 150/ 5000; acc:  23.99; ppl: 17.16; xent: 2.84; lr: 1.00000; 5059/7283 tok/s;      4 sec\n",
      "[2021-01-30 02:48:09,174 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:09,182 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:10,173 INFO] Step 200/ 5000; acc:  26.16; ppl: 14.20; xent: 2.65; lr: 1.00000; 4844/6973 tok/s;      5 sec\n",
      "[2021-01-30 02:48:10,173 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:10,182 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:11,130 INFO] Step 250/ 5000; acc:  30.03; ppl: 12.15; xent: 2.50; lr: 1.00000; 5055/7276 tok/s;      5 sec\n",
      "[2021-01-30 02:48:11,131 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:11,139 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:12,085 INFO] Step 300/ 5000; acc:  35.08; ppl:  9.82; xent: 2.28; lr: 1.00000; 5069/7296 tok/s;      6 sec\n",
      "[2021-01-30 02:48:12,085 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:12,113 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:13,081 INFO] Step 350/ 5000; acc:  38.80; ppl:  8.39; xent: 2.13; lr: 1.00000; 4856/6990 tok/s;      7 sec\n",
      "[2021-01-30 02:48:13,082 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:13,090 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:14,015 INFO] Step 400/ 5000; acc:  42.65; ppl:  7.34; xent: 1.99; lr: 1.00000; 5183/7461 tok/s;      8 sec\n",
      "[2021-01-30 02:48:14,015 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:14,024 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:14,973 INFO] Step 450/ 5000; acc:  45.82; ppl:  6.54; xent: 1.88; lr: 1.00000; 5053/7273 tok/s;      9 sec\n",
      "[2021-01-30 02:48:14,973 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:14,982 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:15,950 INFO] Step 500/ 5000; acc:  47.16; ppl:  5.95; xent: 1.78; lr: 1.00000; 4951/7127 tok/s;     10 sec\n",
      "[2021-01-30 02:48:15,950 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:15,959 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:16,937 INFO] Step 550/ 5000; acc:  51.34; ppl:  5.27; xent: 1.66; lr: 1.00000; 4904/7058 tok/s;     11 sec\n",
      "[2021-01-30 02:48:16,937 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:16,945 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:17,873 INFO] Step 600/ 5000; acc:  55.77; ppl:  4.42; xent: 1.49; lr: 1.00000; 5171/7443 tok/s;     12 sec\n",
      "[2021-01-30 02:48:17,873 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:17,881 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:18,816 INFO] Step 650/ 5000; acc:  61.06; ppl:  3.71; xent: 1.31; lr: 1.00000; 5128/7382 tok/s;     13 sec\n",
      "[2021-01-30 02:48:18,816 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:18,825 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:19,778 INFO] Step 700/ 5000; acc:  65.05; ppl:  3.24; xent: 1.17; lr: 1.00000; 5032/7243 tok/s;     14 sec\n",
      "[2021-01-30 02:48:19,778 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:19,787 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:20,735 INFO] Step 750/ 5000; acc:  70.56; ppl:  2.73; xent: 1.00; lr: 1.00000; 5058/7281 tok/s;     15 sec\n",
      "[2021-01-30 02:48:20,735 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:20,743 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:21,701 INFO] Step 800/ 5000; acc:  75.39; ppl:  2.32; xent: 0.84; lr: 1.00000; 5009/7210 tok/s;     16 sec\n",
      "[2021-01-30 02:48:21,701 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:21,709 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:22,656 INFO] Step 850/ 5000; acc:  78.52; ppl:  2.12; xent: 0.75; lr: 1.00000; 5062/7287 tok/s;     17 sec\n",
      "[2021-01-30 02:48:22,657 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:22,665 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:23,632 INFO] Step 900/ 5000; acc:  82.09; ppl:  1.92; xent: 0.65; lr: 1.00000; 4961/7141 tok/s;     18 sec\n",
      "[2021-01-30 02:48:23,632 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:23,641 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:24,628 INFO] Step 950/ 5000; acc:  85.07; ppl:  1.71; xent: 0.54; lr: 1.00000; 4859/6994 tok/s;     19 sec\n",
      "[2021-01-30 02:48:24,628 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:24,636 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:25,592 INFO] Step 1000/ 5000; acc:  87.89; ppl:  1.55; xent: 0.44; lr: 1.00000; 5019/7225 tok/s;     20 sec\n",
      "[2021-01-30 02:48:25,592 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:25,601 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:26,557 INFO] Step 1050/ 5000; acc:  89.24; ppl:  1.51; xent: 0.41; lr: 1.00000; 5014/7218 tok/s;     21 sec\n",
      "[2021-01-30 02:48:26,557 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:26,584 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:27,537 INFO] Step 1100/ 5000; acc:  90.13; ppl:  1.44; xent: 0.36; lr: 1.00000; 4936/7106 tok/s;     22 sec\n",
      "[2021-01-30 02:48:27,537 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:27,545 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:28,537 INFO] Step 1150/ 5000; acc:  90.06; ppl:  1.46; xent: 0.38; lr: 1.00000; 4839/6965 tok/s;     23 sec\n",
      "[2021-01-30 02:48:28,537 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:28,545 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:29,528 INFO] Step 1200/ 5000; acc:  92.39; ppl:  1.32; xent: 0.28; lr: 1.00000; 4885/7032 tok/s;     24 sec\n",
      "[2021-01-30 02:48:29,528 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:29,536 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:30,470 INFO] Step 1250/ 5000; acc:  92.73; ppl:  1.31; xent: 0.27; lr: 1.00000; 5135/7392 tok/s;     25 sec\n",
      "[2021-01-30 02:48:30,470 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:30,486 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:31,431 INFO] Step 1300/ 5000; acc:  92.98; ppl:  1.30; xent: 0.26; lr: 1.00000; 5036/7248 tok/s;     26 sec\n",
      "[2021-01-30 02:48:31,431 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:31,439 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:32,381 INFO] Step 1350/ 5000; acc:  93.75; ppl:  1.25; xent: 0.22; lr: 1.00000; 5095/7334 tok/s;     27 sec\n",
      "[2021-01-30 02:48:32,381 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:32,389 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:33,392 INFO] Step 1400/ 5000; acc:  94.18; ppl:  1.25; xent: 0.22; lr: 1.00000; 4784/6886 tok/s;     28 sec\n",
      "[2021-01-30 02:48:33,392 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:33,401 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:34,325 INFO] Step 1450/ 5000; acc:  94.90; ppl:  1.21; xent: 0.19; lr: 1.00000; 5186/7465 tok/s;     29 sec\n",
      "[2021-01-30 02:48:34,325 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:34,334 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:35,321 INFO] Step 1500/ 5000; acc:  95.23; ppl:  1.20; xent: 0.18; lr: 1.00000; 4859/6994 tok/s;     30 sec\n",
      "[2021-01-30 02:48:35,321 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:35,330 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:36,303 INFO] Step 1550/ 5000; acc:  94.99; ppl:  1.23; xent: 0.20; lr: 1.00000; 4926/7091 tok/s;     31 sec\n",
      "[2021-01-30 02:48:36,303 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:36,312 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:37,260 INFO] Step 1600/ 5000; acc:  94.93; ppl:  1.20; xent: 0.19; lr: 1.00000; 5057/7280 tok/s;     32 sec\n",
      "[2021-01-30 02:48:37,260 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:37,269 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:38,213 INFO] Step 1650/ 5000; acc:  95.26; ppl:  1.20; xent: 0.18; lr: 1.00000; 5077/7309 tok/s;     33 sec\n",
      "[2021-01-30 02:48:38,213 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:38,222 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:39,248 INFO] Step 1700/ 5000; acc:  95.85; ppl:  1.17; xent: 0.16; lr: 1.00000; 4676/6730 tok/s;     34 sec\n",
      "[2021-01-30 02:48:39,248 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:39,257 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:40,216 INFO] Step 1750/ 5000; acc:  95.71; ppl:  1.18; xent: 0.17; lr: 1.00000; 4997/7192 tok/s;     35 sec\n",
      "[2021-01-30 02:48:40,217 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:40,224 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:41,153 INFO] Step 1800/ 5000; acc:  96.25; ppl:  1.14; xent: 0.13; lr: 1.00000; 5164/7433 tok/s;     36 sec\n",
      "[2021-01-30 02:48:41,154 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:41,181 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:42,102 INFO] Step 1850/ 5000; acc:  95.74; ppl:  1.18; xent: 0.16; lr: 1.00000; 5101/7343 tok/s;     36 sec\n",
      "[2021-01-30 02:48:42,102 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:42,111 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:43,085 INFO] Step 1900/ 5000; acc:  96.35; ppl:  1.15; xent: 0.14; lr: 1.00000; 4925/7089 tok/s;     37 sec\n",
      "[2021-01-30 02:48:43,085 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:43,093 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:44,034 INFO] Step 1950/ 5000; acc:  96.45; ppl:  1.16; xent: 0.15; lr: 1.00000; 5095/7333 tok/s;     38 sec\n",
      "[2021-01-30 02:48:44,035 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:44,043 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:44,960 INFO] Step 2000/ 5000; acc:  95.91; ppl:  1.17; xent: 0.16; lr: 1.00000; 5231/7529 tok/s;     39 sec\n",
      "[2021-01-30 02:48:44,960 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:44,968 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:45,923 INFO] Step 2050/ 5000; acc:  96.60; ppl:  1.14; xent: 0.13; lr: 1.00000; 5020/7226 tok/s;     40 sec\n",
      "[2021-01-30 02:48:45,924 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:45,933 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:46,854 INFO] Step 2100/ 5000; acc:  95.96; ppl:  1.19; xent: 0.17; lr: 1.00000; 5202/7488 tok/s;     41 sec\n",
      "[2021-01-30 02:48:46,854 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:46,862 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:47,830 INFO] Step 2150/ 5000; acc:  96.21; ppl:  1.18; xent: 0.17; lr: 1.00000; 4956/7134 tok/s;     42 sec\n",
      "[2021-01-30 02:48:47,830 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:47,839 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:48,771 INFO] Step 2200/ 5000; acc:  96.78; ppl:  1.14; xent: 0.13; lr: 1.00000; 5141/7401 tok/s;     43 sec\n",
      "[2021-01-30 02:48:48,771 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:48,780 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:49,724 INFO] Step 2250/ 5000; acc:  96.55; ppl:  1.15; xent: 0.14; lr: 1.00000; 5080/7313 tok/s;     44 sec\n",
      "[2021-01-30 02:48:49,724 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:49,738 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:50,664 INFO] Step 2300/ 5000; acc:  96.81; ppl:  1.14; xent: 0.13; lr: 1.00000; 5144/7404 tok/s;     45 sec\n",
      "[2021-01-30 02:48:50,665 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:50,673 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:51,636 INFO] Step 2350/ 5000; acc:  96.48; ppl:  1.17; xent: 0.16; lr: 1.00000; 4981/7170 tok/s;     46 sec\n",
      "[2021-01-30 02:48:51,636 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:51,644 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:52,614 INFO] Step 2400/ 5000; acc:  96.71; ppl:  1.15; xent: 0.14; lr: 1.00000; 4945/7118 tok/s;     47 sec\n",
      "[2021-01-30 02:48:52,615 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:52,623 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:53,618 INFO] Step 2450/ 5000; acc:  96.61; ppl:  1.14; xent: 0.13; lr: 1.00000; 4824/6943 tok/s;     48 sec\n",
      "[2021-01-30 02:48:53,618 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:53,626 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:54,585 INFO] Step 2500/ 5000; acc:  96.24; ppl:  1.17; xent: 0.16; lr: 1.00000; 5002/7200 tok/s;     49 sec\n",
      "[2021-01-30 02:48:54,585 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:54,613 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:55,590 INFO] Step 2550/ 5000; acc:  96.44; ppl:  1.16; xent: 0.15; lr: 1.00000; 4817/6933 tok/s;     50 sec\n",
      "[2021-01-30 02:48:55,590 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:55,598 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:56,540 INFO] Step 2600/ 5000; acc:  96.14; ppl:  1.18; xent: 0.17; lr: 1.00000; 5091/7329 tok/s;     51 sec\n",
      "[2021-01-30 02:48:56,540 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:56,548 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:57,501 INFO] Step 2650/ 5000; acc:  95.94; ppl:  1.22; xent: 0.20; lr: 1.00000; 5037/7251 tok/s;     52 sec\n",
      "[2021-01-30 02:48:57,501 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:57,509 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:58,456 INFO] Step 2700/ 5000; acc:  96.71; ppl:  1.16; xent: 0.15; lr: 1.00000; 5067/7294 tok/s;     53 sec\n",
      "[2021-01-30 02:48:58,456 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:58,465 INFO] number of examples: 1000\n",
      "[2021-01-30 02:48:59,425 INFO] Step 2750/ 5000; acc:  96.41; ppl:  1.18; xent: 0.17; lr: 1.00000; 4991/7184 tok/s;     54 sec\n",
      "[2021-01-30 02:48:59,425 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:48:59,433 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:00,409 INFO] Step 2800/ 5000; acc:  96.31; ppl:  1.17; xent: 0.16; lr: 1.00000; 4918/7079 tok/s;     55 sec\n",
      "[2021-01-30 02:49:00,409 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:00,418 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:01,372 INFO] Step 2850/ 5000; acc:  96.91; ppl:  1.15; xent: 0.14; lr: 1.00000; 5023/7231 tok/s;     56 sec\n",
      "[2021-01-30 02:49:01,373 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:01,381 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:02,339 INFO] Step 2900/ 5000; acc:  97.03; ppl:  1.14; xent: 0.13; lr: 1.00000; 5007/7207 tok/s;     57 sec\n",
      "[2021-01-30 02:49:02,339 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:02,348 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:03,329 INFO] Step 2950/ 5000; acc:  96.34; ppl:  1.17; xent: 0.15; lr: 1.00000; 4887/7035 tok/s;     58 sec\n",
      "[2021-01-30 02:49:03,329 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:03,337 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:04,276 INFO] Step 3000/ 5000; acc:  96.37; ppl:  1.17; xent: 0.16; lr: 1.00000; 5109/7354 tok/s;     59 sec\n",
      "[2021-01-30 02:49:04,276 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:04,285 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:05,251 INFO] Step 3050/ 5000; acc:  96.81; ppl:  1.14; xent: 0.13; lr: 1.00000; 4964/7146 tok/s;     60 sec\n",
      "[2021-01-30 02:49:05,251 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:05,260 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:06,201 INFO] Step 3100/ 5000; acc:  96.67; ppl:  1.16; xent: 0.15; lr: 1.00000; 5093/7331 tok/s;     61 sec\n",
      "[2021-01-30 02:49:06,201 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:06,210 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:07,156 INFO] Step 3150/ 5000; acc:  97.31; ppl:  1.14; xent: 0.13; lr: 1.00000; 5066/7293 tok/s;     62 sec\n",
      "[2021-01-30 02:49:07,156 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:07,164 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:08,109 INFO] Step 3200/ 5000; acc:  96.84; ppl:  1.15; xent: 0.14; lr: 1.00000; 5081/7313 tok/s;     62 sec\n",
      "[2021-01-30 02:49:08,109 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:08,117 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:09,113 INFO] Step 3250/ 5000; acc:  96.96; ppl:  1.14; xent: 0.14; lr: 1.00000; 4818/6935 tok/s;     63 sec\n",
      "[2021-01-30 02:49:09,113 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:09,141 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:10,086 INFO] Step 3300/ 5000; acc:  97.06; ppl:  1.15; xent: 0.14; lr: 1.00000; 4975/7161 tok/s;     64 sec\n",
      "[2021-01-30 02:49:10,086 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:10,095 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:11,057 INFO] Step 3350/ 5000; acc:  96.75; ppl:  1.17; xent: 0.16; lr: 1.00000; 4982/7172 tok/s;     65 sec\n",
      "[2021-01-30 02:49:11,057 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:11,065 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:12,040 INFO] Step 3400/ 5000; acc:  97.06; ppl:  1.16; xent: 0.15; lr: 1.00000; 4920/7082 tok/s;     66 sec\n",
      "[2021-01-30 02:49:12,041 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:12,050 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:13,010 INFO] Step 3450/ 5000; acc:  96.81; ppl:  1.16; xent: 0.15; lr: 1.00000; 4991/7184 tok/s;     67 sec\n",
      "[2021-01-30 02:49:13,010 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:13,019 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:13,973 INFO] Step 3500/ 5000; acc:  97.11; ppl:  1.14; xent: 0.13; lr: 1.00000; 5026/7235 tok/s;     68 sec\n",
      "[2021-01-30 02:49:13,973 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:13,982 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:14,953 INFO] Step 3550/ 5000; acc:  96.61; ppl:  1.16; xent: 0.15; lr: 1.00000; 4935/7104 tok/s;     69 sec\n",
      "[2021-01-30 02:49:14,953 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:14,962 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:15,905 INFO] Step 3600/ 5000; acc:  96.61; ppl:  1.18; xent: 0.17; lr: 1.00000; 5086/7321 tok/s;     70 sec\n",
      "[2021-01-30 02:49:15,905 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:15,913 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:16,865 INFO] Step 3650/ 5000; acc:  96.31; ppl:  1.20; xent: 0.18; lr: 1.00000; 5039/7253 tok/s;     71 sec\n",
      "[2021-01-30 02:49:16,865 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:16,874 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:17,801 INFO] Step 3700/ 5000; acc:  96.37; ppl:  1.21; xent: 0.19; lr: 1.00000; 5172/7444 tok/s;     72 sec\n",
      "[2021-01-30 02:49:17,801 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:17,809 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:18,795 INFO] Step 3750/ 5000; acc:  96.60; ppl:  1.16; xent: 0.15; lr: 1.00000; 4868/7007 tok/s;     73 sec\n",
      "[2021-01-30 02:49:18,795 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:18,803 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:19,737 INFO] Step 3800/ 5000; acc:  96.84; ppl:  1.18; xent: 0.16; lr: 1.00000; 5135/7392 tok/s;     74 sec\n",
      "[2021-01-30 02:49:19,737 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:19,745 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:20,689 INFO] Step 3850/ 5000; acc:  96.50; ppl:  1.18; xent: 0.16; lr: 1.00000; 5084/7318 tok/s;     75 sec\n",
      "[2021-01-30 02:49:20,689 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:20,697 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:21,644 INFO] Step 3900/ 5000; acc:  95.65; ppl:  1.26; xent: 0.23; lr: 1.00000; 5065/7291 tok/s;     76 sec\n",
      "[2021-01-30 02:49:21,644 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:21,653 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:22,614 INFO] Step 3950/ 5000; acc:  96.24; ppl:  1.19; xent: 0.18; lr: 1.00000; 4988/7180 tok/s;     77 sec\n",
      "[2021-01-30 02:49:22,614 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:22,623 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:23,576 INFO] Step 4000/ 5000; acc:  96.57; ppl:  1.17; xent: 0.16; lr: 1.00000; 5029/7239 tok/s;     78 sec\n",
      "[2021-01-30 02:49:23,577 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:23,605 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:24,565 INFO] Step 4050/ 5000; acc:  97.01; ppl:  1.17; xent: 0.16; lr: 1.00000; 4897/7048 tok/s;     79 sec\n",
      "[2021-01-30 02:49:24,565 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:24,574 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:25,580 INFO] Step 4100/ 5000; acc:  96.74; ppl:  1.18; xent: 0.16; lr: 1.00000; 4764/6857 tok/s;     80 sec\n",
      "[2021-01-30 02:49:25,581 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:25,589 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:26,529 INFO] Step 4150/ 5000; acc:  96.84; ppl:  1.16; xent: 0.15; lr: 1.00000; 5104/7347 tok/s;     81 sec\n",
      "[2021-01-30 02:49:26,529 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:26,537 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:27,465 INFO] Step 4200/ 5000; acc:  96.94; ppl:  1.16; xent: 0.15; lr: 1.00000; 5165/7435 tok/s;     82 sec\n",
      "[2021-01-30 02:49:27,466 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:27,474 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:28,422 INFO] Step 4250/ 5000; acc:  96.80; ppl:  1.18; xent: 0.17; lr: 1.00000; 5056/7278 tok/s;     83 sec\n",
      "[2021-01-30 02:49:28,423 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:28,432 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:29,431 INFO] Step 4300/ 5000; acc:  96.45; ppl:  1.19; xent: 0.17; lr: 1.00000; 4798/6906 tok/s;     84 sec\n",
      "[2021-01-30 02:49:29,431 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:29,440 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:30,477 INFO] Step 4350/ 5000; acc:  97.06; ppl:  1.16; xent: 0.15; lr: 1.00000; 4625/6657 tok/s;     85 sec\n",
      "[2021-01-30 02:49:30,477 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:30,486 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:31,480 INFO] Step 4400/ 5000; acc:  96.83; ppl:  1.19; xent: 0.17; lr: 1.00000; 4825/6945 tok/s;     86 sec\n",
      "[2021-01-30 02:49:31,480 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:31,489 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:32,448 INFO] Step 4450/ 5000; acc:  96.04; ppl:  1.21; xent: 0.19; lr: 1.00000; 5000/7197 tok/s;     87 sec\n",
      "[2021-01-30 02:49:32,448 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:32,457 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:33,395 INFO] Step 4500/ 5000; acc:  96.37; ppl:  1.20; xent: 0.18; lr: 1.00000; 5107/7352 tok/s;     88 sec\n",
      "[2021-01-30 02:49:33,396 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:33,404 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:34,378 INFO] Step 4550/ 5000; acc:  95.61; ppl:  1.25; xent: 0.22; lr: 1.00000; 4923/7086 tok/s;     89 sec\n",
      "[2021-01-30 02:49:34,379 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:34,387 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:35,313 INFO] Step 4600/ 5000; acc:  96.28; ppl:  1.22; xent: 0.20; lr: 1.00000; 5177/7452 tok/s;     90 sec\n",
      "[2021-01-30 02:49:35,313 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:35,326 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:36,252 INFO] Step 4650/ 5000; acc:  95.52; ppl:  1.27; xent: 0.24; lr: 1.00000; 5153/7418 tok/s;     91 sec\n",
      "[2021-01-30 02:49:36,252 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:36,261 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:37,203 INFO] Step 4700/ 5000; acc:  95.95; ppl:  1.25; xent: 0.22; lr: 1.00000; 5087/7322 tok/s;     92 sec\n",
      "[2021-01-30 02:49:37,204 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:37,231 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:38,184 INFO] Step 4750/ 5000; acc:  95.81; ppl:  1.25; xent: 0.22; lr: 1.00000; 4933/7101 tok/s;     93 sec\n",
      "[2021-01-30 02:49:38,185 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:38,193 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:39,140 INFO] Step 4800/ 5000; acc:  95.99; ppl:  1.24; xent: 0.21; lr: 1.00000; 5063/7288 tok/s;     93 sec\n",
      "[2021-01-30 02:49:39,140 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:39,149 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:40,129 INFO] Step 4850/ 5000; acc:  96.12; ppl:  1.23; xent: 0.20; lr: 1.00000; 4893/7043 tok/s;     94 sec\n",
      "[2021-01-30 02:49:40,129 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:40,139 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:41,076 INFO] Step 4900/ 5000; acc:  96.38; ppl:  1.22; xent: 0.20; lr: 1.00000; 5110/7356 tok/s;     95 sec\n",
      "[2021-01-30 02:49:41,076 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:41,085 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:42,023 INFO] Step 4950/ 5000; acc:  95.52; ppl:  1.25; xent: 0.22; lr: 1.00000; 5107/7352 tok/s;     96 sec\n",
      "[2021-01-30 02:49:42,024 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_8/processed.train.0.pt\n",
      "[2021-01-30 02:49:42,032 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:43,003 INFO] Step 5000/ 5000; acc:  96.04; ppl:  1.23; xent: 0.21; lr: 1.00000; 4940/7111 tok/s;     97 sec\n",
      "[2021-01-30 02:49:43,004 INFO] Saving checkpoint drive/MyDrive/EnglishToleranceBaseline/output/english_rnn_model_1000_8_step_5000.pt\n",
      "[2021-01-30 02:49:44,707 INFO]  * src vocab size = 43\n",
      "[2021-01-30 02:49:44,707 INFO]  * tgt vocab size = 45\n",
      "[2021-01-30 02:49:44,707 INFO] Building model...\n",
      "[2021-01-30 02:49:49,014 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(43, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(300, 100, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(45, 300, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(400, 100)\n",
      "        (1): LSTMCell(100, 100)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (linear_out): Linear(in_features=200, out_features=100, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=45, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-01-30 02:49:49,014 INFO] encoder: 254500\n",
      "[2021-01-30 02:49:49,014 INFO] decoder: 329645\n",
      "[2021-01-30 02:49:49,014 INFO] * number of parameters: 584145\n",
      "[2021-01-30 02:49:49,017 INFO] Starting training on GPU: [0]\n",
      "[2021-01-30 02:49:49,017 INFO] Start training loop and validate every 10000 steps...\n",
      "[2021-01-30 02:49:49,018 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:49:49,650 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:50,617 INFO] Step 50/ 5000; acc:  13.16; ppl: 31.95; xent: 3.46; lr: 1.00000; 3015/4338 tok/s;      2 sec\n",
      "[2021-01-30 02:49:50,617 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:49:50,626 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:51,598 INFO] Step 100/ 5000; acc:  21.59; ppl: 19.04; xent: 2.95; lr: 1.00000; 4916/7071 tok/s;      3 sec\n",
      "[2021-01-30 02:49:51,599 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:49:51,607 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:52,542 INFO] Step 150/ 5000; acc:  22.53; ppl: 17.24; xent: 2.85; lr: 1.00000; 5113/7355 tok/s;      4 sec\n",
      "[2021-01-30 02:49:52,542 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:49:52,551 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:53,485 INFO] Step 200/ 5000; acc:  24.36; ppl: 15.16; xent: 2.72; lr: 1.00000; 5112/7354 tok/s;      4 sec\n",
      "[2021-01-30 02:49:53,486 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:49:53,494 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:54,425 INFO] Step 250/ 5000; acc:  28.32; ppl: 13.30; xent: 2.59; lr: 1.00000; 5135/7387 tok/s;      5 sec\n",
      "[2021-01-30 02:49:54,425 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:49:54,433 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:55,376 INFO] Step 300/ 5000; acc:  33.22; ppl: 10.99; xent: 2.40; lr: 1.00000; 5074/7299 tok/s;      6 sec\n",
      "[2021-01-30 02:49:55,376 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:49:55,404 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:56,385 INFO] Step 350/ 5000; acc:  35.90; ppl:  9.63; xent: 2.26; lr: 1.00000; 4778/6874 tok/s;      7 sec\n",
      "[2021-01-30 02:49:56,385 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:49:56,393 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:57,328 INFO] Step 400/ 5000; acc:  38.97; ppl:  8.22; xent: 2.11; lr: 1.00000; 5118/7363 tok/s;      8 sec\n",
      "[2021-01-30 02:49:57,328 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:49:57,336 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:58,309 INFO] Step 450/ 5000; acc:  42.69; ppl:  7.14; xent: 1.97; lr: 1.00000; 4916/7072 tok/s;      9 sec\n",
      "[2021-01-30 02:49:58,309 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:49:58,317 INFO] number of examples: 1000\n",
      "[2021-01-30 02:49:59,245 INFO] Step 500/ 5000; acc:  45.53; ppl:  6.43; xent: 1.86; lr: 1.00000; 5153/7412 tok/s;     10 sec\n",
      "[2021-01-30 02:49:59,245 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:49:59,254 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:00,193 INFO] Step 550/ 5000; acc:  48.76; ppl:  5.68; xent: 1.74; lr: 1.00000; 5087/7318 tok/s;     11 sec\n",
      "[2021-01-30 02:50:00,194 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:00,202 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:01,129 INFO] Step 600/ 5000; acc:  52.85; ppl:  4.87; xent: 1.58; lr: 1.00000; 5158/7419 tok/s;     12 sec\n",
      "[2021-01-30 02:50:01,129 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:01,137 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:02,116 INFO] Step 650/ 5000; acc:  56.83; ppl:  4.29; xent: 1.46; lr: 1.00000; 4885/7027 tok/s;     13 sec\n",
      "[2021-01-30 02:50:02,116 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:02,125 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:03,064 INFO] Step 700/ 5000; acc:  63.52; ppl:  3.43; xent: 1.23; lr: 1.00000; 5087/7318 tok/s;     14 sec\n",
      "[2021-01-30 02:50:03,065 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:03,073 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:04,025 INFO] Step 750/ 5000; acc:  69.80; ppl:  2.83; xent: 1.04; lr: 1.00000; 5024/7227 tok/s;     15 sec\n",
      "[2021-01-30 02:50:04,025 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:04,032 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:04,966 INFO] Step 800/ 5000; acc:  75.48; ppl:  2.33; xent: 0.85; lr: 1.00000; 5127/7375 tok/s;     16 sec\n",
      "[2021-01-30 02:50:04,966 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:04,974 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:05,912 INFO] Step 850/ 5000; acc:  81.31; ppl:  1.96; xent: 0.68; lr: 1.00000; 5097/7332 tok/s;     17 sec\n",
      "[2021-01-30 02:50:05,912 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:05,921 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:06,874 INFO] Step 900/ 5000; acc:  85.05; ppl:  1.75; xent: 0.56; lr: 1.00000; 5012/7211 tok/s;     18 sec\n",
      "[2021-01-30 02:50:06,875 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:06,883 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:07,841 INFO] Step 950/ 5000; acc:  87.68; ppl:  1.56; xent: 0.45; lr: 1.00000; 4990/7178 tok/s;     19 sec\n",
      "[2021-01-30 02:50:07,841 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:07,849 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:08,786 INFO] Step 1000/ 5000; acc:  88.48; ppl:  1.55; xent: 0.44; lr: 1.00000; 5106/7345 tok/s;     20 sec\n",
      "[2021-01-30 02:50:08,786 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:08,795 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:09,724 INFO] Step 1050/ 5000; acc:  91.11; ppl:  1.40; xent: 0.34; lr: 1.00000; 5141/7395 tok/s;     21 sec\n",
      "[2021-01-30 02:50:09,725 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:09,752 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:10,697 INFO] Step 1100/ 5000; acc:  92.01; ppl:  1.36; xent: 0.30; lr: 1.00000; 4959/7134 tok/s;     22 sec\n",
      "[2021-01-30 02:50:10,697 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:10,706 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:11,640 INFO] Step 1150/ 5000; acc:  93.08; ppl:  1.29; xent: 0.26; lr: 1.00000; 5116/7360 tok/s;     23 sec\n",
      "[2021-01-30 02:50:11,640 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:11,648 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:12,605 INFO] Step 1200/ 5000; acc:  93.66; ppl:  1.29; xent: 0.26; lr: 1.00000; 4997/7189 tok/s;     24 sec\n",
      "[2021-01-30 02:50:12,606 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:12,614 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:13,614 INFO] Step 1250/ 5000; acc:  93.89; ppl:  1.26; xent: 0.23; lr: 1.00000; 4784/6881 tok/s;     25 sec\n",
      "[2021-01-30 02:50:13,614 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:13,622 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:14,562 INFO] Step 1300/ 5000; acc:  93.76; ppl:  1.26; xent: 0.23; lr: 1.00000; 5089/7320 tok/s;     26 sec\n",
      "[2021-01-30 02:50:14,562 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:14,570 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:15,509 INFO] Step 1350/ 5000; acc:  94.29; ppl:  1.26; xent: 0.23; lr: 1.00000; 5091/7323 tok/s;     26 sec\n",
      "[2021-01-30 02:50:15,510 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:15,518 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:16,477 INFO] Step 1400/ 5000; acc:  93.61; ppl:  1.28; xent: 0.25; lr: 1.00000; 4987/7174 tok/s;     27 sec\n",
      "[2021-01-30 02:50:16,477 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:16,485 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:17,429 INFO] Step 1450/ 5000; acc:  95.03; ppl:  1.21; xent: 0.19; lr: 1.00000; 5065/7286 tok/s;     28 sec\n",
      "[2021-01-30 02:50:17,429 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:17,438 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:18,374 INFO] Step 1500/ 5000; acc:  95.52; ppl:  1.18; xent: 0.17; lr: 1.00000; 5103/7340 tok/s;     29 sec\n",
      "[2021-01-30 02:50:18,375 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:18,383 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:19,308 INFO] Step 1550/ 5000; acc:  96.05; ppl:  1.16; xent: 0.15; lr: 1.00000; 5168/7435 tok/s;     30 sec\n",
      "[2021-01-30 02:50:19,308 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:19,316 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:20,269 INFO] Step 1600/ 5000; acc:  95.24; ppl:  1.22; xent: 0.20; lr: 1.00000; 5018/7219 tok/s;     31 sec\n",
      "[2021-01-30 02:50:20,269 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:20,277 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:21,241 INFO] Step 1650/ 5000; acc:  95.53; ppl:  1.20; xent: 0.18; lr: 1.00000; 4964/7141 tok/s;     32 sec\n",
      "[2021-01-30 02:50:21,241 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:21,250 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:22,191 INFO] Step 1700/ 5000; acc:  96.24; ppl:  1.15; xent: 0.14; lr: 1.00000; 5078/7306 tok/s;     33 sec\n",
      "[2021-01-30 02:50:22,191 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:22,200 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:23,136 INFO] Step 1750/ 5000; acc:  96.32; ppl:  1.15; xent: 0.14; lr: 1.00000; 5103/7340 tok/s;     34 sec\n",
      "[2021-01-30 02:50:23,136 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:23,144 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:24,068 INFO] Step 1800/ 5000; acc:  95.72; ppl:  1.18; xent: 0.17; lr: 1.00000; 5175/7445 tok/s;     35 sec\n",
      "[2021-01-30 02:50:24,068 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:24,094 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:25,081 INFO] Step 1850/ 5000; acc:  96.05; ppl:  1.16; xent: 0.15; lr: 1.00000; 4764/6854 tok/s;     36 sec\n",
      "[2021-01-30 02:50:25,081 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:25,089 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:26,088 INFO] Step 1900/ 5000; acc:  96.43; ppl:  1.15; xent: 0.14; lr: 1.00000; 4788/6888 tok/s;     37 sec\n",
      "[2021-01-30 02:50:26,088 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:26,097 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:27,040 INFO] Step 1950/ 5000; acc:  96.51; ppl:  1.15; xent: 0.14; lr: 1.00000; 5069/7292 tok/s;     38 sec\n",
      "[2021-01-30 02:50:27,040 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:27,048 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:27,979 INFO] Step 2000/ 5000; acc:  96.30; ppl:  1.16; xent: 0.15; lr: 1.00000; 5136/7388 tok/s;     39 sec\n",
      "[2021-01-30 02:50:27,979 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:27,988 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:28,938 INFO] Step 2050/ 5000; acc:  96.31; ppl:  1.17; xent: 0.15; lr: 1.00000; 5028/7233 tok/s;     40 sec\n",
      "[2021-01-30 02:50:28,938 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:28,948 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:29,901 INFO] Step 2100/ 5000; acc:  96.07; ppl:  1.16; xent: 0.15; lr: 1.00000; 5011/7208 tok/s;     41 sec\n",
      "[2021-01-30 02:50:29,901 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:29,910 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:30,873 INFO] Step 2150/ 5000; acc:  96.53; ppl:  1.16; xent: 0.15; lr: 1.00000; 4963/7140 tok/s;     42 sec\n",
      "[2021-01-30 02:50:30,873 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:30,882 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:31,829 INFO] Step 2200/ 5000; acc:  96.38; ppl:  1.16; xent: 0.15; lr: 1.00000; 5043/7254 tok/s;     43 sec\n",
      "[2021-01-30 02:50:31,830 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:31,838 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:32,768 INFO] Step 2250/ 5000; acc:  96.32; ppl:  1.15; xent: 0.14; lr: 1.00000; 5141/7395 tok/s;     44 sec\n",
      "[2021-01-30 02:50:32,768 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:32,776 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:33,681 INFO] Step 2300/ 5000; acc:  96.56; ppl:  1.17; xent: 0.15; lr: 1.00000; 5284/7602 tok/s;     45 sec\n",
      "[2021-01-30 02:50:33,681 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:33,689 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:34,625 INFO] Step 2350/ 5000; acc:  96.67; ppl:  1.15; xent: 0.14; lr: 1.00000; 5111/7353 tok/s;     46 sec\n",
      "[2021-01-30 02:50:34,625 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:34,632 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:35,567 INFO] Step 2400/ 5000; acc:  96.77; ppl:  1.16; xent: 0.15; lr: 1.00000; 5116/7359 tok/s;     47 sec\n",
      "[2021-01-30 02:50:35,568 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:35,576 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:36,534 INFO] Step 2450/ 5000; acc:  96.47; ppl:  1.16; xent: 0.15; lr: 1.00000; 4989/7176 tok/s;     48 sec\n",
      "[2021-01-30 02:50:36,535 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:36,543 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:37,522 INFO] Step 2500/ 5000; acc:  96.41; ppl:  1.16; xent: 0.15; lr: 1.00000; 4886/7028 tok/s;     49 sec\n",
      "[2021-01-30 02:50:37,522 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:37,549 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:38,507 INFO] Step 2550/ 5000; acc:  96.92; ppl:  1.14; xent: 0.13; lr: 1.00000; 4897/7045 tok/s;     49 sec\n",
      "[2021-01-30 02:50:38,507 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:38,515 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:39,453 INFO] Step 2600/ 5000; acc:  96.86; ppl:  1.13; xent: 0.13; lr: 1.00000; 5097/7332 tok/s;     50 sec\n",
      "[2021-01-30 02:50:39,453 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:39,462 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:40,408 INFO] Step 2650/ 5000; acc:  96.47; ppl:  1.16; xent: 0.15; lr: 1.00000; 5052/7267 tok/s;     51 sec\n",
      "[2021-01-30 02:50:40,408 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:40,417 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:41,373 INFO] Step 2700/ 5000; acc:  96.64; ppl:  1.15; xent: 0.14; lr: 1.00000; 5002/7196 tok/s;     52 sec\n",
      "[2021-01-30 02:50:41,373 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:41,382 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:42,379 INFO] Step 2750/ 5000; acc:  97.10; ppl:  1.14; xent: 0.13; lr: 1.00000; 4791/6892 tok/s;     53 sec\n",
      "[2021-01-30 02:50:42,380 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:42,389 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:43,346 INFO] Step 2800/ 5000; acc:  96.94; ppl:  1.14; xent: 0.13; lr: 1.00000; 4988/7176 tok/s;     54 sec\n",
      "[2021-01-30 02:50:43,347 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:43,356 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:44,347 INFO] Step 2850/ 5000; acc:  96.05; ppl:  1.19; xent: 0.18; lr: 1.00000; 4821/6935 tok/s;     55 sec\n",
      "[2021-01-30 02:50:44,347 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:44,357 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:45,314 INFO] Step 2900/ 5000; acc:  96.50; ppl:  1.18; xent: 0.17; lr: 1.00000; 4990/7178 tok/s;     56 sec\n",
      "[2021-01-30 02:50:45,314 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:45,322 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:46,287 INFO] Step 2950/ 5000; acc:  96.37; ppl:  1.17; xent: 0.16; lr: 1.00000; 4957/7131 tok/s;     57 sec\n",
      "[2021-01-30 02:50:46,287 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:46,295 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:47,324 INFO] Step 3000/ 5000; acc:  97.03; ppl:  1.14; xent: 0.14; lr: 1.00000; 4650/6689 tok/s;     58 sec\n",
      "[2021-01-30 02:50:47,324 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:47,333 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:48,307 INFO] Step 3050/ 5000; acc:  97.02; ppl:  1.13; xent: 0.13; lr: 1.00000; 4908/7060 tok/s;     59 sec\n",
      "[2021-01-30 02:50:48,307 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:48,316 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:49,320 INFO] Step 3100/ 5000; acc:  96.31; ppl:  1.17; xent: 0.15; lr: 1.00000; 4762/6850 tok/s;     60 sec\n",
      "[2021-01-30 02:50:49,320 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:49,329 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:50,351 INFO] Step 3150/ 5000; acc:  96.50; ppl:  1.16; xent: 0.14; lr: 1.00000; 4680/6732 tok/s;     61 sec\n",
      "[2021-01-30 02:50:50,351 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:50,360 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:51,324 INFO] Step 3200/ 5000; acc:  96.21; ppl:  1.19; xent: 0.17; lr: 1.00000; 4960/7136 tok/s;     62 sec\n",
      "[2021-01-30 02:50:51,324 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:51,332 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:52,348 INFO] Step 3250/ 5000; acc:  96.77; ppl:  1.16; xent: 0.15; lr: 1.00000; 4707/6771 tok/s;     63 sec\n",
      "[2021-01-30 02:50:52,349 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:52,376 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:53,340 INFO] Step 3300/ 5000; acc:  96.61; ppl:  1.15; xent: 0.14; lr: 1.00000; 4867/7001 tok/s;     64 sec\n",
      "[2021-01-30 02:50:53,340 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:53,350 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:54,291 INFO] Step 3350/ 5000; acc:  96.99; ppl:  1.15; xent: 0.14; lr: 1.00000; 5071/7294 tok/s;     65 sec\n",
      "[2021-01-30 02:50:54,291 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:54,299 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:55,250 INFO] Step 3400/ 5000; acc:  96.90; ppl:  1.18; xent: 0.17; lr: 1.00000; 5027/7232 tok/s;     66 sec\n",
      "[2021-01-30 02:50:55,251 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:55,260 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:56,176 INFO] Step 3450/ 5000; acc:  96.68; ppl:  1.16; xent: 0.15; lr: 1.00000; 5210/7494 tok/s;     67 sec\n",
      "[2021-01-30 02:50:56,177 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:56,185 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:57,124 INFO] Step 3500/ 5000; acc:  97.19; ppl:  1.14; xent: 0.13; lr: 1.00000; 5092/7326 tok/s;     68 sec\n",
      "[2021-01-30 02:50:57,124 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:57,132 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:58,083 INFO] Step 3550/ 5000; acc:  96.57; ppl:  1.17; xent: 0.16; lr: 1.00000; 5028/7232 tok/s;     69 sec\n",
      "[2021-01-30 02:50:58,083 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:58,091 INFO] number of examples: 1000\n",
      "[2021-01-30 02:50:59,047 INFO] Step 3600/ 5000; acc:  96.64; ppl:  1.18; xent: 0.16; lr: 1.00000; 5004/7199 tok/s;     70 sec\n",
      "[2021-01-30 02:50:59,047 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:50:59,056 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:00,013 INFO] Step 3650/ 5000; acc:  96.31; ppl:  1.19; xent: 0.17; lr: 1.00000; 4992/7182 tok/s;     71 sec\n",
      "[2021-01-30 02:51:00,014 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:00,024 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:00,964 INFO] Step 3700/ 5000; acc:  96.92; ppl:  1.15; xent: 0.14; lr: 1.00000; 5072/7297 tok/s;     72 sec\n",
      "[2021-01-30 02:51:00,965 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:00,973 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:01,922 INFO] Step 3750/ 5000; acc:  96.83; ppl:  1.17; xent: 0.15; lr: 1.00000; 5036/7244 tok/s;     73 sec\n",
      "[2021-01-30 02:51:01,923 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:01,930 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:02,857 INFO] Step 3800/ 5000; acc:  96.74; ppl:  1.18; xent: 0.17; lr: 1.00000; 5161/7425 tok/s;     74 sec\n",
      "[2021-01-30 02:51:02,857 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:02,865 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:03,807 INFO] Step 3850/ 5000; acc:  96.71; ppl:  1.16; xent: 0.15; lr: 1.00000; 5078/7305 tok/s;     75 sec\n",
      "[2021-01-30 02:51:03,807 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:03,816 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:04,776 INFO] Step 3900/ 5000; acc:  97.07; ppl:  1.16; xent: 0.15; lr: 1.00000; 4977/7160 tok/s;     76 sec\n",
      "[2021-01-30 02:51:04,776 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:04,785 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:05,753 INFO] Step 3950/ 5000; acc:  96.90; ppl:  1.15; xent: 0.14; lr: 1.00000; 4939/7105 tok/s;     77 sec\n",
      "[2021-01-30 02:51:05,753 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:05,761 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:06,733 INFO] Step 4000/ 5000; acc:  96.67; ppl:  1.18; xent: 0.17; lr: 1.00000; 4922/7081 tok/s;     78 sec\n",
      "[2021-01-30 02:51:06,733 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:06,761 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:07,714 INFO] Step 4050/ 5000; acc:  96.41; ppl:  1.20; xent: 0.18; lr: 1.00000; 4918/7074 tok/s;     79 sec\n",
      "[2021-01-30 02:51:07,714 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:07,723 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:08,698 INFO] Step 4100/ 5000; acc:  96.57; ppl:  1.17; xent: 0.16; lr: 1.00000; 4899/7047 tok/s;     80 sec\n",
      "[2021-01-30 02:51:08,699 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:08,707 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:09,664 INFO] Step 4150/ 5000; acc:  96.77; ppl:  1.16; xent: 0.15; lr: 1.00000; 4995/7185 tok/s;     81 sec\n",
      "[2021-01-30 02:51:09,664 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:09,672 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:10,640 INFO] Step 4200/ 5000; acc:  96.70; ppl:  1.16; xent: 0.15; lr: 1.00000; 4946/7115 tok/s;     82 sec\n",
      "[2021-01-30 02:51:10,640 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:10,648 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:11,603 INFO] Step 4250/ 5000; acc:  96.31; ppl:  1.22; xent: 0.20; lr: 1.00000; 5006/7201 tok/s;     83 sec\n",
      "[2021-01-30 02:51:11,603 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:11,612 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:12,605 INFO] Step 4300/ 5000; acc:  97.02; ppl:  1.16; xent: 0.15; lr: 1.00000; 4816/6928 tok/s;     84 sec\n",
      "[2021-01-30 02:51:12,605 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:12,614 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:13,590 INFO] Step 4350/ 5000; acc:  96.63; ppl:  1.19; xent: 0.17; lr: 1.00000; 4896/7042 tok/s;     85 sec\n",
      "[2021-01-30 02:51:13,591 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:13,604 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:14,561 INFO] Step 4400/ 5000; acc:  96.81; ppl:  1.17; xent: 0.15; lr: 1.00000; 4972/7152 tok/s;     86 sec\n",
      "[2021-01-30 02:51:14,561 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:14,570 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:15,517 INFO] Step 4450/ 5000; acc:  96.53; ppl:  1.17; xent: 0.16; lr: 1.00000; 5042/7253 tok/s;     86 sec\n",
      "[2021-01-30 02:51:15,518 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:15,526 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:16,521 INFO] Step 4500/ 5000; acc:  96.68; ppl:  1.19; xent: 0.18; lr: 1.00000; 4807/6915 tok/s;     88 sec\n",
      "[2021-01-30 02:51:16,521 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:16,530 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:17,513 INFO] Step 4550/ 5000; acc:  96.99; ppl:  1.15; xent: 0.14; lr: 1.00000; 4865/6998 tok/s;     88 sec\n",
      "[2021-01-30 02:51:17,513 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:17,521 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:18,510 INFO] Step 4600/ 5000; acc:  96.92; ppl:  1.16; xent: 0.15; lr: 1.00000; 4839/6961 tok/s;     89 sec\n",
      "[2021-01-30 02:51:18,510 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:18,518 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:19,534 INFO] Step 4650/ 5000; acc:  96.14; ppl:  1.21; xent: 0.19; lr: 1.00000; 4710/6775 tok/s;     91 sec\n",
      "[2021-01-30 02:51:19,534 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:19,543 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:20,509 INFO] Step 4700/ 5000; acc:  96.80; ppl:  1.17; xent: 0.16; lr: 1.00000; 4947/7116 tok/s;     91 sec\n",
      "[2021-01-30 02:51:20,509 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:20,536 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:21,501 INFO] Step 4750/ 5000; acc:  96.70; ppl:  1.17; xent: 0.16; lr: 1.00000; 4864/6996 tok/s;     92 sec\n",
      "[2021-01-30 02:51:21,501 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:21,509 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:22,452 INFO] Step 4800/ 5000; acc:  96.66; ppl:  1.18; xent: 0.16; lr: 1.00000; 5073/7298 tok/s;     93 sec\n",
      "[2021-01-30 02:51:22,452 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:22,460 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:23,391 INFO] Step 4850/ 5000; acc:  96.79; ppl:  1.16; xent: 0.15; lr: 1.00000; 5136/7388 tok/s;     94 sec\n",
      "[2021-01-30 02:51:23,391 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:23,400 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:24,354 INFO] Step 4900/ 5000; acc:  96.94; ppl:  1.17; xent: 0.15; lr: 1.00000; 5007/7202 tok/s;     95 sec\n",
      "[2021-01-30 02:51:24,355 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:24,363 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:25,334 INFO] Step 4950/ 5000; acc:  97.06; ppl:  1.13; xent: 0.12; lr: 1.00000; 4926/7087 tok/s;     96 sec\n",
      "[2021-01-30 02:51:25,334 INFO] Loading dataset from drive/MyDrive/EnglishToleranceBaseline/processed_data/1000_9/processed.train.0.pt\n",
      "[2021-01-30 02:51:25,342 INFO] number of examples: 1000\n",
      "[2021-01-30 02:51:26,309 INFO] Step 5000/ 5000; acc:  96.86; ppl:  1.16; xent: 0.15; lr: 1.00000; 4944/7113 tok/s;     97 sec\n",
      "[2021-01-30 02:51:26,310 INFO] Saving checkpoint drive/MyDrive/EnglishToleranceBaseline/output/english_rnn_model_1000_9_step_5000.pt\n"
     ]
    }
   ],
   "source": [
    "for datasize in datasizes:\n",
    "  epochs, n_examples, batchsize,  = 100, int(datasize.split('_')[0]), 20\n",
    "  steps = str(int(epochs * n_examples / batchsize))\n",
    "\n",
    "  datadir = f'drive/MyDrive/EnglishToleranceBaseline/processed_data/{datasize}'\n",
    "  rnn_modelpath = f'{outdir}/english_rnn_model_{datasize}'\n",
    "  rnn_train_args = ' '.join([\n",
    "    f'-data {datadir}/processed',\n",
    "    '-save_model '+rnn_modelpath,\n",
    "    '-enc_layers 2',\n",
    "    '-dec_layers 2',\n",
    "    '-rnn_size 100',\n",
    "    '-batch_size 20',\n",
    "    '-word_vec_size 300',\n",
    "    '-gpu_ranks 0',\n",
    "    '-train_steps '+steps,\n",
    "    '-save_checkpoint_steps '+steps\n",
    "    ])\n",
    "\n",
    "  !python OpenNMT-py/train.py $rnn_train_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM11J8k9hmmB"
   },
   "source": [
    "## Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDyD7XU3hmI-",
    "outputId": "5111dfc7-177b-48ef-81e6-8fea41500897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "\n",
      "[2021-01-30 02:53:28,092 INFO] \n",
      "SENT 3: ['p', 'i', 'k']\n",
      "PRED 3: p i k t\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-01-30 02:53:28,092 INFO] \n",
      "SENT 4: ['ʧ', 'æ', 't']\n",
      "PRED 4: ʧ æ t ɪ d\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:28,092 INFO] \n",
      "SENT 5: ['s', 'k', 'ɪ', 'd']\n",
      "PRED 5: s k ɪ d ɪ d\n",
      "PRED SCORE: -0.4805\n",
      "\n",
      "[2021-01-30 02:53:28,092 INFO] \n",
      "SENT 6: ['s', 'ə', 'b', 't', 'ɹ', 'æ', 'k', 't']\n",
      "PRED 6: s ə b t ɹ æ k t ɪ d\n",
      "PRED SCORE: -0.0014\n",
      "\n",
      "[2021-01-30 02:53:28,093 INFO] \n",
      "SENT 7: ['n', 'o', 'ʊ']\n",
      "PRED 7: n o ʊ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,093 INFO] \n",
      "SENT 8: ['ɪ', 'n', 'f', 'ɔ', 'ɹ', 'm']\n",
      "PRED 8: ɪ n f ɔ ɹ m d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:28,093 INFO] \n",
      "SENT 9: ['s', 't', 'r', 'ə', 'g', 'ə', 'l']\n",
      "PRED 9: s t r ə g ə l d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,093 INFO] \n",
      "SENT 10: ['ʃ', 'ʌ', 'v', 'ə', 'l']\n",
      "PRED 10: ʃ ʌ v ə l d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:28,093 INFO] \n",
      "SENT 11: ['k', 'r', 'i', 'e', 'ɪ', 't']\n",
      "PRED 11: k r i e ɪ t ɪ d\n",
      "PRED SCORE: -0.1624\n",
      "\n",
      "[2021-01-30 02:53:28,094 INFO] \n",
      "SENT 12: ['ɪ', 'ɡ', 'z', 'ɪ', 's', 't']\n",
      "PRED 12: ɪ ɡ z z s t ɪ d\n",
      "PRED SCORE: -1.1467\n",
      "\n",
      "[2021-01-30 02:53:28,094 INFO] \n",
      "SENT 13: ['l', 'ɛ', 'd']\n",
      "PRED 13: l ɛ d ɪ d\n",
      "PRED SCORE: -0.0016\n",
      "\n",
      "[2021-01-30 02:53:28,094 INFO] \n",
      "SENT 14: ['t', 'i', 't', '͡', 'ʃ']\n",
      "PRED 14: t i t ͡ ʃ t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:28,094 INFO] \n",
      "SENT 15: ['b', 'ɑ', 'r', 'ʤ']\n",
      "PRED 15: b ɑ r ʤ d\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-01-30 02:53:28,094 INFO] \n",
      "SENT 16: ['g', 'r', 'e', 'ɪ', 't']\n",
      "PRED 16: g ɹ e ɪ t ɪ d\n",
      "PRED SCORE: -0.1918\n",
      "\n",
      "[2021-01-30 02:53:28,095 INFO] \n",
      "SENT 17: ['s', 'n', 'æ', 'ʧ']\n",
      "PRED 17: s n æ ʧ t\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:28,095 INFO] \n",
      "SENT 18: ['j', 'u', 'z']\n",
      "PRED 18: j u z d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,095 INFO] \n",
      "SENT 19: ['f', 'r', 'ɛ', 'ʃ', 'ə', 'n']\n",
      "PRED 19: f r ɛ ʃ ə n d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:28,095 INFO] \n",
      "SENT 20: ['θ', 'ə', 'n', 'd', 'ə', 'r']\n",
      "PRED 20: θ ə n d ə r d\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-01-30 02:53:28,095 INFO] \n",
      "SENT 21: ['s', 't', 'ɑ', 'ɹ', 't']\n",
      "PRED 21: s t ɑ ɹ t ɪ d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:28,096 INFO] \n",
      "SENT 22: ['ʧ', 'ɪ', 'p']\n",
      "PRED 22: ʧ ɪ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,097 INFO] \n",
      "SENT 23: ['k', 'ə', 'n', 'ɛ', 'k', 't']\n",
      "PRED 23: k ə n ɛ k t ɪ d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:28,097 INFO] \n",
      "SENT 24: ['s', 'n', 'ə', 'f']\n",
      "PRED 24: s n ə f t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,097 INFO] \n",
      "SENT 25: ['ə', 'b', 'æ', 'n', 'd', 'ə', 'n']\n",
      "PRED 25: ə b æ n d ə n d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:28,097 INFO] \n",
      "SENT 26: ['k', 'ɛ', 'r']\n",
      "PRED 26: k ɛ r d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:28,097 INFO] \n",
      "SENT 27: ['t', 'æ', 'k', 'ə', 'l']\n",
      "PRED 27: t æ k ə l d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,098 INFO] \n",
      "SENT 28: ['s', 'm', 'ɛ', 'l']\n",
      "PRED 28: s m ɛ l d\n",
      "PRED SCORE: -0.2976\n",
      "\n",
      "[2021-01-30 02:53:28,098 INFO] \n",
      "SENT 29: ['l', 'ʊ', 'k']\n",
      "PRED 29: l ʊ k t\n",
      "PRED SCORE: -0.5073\n",
      "\n",
      "[2021-01-30 02:53:28,098 INFO] \n",
      "SENT 30: ['h', 'ɔ', 'l']\n",
      "PRED 30: h ɔ l d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,184 INFO] \n",
      "SENT 31: ['b', 'i', 'm']\n",
      "PRED 31: b i m d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:28,185 INFO] \n",
      "SENT 32: ['l', 'e', 'ɪ', 'ə', 'r']\n",
      "PRED 32: l e ɪ ə r d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,185 INFO] \n",
      "SENT 33: ['l', 'ɪ', 'f', 't']\n",
      "PRED 33: l ɪ f t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,185 INFO] \n",
      "SENT 34: ['t', 'ɹ', 'ɪ', 'm']\n",
      "PRED 34: t ɹ ʌ m\n",
      "PRED SCORE: -0.0061\n",
      "\n",
      "[2021-01-30 02:53:28,185 INFO] \n",
      "SENT 35: ['ʃ', 'ʌ', 'v']\n",
      "PRED 35: ʃ ʌ v d\n",
      "PRED SCORE: -0.0012\n",
      "\n",
      "[2021-01-30 02:53:28,186 INFO] \n",
      "SENT 36: ['k', 'æ', 'p', 's', 'a', 'ɪ', 'z']\n",
      "PRED 36: k æ p s a ɪ z d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,186 INFO] \n",
      "SENT 37: ['s', 'i', 'z']\n",
      "PRED 37: s i z d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:28,186 INFO] \n",
      "SENT 38: ['ʧ', 'e', 'ɪ', 's']\n",
      "PRED 38: ʧ e ɪ s t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,186 INFO] \n",
      "SENT 39: ['g', 'u', 'f']\n",
      "PRED 39: g u f t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,187 INFO] \n",
      "SENT 40: ['h', 'ʌ', 'm']\n",
      "PRED 40: h ʌ m d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,187 INFO] \n",
      "SENT 41: ['b', 'r', 'e', 'ɪ', 'k']\n",
      "PRED 41: b r e ɪ k t\n",
      "PRED SCORE: -0.0157\n",
      "\n",
      "[2021-01-30 02:53:28,187 INFO] \n",
      "SENT 42: ['æ', 'g', 'r', 'ə', 'v', 'e', 'ɪ', 't']\n",
      "PRED 42: æ g r ə v e ɪ t ɪ d\n",
      "PRED SCORE: -0.0150\n",
      "\n",
      "[2021-01-30 02:53:28,187 INFO] \n",
      "SENT 43: ['ɪ', 'v', 'ɑ', 'l', 'v']\n",
      "PRED 43: ɪ l v ɑ l v d\n",
      "PRED SCORE: -0.0344\n",
      "\n",
      "[2021-01-30 02:53:28,187 INFO] \n",
      "SENT 44: ['ɪ', 'n', 's', 'ɪ', 's', 't']\n",
      "PRED 44: ɪ n s t ɪ s t ɪ d\n",
      "PRED SCORE: -0.0050\n",
      "\n",
      "[2021-01-30 02:53:28,188 INFO] \n",
      "SENT 45: ['ɡ', 'l', 'u']\n",
      "PRED 45: ɡ l u d\n",
      "PRED SCORE: -0.0030\n",
      "\n",
      "[2021-01-30 02:53:28,188 INFO] \n",
      "SENT 46: ['ɪ', 'n', 'd', 'u', 's']\n",
      "PRED 46: ɪ n d u s t\n",
      "PRED SCORE: -0.0106\n",
      "\n",
      "[2021-01-30 02:53:28,188 INFO] \n",
      "SENT 47: ['l', 'a', 'ɪ']\n",
      "PRED 47: l a ɪ d\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-01-30 02:53:28,188 INFO] \n",
      "SENT 48: ['k', 'r', 'ɪ', 'p', 'ə', 'l']\n",
      "PRED 48: k r ɪ p ə l d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:28,188 INFO] \n",
      "SENT 49: ['p', 'r', 'ə', 'v', 'a', 'ɪ', 'd']\n",
      "PRED 49: p r ə v a ɪ d ɪ d\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-01-30 02:53:28,189 INFO] \n",
      "SENT 50: ['s', 'k', 'ə', 'l', 'p', 't']\n",
      "PRED 50: s k ə l p t ɪ d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:28,189 INFO] \n",
      "SENT 51: ['s', 't', 'r', 'a', 'ɪ', 'd']\n",
      "PRED 51: s t r ɪ d\n",
      "PRED SCORE: -0.2159\n",
      "\n",
      "[2021-01-30 02:53:28,189 INFO] \n",
      "SENT 52: ['d', 'r', 'e', 'ɪ', 'n']\n",
      "PRED 52: d r e ɪ n d\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-01-30 02:53:28,189 INFO] \n",
      "SENT 53: ['l', 'ɛ', 'n', 'd']\n",
      "PRED 53: l ɛ n d ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,189 INFO] \n",
      "SENT 54: ['l', 'a', 'ɪ', 'n']\n",
      "PRED 54: l a ɪ n d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,190 INFO] \n",
      "SENT 55: ['p', 'ɹ', 'ɪ', 'k']\n",
      "PRED 55: p ɹ ɪ k t\n",
      "PRED SCORE: -0.0043\n",
      "\n",
      "[2021-01-30 02:53:28,190 INFO] \n",
      "SENT 56: ['p', 'ɛ', 'ɡ']\n",
      "PRED 56: p ɛ ɡ d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:28,190 INFO] \n",
      "SENT 57: ['ə', 'k', 'ə', 'r']\n",
      "PRED 57: ə k ə r d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,190 INFO] \n",
      "SENT 58: ['s', 'm', 'æ', 'k']\n",
      "PRED 58: s m æ k t\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-01-30 02:53:28,190 INFO] \n",
      "SENT 59: ['ʃ', 'u', 't']\n",
      "PRED 59: ʃ u t ɪ d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:28,191 INFO] \n",
      "SENT 60: ['g', 'e', 'ɪ', 'n']\n",
      "PRED 60: g e ɪ n d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,280 INFO] \n",
      "SENT 61: ['d', 'ɪ', 'p', 'ɹ', 'ɛ', 's']\n",
      "PRED 61: d ɪ p ɹ ɛ s t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,281 INFO] \n",
      "SENT 62: ['ɪ', 'm', 'p', 'l', 'ɔ', 'ɪ']\n",
      "PRED 62: ɪ m p l ɔ ɪ d\n",
      "PRED SCORE: -0.1243\n",
      "\n",
      "[2021-01-30 02:53:28,281 INFO] \n",
      "SENT 63: ['ɪ', 't', '͡', 'ʃ']\n",
      "PRED 63: ɪ t ͡ ʃ t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,281 INFO] \n",
      "SENT 64: ['ɪ', 'n', 'd', 'ə', 'k', 'e', 'ɪ', 't']\n",
      "PRED 64: ɪ n d ə k e ɪ t ɪ d\n",
      "PRED SCORE: -0.0092\n",
      "\n",
      "[2021-01-30 02:53:28,281 INFO] \n",
      "SENT 65: ['s', 't', 'r', 'o', 'ʊ', 'l']\n",
      "PRED 65: s t r o ʊ l d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,282 INFO] \n",
      "SENT 66: ['n', 'ɪ', 'p']\n",
      "PRED 66: n ɪ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,282 INFO] \n",
      "SENT 67: ['t', 'ʌ', 'ɡ']\n",
      "PRED 67: t ʌ ɡ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,282 INFO] \n",
      "SENT 68: ['ɛ', 's', 'k', 'ɔ', 'ɹ', 't']\n",
      "PRED 68: ɛ s k ɔ ɹ t ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,282 INFO] \n",
      "SENT 69: ['r', 'u', 'ɪ', 'n']\n",
      "PRED 69: r u n ɪ n d\n",
      "PRED SCORE: -0.3962\n",
      "\n",
      "[2021-01-30 02:53:28,282 INFO] \n",
      "SENT 70: ['r', 'a', 'ɪ', 'z']\n",
      "PRED 70: r a ɪ z d\n",
      "PRED SCORE: -0.1110\n",
      "\n",
      "[2021-01-30 02:53:28,283 INFO] \n",
      "SENT 71: ['p', 'ʌ', 'f']\n",
      "PRED 71: p ʌ f t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,283 INFO] \n",
      "SENT 72: ['s', 't', 'e', 'ɪ', 'ʃ', 'ə', 'n']\n",
      "PRED 72: s t ə t ə n d ɪ d\n",
      "PRED SCORE: -0.2621\n",
      "\n",
      "[2021-01-30 02:53:28,283 INFO] \n",
      "SENT 73: ['ɪ', 'n', 't', 'ɹ', 'ə', 's', 't']\n",
      "PRED 73: ɪ n t ɹ ə s t ɪ d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:28,283 INFO] \n",
      "SENT 74: ['b', 'ɔ', 'l']\n",
      "PRED 74: b ɔ l d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,283 INFO] \n",
      "SENT 75: ['ɑ', 'k', 'j', 'ə', 'p', 'a', 'ɪ']\n",
      "PRED 75: ɑ k j ə p a ɪ d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:28,284 INFO] \n",
      "SENT 76: ['d', 'ɹ', 'u', 'l']\n",
      "PRED 76: d ɹ u l d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,284 INFO] \n",
      "SENT 77: ['t', 'u', 't']\n",
      "PRED 77: t u t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,284 INFO] \n",
      "SENT 78: ['s', 'k', 'æ', 'n']\n",
      "PRED 78: s k æ n d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,284 INFO] \n",
      "SENT 79: ['w', 'e', 'ɪ']\n",
      "PRED 79: w e ɪ d\n",
      "PRED SCORE: -0.0014\n",
      "\n",
      "[2021-01-30 02:53:28,284 INFO] \n",
      "SENT 80: ['l', 'ɪ', 's', 't']\n",
      "PRED 80: l ɪ s t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,285 INFO] \n",
      "SENT 81: ['l', 'u', 's']\n",
      "PRED 81: l u s t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:28,285 INFO] \n",
      "SENT 82: ['s', 'm', 'ə', 'ð', 'ə', 'r']\n",
      "PRED 82: s m ə ð ə r d\n",
      "PRED SCORE: -0.0362\n",
      "\n",
      "[2021-01-30 02:53:28,285 INFO] \n",
      "SENT 83: ['s', 'p', 'ɹ', 'ɪ', 'ŋ']\n",
      "PRED 83: s p ɹ ʌ ŋ\n",
      "PRED SCORE: -0.0015\n",
      "\n",
      "[2021-01-30 02:53:28,285 INFO] \n",
      "SENT 84: ['m', 'ɛ', 'r', 'i']\n",
      "PRED 84: m ɛ r i d\n",
      "PRED SCORE: -0.0617\n",
      "\n",
      "[2021-01-30 02:53:28,285 INFO] \n",
      "SENT 85: ['s', 'm', 'ɪ', 'r']\n",
      "PRED 85: s m ɪ r d\n",
      "PRED SCORE: -0.0018\n",
      "\n",
      "[2021-01-30 02:53:28,286 INFO] \n",
      "SENT 86: ['g', 'l', 'a', 'ɪ', 'd']\n",
      "PRED 86: g l a ɪ d ɪ d\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-01-30 02:53:28,286 INFO] \n",
      "SENT 87: ['d', 'ɪ', 'v', 'a', 'ɪ', 'd']\n",
      "PRED 87: d ɪ v a ɪ d ɪ d\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-01-30 02:53:28,286 INFO] \n",
      "SENT 88: ['r', 'ɑ', 't']\n",
      "PRED 88: r ɑ t ɪ d\n",
      "PRED SCORE: -0.4867\n",
      "\n",
      "[2021-01-30 02:53:28,286 INFO] \n",
      "SENT 89: ['ɛ', 'n', 'd', 'a', 'ʊ']\n",
      "PRED 89: ɛ n d a ʊ d\n",
      "PRED SCORE: -0.0048\n",
      "\n",
      "[2021-01-30 02:53:28,286 INFO] \n",
      "SENT 90: ['t', 'æ', 'k']\n",
      "PRED 90: t æ k t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,394 INFO] \n",
      "SENT 91: ['b', 'a', 'ʊ', 'n', 'd']\n",
      "PRED 91: b a ʊ n d ɪ d\n",
      "PRED SCORE: -0.0101\n",
      "\n",
      "[2021-01-30 02:53:28,395 INFO] \n",
      "SENT 92: ['f', 'l', 'ɛ', 'r']\n",
      "PRED 92: f l ɛ r d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:28,395 INFO] \n",
      "SENT 93: ['s', 'ə', 'r', 'a', 'ʊ', 'n', 'd']\n",
      "PRED 93: s ə r a ʊ n d ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,395 INFO] \n",
      "SENT 94: ['ɪ', 'k', 's', 'p', 'l', 'e', 'ɪ', 'n']\n",
      "PRED 94: ɪ k s p l e ɪ n d\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-01-30 02:53:28,396 INFO] \n",
      "SENT 95: ['b', 'l', 'ɪ', 'ŋ', 'k']\n",
      "PRED 95: b l æ ŋ k\n",
      "PRED SCORE: -0.9746\n",
      "\n",
      "[2021-01-30 02:53:28,396 INFO] \n",
      "SENT 96: ['h', 'ɪ', 'p', 'n', 'ə', 't', 'a', 'j', 'z']\n",
      "PRED 96: h ɪ p n ə t a j z d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:28,396 INFO] \n",
      "SENT 97: ['n', 'æ', 'p']\n",
      "PRED 97: n æ p t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,396 INFO] \n",
      "SENT 98: ['j', 'ɛ', 'l', 'p']\n",
      "PRED 98: j ɛ l p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,397 INFO] \n",
      "SENT 99: ['ɪ', 'm', 'b', 'ɛ', 'd']\n",
      "PRED 99: ɪ m b ɛ d ɪ d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:28,397 INFO] \n",
      "SENT 100: ['h', 'o', 'ʊ', 'l', 'd']\n",
      "PRED 100: h o ʊ l d ɪ d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:28,397 INFO] \n",
      "SENT 101: ['p', 'r', 'ɑ', 'm', 'ə', 's']\n",
      "PRED 101: p r ɑ m ə s t\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:28,397 INFO] \n",
      "SENT 102: ['p', 'l', 'i', 't']\n",
      "PRED 102: p l i t ɪ d\n",
      "PRED SCORE: -0.0647\n",
      "\n",
      "[2021-01-30 02:53:28,398 INFO] \n",
      "SENT 103: ['f', 'ɪ', 'k', 's']\n",
      "PRED 103: f ɪ k s t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,398 INFO] \n",
      "SENT 104: ['l', 'u', 'z']\n",
      "PRED 104: l u z d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,399 INFO] \n",
      "SENT 105: ['s', 'ə', 'r', 'k', 'ə', 'l']\n",
      "PRED 105: s ə r k ə l d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,399 INFO] \n",
      "SENT 106: ['ə', 'n', 'ɔ', 'ɪ']\n",
      "PRED 106: ə n ɔ ɪ d\n",
      "PRED SCORE: -0.0215\n",
      "\n",
      "[2021-01-30 02:53:28,399 INFO] \n",
      "SENT 107: ['h', 'a', 'ɪ', 'r']\n",
      "PRED 107: h a ɪ r d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,399 INFO] \n",
      "SENT 108: ['t', 'ɹ', 'ɪ', 'k']\n",
      "PRED 108: t ɹ ɪ k t\n",
      "PRED SCORE: -0.0129\n",
      "\n",
      "[2021-01-30 02:53:28,400 INFO] \n",
      "SENT 109: ['p', 'ɔ', 'ɪ', 'n', 't']\n",
      "PRED 109: p ɔ ɪ n t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,400 INFO] \n",
      "SENT 110: ['f', 'r', 'ə', 's', 't', 'r', 'e', 'ɪ', 't']\n",
      "PRED 110: f r ə s t r e ɪ t ɪ d\n",
      "PRED SCORE: -0.0148\n",
      "\n",
      "[2021-01-30 02:53:28,400 INFO] \n",
      "SENT 111: ['t', 'ɹ', 'ɛ', 'd']\n",
      "PRED 111: t ɹ ɛ d ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,400 INFO] \n",
      "SENT 112: ['d', 'u', 'p', 'l', 'ə', 'k', 'e', 'ɪ', 't']\n",
      "PRED 112: d u p l ə k e ɪ t ɪ d\n",
      "PRED SCORE: -0.9261\n",
      "\n",
      "[2021-01-30 02:53:28,400 INFO] \n",
      "SENT 113: ['d', 'ɛ', 'k', 'ə', 'r', 'e', 'ɪ', 't']\n",
      "PRED 113: d ɛ k ə r e ɪ t ɪ d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:28,401 INFO] \n",
      "SENT 114: ['d', 'o', 'ʊ', 'p']\n",
      "PRED 114: d o ʊ p t\n",
      "PRED SCORE: -0.0037\n",
      "\n",
      "[2021-01-30 02:53:28,401 INFO] \n",
      "SENT 115: ['b', 'æ', 'l', 'ə', 'n', 's']\n",
      "PRED 115: b æ l ə n s t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,401 INFO] \n",
      "SENT 116: ['w', 'ə', 'r', 'k']\n",
      "PRED 116: w ə r k t\n",
      "PRED SCORE: -0.0338\n",
      "\n",
      "[2021-01-30 02:53:28,401 INFO] \n",
      "SENT 117: ['s', 't', 'ɑ', 'ɹ']\n",
      "PRED 117: s t ɑ ɹ d\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-01-30 02:53:28,402 INFO] \n",
      "SENT 118: ['l', 'o', 'ʊ', 'k', 'e', 'ɪ', 't']\n",
      "PRED 118: l o ʊ k e ɪ t ɪ d\n",
      "PRED SCORE: -0.0032\n",
      "\n",
      "[2021-01-30 02:53:28,402 INFO] \n",
      "SENT 119: ['b', 'ɹ', 'u', 'm']\n",
      "PRED 119: b ɹ u m d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,402 INFO] \n",
      "SENT 120: ['t', '͡', 'ʃ', 'ɪ', 'ɹ']\n",
      "PRED 120: t ͡ ʃ ɪ ɹ d\n",
      "PRED SCORE: -0.0043\n",
      "\n",
      "[2021-01-30 02:53:28,497 INFO] \n",
      "SENT 121: ['s', 'ɔ', 'ɹ']\n",
      "PRED 121: s ɔ ɹ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,497 INFO] \n",
      "SENT 122: ['ʃ', 'ʌ', 'f', 'ə', 'l']\n",
      "PRED 122: ʃ ʌ f ə l d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,497 INFO] \n",
      "SENT 123: ['k', 'l', 'ʌ', 't', '͡', 'ʃ']\n",
      "PRED 123: k l ʌ t ͡ ʃ t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,498 INFO] \n",
      "SENT 124: ['k', 'ɑ', 'n', 'v', 'ə', 'r', 't']\n",
      "PRED 124: k ɑ n v ə r t ɪ d\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-01-30 02:53:28,498 INFO] \n",
      "SENT 125: ['m', 'æ', 'ʃ']\n",
      "PRED 125: m æ ʃ t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,499 INFO] \n",
      "SENT 126: ['ə', 'l', 'ɑ', 'ɹ', 'm']\n",
      "PRED 126: ə l ɑ ɹ m d\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:28,499 INFO] \n",
      "SENT 127: ['k', 'ə', 'm', 'ɪ', 't']\n",
      "PRED 127: k ə m ɪ t ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,499 INFO] \n",
      "SENT 128: ['s', 'k', 'o', 'ʊ', 'l', 'd']\n",
      "PRED 128: s k o ʊ l d ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,499 INFO] \n",
      "SENT 129: ['ʧ', 'ə', 'g']\n",
      "PRED 129: ʧ ə g d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,500 INFO] \n",
      "SENT 130: ['w', 'ɔ', 'k']\n",
      "PRED 130: w ɔ k t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,500 INFO] \n",
      "SENT 131: ['n', 'ɔ']\n",
      "PRED 131: n ɔ d\n",
      "PRED SCORE: -0.0069\n",
      "\n",
      "[2021-01-30 02:53:28,500 INFO] \n",
      "SENT 132: ['w', 'u', 'ʃ']\n",
      "PRED 132: w u ʃ t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,500 INFO] \n",
      "SENT 133: ['h', 'ə', 'v', 'ə', 'r']\n",
      "PRED 133: h ə v ə r d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:28,501 INFO] \n",
      "SENT 134: ['f', 'a', 'ɪ', 'n']\n",
      "PRED 134: f a ɪ n d\n",
      "PRED SCORE: -0.0082\n",
      "\n",
      "[2021-01-30 02:53:28,501 INFO] \n",
      "SENT 135: ['b', 'ʌ', 't', 'ə', 'n']\n",
      "PRED 135: b ʌ t ə n d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,501 INFO] \n",
      "SENT 136: ['t', 'r', 'e', 'ɪ', 'd']\n",
      "PRED 136: t r e ɪ d ɪ d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:28,501 INFO] \n",
      "SENT 137: ['p', 'ə', 'z', 'ɛ', 's']\n",
      "PRED 137: p ə z ɛ s t\n",
      "PRED SCORE: -0.0090\n",
      "\n",
      "[2021-01-30 02:53:28,501 INFO] \n",
      "SENT 138: ['ɹ', 'ɪ', 's', 'p', 'ɛ', 'k', 't']\n",
      "PRED 138: ɹ ɪ s p ɛ k t ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,502 INFO] \n",
      "SENT 139: ['t', 'ɔ', 'r', 'ʧ', 'ə', 'r']\n",
      "PRED 139: t ɔ r ʧ ə r d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:28,502 INFO] \n",
      "SENT 140: ['f', 'r', 'a', 'ʊ', 'n']\n",
      "PRED 140: f r a ʊ n d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,502 INFO] \n",
      "SENT 141: ['m', 'æ', 't', '͡', 'ʃ']\n",
      "PRED 141: m æ t ͡ ʃ t\n",
      "PRED SCORE: -0.4338\n",
      "\n",
      "[2021-01-30 02:53:28,502 INFO] \n",
      "SENT 142: ['d', 'o', 'ʊ', 'z']\n",
      "PRED 142: d o ʊ z d\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-01-30 02:53:28,503 INFO] \n",
      "SENT 143: ['f', 'ɔ', 'l', 't', 'ə', 'r']\n",
      "PRED 143: f ɛ l t ə r d\n",
      "PRED SCORE: -0.0155\n",
      "\n",
      "[2021-01-30 02:53:28,503 INFO] \n",
      "SENT 144: ['v', 'ɪ', 'd', 'i', 'o', 'ʊ']\n",
      "PRED 144: v ɪ d i d i d\n",
      "PRED SCORE: -0.0556\n",
      "\n",
      "[2021-01-30 02:53:28,503 INFO] \n",
      "SENT 145: ['ʃ', 'ɑ', 'k']\n",
      "PRED 145: ʃ ɑ k t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,503 INFO] \n",
      "SENT 146: ['r', 'ɪ', 'l', 'e', 'ɪ', 't']\n",
      "PRED 146: ɹ ɪ l e ɪ t ɪ d\n",
      "PRED SCORE: -0.0969\n",
      "\n",
      "[2021-01-30 02:53:28,503 INFO] \n",
      "SENT 147: ['p', 'ɪ', 'k', 'ə', 'l']\n",
      "PRED 147: p ɪ k ə l d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,504 INFO] \n",
      "SENT 148: ['b', 'ʌ', 'ɡ']\n",
      "PRED 148: b ʌ ɡ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,504 INFO] \n",
      "SENT 149: ['k', 'r', 'ə', 'n', 'ʧ']\n",
      "PRED 149: k r ə n ʧ t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,504 INFO] \n",
      "SENT 150: ['h', 'ɔ', 'ŋ', 'k']\n",
      "PRED 150: h ɔ ŋ k t\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:28,609 INFO] \n",
      "SENT 151: ['t', 'r', 'e', 'ɪ', 's']\n",
      "PRED 151: t r e ɪ s t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,610 INFO] \n",
      "SENT 152: ['m', 'ɪ', 'k', 's']\n",
      "PRED 152: m ɪ k s t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,610 INFO] \n",
      "SENT 153: ['ɪ', 'g', 'z', 'ɔ', 's', 't']\n",
      "PRED 153: ɪ g z ɔ s t ɪ d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:28,610 INFO] \n",
      "SENT 154: ['l', 'ɔ', 'n', 't', '͡', 'ʃ']\n",
      "PRED 154: l ɔ n t ͡ ʃ t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,610 INFO] \n",
      "SENT 155: ['z', 'u', 'm']\n",
      "PRED 155: z u m d\n",
      "PRED SCORE: -0.0117\n",
      "\n",
      "[2021-01-30 02:53:28,611 INFO] \n",
      "SENT 156: ['s', 'ɪ', 'ŋ']\n",
      "PRED 156: s æ ŋ\n",
      "PRED SCORE: -0.0011\n",
      "\n",
      "[2021-01-30 02:53:28,611 INFO] \n",
      "SENT 157: ['p', 'ɛ', 'p', 'ə', 'r']\n",
      "PRED 157: p ɛ p ə r d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,611 INFO] \n",
      "SENT 158: ['k', 'ɑ', 'm']\n",
      "PRED 158: k ɑ m d\n",
      "PRED SCORE: -0.0021\n",
      "\n",
      "[2021-01-30 02:53:28,611 INFO] \n",
      "SENT 159: ['k', 'ə', 'n', 'd', 'ɛ', 'n', 's']\n",
      "PRED 159: k ə n d ɛ n s t\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:28,612 INFO] \n",
      "SENT 160: ['s', 'k', 'ɹ', 'ʌ', 'b']\n",
      "PRED 160: s k ɹ ʌ b d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:28,612 INFO] \n",
      "SENT 161: ['r', 'i', 's', 'a', 'ɪ', 'k', 'ə', 'l']\n",
      "PRED 161: r i s a ɪ s a ɪ l d\n",
      "PRED SCORE: -0.3665\n",
      "\n",
      "[2021-01-30 02:53:28,612 INFO] \n",
      "SENT 162: ['ɪ', 'n', 'k', 'r', 'i', 's']\n",
      "PRED 162: ɪ n k r i s t\n",
      "PRED SCORE: -0.0020\n",
      "\n",
      "[2021-01-30 02:53:28,612 INFO] \n",
      "SENT 163: ['ə', 'd', 'ɹ', 'ɛ', 's']\n",
      "PRED 163: ə d ɹ ɛ s t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:28,612 INFO] \n",
      "SENT 164: ['m', 'ɛ', 'ʒ', 'ə', 'r']\n",
      "PRED 164: m ɛ ð ə r d\n",
      "PRED SCORE: -0.0128\n",
      "\n",
      "[2021-01-30 02:53:28,612 INFO] \n",
      "SENT 165: ['f', 'o', 'ʊ', 'l', 'd']\n",
      "PRED 165: f o ʊ l d ɪ d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:28,613 INFO] \n",
      "SENT 166: ['ɛ', 'n', 'l', 'ɑ', 'r', 'ʤ']\n",
      "PRED 166: ɛ n l ɑ n ʤ d\n",
      "PRED SCORE: -0.2391\n",
      "\n",
      "[2021-01-30 02:53:28,613 INFO] \n",
      "SENT 167: ['w', 'ɪ', 'p']\n",
      "PRED 167: w ɪ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,613 INFO] \n",
      "SENT 168: ['b', 'r', 'e', 'ɪ', 'd']\n",
      "PRED 168: b r e ɪ d ɪ d\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:28,613 INFO] \n",
      "SENT 169: ['t', 'æ', 'l', 'i']\n",
      "PRED 169: t æ l i d\n",
      "PRED SCORE: -0.1436\n",
      "\n",
      "[2021-01-30 02:53:28,613 INFO] \n",
      "SENT 170: ['ɪ', 'm', 'æ', 'ʤ', 'ə', 'n']\n",
      "PRED 170: ɪ m æ ʤ ə n d\n",
      "PRED SCORE: -0.0017\n",
      "\n",
      "[2021-01-30 02:53:28,614 INFO] \n",
      "SENT 171: ['n', 'ɑ', 'k']\n",
      "PRED 171: n ɑ k t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,614 INFO] \n",
      "SENT 172: ['r', 'i', 'ʧ']\n",
      "PRED 172: r i ʧ t\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:28,614 INFO] \n",
      "SENT 173: ['s', 't', 'ɪ', 'k']\n",
      "PRED 173: s t ɪ k t\n",
      "PRED SCORE: -0.0014\n",
      "\n",
      "[2021-01-30 02:53:28,614 INFO] \n",
      "SENT 174: ['p', 'ɛ', 'd', 'ə', 'l']\n",
      "PRED 174: p ɛ d ə l d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,614 INFO] \n",
      "SENT 175: ['t', 'w', 'ɪ', 's', 't']\n",
      "PRED 175: t w ɪ s t ɪ d\n",
      "PRED SCORE: -0.0034\n",
      "\n",
      "[2021-01-30 02:53:28,615 INFO] \n",
      "SENT 176: ['p', 'ə', 'r', 's', 'ə', 'v', 'ɪ', 'r']\n",
      "PRED 176: p ə r s ə v ɪ r d\n",
      "PRED SCORE: -0.0016\n",
      "\n",
      "[2021-01-30 02:53:28,615 INFO] \n",
      "SENT 177: ['t', 'r', 'a', 'ɪ']\n",
      "PRED 177: t r a ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,615 INFO] \n",
      "SENT 178: ['k', 'ə', 'm', 'p', 'l', 'i', 't']\n",
      "PRED 178: k ə m p l i t ɪ d\n",
      "PRED SCORE: -0.1545\n",
      "\n",
      "[2021-01-30 02:53:28,615 INFO] \n",
      "SENT 179: ['d', 'ɑ', 'k', 't', 'ɹ', '̩']\n",
      "PRED 179: d ɑ k t ɹ ̩ d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:28,615 INFO] \n",
      "SENT 180: ['m', 'ʌ', 'm', 'b', 'ə', 'l']\n",
      "PRED 180: m ʌ m b ə l d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:28,677 INFO] \n",
      "SENT 181: ['ɪ', 'n', 't', 'ə', 'r', 'v', 'j', 'u']\n",
      "PRED 181: ɪ n t ə r v j u d\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-01-30 02:53:28,677 INFO] \n",
      "SENT 182: ['s', 'ɪ', 'f', 't']\n",
      "PRED 182: s ɪ f t ɪ d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:28,678 INFO] \n",
      "SENT 183: ['f', 'e', 'ɪ', 'l']\n",
      "PRED 183: f e ɪ l d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,678 INFO] \n",
      "SENT 184: ['b', 'ɑ', 'b']\n",
      "PRED 184: b ɑ b d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,678 INFO] \n",
      "SENT 185: ['k', 'j', 'u']\n",
      "PRED 185: k j u d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,678 INFO] \n",
      "SENT 186: ['s', 'k', 'ɪ', 'm']\n",
      "PRED 186: s k æ m\n",
      "PRED SCORE: -0.0269\n",
      "\n",
      "[2021-01-30 02:53:28,678 INFO] \n",
      "SENT 187: ['p', 'æ', 't']\n",
      "PRED 187: p æ t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,679 INFO] \n",
      "SENT 188: ['b', 'ʌ', 'ŋ', 'ɡ', 'ə', 'l']\n",
      "PRED 188: b ʌ ŋ ɡ ə l d\n",
      "PRED SCORE: -0.0018\n",
      "\n",
      "[2021-01-30 02:53:28,679 INFO] \n",
      "SENT 189: ['p', 'e', 'ɪ', 'n', 't']\n",
      "PRED 189: p e ɪ n t ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,679 INFO] \n",
      "SENT 190: ['k', 'ɹ', 'o', 'w', 'k']\n",
      "PRED 190: k ɹ o w k t\n",
      "PRED SCORE: -0.0048\n",
      "\n",
      "[2021-01-30 02:53:28,679 INFO] \n",
      "SENT 191: ['d', 'ɛ', 'd', 'ɪ', 'k', 'e', 'ɪ', 't']\n",
      "PRED 191: d ɛ d ɪ k e ɪ t ɪ d\n",
      "PRED SCORE: -0.0034\n",
      "\n",
      "[2021-01-30 02:53:28,680 INFO] \n",
      "SENT 192: ['g', 'o', 'ʊ']\n",
      "PRED 192: g o ʊ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,680 INFO] \n",
      "SENT 193: ['h', 'ɛ', 'd']\n",
      "PRED 193: h ɛ d ɪ d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:28,680 INFO] \n",
      "SENT 194: ['n', 'ɪ', 'ɹ']\n",
      "PRED 194: n ɪ ɹ d\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-01-30 02:53:28,680 INFO] \n",
      "SENT 195: ['k', 'ə', 'n', 's', 'ɪ', 'd', 'ə', 'r']\n",
      "PRED 195: k ə n s ɪ d ə r d\n",
      "PRED SCORE: -0.0205\n",
      "\n",
      "[2021-01-30 02:53:28,680 INFO] \n",
      "SENT 196: ['s', 'k', 'æ', 't', 'ə', 'r']\n",
      "PRED 196: s k æ t ə r d\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-01-30 02:53:28,681 INFO] \n",
      "SENT 197: ['ɹ', 'ʌ', 'n']\n",
      "PRED 197: ɹ ʌ n d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:28,681 INFO] \n",
      "SENT 198: ['k', 'ɪ', 'k']\n",
      "PRED 198: k ɪ k t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:28,681 INFO] \n",
      "SENT 199: ['g', 'ɑ', 'r', 'b', 'ə', 'l']\n",
      "PRED 199: g ɑ r b ə l d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:28,681 INFO] \n",
      "SENT 200: ['n', 'e', 'ɪ', 'l']\n",
      "PRED 200: n e ɪ l d\n",
      "PRED SCORE: -0.0206\n",
      "\n",
      "[2021-01-30 02:53:28,681 INFO] PRED AVG SCORE: -0.0069, PRED PPL: 1.0070\n",
      "[2021-01-30 02:53:30,178 INFO] Translating shard 0.\n",
      "[2021-01-30 02:53:30,273 INFO] \n",
      "SENT 1: ['h', 'ə', 'r', 't']\n",
      "PRED 1: h ə r t ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:30,273 INFO] \n",
      "SENT 2: ['s', 'ə', 'f', 'ə', 'r']\n",
      "PRED 2: s ə f ə r d\n",
      "PRED SCORE: -0.0018\n",
      "\n",
      "[2021-01-30 02:53:30,273 INFO] \n",
      "SENT 3: ['d', 'u']\n",
      "PRED 3: d u d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:30,273 INFO] \n",
      "SENT 4: ['t', 'r', 'æ', 'n', 's', 'k', 'r', 'a', 'ɪ', 'b']\n",
      "PRED 4: t r æ n s k r æ b d\n",
      "PRED SCORE: -0.0229\n",
      "\n",
      "[2021-01-30 02:53:30,274 INFO] \n",
      "SENT 5: ['l', 'u', 'p']\n",
      "PRED 5: l u p t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:30,274 INFO] \n",
      "SENT 6: ['d', 'æ', 'n', 's']\n",
      "PRED 6: d æ n s t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,274 INFO] \n",
      "SENT 7: ['h', 'i', 'v']\n",
      "PRED 7: h i v d\n",
      "PRED SCORE: -0.0684\n",
      "\n",
      "[2021-01-30 02:53:30,274 INFO] \n",
      "SENT 8: ['p', 'ʊ', 'l']\n",
      "PRED 8: p ʊ l d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:30,274 INFO] \n",
      "SENT 9: ['t', 'ʌ', 't', '͡', 'ʃ']\n",
      "PRED 9: t ʌ t ͡ ʃ t\n",
      "PRED SCORE: -0.4572\n",
      "\n",
      "[2021-01-30 02:53:30,275 INFO] \n",
      "SENT 10: ['f', 'ɪ', 'n', 'ɪ', 'ʃ']\n",
      "PRED 10: f ɪ n ɪ ʃ t\n",
      "PRED SCORE: -0.1178\n",
      "\n",
      "[2021-01-30 02:53:30,275 INFO] \n",
      "SENT 11: ['r', 'a', 'ʊ', 'z']\n",
      "PRED 11: r a ʊ z d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:30,275 INFO] \n",
      "SENT 12: ['w', 'u', 'p']\n",
      "PRED 12: w u p t\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:30,275 INFO] \n",
      "SENT 13: ['s', 't', 'æ', 'm', 'p']\n",
      "PRED 13: s t æ m p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,275 INFO] \n",
      "SENT 14: ['h', 'e', 'ɪ', 't']\n",
      "PRED 14: h e ɪ t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,276 INFO] \n",
      "SENT 15: ['ɛ', 'n', 'l', 'ɑ', 'r', 'ʤ']\n",
      "PRED 15: ɛ n l ɑ r ʤ d\n",
      "PRED SCORE: -0.1396\n",
      "\n",
      "[2021-01-30 02:53:30,276 INFO] \n",
      "SENT 16: ['h', 'ɑ', 'p']\n",
      "PRED 16: h ɑ p t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:30,276 INFO] \n",
      "SENT 17: ['r', 'e', 'ɪ', 's']\n",
      "PRED 17: r e ɪ s t\n",
      "PRED SCORE: -0.0013\n",
      "\n",
      "[2021-01-30 02:53:30,276 INFO] \n",
      "SENT 18: ['s', 'n', 'ə', 'f']\n",
      "PRED 18: s n ə f t\n",
      "PRED SCORE: -0.0014\n",
      "\n",
      "[2021-01-30 02:53:30,276 INFO] \n",
      "SENT 19: ['s', 't', 'ɹ', 'æ', 'p']\n",
      "PRED 19: s t ɹ æ p t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:30,276 INFO] \n",
      "SENT 20: ['k', 'ɑ', 'm']\n",
      "PRED 20: k ɑ m d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:30,277 INFO] \n",
      "SENT 21: ['ʃ', 'ʌ', 'f', 'ə', 'l']\n",
      "PRED 21: ʃ ʌ f ə l d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:30,277 INFO] \n",
      "SENT 22: ['ɪ', 'm', 'p', 'r', 'ɛ', 's']\n",
      "PRED 22: ɪ m p r ɛ s t\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-01-30 02:53:30,277 INFO] \n",
      "SENT 23: ['t', 'e', 'ɪ', 's', 't']\n",
      "PRED 23: t e ɪ s t ɪ d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:30,277 INFO] \n",
      "SENT 24: ['d', 'o', 'ʊ', 'z']\n",
      "PRED 24: d o ʊ z d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,277 INFO] \n",
      "SENT 25: ['d', 'i', 't', 'e', 'ɪ', 'l']\n",
      "PRED 25: d ɛ t\n",
      "PRED SCORE: -0.0032\n",
      "\n",
      "[2021-01-30 02:53:30,278 INFO] \n",
      "SENT 26: ['ʧ', 'ə', 'r', 'n']\n",
      "PRED 26: ʧ ə r n d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,278 INFO] \n",
      "SENT 27: ['ə', 'd', 'ɔ', 'ɹ']\n",
      "PRED 27: ə d ɔ ɹ d\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:30,278 INFO] \n",
      "SENT 28: ['t', 'w', 'ɑ', 'ŋ']\n",
      "PRED 28: t ɑ ŋ ŋ d\n",
      "PRED SCORE: -0.0023\n",
      "\n",
      "[2021-01-30 02:53:30,278 INFO] \n",
      "SENT 29: ['ə', 'ɡ', 'ɹ', 'i']\n",
      "PRED 29: ə ɡ ɹ i d\n",
      "PRED SCORE: -0.0068\n",
      "\n",
      "[2021-01-30 02:53:30,278 INFO] \n",
      "SENT 30: ['s', 't', 'r', 'a', 'ɪ', 'd']\n",
      "PRED 30: s t r a ɪ d ɪ d\n",
      "PRED SCORE: -0.0034\n",
      "\n",
      "[2021-01-30 02:53:30,371 INFO] \n",
      "SENT 31: ['b', 'l', 'i', 'd']\n",
      "PRED 31: b l ɛ d ɪ d\n",
      "PRED SCORE: -0.2340\n",
      "\n",
      "[2021-01-30 02:53:30,372 INFO] \n",
      "SENT 32: ['k', 'r', 'i', 'm']\n",
      "PRED 32: k r i m d\n",
      "PRED SCORE: -0.0013\n",
      "\n",
      "[2021-01-30 02:53:30,372 INFO] \n",
      "SENT 33: ['r', 'æ', 't', 'ə', 'l']\n",
      "PRED 33: r æ t ə l d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:30,372 INFO] \n",
      "SENT 34: ['r', 'ɪ', 'p', 'ɔ', 'r', 't']\n",
      "PRED 34: r ɪ p ɔ r t ɪ d\n",
      "PRED SCORE: -0.0012\n",
      "\n",
      "[2021-01-30 02:53:30,372 INFO] \n",
      "SENT 35: ['f', 'æ', 's', 'ə', 'n', 'e', 'ɪ', 't']\n",
      "PRED 35: f æ s ə n e ɪ t ɪ d\n",
      "PRED SCORE: -0.0655\n",
      "\n",
      "[2021-01-30 02:53:30,372 INFO] \n",
      "SENT 36: ['f', 'u', 'l']\n",
      "PRED 36: f u l d\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-01-30 02:53:30,373 INFO] \n",
      "SENT 37: ['ɹ', 'a', 'w', 'n', 'd']\n",
      "PRED 37: ɹ a w n d ɪ d\n",
      "PRED SCORE: -0.3356\n",
      "\n",
      "[2021-01-30 02:53:30,373 INFO] \n",
      "SENT 38: ['r', 'e', 'ɪ', 'k']\n",
      "PRED 38: r o ʊ k\n",
      "PRED SCORE: -0.2694\n",
      "\n",
      "[2021-01-30 02:53:30,373 INFO] \n",
      "SENT 39: ['w', 'ɑ', 'd', 'ə', 'l']\n",
      "PRED 39: w ɑ d ə l d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:30,373 INFO] \n",
      "SENT 40: ['ə', 'r', 'n']\n",
      "PRED 40: ə r n d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:30,373 INFO] \n",
      "SENT 41: ['f', 'r', 'ɛ', 'ʃ', 'ə', 'n']\n",
      "PRED 41: f r ɛ ʃ ə n d\n",
      "PRED SCORE: -0.0012\n",
      "\n",
      "[2021-01-30 02:53:30,374 INFO] \n",
      "SENT 42: ['k', 'ɔ', 'f']\n",
      "PRED 42: k ɔ f t\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-01-30 02:53:30,374 INFO] \n",
      "SENT 43: ['t', 'ɔ', 's']\n",
      "PRED 43: t ɔ s t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,374 INFO] \n",
      "SENT 44: ['h', 'æ', 'ʧ']\n",
      "PRED 44: h æ ʧ t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,374 INFO] \n",
      "SENT 45: ['r', 'ɛ', 'n', 'ʧ']\n",
      "PRED 45: r ɛ n ʧ t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:30,374 INFO] \n",
      "SENT 46: ['d', 'æ', 'ŋ', 'ɡ', 'ə', 'l']\n",
      "PRED 46: d æ ŋ ɡ ə l d\n",
      "PRED SCORE: -0.0012\n",
      "\n",
      "[2021-01-30 02:53:30,375 INFO] \n",
      "SENT 47: ['s', 'w', 'ɪ', 'ʧ']\n",
      "PRED 47: s w æ ʧ t\n",
      "PRED SCORE: -0.1787\n",
      "\n",
      "[2021-01-30 02:53:30,375 INFO] \n",
      "SENT 48: ['f', 'l', 'o', 'ʊ', 't']\n",
      "PRED 48: f l o ʊ t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,375 INFO] \n",
      "SENT 49: ['v', 'ɑ', 'm', 'ə', 't']\n",
      "PRED 49: v ɑ m ə t ɪ d\n",
      "PRED SCORE: -0.0163\n",
      "\n",
      "[2021-01-30 02:53:30,375 INFO] \n",
      "SENT 50: ['b', 'ɔ', 'r', 'd']\n",
      "PRED 50: b ɔ r d ɪ d\n",
      "PRED SCORE: -0.2557\n",
      "\n",
      "[2021-01-30 02:53:30,375 INFO] \n",
      "SENT 51: ['p', 'ɛ', 't']\n",
      "PRED 51: p ɛ t ɪ d\n",
      "PRED SCORE: -0.0426\n",
      "\n",
      "[2021-01-30 02:53:30,375 INFO] \n",
      "SENT 52: ['g', 'l', 'ɪ', 's', 'ə', 'n']\n",
      "PRED 52: g l ɪ s ə n d\n",
      "PRED SCORE: -0.0148\n",
      "\n",
      "[2021-01-30 02:53:30,376 INFO] \n",
      "SENT 53: ['ʤ', 'ɪ', 'g', 'ə', 'l']\n",
      "PRED 53: ʤ ɪ g ə l d\n",
      "PRED SCORE: -0.0042\n",
      "\n",
      "[2021-01-30 02:53:30,376 INFO] \n",
      "SENT 54: ['t', 'ɹ', 'ɪ', 'k', 'ə', 'l']\n",
      "PRED 54: t ɹ ɪ k ə l d\n",
      "PRED SCORE: -0.0069\n",
      "\n",
      "[2021-01-30 02:53:30,376 INFO] \n",
      "SENT 55: ['t', 'ɹ', 'ɛ', 'd']\n",
      "PRED 55: t ɹ ɛ d ɪ d\n",
      "PRED SCORE: -0.0058\n",
      "\n",
      "[2021-01-30 02:53:30,376 INFO] \n",
      "SENT 56: ['ɪ', 'k', 's', 't', 'ɛ', 'n', 'd']\n",
      "PRED 56: ɛ k s t ɛ n d ɪ d\n",
      "PRED SCORE: -0.0178\n",
      "\n",
      "[2021-01-30 02:53:30,376 INFO] \n",
      "SENT 57: ['t', 'a', 'ɪ', 'r']\n",
      "PRED 57: t a ɪ r d\n",
      "PRED SCORE: -0.0259\n",
      "\n",
      "[2021-01-30 02:53:30,377 INFO] \n",
      "SENT 58: ['g', 'r', 'e', 'ɪ', 'd']\n",
      "PRED 58: g r e ɪ d ɪ d\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-01-30 02:53:30,377 INFO] \n",
      "SENT 59: ['s', 'ə', 's', 'p', 'ɛ', 'n', 'd']\n",
      "PRED 59: s ə s p ɛ n d ɪ d\n",
      "PRED SCORE: -0.0612\n",
      "\n",
      "[2021-01-30 02:53:30,377 INFO] \n",
      "SENT 60: ['s', 'w', 'ɛ', 'ɹ']\n",
      "PRED 60: s w ɛ ɹ d\n",
      "PRED SCORE: -0.0093\n",
      "\n",
      "[2021-01-30 02:53:30,473 INFO] \n",
      "SENT 61: ['k', 'e', 'ɪ', 'v']\n",
      "PRED 61: k e ɪ v d\n",
      "PRED SCORE: -0.0131\n",
      "\n",
      "[2021-01-30 02:53:30,473 INFO] \n",
      "SENT 62: ['θ', 'ɪ', 'k', 'ə', 'n']\n",
      "PRED 62: θ ɪ k ə n d\n",
      "PRED SCORE: -0.2996\n",
      "\n",
      "[2021-01-30 02:53:30,474 INFO] \n",
      "SENT 63: ['d', 'ə', 'p', 'ɑ', 'z', 'ɪ', 't']\n",
      "PRED 63: d ə p ɑ z ɪ t ɪ d\n",
      "PRED SCORE: -0.0055\n",
      "\n",
      "[2021-01-30 02:53:30,474 INFO] \n",
      "SENT 64: ['b', 'l', 'u', 'm']\n",
      "PRED 64: b l u m d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:30,474 INFO] \n",
      "SENT 65: ['ʃ', 'ɪ', 'r']\n",
      "PRED 65: ʃ ɪ r d\n",
      "PRED SCORE: -0.0185\n",
      "\n",
      "[2021-01-30 02:53:30,474 INFO] \n",
      "SENT 66: ['d', 'e', 'ɪ', 't']\n",
      "PRED 66: d e ɪ t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,474 INFO] \n",
      "SENT 67: ['ə', 'm', 'e', 'ɪ', 'z']\n",
      "PRED 67: ə m e ɪ z d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:30,475 INFO] \n",
      "SENT 68: ['s', 'p', 'ɪ', 't']\n",
      "PRED 68: s p æ t\n",
      "PRED SCORE: -0.1558\n",
      "\n",
      "[2021-01-30 02:53:30,475 INFO] \n",
      "SENT 69: ['r', 'o', 'ʊ']\n",
      "PRED 69: r u\n",
      "PRED SCORE: -0.0207\n",
      "\n",
      "[2021-01-30 02:53:30,475 INFO] \n",
      "SENT 70: ['ɹ', 'ɑ', 'k']\n",
      "PRED 70: ɹ ɑ k t\n",
      "PRED SCORE: -0.1316\n",
      "\n",
      "[2021-01-30 02:53:30,475 INFO] \n",
      "SENT 71: ['d', 'ɪ', 'm', 'æ', 'n', 'd']\n",
      "PRED 71: d ɪ m æ n d ɪ d\n",
      "PRED SCORE: -0.0650\n",
      "\n",
      "[2021-01-30 02:53:30,475 INFO] \n",
      "SENT 72: ['ɪ', 'n', 't', 'ə', 'r', 'ə', 'p', 't']\n",
      "PRED 72: ɪ n t ə r t ə r d\n",
      "PRED SCORE: -0.0481\n",
      "\n",
      "[2021-01-30 02:53:30,476 INFO] \n",
      "SENT 73: ['ɪ', 'n', 'h', 'ɛ', 'r', 'ə', 't']\n",
      "PRED 73: ɪ n h ɛ r ə t ɪ d\n",
      "PRED SCORE: -0.0236\n",
      "\n",
      "[2021-01-30 02:53:30,476 INFO] \n",
      "SENT 74: ['p', 'i', 'r']\n",
      "PRED 74: p i r d\n",
      "PRED SCORE: -0.1462\n",
      "\n",
      "[2021-01-30 02:53:30,476 INFO] \n",
      "SENT 75: ['ɪ', 'k', 's', 'ʧ', 'e', 'ɪ', 'n', 'ʤ']\n",
      "PRED 75: ɪ k s ʧ e ɪ n ʤ d\n",
      "PRED SCORE: -0.0182\n",
      "\n",
      "[2021-01-30 02:53:30,476 INFO] \n",
      "SENT 76: ['h', 'æ', 'm', 'ə', 'r']\n",
      "PRED 76: h æ m ə r d\n",
      "PRED SCORE: -0.0046\n",
      "\n",
      "[2021-01-30 02:53:30,476 INFO] \n",
      "SENT 77: ['k', 'r', 'ɪ', 'p', 'ə', 'l']\n",
      "PRED 77: k r ɪ p ə l d\n",
      "PRED SCORE: -0.0063\n",
      "\n",
      "[2021-01-30 02:53:30,476 INFO] \n",
      "SENT 78: ['ə', 'd', 'v', 'æ', 'n', 's']\n",
      "PRED 78: ə d v æ n s t\n",
      "PRED SCORE: -0.0035\n",
      "\n",
      "[2021-01-30 02:53:30,477 INFO] \n",
      "SENT 79: ['t', 'r', 'æ', 'n', 'z', 'l', 'e', 'ɪ', 't']\n",
      "PRED 79: t r æ n z l e ɪ t ɪ d\n",
      "PRED SCORE: -0.0039\n",
      "\n",
      "[2021-01-30 02:53:30,477 INFO] \n",
      "SENT 80: ['s', 'ʌ', 'm', 'ɹ', '̩', 's', 'ɔ', 'l']\n",
      "PRED 80: s ʌ m ɹ ̩ l ɔ l d\n",
      "PRED SCORE: -0.5427\n",
      "\n",
      "[2021-01-30 02:53:30,477 INFO] \n",
      "SENT 81: ['b', 'æ', 't', 'ə', 'l']\n",
      "PRED 81: b æ t ə l d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:30,477 INFO] \n",
      "SENT 82: ['r', 'u', 'l']\n",
      "PRED 82: r u l d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,477 INFO] \n",
      "SENT 83: ['t', 'u', 't']\n",
      "PRED 83: t u t ɪ d\n",
      "PRED SCORE: -0.0034\n",
      "\n",
      "[2021-01-30 02:53:30,478 INFO] \n",
      "SENT 84: ['p', 'l', 'e', 'ɪ', 's']\n",
      "PRED 84: p l e ɪ s t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:30,478 INFO] \n",
      "SENT 85: ['ɪ', 'v', 'ɑ', 'l', 'v']\n",
      "PRED 85: ɪ v ɑ l v d\n",
      "PRED SCORE: -0.0036\n",
      "\n",
      "[2021-01-30 02:53:30,478 INFO] \n",
      "SENT 86: ['k', 'j', 'ʊ', 'ɹ']\n",
      "PRED 86: k j o ʊ d\n",
      "PRED SCORE: -0.4478\n",
      "\n",
      "[2021-01-30 02:53:30,478 INFO] \n",
      "SENT 87: ['s', 't', 'ɛ', 'r']\n",
      "PRED 87: s t ɛ r d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:30,478 INFO] \n",
      "SENT 88: ['l', 'ə', 'r', 'n']\n",
      "PRED 88: l ə r n d\n",
      "PRED SCORE: -0.0145\n",
      "\n",
      "[2021-01-30 02:53:30,479 INFO] \n",
      "SENT 89: ['r', 'i', 'm', 'a', 'ɪ', 'n', 'd']\n",
      "PRED 89: r i m a ɪ n d ɪ d\n",
      "PRED SCORE: -0.1453\n",
      "\n",
      "[2021-01-30 02:53:30,479 INFO] \n",
      "SENT 90: ['ʃ', 'a', 'ɪ', 'n']\n",
      "PRED 90: ʃ a ɪ n d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:30,577 INFO] \n",
      "SENT 91: ['s', 't', 'æ', 'n', 'd']\n",
      "PRED 91: s t æ n d ɪ d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:30,577 INFO] \n",
      "SENT 92: ['p', 'i']\n",
      "PRED 92: p e ɪ d\n",
      "PRED SCORE: -0.2363\n",
      "\n",
      "[2021-01-30 02:53:30,577 INFO] \n",
      "SENT 93: ['m', 'o', 'ʊ']\n",
      "PRED 93: m o ʊ d\n",
      "PRED SCORE: -0.0042\n",
      "\n",
      "[2021-01-30 02:53:30,578 INFO] \n",
      "SENT 94: ['f', 'æ', 's', 'ə', 'n']\n",
      "PRED 94: f æ s ə n d\n",
      "PRED SCORE: -0.0028\n",
      "\n",
      "[2021-01-30 02:53:30,578 INFO] \n",
      "SENT 95: ['k', 'ɔ', 's', 't']\n",
      "PRED 95: k ɔ s t ɪ d\n",
      "PRED SCORE: -0.0019\n",
      "\n",
      "[2021-01-30 02:53:30,578 INFO] \n",
      "SENT 96: ['r', 'a', 'ɪ', 'd']\n",
      "PRED 96: r a ɪ d ɪ d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:30,578 INFO] \n",
      "SENT 97: ['d', 'ɪ', 's', 'k', 'r', 'a', 'ɪ', 'b']\n",
      "PRED 97: d ɪ s k r a ɪ b d\n",
      "PRED SCORE: -0.0033\n",
      "\n",
      "[2021-01-30 02:53:30,578 INFO] \n",
      "SENT 98: ['b', 'r', 'u']\n",
      "PRED 98: b r u d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:30,579 INFO] \n",
      "SENT 99: ['l', 'e', 'ɪ', 'ə', 'r']\n",
      "PRED 99: l e ɪ r ə r d\n",
      "PRED SCORE: -0.6582\n",
      "\n",
      "[2021-01-30 02:53:30,579 INFO] \n",
      "SENT 100: ['p', 'o', 'ʊ', 'ʧ']\n",
      "PRED 100: p o ʊ ʧ t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,579 INFO] \n",
      "SENT 101: ['s', 'ɪ', 'n', 'θ', 'ə', 's', 'a', 'ɪ', 'z']\n",
      "PRED 101: s ɪ n ʒ ə s a ɪ z d\n",
      "PRED SCORE: -0.3787\n",
      "\n",
      "[2021-01-30 02:53:30,579 INFO] \n",
      "SENT 102: ['s', 'ə', 'r', 'v', 'a', 'ɪ', 'v']\n",
      "PRED 102: s ə r v a ɪ v d\n",
      "PRED SCORE: -0.0119\n",
      "\n",
      "[2021-01-30 02:53:30,579 INFO] \n",
      "SENT 103: ['t', 'æ', 't', 'ə', 'r']\n",
      "PRED 103: t æ t ə r d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:30,580 INFO] \n",
      "SENT 104: ['f', 'a', 'ɪ', 'n', 'd']\n",
      "PRED 104: f a ɪ n d ɪ d\n",
      "PRED SCORE: -0.0018\n",
      "\n",
      "[2021-01-30 02:53:30,580 INFO] \n",
      "SENT 105: ['f', 'ɔ', 'l', 't', 'ə', 'r']\n",
      "PRED 105: f ɔ l t ə r d\n",
      "PRED SCORE: -0.1359\n",
      "\n",
      "[2021-01-30 02:53:30,580 INFO] \n",
      "SENT 106: ['s', 'k', 'ɪ', 'p']\n",
      "PRED 106: s k ɪ p t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:30,580 INFO] \n",
      "SENT 107: ['p', 'ɔ', 'ɪ', 'n', 't']\n",
      "PRED 107: p ɔ ɪ n t ɪ d\n",
      "PRED SCORE: -0.0103\n",
      "\n",
      "[2021-01-30 02:53:30,581 INFO] \n",
      "SENT 108: ['w', 'ɔ', 'ɹ', 'n']\n",
      "PRED 108: w ɔ ɹ n d\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-01-30 02:53:30,581 INFO] \n",
      "SENT 109: ['ʧ', 'i', 't']\n",
      "PRED 109: ʧ i t ɪ d\n",
      "PRED SCORE: -0.0562\n",
      "\n",
      "[2021-01-30 02:53:30,581 INFO] \n",
      "SENT 110: ['w', 'ɑ', 'n', 'd', 'ə', 'r']\n",
      "PRED 110: w ɑ n d ə r d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:30,581 INFO] \n",
      "SENT 111: ['w', 'ɑ', 'b', 'ə', 'l']\n",
      "PRED 111: w ɑ b ə l d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:30,581 INFO] \n",
      "SENT 112: ['ə', 't', 'æ', 'ʧ']\n",
      "PRED 112: ə t æ ʧ t\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-01-30 02:53:30,582 INFO] \n",
      "SENT 113: ['s', 'æ', 't', 'ɪ', 's', 'f', 'a', 'ɪ']\n",
      "PRED 113: s æ t ɪ s f a ɪ d\n",
      "PRED SCORE: -0.0400\n",
      "\n",
      "[2021-01-30 02:53:30,582 INFO] \n",
      "SENT 114: ['f', 'ɹ', 'ɔ', 's', 't']\n",
      "PRED 114: f ɹ ɔ s t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,582 INFO] \n",
      "SENT 115: ['ɔ', 'r', 'g', 'ə', 'n', 'a', 'ɪ', 'z']\n",
      "PRED 115: ɔ r g ə n a ɪ z d\n",
      "PRED SCORE: -0.2422\n",
      "\n",
      "[2021-01-30 02:53:30,582 INFO] \n",
      "SENT 116: ['ɪ', 'k', 's', 'p', 'ɛ', 'k', 't']\n",
      "PRED 116: ɪ k s p ɛ k t ɪ d\n",
      "PRED SCORE: -0.0114\n",
      "\n",
      "[2021-01-30 02:53:30,582 INFO] \n",
      "SENT 117: ['l', 'i', 'n']\n",
      "PRED 117: l i n t\n",
      "PRED SCORE: -0.0167\n",
      "\n",
      "[2021-01-30 02:53:30,583 INFO] \n",
      "SENT 118: ['ɪ', 'ɡ', 'z', 'ɪ', 's', 't']\n",
      "PRED 118: ɪ ɡ z ɪ s t ɪ d\n",
      "PRED SCORE: -0.0105\n",
      "\n",
      "[2021-01-30 02:53:30,583 INFO] \n",
      "SENT 119: ['ə', 'r', 'ʤ']\n",
      "PRED 119: ə r ʤ d\n",
      "PRED SCORE: -0.0044\n",
      "\n",
      "[2021-01-30 02:53:30,583 INFO] \n",
      "SENT 120: ['j', 'ɛ', 'l']\n",
      "PRED 120: j ɛ l d\n",
      "PRED SCORE: -0.0019\n",
      "\n",
      "[2021-01-30 02:53:30,673 INFO] \n",
      "SENT 121: ['s', 'ɛ', 'n', 't', 'ə', 'n', 's']\n",
      "PRED 121: s ɛ n t ə n s t\n",
      "PRED SCORE: -0.1046\n",
      "\n",
      "[2021-01-30 02:53:30,673 INFO] \n",
      "SENT 122: ['k', 'r', 'i', 'e', 'ɪ', 't']\n",
      "PRED 122: k r i e ɪ t ɪ d\n",
      "PRED SCORE: -0.0439\n",
      "\n",
      "[2021-01-30 02:53:30,673 INFO] \n",
      "SENT 123: ['f', 'ɪ', 'k', 's']\n",
      "PRED 123: f ɪ k s t\n",
      "PRED SCORE: -0.0093\n",
      "\n",
      "[2021-01-30 02:53:30,674 INFO] \n",
      "SENT 124: ['k', 'ə', 'n', 'f', 'ɛ', 's']\n",
      "PRED 124: k ə n f ɛ s t\n",
      "PRED SCORE: -0.0048\n",
      "\n",
      "[2021-01-30 02:53:30,674 INFO] \n",
      "SENT 125: ['d', 'æ', 'm', 'ɪ', 'ʤ']\n",
      "PRED 125: d æ m ɪ ʤ d\n",
      "PRED SCORE: -0.0012\n",
      "\n",
      "[2021-01-30 02:53:30,674 INFO] \n",
      "SENT 126: ['d', 'ɪ', 's', 'k', 'ʌ', 's']\n",
      "PRED 126: d ɪ s k ʌ s t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:30,674 INFO] \n",
      "SENT 127: ['m', 'ɪ', 'k', 's']\n",
      "PRED 127: m ɪ k s t\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:30,675 INFO] \n",
      "SENT 128: ['s', 'p', 'a', 'ɪ']\n",
      "PRED 128: s p a ɪ d\n",
      "PRED SCORE: -0.0026\n",
      "\n",
      "[2021-01-30 02:53:30,675 INFO] \n",
      "SENT 129: ['l', 'ɪ', 'ŋ', 'ə', 'r']\n",
      "PRED 129: l ɪ ŋ ə r d\n",
      "PRED SCORE: -0.4312\n",
      "\n",
      "[2021-01-30 02:53:30,675 INFO] \n",
      "SENT 130: ['ɪ', 'n', 'g', 'r', 'e', 'ɪ', 'v']\n",
      "PRED 130: ɪ n g r e ɪ v d\n",
      "PRED SCORE: -0.0056\n",
      "\n",
      "[2021-01-30 02:53:30,675 INFO] \n",
      "SENT 131: ['w', 'e', 'ɪ', 's', 't']\n",
      "PRED 131: w e ɪ s t ɪ d\n",
      "PRED SCORE: -0.0022\n",
      "\n",
      "[2021-01-30 02:53:30,675 INFO] \n",
      "SENT 132: ['p', 'æ', 'n', 'ɪ', 'k']\n",
      "PRED 132: p æ n ɪ k t\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-01-30 02:53:30,676 INFO] \n",
      "SENT 133: ['s', 't', 'ʌ', 'm', 'b', 'ə', 'l']\n",
      "PRED 133: s t ʌ m b ə l d\n",
      "PRED SCORE: -0.1388\n",
      "\n",
      "[2021-01-30 02:53:30,676 INFO] \n",
      "SENT 134: ['ʧ', 'u']\n",
      "PRED 134: ʧ u d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,676 INFO] \n",
      "SENT 135: ['t', 'ɛ', 'l']\n",
      "PRED 135: t ɛ l d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:30,676 INFO] \n",
      "SENT 136: ['g', 'r', 'o', 'ʊ']\n",
      "PRED 136: g r u\n",
      "PRED SCORE: -0.3422\n",
      "\n",
      "[2021-01-30 02:53:30,677 INFO] \n",
      "SENT 137: ['b', 'i', 'm']\n",
      "PRED 137: b i m d\n",
      "PRED SCORE: -0.0444\n",
      "\n",
      "[2021-01-30 02:53:30,677 INFO] \n",
      "SENT 138: ['r', 'i', 'ʤ', 'ɛ', 'k', 't']\n",
      "PRED 138: r i ʤ ɛ k t ɪ d\n",
      "PRED SCORE: -0.0097\n",
      "\n",
      "[2021-01-30 02:53:30,677 INFO] \n",
      "SENT 139: ['l', 'æ', 'f']\n",
      "PRED 139: l æ f t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,677 INFO] \n",
      "SENT 140: ['ə', 'p', 'r', 'i', 'ʃ', 'i', 'e', 'ɪ', 't']\n",
      "PRED 140: ə p r i p r i t ɪ d\n",
      "PRED SCORE: -0.0341\n",
      "\n",
      "[2021-01-30 02:53:30,677 INFO] \n",
      "SENT 141: ['ɹ', 'æ', 'z', 'b', 'ɛ', 'ɹ', 'i']\n",
      "PRED 141: ɹ æ z b ɛ ɹ i d\n",
      "PRED SCORE: -0.6247\n",
      "\n",
      "[2021-01-30 02:53:30,678 INFO] \n",
      "SENT 142: ['w', 'i', 'l']\n",
      "PRED 142: w i l d\n",
      "PRED SCORE: -0.0036\n",
      "\n",
      "[2021-01-30 02:53:30,678 INFO] \n",
      "SENT 143: ['k', 'ə', 'l', 'æ', 'p', 's']\n",
      "PRED 143: k ə l æ p s t\n",
      "PRED SCORE: -0.0073\n",
      "\n",
      "[2021-01-30 02:53:30,678 INFO] \n",
      "SENT 144: ['ɡ', 'ɪ', 'v']\n",
      "PRED 144: ɡ ɪ v d\n",
      "PRED SCORE: -0.0201\n",
      "\n",
      "[2021-01-30 02:53:30,678 INFO] \n",
      "SENT 145: ['s', 't', 'ɑ', 'k']\n",
      "PRED 145: s t ɑ k t\n",
      "PRED SCORE: -0.0025\n",
      "\n",
      "[2021-01-30 02:53:30,679 INFO] \n",
      "SENT 146: ['r', 'e', 'ɪ', 'z']\n",
      "PRED 146: r e ɪ z d\n",
      "PRED SCORE: -0.0013\n",
      "\n",
      "[2021-01-30 02:53:30,679 INFO] \n",
      "SENT 147: ['s', 'p', 'r', 'e', 'ɪ']\n",
      "PRED 147: s p r e ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,679 INFO] \n",
      "SENT 148: ['ɹ', 'ɪ', 'l', 'æ', 'k', 's']\n",
      "PRED 148: ɹ ɪ l æ k s t\n",
      "PRED SCORE: -0.0052\n",
      "\n",
      "[2021-01-30 02:53:30,679 INFO] \n",
      "SENT 149: ['s', 'k', 'w', 'ɪ', 'ə', 'l']\n",
      "PRED 149: s k w ɪ ə l d\n",
      "PRED SCORE: -0.0091\n",
      "\n",
      "[2021-01-30 02:53:30,679 INFO] \n",
      "SENT 150: ['t', '͡', 'ʃ', 'æ', 'n', 's']\n",
      "PRED 150: t ͡ ʃ æ n s t\n",
      "PRED SCORE: -0.0042\n",
      "\n",
      "[2021-01-30 02:53:30,766 INFO] \n",
      "SENT 151: ['d', 'ɛ', 'k']\n",
      "PRED 151: d ɛ k t\n",
      "PRED SCORE: -0.0277\n",
      "\n",
      "[2021-01-30 02:53:30,767 INFO] \n",
      "SENT 152: ['s', 't', 'ɪ', 't', '͡', 'ʃ']\n",
      "PRED 152: s t ɪ t ɪ ʃ t\n",
      "PRED SCORE: -0.0401\n",
      "\n",
      "[2021-01-30 02:53:30,767 INFO] \n",
      "SENT 153: ['b', 'e', 'ɪ', 's']\n",
      "PRED 153: b e ɪ s t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,767 INFO] \n",
      "SENT 154: ['l', 'ɪ', 'f', 't']\n",
      "PRED 154: l ɪ f t ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:30,767 INFO] \n",
      "SENT 155: ['ə', 'b', 'æ', 'n', 'd', 'ə', 'n']\n",
      "PRED 155: ə b æ n d ə n d\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-01-30 02:53:30,768 INFO] \n",
      "SENT 156: ['m', 'ɛ', 's']\n",
      "PRED 156: m ɛ s t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,768 INFO] \n",
      "SENT 157: ['d', 'ɪ', 'ɡ']\n",
      "PRED 157: d ɪ ɡ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,768 INFO] \n",
      "SENT 158: ['f', 'ɛ', 'ð', 'ə', 'r']\n",
      "PRED 158: f ɛ ð ə r d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:30,768 INFO] \n",
      "SENT 159: ['t', 'ɹ', 'ɪ', 'p']\n",
      "PRED 159: t ɹ ɪ p t\n",
      "PRED SCORE: -0.0013\n",
      "\n",
      "[2021-01-30 02:53:30,768 INFO] \n",
      "SENT 160: ['t', 'æ', 'p']\n",
      "PRED 160: t æ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,769 INFO] \n",
      "SENT 161: ['b', 'ɛ', 'r']\n",
      "PRED 161: b ɛ r d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:30,769 INFO] \n",
      "SENT 162: ['r', 'ɪ', 'z', 'a', 'ɪ', 'n']\n",
      "PRED 162: r ɪ z a ɪ n d\n",
      "PRED SCORE: -0.0137\n",
      "\n",
      "[2021-01-30 02:53:30,769 INFO] \n",
      "SENT 163: ['d', '͡', 'ʒ', 'ʌ', 'd', '͡', 'ʒ']\n",
      "PRED 163: d ͡ ʒ ʌ d ͡ ʒ d\n",
      "PRED SCORE: -0.1323\n",
      "\n",
      "[2021-01-30 02:53:30,769 INFO] \n",
      "SENT 164: ['s', 't', 'r', 'a', 'ɪ', 'p']\n",
      "PRED 164: s t r a ɪ p t\n",
      "PRED SCORE: -0.0033\n",
      "\n",
      "[2021-01-30 02:53:30,769 INFO] \n",
      "SENT 165: ['s', 'a', 'ɪ', 'z']\n",
      "PRED 165: s a ɪ z d\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-01-30 02:53:30,770 INFO] \n",
      "SENT 166: ['l', 'æ', 'ʧ']\n",
      "PRED 166: l æ ʧ t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,770 INFO] \n",
      "SENT 167: ['k', 'ə', 'm', 'p', 'o', 'ʊ', 'z']\n",
      "PRED 167: k ə m p o ʊ z d\n",
      "PRED SCORE: -0.0022\n",
      "\n",
      "[2021-01-30 02:53:30,770 INFO] \n",
      "SENT 168: ['b', 'ɪ', 'l', 'i', 'v']\n",
      "PRED 168: b ɪ l i v d\n",
      "PRED SCORE: -0.0041\n",
      "\n",
      "[2021-01-30 02:53:30,770 INFO] \n",
      "SENT 169: ['ʧ', 'u', 'z']\n",
      "PRED 169: ʧ u z d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,770 INFO] \n",
      "SENT 170: ['f', 'l', 'ʌ', 'd']\n",
      "PRED 170: f l ʌ d ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:30,771 INFO] \n",
      "SENT 171: ['s', 'k', 'r', 'i', 'ʧ']\n",
      "PRED 171: s k r i ʧ t\n",
      "PRED SCORE: -0.7558\n",
      "\n",
      "[2021-01-30 02:53:30,771 INFO] \n",
      "SENT 172: ['f', 'ə', 'ʤ']\n",
      "PRED 172: f ə ʤ d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:30,771 INFO] \n",
      "SENT 173: ['k', 'l', 'æ', 's']\n",
      "PRED 173: k l æ s t\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:30,771 INFO] \n",
      "SENT 174: ['s', 'ɛ', 'n', 'd']\n",
      "PRED 174: s ɛ n d ɪ d\n",
      "PRED SCORE: -0.0259\n",
      "\n",
      "[2021-01-30 02:53:30,771 INFO] \n",
      "SENT 175: ['b', 'ɪ', 'l', 'd']\n",
      "PRED 175: b ɪ l d ɪ d\n",
      "PRED SCORE: -0.0023\n",
      "\n",
      "[2021-01-30 02:53:30,772 INFO] \n",
      "SENT 176: ['p', 'r', 'ə', 'p', 'o', 'ʊ', 'z']\n",
      "PRED 176: p r ə p o ʊ z d\n",
      "PRED SCORE: -0.0053\n",
      "\n",
      "[2021-01-30 02:53:30,772 INFO] \n",
      "SENT 177: ['b', 'ɹ', 'ɪ', 'ŋ']\n",
      "PRED 177: b ɹ æ ŋ\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-01-30 02:53:30,772 INFO] \n",
      "SENT 178: ['θ', 'ʌ', 'm', 'p']\n",
      "PRED 178: θ ʌ m p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,772 INFO] \n",
      "SENT 179: ['k', 'ɑ', 'n', 's', 'o', 'ʊ', 'l']\n",
      "PRED 179: k ɑ n s o ʊ l d\n",
      "PRED SCORE: -0.0229\n",
      "\n",
      "[2021-01-30 02:53:30,773 INFO] \n",
      "SENT 180: ['p', 'ɪ', 'ʧ']\n",
      "PRED 180: p ɪ ʧ t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,842 INFO] \n",
      "SENT 181: ['a', 'ɪ', 'ə', 'r', 'n']\n",
      "PRED 181: a ɪ ə r n d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:30,842 INFO] \n",
      "SENT 182: ['s', 'ɛ', 'p', 'ə', 'r', 'e', 'ɪ', 't']\n",
      "PRED 182: s ɛ p ə r e ɪ t ɪ d\n",
      "PRED SCORE: -0.1178\n",
      "\n",
      "[2021-01-30 02:53:30,842 INFO] \n",
      "SENT 183: ['w', 'ɪ', 's', 'ə', 'l']\n",
      "PRED 183: w ɪ s ə l d\n",
      "PRED SCORE: -0.0431\n",
      "\n",
      "[2021-01-30 02:53:30,843 INFO] \n",
      "SENT 184: ['k', 'a', 'ʊ', 'n', 't']\n",
      "PRED 184: k a ʊ n t ɪ d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:30,843 INFO] \n",
      "SENT 185: ['m', 'u']\n",
      "PRED 185: m u d\n",
      "PRED SCORE: -0.0020\n",
      "\n",
      "[2021-01-30 02:53:30,843 INFO] \n",
      "SENT 186: ['l', 'u', 'z']\n",
      "PRED 186: l u z d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,843 INFO] \n",
      "SENT 187: ['t', 'æ', 'ɡ']\n",
      "PRED 187: t æ ɡ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:30,844 INFO] \n",
      "SENT 188: ['ɹ', 'ɪ', 'ɡ', 'ə', 'l']\n",
      "PRED 188: ɹ ɪ ɡ ə l d\n",
      "PRED SCORE: -0.0021\n",
      "\n",
      "[2021-01-30 02:53:30,844 INFO] \n",
      "SENT 189: ['w', 'i']\n",
      "PRED 189: w i d\n",
      "PRED SCORE: -0.0435\n",
      "\n",
      "[2021-01-30 02:53:30,844 INFO] \n",
      "SENT 190: ['g', 'r', 'i', 't']\n",
      "PRED 190: g r i t ɪ d\n",
      "PRED SCORE: -0.0019\n",
      "\n",
      "[2021-01-30 02:53:30,845 INFO] \n",
      "SENT 191: ['t', '͡', 'ʃ', 'ɹ', '̩', 'p']\n",
      "PRED 191: t ͡ ʃ ɹ ̩ p t\n",
      "PRED SCORE: -0.2736\n",
      "\n",
      "[2021-01-30 02:53:30,845 INFO] \n",
      "SENT 192: ['k', 'l', 'æ', 's', 'p']\n",
      "PRED 192: k l æ s p t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:30,845 INFO] \n",
      "SENT 193: ['s', 't', 'ʌ', 'd', 'i']\n",
      "PRED 193: s t ʌ d i d\n",
      "PRED SCORE: -0.0018\n",
      "\n",
      "[2021-01-30 02:53:30,845 INFO] \n",
      "SENT 194: ['p', 'l', 'i', 'z']\n",
      "PRED 194: p l o ʊ z\n",
      "PRED SCORE: -0.3160\n",
      "\n",
      "[2021-01-30 02:53:30,846 INFO] \n",
      "SENT 195: ['ʃ', 'ɹ', 'ɪ', 'v', 'ə', 'l']\n",
      "PRED 195: ʃ ɹ ɪ v ə l d\n",
      "PRED SCORE: -0.0088\n",
      "\n",
      "[2021-01-30 02:53:30,846 INFO] \n",
      "SENT 196: ['ʃ', 'u', 't']\n",
      "PRED 196: ʃ u t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,846 INFO] \n",
      "SENT 197: ['ɹ', 'æ', 'p']\n",
      "PRED 197: ɹ æ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,846 INFO] \n",
      "SENT 198: ['p', 'ɔ']\n",
      "PRED 198: p ɔ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:30,846 INFO] \n",
      "SENT 199: ['f', 'ɪ', 'g', 'j', 'ə', 'r']\n",
      "PRED 199: f ɪ g j ə r d\n",
      "PRED SCORE: -0.0018\n",
      "\n",
      "[2021-01-30 02:53:30,847 INFO] \n",
      "SENT 200: ['k', 'ɑ', 'n', 'v', 'ə', 'r', 't']\n",
      "PRED 200: k ɑ n v ə r t ɪ d\n",
      "PRED SCORE: -0.0114\n",
      "\n",
      "[2021-01-30 02:53:30,847 INFO] PRED AVG SCORE: -0.0085, PRED PPL: 1.0085\n",
      "[2021-01-30 02:53:32,357 INFO] Translating shard 0.\n",
      "[2021-01-30 02:53:32,456 INFO] \n",
      "SENT 1: ['ɛ', 'k', 'o', 'ʊ']\n",
      "PRED 1: ɛ k o ʊ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,456 INFO] \n",
      "SENT 2: ['s', 'ɛ', 'n', 'd']\n",
      "PRED 2: s ɛ n t\n",
      "PRED SCORE: -0.2655\n",
      "\n",
      "[2021-01-30 02:53:32,456 INFO] \n",
      "SENT 3: ['s', 'l', 'a', 'ɪ', 'd']\n",
      "PRED 3: s l a ɪ d ɪ d\n",
      "PRED SCORE: -0.0067\n",
      "\n",
      "[2021-01-30 02:53:32,457 INFO] \n",
      "SENT 4: ['s', 't', 'ɹ', 'ɪ', 'p']\n",
      "PRED 4: s t ɹ ɪ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,457 INFO] \n",
      "SENT 5: ['d', 'r', 'a', 'ɪ', 'v']\n",
      "PRED 5: d r a ɪ v d\n",
      "PRED SCORE: -0.0046\n",
      "\n",
      "[2021-01-30 02:53:32,457 INFO] \n",
      "SENT 6: ['ə', 'r', 'a', 'ɪ', 'v']\n",
      "PRED 6: ə r a ɪ v d\n",
      "PRED SCORE: -0.0035\n",
      "\n",
      "[2021-01-30 02:53:32,458 INFO] \n",
      "SENT 7: ['r', 'e', 'ɪ', 's']\n",
      "PRED 7: r e ɪ s t\n",
      "PRED SCORE: -0.0178\n",
      "\n",
      "[2021-01-30 02:53:32,458 INFO] \n",
      "SENT 8: ['r', 'o', 'ʊ', 'l']\n",
      "PRED 8: r o ʊ l d\n",
      "PRED SCORE: -0.0040\n",
      "\n",
      "[2021-01-30 02:53:32,458 INFO] \n",
      "SENT 9: ['æ', 'n', 's', 'ə', 'r']\n",
      "PRED 9: æ n s ə r d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,459 INFO] \n",
      "SENT 10: ['k', 'r', 'u', 's', 'ə', 'f', 'a', 'ɪ']\n",
      "PRED 10: k r u s ə f a ɪ d\n",
      "PRED SCORE: -0.0144\n",
      "\n",
      "[2021-01-30 02:53:32,459 INFO] \n",
      "SENT 11: ['h', 'o', 'ʊ', 'p']\n",
      "PRED 11: h ɛ p t\n",
      "PRED SCORE: -0.0045\n",
      "\n",
      "[2021-01-30 02:53:32,459 INFO] \n",
      "SENT 12: ['d', 'ə', 'p', 'ɑ', 'z', 'ɪ', 't']\n",
      "PRED 12: d ə p ɑ z ɪ t ɪ d\n",
      "PRED SCORE: -0.0640\n",
      "\n",
      "[2021-01-30 02:53:32,459 INFO] \n",
      "SENT 13: ['p', 'æ', 'ʧ']\n",
      "PRED 13: p æ ʧ t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,460 INFO] \n",
      "SENT 14: ['ɡ', 'ɹ', 'ʌ', 'n', 't']\n",
      "PRED 14: ɡ ɹ ʌ n t ɪ d\n",
      "PRED SCORE: -0.6297\n",
      "\n",
      "[2021-01-30 02:53:32,460 INFO] \n",
      "SENT 15: ['s', 'ɑ', 'l', 'v']\n",
      "PRED 15: s ɑ l v d\n",
      "PRED SCORE: -0.0036\n",
      "\n",
      "[2021-01-30 02:53:32,460 INFO] \n",
      "SENT 16: ['l', 'e', 'ɪ', 'ə', 'r']\n",
      "PRED 16: l e ɪ ə r d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:32,460 INFO] \n",
      "SENT 17: ['ɪ', 'l', 'ə', 's', 't', 'r', 'e', 'ɪ', 't']\n",
      "PRED 17: ɪ l ə s t r e ɪ t ɪ d\n",
      "PRED SCORE: -0.0049\n",
      "\n",
      "[2021-01-30 02:53:32,460 INFO] \n",
      "SENT 18: ['m', 'æ', 'n', 'ə', 'ʤ']\n",
      "PRED 18: m æ n ə ʤ d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:32,461 INFO] \n",
      "SENT 19: ['p', 'ʌ', 'm', 'p']\n",
      "PRED 19: p ʌ m p t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:32,461 INFO] \n",
      "SENT 20: ['d', 'ɪ', 's', 'ɡ', 'ʌ', 's', 't']\n",
      "PRED 20: d ɪ s ɡ ʌ s t ɪ d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:32,461 INFO] \n",
      "SENT 21: ['p', 'ʌ', 'n', 'ɪ', 'ʃ']\n",
      "PRED 21: p ʌ n ɪ ʃ t\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:32,461 INFO] \n",
      "SENT 22: ['d', 'ɪ', 's', 'ɛ', 'n', 'd']\n",
      "PRED 22: d ɪ s ɛ n d ɪ d\n",
      "PRED SCORE: -0.0024\n",
      "\n",
      "[2021-01-30 02:53:32,461 INFO] \n",
      "SENT 23: ['s', 't', 'ɹ', 'æ', 'ŋ', 'ɡ', 'ə', 'l']\n",
      "PRED 23: s t ɹ æ ŋ ɡ ə l d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:32,462 INFO] \n",
      "SENT 24: ['s', 'ə', 'r', 'ʧ']\n",
      "PRED 24: s ə r ʧ t\n",
      "PRED SCORE: -0.0018\n",
      "\n",
      "[2021-01-30 02:53:32,462 INFO] \n",
      "SENT 25: ['h', 'ʌ', 'n', 't']\n",
      "PRED 25: h ʌ n t\n",
      "PRED SCORE: -0.3600\n",
      "\n",
      "[2021-01-30 02:53:32,462 INFO] \n",
      "SENT 26: ['b', 'ɑ', 'r', 'ʤ']\n",
      "PRED 26: b ɑ r ʤ d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:32,462 INFO] \n",
      "SENT 27: ['h', 'ʌ', 'm']\n",
      "PRED 27: h ʌ m d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:32,462 INFO] \n",
      "SENT 28: ['m', 'i', 'n']\n",
      "PRED 28: m ɛ n d\n",
      "PRED SCORE: -0.1642\n",
      "\n",
      "[2021-01-30 02:53:32,463 INFO] \n",
      "SENT 29: ['d', 'ɪ', 'z', 'ə', 'r', 'v']\n",
      "PRED 29: d ɪ z ə r v d\n",
      "PRED SCORE: -0.0055\n",
      "\n",
      "[2021-01-30 02:53:32,463 INFO] \n",
      "SENT 30: ['s', 'k', 'ə', 'r', 'i']\n",
      "PRED 30: s k ə r i d\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-01-30 02:53:32,556 INFO] \n",
      "SENT 31: ['r', 'ɪ', 'k', 'ɔ', 'r', 'd']\n",
      "PRED 31: r ɪ k ɔ r d ɪ d\n",
      "PRED SCORE: -0.0302\n",
      "\n",
      "[2021-01-30 02:53:32,556 INFO] \n",
      "SENT 32: ['j', 'u', 'n', 'a', 'ɪ', 't']\n",
      "PRED 32: j u n a ɪ t ɪ d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:32,556 INFO] \n",
      "SENT 33: ['b', 'æ', 'k']\n",
      "PRED 33: b æ k t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,556 INFO] \n",
      "SENT 34: ['ɪ', 'g', 'z', 'ɔ', 's', 't']\n",
      "PRED 34: ɪ g z ɔ s t ɪ d\n",
      "PRED SCORE: -0.6393\n",
      "\n",
      "[2021-01-30 02:53:32,557 INFO] \n",
      "SENT 35: ['p', 'a', 'ʊ', 'n', 's']\n",
      "PRED 35: p a ʊ n s t\n",
      "PRED SCORE: -0.0115\n",
      "\n",
      "[2021-01-30 02:53:32,557 INFO] \n",
      "SENT 36: ['k', 'ɑ', 'ɹ', 'v']\n",
      "PRED 36: k ɑ ɹ v d\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-01-30 02:53:32,557 INFO] \n",
      "SENT 37: ['s', 't', 'ʌ', 'm', 'b', 'ə', 'l']\n",
      "PRED 37: s t ʌ m b ə l d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,557 INFO] \n",
      "SENT 38: ['o', 'ʊ', 'p', 'ə', 'n']\n",
      "PRED 38: o ʊ p ə n d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:32,557 INFO] \n",
      "SENT 39: ['t', 'r', 'æ', 'n', 's', 'k', 'r', 'a', 'ɪ', 'b']\n",
      "PRED 39: t r æ n s k r a ɪ b d\n",
      "PRED SCORE: -0.0172\n",
      "\n",
      "[2021-01-30 02:53:32,558 INFO] \n",
      "SENT 40: ['v', 'ɑ', 'l', 'ə', 'n', 't', 'ɪ', 'ɹ']\n",
      "PRED 40: v ɑ l ə n t ɪ ɹ d\n",
      "PRED SCORE: -0.0022\n",
      "\n",
      "[2021-01-30 02:53:32,558 INFO] \n",
      "SENT 41: ['h', 'a', 'ʊ', 'l']\n",
      "PRED 41: h a ʊ l d\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-01-30 02:53:32,558 INFO] \n",
      "SENT 42: ['s', 'p', 'i', 'k']\n",
      "PRED 42: s p i k t\n",
      "PRED SCORE: -0.0207\n",
      "\n",
      "[2021-01-30 02:53:32,558 INFO] \n",
      "SENT 43: ['s', 'ə', 's', 'p', 'ɛ', 'k', 't']\n",
      "PRED 43: s ə s p ɛ k t ɪ d\n",
      "PRED SCORE: -0.0292\n",
      "\n",
      "[2021-01-30 02:53:32,559 INFO] \n",
      "SENT 44: ['ə', 'f', 'ɛ', 'k', 't']\n",
      "PRED 44: ə f ɛ k t ɪ d\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-01-30 02:53:32,559 INFO] \n",
      "SENT 45: ['p', 'a', 'ʊ', 'n', 'd']\n",
      "PRED 45: p a ʊ n d ɪ d\n",
      "PRED SCORE: -0.0392\n",
      "\n",
      "[2021-01-30 02:53:32,559 INFO] \n",
      "SENT 46: ['k', 'ə', 'm', 'p', 'l', 'e', 'ɪ', 'n']\n",
      "PRED 46: k ə m p l e ɪ n d\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-01-30 02:53:32,559 INFO] \n",
      "SENT 47: ['k', 'ɹ', 'i', 'p']\n",
      "PRED 47: k ɹ æ p t\n",
      "PRED SCORE: -0.4174\n",
      "\n",
      "[2021-01-30 02:53:32,560 INFO] \n",
      "SENT 48: ['s', 'ə', 'r', 'v', 'a', 'ɪ', 'v']\n",
      "PRED 48: s ə r v a ɪ v d\n",
      "PRED SCORE: -0.0337\n",
      "\n",
      "[2021-01-30 02:53:32,560 INFO] \n",
      "SENT 49: ['k', 'ə', 'n', 's', 't', 'ɹ', 'ʌ', 'k', 't']\n",
      "PRED 49: k ə n s t ɹ ʌ k t ɪ d\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-01-30 02:53:32,560 INFO] \n",
      "SENT 50: ['s', 'ə', 'ʤ', 'ɛ', 's', 't']\n",
      "PRED 50: s ə ʤ ɛ s t ɪ d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:32,560 INFO] \n",
      "SENT 51: ['b', 'ɑ', 't', 'ə', 'l']\n",
      "PRED 51: b ɑ t ə l d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,560 INFO] \n",
      "SENT 52: ['b', 'i', 'p']\n",
      "PRED 52: b ɛ p t\n",
      "PRED SCORE: -0.1523\n",
      "\n",
      "[2021-01-30 02:53:32,561 INFO] \n",
      "SENT 53: ['p', 'ɪ', 'n']\n",
      "PRED 53: p ɪ n d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:32,561 INFO] \n",
      "SENT 54: ['ɪ', 'm', 'b', 'a', 'ɪ', 'b']\n",
      "PRED 54: ɪ m b a ɪ b d\n",
      "PRED SCORE: -0.0037\n",
      "\n",
      "[2021-01-30 02:53:32,561 INFO] \n",
      "SENT 55: ['l', 'u', 's', 'ə', 'n']\n",
      "PRED 55: l ɔ s ə n d\n",
      "PRED SCORE: -0.6202\n",
      "\n",
      "[2021-01-30 02:53:32,561 INFO] \n",
      "SENT 56: ['t', 'ɪ', 'l', 't']\n",
      "PRED 56: t ɪ l t ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,561 INFO] \n",
      "SENT 57: ['m', 'o', 'ʊ']\n",
      "PRED 57: m o ʊ d\n",
      "PRED SCORE: -0.2126\n",
      "\n",
      "[2021-01-30 02:53:32,562 INFO] \n",
      "SENT 58: ['p', 'ɛ', 't', 'r', 'ə', 'f', 'a', 'ɪ']\n",
      "PRED 58: p ɛ t r ə f a ɪ d\n",
      "PRED SCORE: -0.0024\n",
      "\n",
      "[2021-01-30 02:53:32,562 INFO] \n",
      "SENT 59: ['ɹ', 'ɪ', 'f', 'j', 'u', 'ː', 'z']\n",
      "PRED 59: ɹ ɪ f j u z d\n",
      "PRED SCORE: -0.8985\n",
      "\n",
      "[2021-01-30 02:53:32,562 INFO] \n",
      "SENT 60: ['p', 'r', 'u', 'v']\n",
      "PRED 60: p r u v d\n",
      "PRED SCORE: -0.0036\n",
      "\n",
      "[2021-01-30 02:53:32,651 INFO] \n",
      "SENT 61: ['k', 'æ', 'm', 'p']\n",
      "PRED 61: k æ m p t\n",
      "PRED SCORE: -0.0132\n",
      "\n",
      "[2021-01-30 02:53:32,652 INFO] \n",
      "SENT 62: ['l', 'ə', 'r', 'n']\n",
      "PRED 62: l ə r n d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:32,652 INFO] \n",
      "SENT 63: ['k', 'ə', 'l', 'æ', 'p', 's']\n",
      "PRED 63: k ə l æ p s t\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-01-30 02:53:32,652 INFO] \n",
      "SENT 64: ['ʃ', 'ʌ', 'v']\n",
      "PRED 64: ʃ ʌ v d\n",
      "PRED SCORE: -0.0101\n",
      "\n",
      "[2021-01-30 02:53:32,652 INFO] \n",
      "SENT 65: ['ə', 'p', 'ɑ', 'l', 'ə', 'ʤ', 'a', 'ɪ', 'z']\n",
      "PRED 65: ə p ɑ l ə ʤ a ɪ z d\n",
      "PRED SCORE: -0.0025\n",
      "\n",
      "[2021-01-30 02:53:32,653 INFO] \n",
      "SENT 66: ['z', 'u', 'm']\n",
      "PRED 66: z u m d\n",
      "PRED SCORE: -0.0027\n",
      "\n",
      "[2021-01-30 02:53:32,653 INFO] \n",
      "SENT 67: ['h', 'ɪ', 'ɹ']\n",
      "PRED 67: h ɪ ɹ d\n",
      "PRED SCORE: -0.1890\n",
      "\n",
      "[2021-01-30 02:53:32,653 INFO] \n",
      "SENT 68: ['s', 't', 'i', 'm']\n",
      "PRED 68: s t ɛ m d\n",
      "PRED SCORE: -0.3266\n",
      "\n",
      "[2021-01-30 02:53:32,653 INFO] \n",
      "SENT 69: ['k', 'ɔ', 'r', 'n', 'ə', 'r']\n",
      "PRED 69: k ɔ r n n ə r d\n",
      "PRED SCORE: -0.0017\n",
      "\n",
      "[2021-01-30 02:53:32,654 INFO] \n",
      "SENT 70: ['θ', 'ɹ', 'ɛ', 'd']\n",
      "PRED 70: θ ɹ ɛ d ɪ d\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:32,654 INFO] \n",
      "SENT 71: ['b', 'e', 'ɪ', 'k']\n",
      "PRED 71: b e ɪ k t\n",
      "PRED SCORE: -0.1762\n",
      "\n",
      "[2021-01-30 02:53:32,654 INFO] \n",
      "SENT 72: ['b', 'ʌ', 's', 't']\n",
      "PRED 72: b ʌ s t ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,654 INFO] \n",
      "SENT 73: ['k', 'ɔ', 's', 't']\n",
      "PRED 73: k ɔ s t ɪ d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:32,654 INFO] \n",
      "SENT 74: ['ʃ', 'ə', 'n', 't']\n",
      "PRED 74: ʃ ə n t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,655 INFO] \n",
      "SENT 75: ['b', 'e', 'ɪ', 'ð']\n",
      "PRED 75: b e ɪ ð d\n",
      "PRED SCORE: -0.1578\n",
      "\n",
      "[2021-01-30 02:53:32,655 INFO] \n",
      "SENT 76: ['s', 'l', 'æ', 'm']\n",
      "PRED 76: s l æ m d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,655 INFO] \n",
      "SENT 77: ['s', 'n', 'i', 'k']\n",
      "PRED 77: s n i k t\n",
      "PRED SCORE: -0.0038\n",
      "\n",
      "[2021-01-30 02:53:32,655 INFO] \n",
      "SENT 78: ['p', 'a', 'ɪ', 'p']\n",
      "PRED 78: p a ɪ p t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:32,655 INFO] \n",
      "SENT 79: ['s', 'n', 'ʌ', 'ɡ', 'ə', 'l']\n",
      "PRED 79: s n ʌ ɡ ə l d\n",
      "PRED SCORE: -0.0013\n",
      "\n",
      "[2021-01-30 02:53:32,656 INFO] \n",
      "SENT 80: ['s', 't', 'ɪ', 't', '͡', 'ʃ']\n",
      "PRED 80: s t ɪ t ͡ ʃ t\n",
      "PRED SCORE: -0.3015\n",
      "\n",
      "[2021-01-30 02:53:32,656 INFO] \n",
      "SENT 81: ['p', 'r', 'ə', 'v', 'a', 'ɪ', 'd']\n",
      "PRED 81: p r ə v a ɪ d ɪ d\n",
      "PRED SCORE: -0.0079\n",
      "\n",
      "[2021-01-30 02:53:32,656 INFO] \n",
      "SENT 82: ['k', 'ə', 'n', 't', 'r', 'o', 'ʊ', 'l']\n",
      "PRED 82: k ə n t r o ʊ l d\n",
      "PRED SCORE: -0.0271\n",
      "\n",
      "[2021-01-30 02:53:32,656 INFO] \n",
      "SENT 83: ['b', 'ɔ', 'ɹ']\n",
      "PRED 83: b ɔ ɹ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,656 INFO] \n",
      "SENT 84: ['t', 'w', 'ɑ', 'ŋ']\n",
      "PRED 84: t ɑ ŋ ŋ d\n",
      "PRED SCORE: -0.2838\n",
      "\n",
      "[2021-01-30 02:53:32,657 INFO] \n",
      "SENT 85: ['k', 'ə', 'n', 's', 'ɪ', 'd', 'ə', 'r']\n",
      "PRED 85: k ə n s ɪ d ə r d\n",
      "PRED SCORE: -0.0011\n",
      "\n",
      "[2021-01-30 02:53:32,657 INFO] \n",
      "SENT 86: ['g', 'e', 'ɪ', 'z']\n",
      "PRED 86: g e ɪ z d\n",
      "PRED SCORE: -0.1304\n",
      "\n",
      "[2021-01-30 02:53:32,657 INFO] \n",
      "SENT 87: ['h', 'ɔ', 'l']\n",
      "PRED 87: h ɔ l d\n",
      "PRED SCORE: -0.0020\n",
      "\n",
      "[2021-01-30 02:53:32,657 INFO] \n",
      "SENT 88: ['g', 'ɪ', 'g', 'ə', 'l']\n",
      "PRED 88: g ɪ g ə l d\n",
      "PRED SCORE: -0.0896\n",
      "\n",
      "[2021-01-30 02:53:32,657 INFO] \n",
      "SENT 89: ['s', 'e', 'ɪ', 'l']\n",
      "PRED 89: s e ɪ l d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,658 INFO] \n",
      "SENT 90: ['k', 'l', 'æ', 's']\n",
      "PRED 90: k l æ s t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,742 INFO] \n",
      "SENT 91: ['k', 'l', 'ʌ', 't', '͡', 'ʃ']\n",
      "PRED 91: k l ʌ t ͡ ʃ t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,742 INFO] \n",
      "SENT 92: ['ɹ', 'ɪ', 'n', 's']\n",
      "PRED 92: ɹ ɪ n s t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,742 INFO] \n",
      "SENT 93: ['p', 'ɹ', 'ɪ', 'k']\n",
      "PRED 93: p ɹ ɪ k t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,743 INFO] \n",
      "SENT 94: ['ɡ', 'l', 'æ', 'n', 's']\n",
      "PRED 94: ɡ l æ n s t\n",
      "PRED SCORE: -0.0052\n",
      "\n",
      "[2021-01-30 02:53:32,743 INFO] \n",
      "SENT 95: ['ɡ', 'ɹ', 'æ', 's', 'p']\n",
      "PRED 95: ɡ ɹ æ s p t\n",
      "PRED SCORE: -0.0022\n",
      "\n",
      "[2021-01-30 02:53:32,743 INFO] \n",
      "SENT 96: ['h', 'a', 'ɪ', 'ʤ', 'æ', 'k']\n",
      "PRED 96: h a ɪ ʤ æ k t\n",
      "PRED SCORE: -0.0043\n",
      "\n",
      "[2021-01-30 02:53:32,743 INFO] \n",
      "SENT 97: ['s', 'm', 'ə', 'ð', 'ə', 'r']\n",
      "PRED 97: s m ə ð ə r d\n",
      "PRED SCORE: -0.0163\n",
      "\n",
      "[2021-01-30 02:53:32,743 INFO] \n",
      "SENT 98: ['s', 'ɪ', 'v', 'ə', 'l', 'a', 'ɪ', 'z']\n",
      "PRED 98: s ɪ v ə l a ɪ z d\n",
      "PRED SCORE: -0.0029\n",
      "\n",
      "[2021-01-30 02:53:32,744 INFO] \n",
      "SENT 99: ['d', 'ɛ', 'k', 'ə', 'r', 'e', 'ɪ', 't']\n",
      "PRED 99: d ɛ k ə r e ɪ t ɪ d\n",
      "PRED SCORE: -0.0473\n",
      "\n",
      "[2021-01-30 02:53:32,744 INFO] \n",
      "SENT 100: ['p', 'æ', 's']\n",
      "PRED 100: p æ s t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,744 INFO] \n",
      "SENT 101: ['k', 'ɹ', 'æ', 'k', 'ə', 'l']\n",
      "PRED 101: k ɹ æ k ə l d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:32,744 INFO] \n",
      "SENT 102: ['b', 'ɛ', 'n', 'd']\n",
      "PRED 102: b ɛ n d ɪ d\n",
      "PRED SCORE: -0.0068\n",
      "\n",
      "[2021-01-30 02:53:32,745 INFO] \n",
      "SENT 103: ['b', 'ʌ', 'ŋ', 'ɡ', 'ə', 'l']\n",
      "PRED 103: b ʌ ŋ ɡ ə l d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,745 INFO] \n",
      "SENT 104: ['k', 'l', 'o', 'ʊ', 'z']\n",
      "PRED 104: k l o ʊ z d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,745 INFO] \n",
      "SENT 105: ['p', 'ɛ', 'p', 'ə', 'r']\n",
      "PRED 105: p ɛ p ə r d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,745 INFO] \n",
      "SENT 106: ['ʃ', 'ɪ', 'v', 'ə', 'r']\n",
      "PRED 106: ʃ ɪ v ə r d\n",
      "PRED SCORE: -0.0022\n",
      "\n",
      "[2021-01-30 02:53:32,745 INFO] \n",
      "SENT 107: ['θ', 'r', 'æ', 'ʃ']\n",
      "PRED 107: θ r æ ʃ t\n",
      "PRED SCORE: -0.0096\n",
      "\n",
      "[2021-01-30 02:53:32,746 INFO] \n",
      "SENT 108: ['s', 'k', 'æ', 'n']\n",
      "PRED 108: s k æ n d\n",
      "PRED SCORE: -0.1931\n",
      "\n",
      "[2021-01-30 02:53:32,746 INFO] \n",
      "SENT 109: ['s', 'k', 'r', 'e', 'ɪ', 'p']\n",
      "PRED 109: s k r e ɪ p t\n",
      "PRED SCORE: -0.0140\n",
      "\n",
      "[2021-01-30 02:53:32,746 INFO] \n",
      "SENT 110: ['ɔ', 'r', 'g', 'ə', 'n', 'a', 'ɪ', 'z']\n",
      "PRED 110: ɔ r g e ɪ z ə n d\n",
      "PRED SCORE: -0.0132\n",
      "\n",
      "[2021-01-30 02:53:32,746 INFO] \n",
      "SENT 111: ['ɛ', 's', 'k', 'ɔ', 'ɹ', 't']\n",
      "PRED 111: ɛ s k ɔ ɹ t ɪ d\n",
      "PRED SCORE: -0.0030\n",
      "\n",
      "[2021-01-30 02:53:32,746 INFO] \n",
      "SENT 112: ['r', 'o', 'ʊ']\n",
      "PRED 112: r u\n",
      "PRED SCORE: -0.0028\n",
      "\n",
      "[2021-01-30 02:53:32,747 INFO] \n",
      "SENT 113: ['s', 't', 'ʌ', 'm', 'p']\n",
      "PRED 113: s t ʌ m p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,747 INFO] \n",
      "SENT 114: ['t', 'i', 'z']\n",
      "PRED 114: t i z d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,748 INFO] \n",
      "SENT 115: ['t', 'ɛ', 's', 't']\n",
      "PRED 115: t ɛ s t ɪ d\n",
      "PRED SCORE: -0.0869\n",
      "\n",
      "[2021-01-30 02:53:32,748 INFO] \n",
      "SENT 116: ['h', 'o', 'ʊ', 'n']\n",
      "PRED 116: h ɛ n d\n",
      "PRED SCORE: -0.0587\n",
      "\n",
      "[2021-01-30 02:53:32,748 INFO] \n",
      "SENT 117: ['h', 'ɑ', 'b', 'ə', 'l']\n",
      "PRED 117: h ɑ b ə l d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:32,749 INFO] \n",
      "SENT 118: ['ʃ', 'ɹ', 'ɪ', 'ŋ', 'k']\n",
      "PRED 118: ʃ ɹ æ ŋ k\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:32,749 INFO] \n",
      "SENT 119: ['s', 'k', 'w', 'i', 'l']\n",
      "PRED 119: s k w i l d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:32,749 INFO] \n",
      "SENT 120: ['d', 'r', 'a', 'ʊ', 'n']\n",
      "PRED 120: d r a ʊ n d\n",
      "PRED SCORE: -0.0046\n",
      "\n",
      "[2021-01-30 02:53:32,838 INFO] \n",
      "SENT 121: ['b', 'ɑ', 'k', 's']\n",
      "PRED 121: b ɑ k s t\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:32,838 INFO] \n",
      "SENT 122: ['d', 'ɑ', 'k', 't', 'ɹ', '̩']\n",
      "PRED 122: d ɑ k t ɹ ̩ d\n",
      "PRED SCORE: -0.0011\n",
      "\n",
      "[2021-01-30 02:53:32,839 INFO] \n",
      "SENT 123: ['k', 'l', 'ɪ', 'ɹ']\n",
      "PRED 123: k l ɪ ɹ d\n",
      "PRED SCORE: -0.0151\n",
      "\n",
      "[2021-01-30 02:53:32,839 INFO] \n",
      "SENT 124: ['p', 'ɪ', 'ɡ']\n",
      "PRED 124: p ɪ ɡ d\n",
      "PRED SCORE: -0.0040\n",
      "\n",
      "[2021-01-30 02:53:32,839 INFO] \n",
      "SENT 125: ['k', 'ə', 'v', 'ə', 'r']\n",
      "PRED 125: k ə v ə r d\n",
      "PRED SCORE: -0.0036\n",
      "\n",
      "[2021-01-30 02:53:32,839 INFO] \n",
      "SENT 126: ['p', 'ɔ', 'ɹ']\n",
      "PRED 126: p ɔ ɹ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,840 INFO] \n",
      "SENT 127: ['ʤ', 'ə', 'g', 'ə', 'l']\n",
      "PRED 127: ʤ ə g ə l d\n",
      "PRED SCORE: -0.0048\n",
      "\n",
      "[2021-01-30 02:53:32,840 INFO] \n",
      "SENT 128: ['s', 'w', 'ɔ', 'ɹ', 'm']\n",
      "PRED 128: s w ɔ ɹ m d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,840 INFO] \n",
      "SENT 129: ['d', 'ɛ', 'k']\n",
      "PRED 129: d ɛ k t\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-01-30 02:53:32,840 INFO] \n",
      "SENT 130: ['n', 'e', 'ɪ', 'm']\n",
      "PRED 130: n e ɪ m d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:32,841 INFO] \n",
      "SENT 131: ['s', 'ə', 'b', 't', 'ɹ', 'æ', 'k', 't']\n",
      "PRED 131: s ə b t ɹ æ k t ɪ d\n",
      "PRED SCORE: -0.3573\n",
      "\n",
      "[2021-01-30 02:53:32,841 INFO] \n",
      "SENT 132: ['k', 'ɹ', 'o', 'w', 'k']\n",
      "PRED 132: k ɹ o w k t\n",
      "PRED SCORE: -0.0137\n",
      "\n",
      "[2021-01-30 02:53:32,841 INFO] \n",
      "SENT 133: ['ɪ', 'k', 's', 'k', 'l', 'e', 'ɪ', 'm']\n",
      "PRED 133: ɪ k s k l e ɪ m d\n",
      "PRED SCORE: -0.0222\n",
      "\n",
      "[2021-01-30 02:53:32,841 INFO] \n",
      "SENT 134: ['s', 'a', 'ʊ', 'n', 'd']\n",
      "PRED 134: s a ʊ n d ɪ d\n",
      "PRED SCORE: -0.1308\n",
      "\n",
      "[2021-01-30 02:53:32,841 INFO] \n",
      "SENT 135: ['f', 'e', 'ɪ', 'n', 't']\n",
      "PRED 135: f e ɪ n t ɪ d\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:32,842 INFO] \n",
      "SENT 136: ['n', 'ə', 'ʤ']\n",
      "PRED 136: n ə ʤ d\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-01-30 02:53:32,842 INFO] \n",
      "SENT 137: ['t', 'w', 'ɪ', 'ŋ', 'k', 'ə', 'l']\n",
      "PRED 137: t ɪ ŋ k ə l d\n",
      "PRED SCORE: -0.0020\n",
      "\n",
      "[2021-01-30 02:53:32,842 INFO] \n",
      "SENT 138: ['k', 'o', 'ʊ', 'm']\n",
      "PRED 138: k o ʊ m d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,842 INFO] \n",
      "SENT 139: ['s', 'k', 'r', 'u']\n",
      "PRED 139: s k r u d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:32,842 INFO] \n",
      "SENT 140: ['k', 'ɛ', 'r']\n",
      "PRED 140: k ɛ r d\n",
      "PRED SCORE: -0.0212\n",
      "\n",
      "[2021-01-30 02:53:32,842 INFO] \n",
      "SENT 141: ['m', 'ʌ', 'ʃ']\n",
      "PRED 141: m ʌ ʃ t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:32,843 INFO] \n",
      "SENT 142: ['t', 'ɪ', 'n']\n",
      "PRED 142: t ɪ n d\n",
      "PRED SCORE: -0.0510\n",
      "\n",
      "[2021-01-30 02:53:32,843 INFO] \n",
      "SENT 143: ['h', 'e', 'ɪ', 'l']\n",
      "PRED 143: h e ɪ l d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:32,843 INFO] \n",
      "SENT 144: ['g', 'ɑ', 'b', 'ə', 'l']\n",
      "PRED 144: g ɑ b ə l d\n",
      "PRED SCORE: -0.0852\n",
      "\n",
      "[2021-01-30 02:53:32,843 INFO] \n",
      "SENT 145: ['t', 'a', 'ɪ', 't', 'ə', 'n']\n",
      "PRED 145: t a ɪ t ə n d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,843 INFO] \n",
      "SENT 146: ['b', 'r', 'e', 'ɪ']\n",
      "PRED 146: b r e ɪ d\n",
      "PRED SCORE: -0.0211\n",
      "\n",
      "[2021-01-30 02:53:32,843 INFO] \n",
      "SENT 147: ['b', 'ʌ', 'ŋ']\n",
      "PRED 147: b ʌ ŋ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,844 INFO] \n",
      "SENT 148: ['ʃ', 'æ', 't', 'ə', 'r']\n",
      "PRED 148: ʃ æ t ə r d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,844 INFO] \n",
      "SENT 149: ['s', 'ə', 'p', 'o', 'ʊ', 'z']\n",
      "PRED 149: s ə p o ʊ z d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,844 INFO] \n",
      "SENT 150: ['d', 'r', 'a', 'ɪ']\n",
      "PRED 150: d r u\n",
      "PRED SCORE: -0.0658\n",
      "\n",
      "[2021-01-30 02:53:32,928 INFO] \n",
      "SENT 151: ['o', 'ʊ', 'n']\n",
      "PRED 151: o ʊ n d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,928 INFO] \n",
      "SENT 152: ['ɡ', 'ɹ', 'ɪ', 'l']\n",
      "PRED 152: ɡ ɹ ɪ l d\n",
      "PRED SCORE: -0.0138\n",
      "\n",
      "[2021-01-30 02:53:32,929 INFO] \n",
      "SENT 153: ['t', 'i', 't', '͡', 'ʃ']\n",
      "PRED 153: t i t ͡ ʃ t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,929 INFO] \n",
      "SENT 154: ['ʧ', 'u']\n",
      "PRED 154: ʧ u d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:32,929 INFO] \n",
      "SENT 155: ['b', 'ʌ', 'ɡ']\n",
      "PRED 155: b ʌ ɡ d\n",
      "PRED SCORE: -0.0017\n",
      "\n",
      "[2021-01-30 02:53:32,929 INFO] \n",
      "SENT 156: ['t', 'e', 'ɪ', 'p']\n",
      "PRED 156: t e ɪ p t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,930 INFO] \n",
      "SENT 157: ['ɛ', 'ʤ']\n",
      "PRED 157: ɛ ʤ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,930 INFO] \n",
      "SENT 158: ['ɪ', 'ɡ', 'z', 'æ', 'm', 'ɪ', 'n']\n",
      "PRED 158: ɪ ɡ z æ m ɪ n d\n",
      "PRED SCORE: -0.5714\n",
      "\n",
      "[2021-01-30 02:53:32,930 INFO] \n",
      "SENT 159: ['ə', 'ɡ', 'ɹ', 'i']\n",
      "PRED 159: ə ɡ ɹ i d\n",
      "PRED SCORE: -0.3787\n",
      "\n",
      "[2021-01-30 02:53:32,930 INFO] \n",
      "SENT 160: ['ɹ', 'ʌ', 'm', 'b', 'ə', 'l']\n",
      "PRED 160: ɹ ʌ m b ə l d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:32,930 INFO] \n",
      "SENT 161: ['k', 'ə', 'n', 'g', 'r', 'æ', 'ʧ', 'ə', 'l', 'e', 'ɪ', 't']\n",
      "PRED 161: k ə n g r e ɪ t ə l e ɪ t\n",
      "PRED SCORE: -0.1544\n",
      "\n",
      "[2021-01-30 02:53:32,931 INFO] \n",
      "SENT 162: ['g', 'r', 'æ', 'ʤ', 'ə', 'w', 'e', 'ɪ', 't']\n",
      "PRED 162: g r æ ʤ ə w e ɪ t ɪ d\n",
      "PRED SCORE: -0.3364\n",
      "\n",
      "[2021-01-30 02:53:32,931 INFO] \n",
      "SENT 163: ['h', 'ɑ', 'r', 'd', 'ə', 'n']\n",
      "PRED 163: h ɑ r d ə n d\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-01-30 02:53:32,931 INFO] \n",
      "SENT 164: ['n', 'ɪ', 'b', 'ə', 'l']\n",
      "PRED 164: n ɪ b ə l d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:32,931 INFO] \n",
      "SENT 165: ['f', 'e', 'ɪ', 'd']\n",
      "PRED 165: f e ɪ d ɪ d\n",
      "PRED SCORE: -0.0017\n",
      "\n",
      "[2021-01-30 02:53:32,931 INFO] \n",
      "SENT 166: ['ə', 'r', 'ɛ', 's', 't']\n",
      "PRED 166: ə r ɛ s t ɪ d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:32,932 INFO] \n",
      "SENT 167: ['b', 'ʌ', 'z']\n",
      "PRED 167: b ʌ z d\n",
      "PRED SCORE: -0.0143\n",
      "\n",
      "[2021-01-30 02:53:32,932 INFO] \n",
      "SENT 168: ['s', 'ɛ', 'p', 'ə', 'r', 'e', 'ɪ', 't']\n",
      "PRED 168: s ɛ p ə r e ɪ t ɪ d\n",
      "PRED SCORE: -0.0021\n",
      "\n",
      "[2021-01-30 02:53:32,932 INFO] \n",
      "SENT 169: ['ɹ', 'æ', 'p']\n",
      "PRED 169: ɹ æ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,932 INFO] \n",
      "SENT 170: ['e', 'ɪ', 'm']\n",
      "PRED 170: e ɪ m d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,932 INFO] \n",
      "SENT 171: ['f', 'ɪ', 'k', 's']\n",
      "PRED 171: f ɪ k s t\n",
      "PRED SCORE: -0.0012\n",
      "\n",
      "[2021-01-30 02:53:32,932 INFO] \n",
      "SENT 172: ['k', 'æ', 't', '͡', 'ʃ']\n",
      "PRED 172: k æ t ͡ ʃ t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,933 INFO] \n",
      "SENT 173: ['b', 'ɔ', 'l']\n",
      "PRED 173: b ɔ l d\n",
      "PRED SCORE: -0.1465\n",
      "\n",
      "[2021-01-30 02:53:32,933 INFO] \n",
      "SENT 174: ['s', 'k', 'u', 'p']\n",
      "PRED 174: s k u p t\n",
      "PRED SCORE: -0.0240\n",
      "\n",
      "[2021-01-30 02:53:32,933 INFO] \n",
      "SENT 175: ['t', 'ʌ', 'k']\n",
      "PRED 175: t ʌ k t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,933 INFO] \n",
      "SENT 176: ['θ', 'r', 'ɛ', 't', 'ə', 'n']\n",
      "PRED 176: θ r ɛ t ə n d\n",
      "PRED SCORE: -0.0048\n",
      "\n",
      "[2021-01-30 02:53:32,933 INFO] \n",
      "SENT 177: ['r', 'ɪ', 's', 'i', 'v']\n",
      "PRED 177: r ɪ s i v d\n",
      "PRED SCORE: -0.0470\n",
      "\n",
      "[2021-01-30 02:53:32,934 INFO] \n",
      "SENT 178: ['j', 'ɔ', 'n']\n",
      "PRED 178: j ɔ n d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,934 INFO] \n",
      "SENT 179: ['s', 'ʌ', 'm', 'ə', 'n']\n",
      "PRED 179: s ʌ m ə n d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:32,934 INFO] \n",
      "SENT 180: ['b', 'o', 'ʊ']\n",
      "PRED 180: b ɛ n d\n",
      "PRED SCORE: -1.1251\n",
      "\n",
      "[2021-01-30 02:53:32,997 INFO] \n",
      "SENT 181: ['h', 'ɪ', 'ʧ']\n",
      "PRED 181: h ɪ ʧ t\n",
      "PRED SCORE: -0.0019\n",
      "\n",
      "[2021-01-30 02:53:32,997 INFO] \n",
      "SENT 182: ['ʧ', 'ə', 'r', 'n']\n",
      "PRED 182: ʧ ə r n d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,997 INFO] \n",
      "SENT 183: ['o', 'ʊ', 'v', 'ə', 'r', 'w', 'ɛ', 'l', 'm']\n",
      "PRED 183: o ʊ v ə r w ɛ l s t\n",
      "PRED SCORE: -0.6212\n",
      "\n",
      "[2021-01-30 02:53:32,998 INFO] \n",
      "SENT 184: ['w', 'ɪ', 'n']\n",
      "PRED 184: w ɪ n d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,998 INFO] \n",
      "SENT 185: ['p', 'e', 'ɪ', 'ʤ']\n",
      "PRED 185: p e ɪ ʤ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,998 INFO] \n",
      "SENT 186: ['r', 'e', 'ɪ', 'v']\n",
      "PRED 186: r e ɪ v d\n",
      "PRED SCORE: -0.0375\n",
      "\n",
      "[2021-01-30 02:53:32,998 INFO] \n",
      "SENT 187: ['f', 'l', 'æ', 't', 'ə', 'r']\n",
      "PRED 187: f l æ t ə r d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:32,999 INFO] \n",
      "SENT 188: ['o', 'ʊ', 'm', 'ɪ', 't']\n",
      "PRED 188: o ʊ m ɪ t ɪ d\n",
      "PRED SCORE: -0.0799\n",
      "\n",
      "[2021-01-30 02:53:32,999 INFO] \n",
      "SENT 189: ['f', 'ɑ', 'ɹ', 't']\n",
      "PRED 189: f ɑ ɹ t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,999 INFO] \n",
      "SENT 190: ['n', 'i', 'd']\n",
      "PRED 190: n i d ɪ d\n",
      "PRED SCORE: -0.0021\n",
      "\n",
      "[2021-01-30 02:53:32,999 INFO] \n",
      "SENT 191: ['b', 'l', 'æ', 's', 't']\n",
      "PRED 191: b l æ s t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:32,999 INFO] \n",
      "SENT 192: ['h', 'o', 'ʊ']\n",
      "PRED 192: h ɛ o ʊ d\n",
      "PRED SCORE: -0.0504\n",
      "\n",
      "[2021-01-30 02:53:33,000 INFO] \n",
      "SENT 193: ['ɡ', 'ɹ', '̩', 'd']\n",
      "PRED 193: ɡ ɹ ̩ d\n",
      "PRED SCORE: -0.7074\n",
      "\n",
      "[2021-01-30 02:53:33,000 INFO] \n",
      "SENT 194: ['l', 'o', 'ʊ']\n",
      "PRED 194: l u\n",
      "PRED SCORE: -0.3374\n",
      "\n",
      "[2021-01-30 02:53:33,000 INFO] \n",
      "SENT 195: ['l', 'ʌ', 'v']\n",
      "PRED 195: l ʌ v d\n",
      "PRED SCORE: -0.4329\n",
      "\n",
      "[2021-01-30 02:53:33,000 INFO] \n",
      "SENT 196: ['k', 'j', 'ʊ', 'ɹ']\n",
      "PRED 196: k j ʊ ɹ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:33,000 INFO] \n",
      "SENT 197: ['p', 'ɹ', 'i', 'p', 'ɛ', 'ɹ']\n",
      "PRED 197: p ɹ i p ɛ ɹ d\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-01-30 02:53:33,001 INFO] \n",
      "SENT 198: ['g', 'l', 'ɪ', 's', 'ə', 'n']\n",
      "PRED 198: g l ɪ s ə n d\n",
      "PRED SCORE: -0.1154\n",
      "\n",
      "[2021-01-30 02:53:33,001 INFO] \n",
      "SENT 199: ['w', 'i', 'l']\n",
      "PRED 199: w i l d\n",
      "PRED SCORE: -0.0817\n",
      "\n",
      "[2021-01-30 02:53:33,001 INFO] \n",
      "SENT 200: ['ɹ', 'ʌ', 's', 't']\n",
      "PRED 200: ɹ ʌ s t ɪ d\n",
      "PRED SCORE: -0.0037\n",
      "\n",
      "[2021-01-30 02:53:33,001 INFO] PRED AVG SCORE: -0.0109, PRED PPL: 1.0109\n",
      "[2021-01-30 02:53:34,486 INFO] Translating shard 0.\n",
      "[2021-01-30 02:53:34,586 INFO] \n",
      "SENT 1: ['p', 'ʌ', 'n', 'ɪ', 'ʃ']\n",
      "PRED 1: p ʌ n ɪ ʃ t\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:34,586 INFO] \n",
      "SENT 2: ['d', 'ɪ', 'm', 'æ', 'n', 'd']\n",
      "PRED 2: d ɪ m æ n d ɪ d\n",
      "PRED SCORE: -0.0629\n",
      "\n",
      "[2021-01-30 02:53:34,587 INFO] \n",
      "SENT 3: ['k', 'e', 'j', 'd', '͡', 'ʒ']\n",
      "PRED 3: k e j d j ʒ d\n",
      "PRED SCORE: -0.0730\n",
      "\n",
      "[2021-01-30 02:53:34,587 INFO] \n",
      "SENT 4: ['t', 'æ', 'n']\n",
      "PRED 4: t æ n d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:34,587 INFO] \n",
      "SENT 5: ['s', 'ɪ', 'ʧ', 'u', 'e', 'ɪ', 't']\n",
      "PRED 5: s ɪ ʧ u e ɪ t ɪ d\n",
      "PRED SCORE: -0.0089\n",
      "\n",
      "[2021-01-30 02:53:34,587 INFO] \n",
      "SENT 6: ['ʧ', 'ə', 'r', 'n']\n",
      "PRED 6: ʧ ə r n d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:34,588 INFO] \n",
      "SENT 7: ['s', 'o', 'ʊ']\n",
      "PRED 7: s o ʊ d\n",
      "PRED SCORE: -0.0069\n",
      "\n",
      "[2021-01-30 02:53:34,588 INFO] \n",
      "SENT 8: ['t', 'o', 'ʊ']\n",
      "PRED 8: t o ʊ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:34,588 INFO] \n",
      "SENT 9: ['k', 'ə', 'd', 'l']\n",
      "PRED 9: k ə d l l d\n",
      "PRED SCORE: -0.1102\n",
      "\n",
      "[2021-01-30 02:53:34,588 INFO] \n",
      "SENT 10: ['t', 'r', 'e', 'ɪ', 'l']\n",
      "PRED 10: t r e ɪ l d\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-01-30 02:53:34,588 INFO] \n",
      "SENT 11: ['s', 'o', 'ʊ']\n",
      "PRED 11: s o ʊ d\n",
      "PRED SCORE: -0.0069\n",
      "\n",
      "[2021-01-30 02:53:34,589 INFO] \n",
      "SENT 12: ['a', 'ɪ']\n",
      "PRED 12: a ɪ d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:34,589 INFO] \n",
      "SENT 13: ['d', 'o', 'ʊ', 'z']\n",
      "PRED 13: d o ʊ z d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:34,589 INFO] \n",
      "SENT 14: ['m', 'ɛ', 'm', 'ə', 'r', 'a', 'ɪ', 'z']\n",
      "PRED 14: m ɛ m ə r a ɪ z d\n",
      "PRED SCORE: -0.3086\n",
      "\n",
      "[2021-01-30 02:53:34,589 INFO] \n",
      "SENT 15: ['k', 'l', 'ɪ', 'ŋ']\n",
      "PRED 15: k l ɪ ŋ d\n",
      "PRED SCORE: -0.9215\n",
      "\n",
      "[2021-01-30 02:53:34,589 INFO] \n",
      "SENT 16: ['z', 'ɪ', 'p']\n",
      "PRED 16: z ɪ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:34,590 INFO] \n",
      "SENT 17: ['æ', 'g', 'r', 'ə', 'v', 'e', 'ɪ', 't']\n",
      "PRED 17: æ g r ə v e ɪ t ɪ d\n",
      "PRED SCORE: -0.7519\n",
      "\n",
      "[2021-01-30 02:53:34,590 INFO] \n",
      "SENT 18: ['θ', 'ɪ', 'k', 'ə', 'n']\n",
      "PRED 18: θ ɪ k ə n d\n",
      "PRED SCORE: -0.0017\n",
      "\n",
      "[2021-01-30 02:53:34,590 INFO] \n",
      "SENT 19: ['ɪ', 't', '͡', 'ʃ']\n",
      "PRED 19: ɪ t ͡ ʃ t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:34,590 INFO] \n",
      "SENT 20: ['t', 'i', 't', '͡', 'ʃ']\n",
      "PRED 20: t i t ͡ ʃ t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:34,590 INFO] \n",
      "SENT 21: ['æ', 'n', 'ə', 't', 'e', 'ɪ', 't']\n",
      "PRED 21: æ n ə t e ɪ t ɪ d\n",
      "PRED SCORE: -0.0026\n",
      "\n",
      "[2021-01-30 02:53:34,591 INFO] \n",
      "SENT 22: ['ɹ', 'ʌ', 's', 't']\n",
      "PRED 22: ɹ ʌ s t ɪ d\n",
      "PRED SCORE: -0.5335\n",
      "\n",
      "[2021-01-30 02:53:34,591 INFO] \n",
      "SENT 23: ['s', 'ə', 'p', 'r', 'a', 'ɪ', 'z']\n",
      "PRED 23: s ə p r a ɪ z d\n",
      "PRED SCORE: -0.0015\n",
      "\n",
      "[2021-01-30 02:53:34,591 INFO] \n",
      "SENT 24: ['s', 'ə', 'r', 'v', 'a', 'ɪ', 'v']\n",
      "PRED 24: s ə r v a ɪ v d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:34,592 INFO] \n",
      "SENT 25: ['t', 'w', 'ɪ', 'd', 'ə', 'l']\n",
      "PRED 25: t w ɪ d ə l d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:34,592 INFO] \n",
      "SENT 26: ['ɑ', 'p', 'ɹ', '̩', 'e', 'j', 't']\n",
      "PRED 26: ɑ p ɹ ɑ p ɹ ̩ d ɪ d\n",
      "PRED SCORE: -0.1935\n",
      "\n",
      "[2021-01-30 02:53:34,592 INFO] \n",
      "SENT 27: ['b', 'ɹ', 'u', 'm']\n",
      "PRED 27: b ɹ u m d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:34,592 INFO] \n",
      "SENT 28: ['p', 'ɹ', 'ɪ', 'k', 'ə', 'l']\n",
      "PRED 28: p ɹ ɪ k ə l d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:34,592 INFO] \n",
      "SENT 29: ['h', 'æ', 'n', 'd']\n",
      "PRED 29: h æ n d ɪ d\n",
      "PRED SCORE: -0.0020\n",
      "\n",
      "[2021-01-30 02:53:34,593 INFO] \n",
      "SENT 30: ['m', 'ɑ', 'ɹ', 't', '͡', 'ʃ']\n",
      "PRED 30: m ɑ ɹ t ͡ ʃ t\n",
      "PRED SCORE: -0.0034\n",
      "\n",
      "[2021-01-30 02:53:34,684 INFO] \n",
      "SENT 31: ['w', 'ɛ', 'ʤ']\n",
      "PRED 31: w ɛ ʤ d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:34,684 INFO] \n",
      "SENT 32: ['b', 'ɑ', 'p']\n",
      "PRED 32: b ɑ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:34,684 INFO] \n",
      "SENT 33: ['ə', 'p', 'r', 'i', 'ʃ', 'i', 'e', 'ɪ', 't']\n",
      "PRED 33: ə p r i ʃ i t ɪ d\n",
      "PRED SCORE: -0.0117\n",
      "\n",
      "[2021-01-30 02:53:34,685 INFO] \n",
      "SENT 34: ['ɛ', 'm', 'p', 't', 'i']\n",
      "PRED 34: ɛ m p i t ɪ d\n",
      "PRED SCORE: -0.0012\n",
      "\n",
      "[2021-01-30 02:53:34,685 INFO] \n",
      "SENT 35: ['m', 'ɔ', 'l']\n",
      "PRED 35: m ɔ l d\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-01-30 02:53:34,685 INFO] \n",
      "SENT 36: ['t', 'ə', 'm', 'b', 'ə', 'l']\n",
      "PRED 36: t ə m b ə l d\n",
      "PRED SCORE: -0.0065\n",
      "\n",
      "[2021-01-30 02:53:34,685 INFO] \n",
      "SENT 37: ['l', 'u', 'p']\n",
      "PRED 37: l u p t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:34,685 INFO] \n",
      "SENT 38: ['s', 'n', 'ə', 'f']\n",
      "PRED 38: s n ə f t\n",
      "PRED SCORE: -0.0040\n",
      "\n",
      "[2021-01-30 02:53:34,686 INFO] \n",
      "SENT 39: ['d', 'ɹ', 'ɔ']\n",
      "PRED 39: d ɹ ɔ d\n",
      "PRED SCORE: -0.1160\n",
      "\n",
      "[2021-01-30 02:53:34,686 INFO] \n",
      "SENT 40: ['ɪ', 'n', 't', 'ɹ', 'ə', 's', 't']\n",
      "PRED 40: ɪ n t ɹ ə s t ɪ d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:34,686 INFO] \n",
      "SENT 41: ['k', 'æ', 't', 'ə', 'p', 'ʌ', 'l', 't']\n",
      "PRED 41: k æ t ə p ʌ l t ɪ d\n",
      "PRED SCORE: -0.0392\n",
      "\n",
      "[2021-01-30 02:53:34,686 INFO] \n",
      "SENT 42: ['t', 'æ', 'k']\n",
      "PRED 42: t æ k t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:34,686 INFO] \n",
      "SENT 43: ['s', 't', 'r', 'o', 'ʊ', 'k']\n",
      "PRED 43: s t r o ʊ k t\n",
      "PRED SCORE: -0.2260\n",
      "\n",
      "[2021-01-30 02:53:34,687 INFO] \n",
      "SENT 44: ['t', 'ɹ', 'ɪ', 'p']\n",
      "PRED 44: t ɹ ɪ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:34,687 INFO] \n",
      "SENT 45: ['s', 'l', 'æ', 'p']\n",
      "PRED 45: s l æ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:34,687 INFO] \n",
      "SENT 46: ['f', 'ɛ', 's', 't', 'u', 'n']\n",
      "PRED 46: f ɛ s t u n d\n",
      "PRED SCORE: -0.0242\n",
      "\n",
      "[2021-01-30 02:53:34,687 INFO] \n",
      "SENT 47: ['s', 'ə', 'ʤ', 'ɛ', 's', 't']\n",
      "PRED 47: s ə ʤ ɛ s t ɪ d\n",
      "PRED SCORE: -0.0016\n",
      "\n",
      "[2021-01-30 02:53:34,687 INFO] \n",
      "SENT 48: ['ɪ', 'n', 't', 'ə', 'r', 'p', 'r', 'ɛ', 't']\n",
      "PRED 48: ɪ n t ə r p r ɛ t ə r d\n",
      "PRED SCORE: -0.5091\n",
      "\n",
      "[2021-01-30 02:53:34,688 INFO] \n",
      "SENT 49: ['ə', 'p', 'ɑ', 'l', 'ə', 'ʤ', 'a', 'ɪ', 'z']\n",
      "PRED 49: ə p ɑ l ə p a ɪ z d\n",
      "PRED SCORE: -0.0836\n",
      "\n",
      "[2021-01-30 02:53:34,688 INFO] \n",
      "SENT 50: ['d', 'æ', 'n', 's']\n",
      "PRED 50: d æ n s t\n",
      "PRED SCORE: -0.0675\n",
      "\n",
      "[2021-01-30 02:53:34,688 INFO] \n",
      "SENT 51: ['ɪ', 'k', 's', 'p', 'l', 'o', 'ʊ', 'd']\n",
      "PRED 51: ɪ k s p l o ʊ d ɪ d\n",
      "PRED SCORE: -0.0040\n",
      "\n",
      "[2021-01-30 02:53:34,688 INFO] \n",
      "SENT 52: ['b', 'æ', 'n', 'ɪ', 'ʃ']\n",
      "PRED 52: b æ n ɪ ʃ t\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-01-30 02:53:34,688 INFO] \n",
      "SENT 53: ['t', '͡', 'ʃ', 'ɑ', 'm', 'p']\n",
      "PRED 53: t ͡ ʃ ɑ m p t\n",
      "PRED SCORE: -0.0173\n",
      "\n",
      "[2021-01-30 02:53:34,689 INFO] \n",
      "SENT 54: ['b', 'ə', 't', 'ə', 'r']\n",
      "PRED 54: b ə t ə r d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:34,689 INFO] \n",
      "SENT 55: ['d', 'e', 'ɪ', 't']\n",
      "PRED 55: d e ɪ t ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:34,689 INFO] \n",
      "SENT 56: ['k', 'æ', 'm', 'p']\n",
      "PRED 56: k æ m p t\n",
      "PRED SCORE: -0.0435\n",
      "\n",
      "[2021-01-30 02:53:34,689 INFO] \n",
      "SENT 57: ['s', 'ə', 'r', 'v', 'ɪ', 's']\n",
      "PRED 57: s ə r v ɪ s t\n",
      "PRED SCORE: -0.0011\n",
      "\n",
      "[2021-01-30 02:53:34,689 INFO] \n",
      "SENT 58: ['k', 'ɔ', 'l']\n",
      "PRED 58: k ɔ l d\n",
      "PRED SCORE: -0.0079\n",
      "\n",
      "[2021-01-30 02:53:34,690 INFO] \n",
      "SENT 59: ['h', 'i', 'v']\n",
      "PRED 59: h i v d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:34,690 INFO] \n",
      "SENT 60: ['m', 'ə', 'n', 'ʧ']\n",
      "PRED 60: m ə n ʧ t\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:34,805 INFO] \n",
      "SENT 61: ['p', 'ɑ', 'ɹ', 'k']\n",
      "PRED 61: p ɑ ɹ k t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:34,805 INFO] \n",
      "SENT 62: ['v', 'o', 'ʊ', 't']\n",
      "PRED 62: v o ʊ t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:34,805 INFO] \n",
      "SENT 63: ['s', 'k', 'o', 'ʊ', 'l', 'd']\n",
      "PRED 63: s k o ʊ l d ɪ d\n",
      "PRED SCORE: -0.3266\n",
      "\n",
      "[2021-01-30 02:53:34,806 INFO] \n",
      "SENT 64: ['f', 'i', 'd']\n",
      "PRED 64: f i d ɪ d\n",
      "PRED SCORE: -0.0792\n",
      "\n",
      "[2021-01-30 02:53:34,806 INFO] \n",
      "SENT 65: ['ə', 'n', 'a', 'ʊ', 'n', 's']\n",
      "PRED 65: ə n a ʊ n s t\n",
      "PRED SCORE: -0.0221\n",
      "\n",
      "[2021-01-30 02:53:34,806 INFO] \n",
      "SENT 66: ['ɪ', 'n', 'k', 'r', 'i', 's']\n",
      "PRED 66: ɪ n k r i s t\n",
      "PRED SCORE: -0.0105\n",
      "\n",
      "[2021-01-30 02:53:34,806 INFO] \n",
      "SENT 67: ['p', 'ɑ', 'p', 'j', 'ə', 'l', 'ə', 'r', 'a', 'ɪ', 'z']\n",
      "PRED 67: p ə p j l ə r a ɪ z d\n",
      "PRED SCORE: -0.9604\n",
      "\n",
      "[2021-01-30 02:53:34,806 INFO] \n",
      "SENT 68: ['b', 'e', 'ɪ', 'k']\n",
      "PRED 68: b e ɪ k t\n",
      "PRED SCORE: -0.6424\n",
      "\n",
      "[2021-01-30 02:53:34,807 INFO] \n",
      "SENT 69: ['k', 'ɹ', 'i', 'k']\n",
      "PRED 69: k ɹ i k t\n",
      "PRED SCORE: -0.0014\n",
      "\n",
      "[2021-01-30 02:53:34,807 INFO] \n",
      "SENT 70: ['k', 'ə', 'n', 'f', 'j', 'u', 'z']\n",
      "PRED 70: k ə n f j u z d\n",
      "PRED SCORE: -0.0252\n",
      "\n",
      "[2021-01-30 02:53:34,807 INFO] \n",
      "SENT 71: ['ɪ', 'n', 'h', 'ɛ', 'r', 'ə', 't']\n",
      "PRED 71: ɪ n h ɛ r ə t\n",
      "PRED SCORE: -0.3627\n",
      "\n",
      "[2021-01-30 02:53:34,807 INFO] \n",
      "SENT 72: ['d', 'ɪ', 's', 't', 'ɹ', 'æ', 'k', 't']\n",
      "PRED 72: d ɪ s t ɹ æ k t ɪ d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:34,807 INFO] \n",
      "SENT 73: ['k', 'ʊ', 'ʃ', 'ə', 'n']\n",
      "PRED 73: k ʊ ʃ ə n d\n",
      "PRED SCORE: -0.0216\n",
      "\n",
      "[2021-01-30 02:53:34,808 INFO] \n",
      "SENT 74: ['b', 'ə', 'n', 'ʧ']\n",
      "PRED 74: b ə n ʧ t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:34,808 INFO] \n",
      "SENT 75: ['ə', 'v', 'ɔ', 'ɪ', 'd']\n",
      "PRED 75: ə v ɔ ɪ d ɪ d\n",
      "PRED SCORE: -0.0028\n",
      "\n",
      "[2021-01-30 02:53:34,808 INFO] \n",
      "SENT 76: ['ɛ', 'd', 'ə', 't']\n",
      "PRED 76: ɛ d ə t ɪ d\n",
      "PRED SCORE: -0.0035\n",
      "\n",
      "[2021-01-30 02:53:34,808 INFO] \n",
      "SENT 77: ['k', 'l', 'æ', 'ŋ', 'k']\n",
      "PRED 77: k l æ ŋ k t\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:34,809 INFO] \n",
      "SENT 78: ['l', 'ɑ', 'ʤ']\n",
      "PRED 78: l ɑ ʤ d\n",
      "PRED SCORE: -0.0015\n",
      "\n",
      "[2021-01-30 02:53:34,809 INFO] \n",
      "SENT 79: ['s', 'l', 'ɑ', 't']\n",
      "PRED 79: s l ɑ t ɪ d\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:34,809 INFO] \n",
      "SENT 80: ['h', 'ɛ', 'l', 'p']\n",
      "PRED 80: h ɛ l p t\n",
      "PRED SCORE: -0.0018\n",
      "\n",
      "[2021-01-30 02:53:34,809 INFO] \n",
      "SENT 81: ['d', 'ɪ', 's', 'k', 'ə', 'r', 'ɪ', 'ʤ']\n",
      "PRED 81: d ɪ s k ə r ɪ ʤ d\n",
      "PRED SCORE: -0.0018\n",
      "\n",
      "[2021-01-30 02:53:34,809 INFO] \n",
      "SENT 82: ['w', 'a', 'ɪ', 'ə', 'r']\n",
      "PRED 82: w a ɪ ə r d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:34,810 INFO] \n",
      "SENT 83: ['b', 'r', 'e', 'ɪ', 'k']\n",
      "PRED 83: b r e ɪ k t\n",
      "PRED SCORE: -0.0209\n",
      "\n",
      "[2021-01-30 02:53:34,810 INFO] \n",
      "SENT 84: ['d', 'ɛ', 's', 't', 'i', 'n']\n",
      "PRED 84: d ɛ s t i n d\n",
      "PRED SCORE: -0.0699\n",
      "\n",
      "[2021-01-30 02:53:34,810 INFO] \n",
      "SENT 85: ['t', '͡', 'ʃ', 'ɹ', '̩', 'p']\n",
      "PRED 85: t ͡ ʃ ɹ ̩ p t\n",
      "PRED SCORE: -0.0117\n",
      "\n",
      "[2021-01-30 02:53:34,810 INFO] \n",
      "SENT 86: ['s', 'k', 'w', 'i', 'z']\n",
      "PRED 86: s k w i z d\n",
      "PRED SCORE: -0.0026\n",
      "\n",
      "[2021-01-30 02:53:34,810 INFO] \n",
      "SENT 87: ['ɹ', 'ɪ', 's', 'p', 'ɛ', 'k', 't']\n",
      "PRED 87: ɹ ɪ s p ɛ k t ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:34,811 INFO] \n",
      "SENT 88: ['b', 'l', 'e', 'ɪ', 'm']\n",
      "PRED 88: b l e ɪ m d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:34,811 INFO] \n",
      "SENT 89: ['k', 'o', 'ʊ', 'k', 's']\n",
      "PRED 89: k o ʊ k s t\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:34,811 INFO] \n",
      "SENT 90: ['p', 'ɑ', 'p']\n",
      "PRED 90: p ɑ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:34,902 INFO] \n",
      "SENT 91: ['l', 'a', 'ɪ', 't']\n",
      "PRED 91: l ɪ t\n",
      "PRED SCORE: -0.1465\n",
      "\n",
      "[2021-01-30 02:53:34,902 INFO] \n",
      "SENT 92: ['p', 'i', 'p']\n",
      "PRED 92: p i p t\n",
      "PRED SCORE: -0.1692\n",
      "\n",
      "[2021-01-30 02:53:34,902 INFO] \n",
      "SENT 93: ['ɹ', 'ɛ', 'n', 't']\n",
      "PRED 93: ɹ ɛ n t ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:34,903 INFO] \n",
      "SENT 94: ['k', 'ə', 'm', 'ɪ', 't']\n",
      "PRED 94: k ə m ɪ t ɪ d\n",
      "PRED SCORE: -0.0659\n",
      "\n",
      "[2021-01-30 02:53:34,908 INFO] \n",
      "SENT 95: ['d', 'ɪ', 's', 'k', 'ʌ', 's']\n",
      "PRED 95: d ɪ s k ʌ s t\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-01-30 02:53:34,909 INFO] \n",
      "SENT 96: ['p', 'ɔ', 'z']\n",
      "PRED 96: p ɔ z d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:34,909 INFO] \n",
      "SENT 97: ['l', 'a', 'ɪ', 'n']\n",
      "PRED 97: l a ɪ n d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:34,909 INFO] \n",
      "SENT 98: ['i', 't']\n",
      "PRED 98: i t ɪ d\n",
      "PRED SCORE: -0.0583\n",
      "\n",
      "[2021-01-30 02:53:34,910 INFO] \n",
      "SENT 99: ['r', 'o', 'ʊ', 'm']\n",
      "PRED 99: r o ʊ m d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:34,910 INFO] \n",
      "SENT 100: ['p', 'r', 'ɑ', 'p']\n",
      "PRED 100: p r ɑ p t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:34,910 INFO] \n",
      "SENT 101: ['s', 't', 'ɑ', 'p']\n",
      "PRED 101: s t ɑ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:34,910 INFO] \n",
      "SENT 102: ['o', 'ʊ', 'v', 'ə', 'r', 'w', 'ɛ', 'l', 'm']\n",
      "PRED 102: o ʊ v ə r m ə r m d\n",
      "PRED SCORE: -0.0269\n",
      "\n",
      "[2021-01-30 02:53:34,910 INFO] \n",
      "SENT 103: ['d', 'ɑ', 'ɹ', 't']\n",
      "PRED 103: d ɑ ɹ t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:34,911 INFO] \n",
      "SENT 104: ['s', 't', 'r', 'e', 'ɪ', 't', 'ə', 'n']\n",
      "PRED 104: s t r e ɪ t ə n d\n",
      "PRED SCORE: -0.0031\n",
      "\n",
      "[2021-01-30 02:53:34,911 INFO] \n",
      "SENT 105: ['m', 'ə', 't', 'ə', 'r']\n",
      "PRED 105: m ə t ə r d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:34,911 INFO] \n",
      "SENT 106: ['k', 'ɑ', 'n']\n",
      "PRED 106: k ɑ n d\n",
      "PRED SCORE: -0.2121\n",
      "\n",
      "[2021-01-30 02:53:34,911 INFO] \n",
      "SENT 107: ['k', 'ə', 'l', 'æ', 'p', 's']\n",
      "PRED 107: k ə l æ p t ɪ d\n",
      "PRED SCORE: -0.1253\n",
      "\n",
      "[2021-01-30 02:53:34,911 INFO] \n",
      "SENT 108: ['n', 'o', 'ʊ', 't', 'ɪ', 's']\n",
      "PRED 108: n o ʊ t ɪ s t\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-01-30 02:53:34,912 INFO] \n",
      "SENT 109: ['w', 'ɪ', 's', 'p', 'ə', 'r']\n",
      "PRED 109: w ɪ s p ə r d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:34,912 INFO] \n",
      "SENT 110: ['k', 'u', 'l']\n",
      "PRED 110: k u l d\n",
      "PRED SCORE: -0.0029\n",
      "\n",
      "[2021-01-30 02:53:34,912 INFO] \n",
      "SENT 111: ['k', 'ɑ', 'm', 'p', 'l', 'ə', 'k', 'e', 'ɪ', 't']\n",
      "PRED 111: k ɑ m p l ə k e ɪ t ɪ d\n",
      "PRED SCORE: -0.1552\n",
      "\n",
      "[2021-01-30 02:53:34,913 INFO] \n",
      "SENT 112: ['m', 'ʌ', 'l']\n",
      "PRED 112: m ʌ l d\n",
      "PRED SCORE: -0.0021\n",
      "\n",
      "[2021-01-30 02:53:34,913 INFO] \n",
      "SENT 113: ['s', 'n', 'ɪ', 'p']\n",
      "PRED 113: s n ɪ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:34,913 INFO] \n",
      "SENT 114: ['k', 'ɑ', 'ɹ', 'v']\n",
      "PRED 114: k ə ɹ v d\n",
      "PRED SCORE: -0.0565\n",
      "\n",
      "[2021-01-30 02:53:34,913 INFO] \n",
      "SENT 115: ['h', 'u', 'v', 'ɹ', '̩']\n",
      "PRED 115: h u v ɹ ̩ d\n",
      "PRED SCORE: -0.0264\n",
      "\n",
      "[2021-01-30 02:53:34,914 INFO] \n",
      "SENT 116: ['ɹ', 'ɛ', 's', 't']\n",
      "PRED 116: ɹ ɛ s t ɪ d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:34,914 INFO] \n",
      "SENT 117: ['k', 'ɑ', 'p', 'i']\n",
      "PRED 117: k ɑ p i d\n",
      "PRED SCORE: -0.2959\n",
      "\n",
      "[2021-01-30 02:53:34,914 INFO] \n",
      "SENT 118: ['p', 'æ', 'd', 'ə', 'l']\n",
      "PRED 118: p æ d ə l d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:34,914 INFO] \n",
      "SENT 119: ['p', 'a', 'ɪ', 'l']\n",
      "PRED 119: p a ɪ l d\n",
      "PRED SCORE: -0.0160\n",
      "\n",
      "[2021-01-30 02:53:34,914 INFO] \n",
      "SENT 120: ['æ', 'd', 'ɪ', 'k', 't']\n",
      "PRED 120: æ d ɪ d ɪ d\n",
      "PRED SCORE: -0.0889\n",
      "\n",
      "[2021-01-30 02:53:35,007 INFO] \n",
      "SENT 121: ['b', 'ɪ', 'ɡ', 'ɪ', 'n']\n",
      "PRED 121: b ɪ ɡ ɪ n d\n",
      "PRED SCORE: -0.0411\n",
      "\n",
      "[2021-01-30 02:53:35,007 INFO] \n",
      "SENT 122: ['b', 'e', 'ɪ', 'ð']\n",
      "PRED 122: b e ɪ d\n",
      "PRED SCORE: -0.0565\n",
      "\n",
      "[2021-01-30 02:53:35,008 INFO] \n",
      "SENT 123: ['d', 'ɹ', 'ɪ', 'f', 't']\n",
      "PRED 123: d ɹ ɪ f t ɪ d\n",
      "PRED SCORE: -0.0117\n",
      "\n",
      "[2021-01-30 02:53:35,008 INFO] \n",
      "SENT 124: ['l', 'ə', 'n', 'ʤ']\n",
      "PRED 124: l ə n ʤ d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:35,008 INFO] \n",
      "SENT 125: ['r', 'o', 'ʊ', 's', 't']\n",
      "PRED 125: r o ʊ s t ɪ d\n",
      "PRED SCORE: -0.0020\n",
      "\n",
      "[2021-01-30 02:53:35,008 INFO] \n",
      "SENT 126: ['s', 'ɔ', 'f', 'ə', 'n']\n",
      "PRED 126: s ɔ f ə n d\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-01-30 02:53:35,009 INFO] \n",
      "SENT 127: ['s', 'l', 'a', 'ɪ', 's']\n",
      "PRED 127: s l a ɪ s t\n",
      "PRED SCORE: -0.0045\n",
      "\n",
      "[2021-01-30 02:53:35,009 INFO] \n",
      "SENT 128: ['s', 'l', 'o', 'ʊ']\n",
      "PRED 128: s l o ʊ d\n",
      "PRED SCORE: -0.0011\n",
      "\n",
      "[2021-01-30 02:53:35,009 INFO] \n",
      "SENT 129: ['ʃ', 'ɛ', 'r']\n",
      "PRED 129: ʃ ɛ r d\n",
      "PRED SCORE: -0.0027\n",
      "\n",
      "[2021-01-30 02:53:35,009 INFO] \n",
      "SENT 130: ['d', 'ɑ', 'ɹ', 'k', 'ə', 'n']\n",
      "PRED 130: d ɑ ɹ k ə n d\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-01-30 02:53:35,009 INFO] \n",
      "SENT 131: ['d', 'r', 'a', 'ɪ', 'v']\n",
      "PRED 131: d r a ɪ v d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:35,010 INFO] \n",
      "SENT 132: ['s', 't', 'r', 'o', 'ʊ', 'l']\n",
      "PRED 132: s t r o ʊ l d\n",
      "PRED SCORE: -0.0011\n",
      "\n",
      "[2021-01-30 02:53:35,010 INFO] \n",
      "SENT 133: ['p', 'o', 'ʊ', 'ʧ']\n",
      "PRED 133: p o ʊ ʧ t\n",
      "PRED SCORE: -0.0014\n",
      "\n",
      "[2021-01-30 02:53:35,010 INFO] \n",
      "SENT 134: ['ɔ', 'f', 'ə', 'r']\n",
      "PRED 134: ɔ f ə r d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:35,010 INFO] \n",
      "SENT 135: ['t', 'r', 'æ', 'n', 's', 'k', 'r', 'a', 'ɪ', 'b']\n",
      "PRED 135: t r æ n s k r a ɪ v d\n",
      "PRED SCORE: -0.6877\n",
      "\n",
      "[2021-01-30 02:53:35,010 INFO] \n",
      "SENT 136: ['j', 'ɛ', 'l', 'p']\n",
      "PRED 136: j ɛ l p t\n",
      "PRED SCORE: -0.0011\n",
      "\n",
      "[2021-01-30 02:53:35,011 INFO] \n",
      "SENT 137: ['ə', 'p', 'ɪ', 'r']\n",
      "PRED 137: ə p ɪ r d\n",
      "PRED SCORE: -0.0014\n",
      "\n",
      "[2021-01-30 02:53:35,011 INFO] \n",
      "SENT 138: ['θ', 'ʌ', 'm', 'p']\n",
      "PRED 138: θ ʌ m p t\n",
      "PRED SCORE: -0.1243\n",
      "\n",
      "[2021-01-30 02:53:35,011 INFO] \n",
      "SENT 139: ['m', 'ɪ', 'k', 's']\n",
      "PRED 139: m ɪ k s t\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:35,011 INFO] \n",
      "SENT 140: ['b', 'ɪ', 'h', 'e', 'ɪ', 'v']\n",
      "PRED 140: b ɪ h e ɪ v d\n",
      "PRED SCORE: -0.1863\n",
      "\n",
      "[2021-01-30 02:53:35,011 INFO] \n",
      "SENT 141: ['b', 'æ', 't', 'ə', 'l']\n",
      "PRED 141: b æ t ə l d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:35,012 INFO] \n",
      "SENT 142: ['t', 'æ', 't', 'ə', 'r']\n",
      "PRED 142: t æ t ə r d\n",
      "PRED SCORE: -0.0068\n",
      "\n",
      "[2021-01-30 02:53:35,012 INFO] \n",
      "SENT 143: ['m', 'ɛ', 'z', 'm', 'ə', 'r', 'a', 'ɪ', 'z']\n",
      "PRED 143: m ɛ z ə r a ɪ z d\n",
      "PRED SCORE: -0.3162\n",
      "\n",
      "[2021-01-30 02:53:35,012 INFO] \n",
      "SENT 144: ['p', 'ɛ', 'd', 'ə', 'l']\n",
      "PRED 144: p ɛ d ə l d\n",
      "PRED SCORE: -0.0239\n",
      "\n",
      "[2021-01-30 02:53:35,012 INFO] \n",
      "SENT 145: ['k', 'æ', 't', '͡', 'ʃ']\n",
      "PRED 145: k æ t ͡ ʃ t\n",
      "PRED SCORE: -0.0016\n",
      "\n",
      "[2021-01-30 02:53:35,012 INFO] \n",
      "SENT 146: ['j', 'u', 'z']\n",
      "PRED 146: j u z d\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-01-30 02:53:35,013 INFO] \n",
      "SENT 147: ['ʃ', 'o', 'ʊ']\n",
      "PRED 147: ʃ o ʊ d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:35,013 INFO] \n",
      "SENT 148: ['s', 'a', 'ɪ']\n",
      "PRED 148: s a ɪ d\n",
      "PRED SCORE: -0.0067\n",
      "\n",
      "[2021-01-30 02:53:35,013 INFO] \n",
      "SENT 149: ['k', 'æ', 'p', 's', 'a', 'ɪ', 'z']\n",
      "PRED 149: k æ p s a ɪ z d\n",
      "PRED SCORE: -0.0253\n",
      "\n",
      "[2021-01-30 02:53:35,013 INFO] \n",
      "SENT 150: ['p', 'ɹ', 'ɛ', 's']\n",
      "PRED 150: p ɹ ɛ s t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:35,104 INFO] \n",
      "SENT 151: ['ɹ', 'ɪ', 'l', 'æ', 'k', 's']\n",
      "PRED 151: ɹ ɪ l æ k s t\n",
      "PRED SCORE: -0.0476\n",
      "\n",
      "[2021-01-30 02:53:35,105 INFO] \n",
      "SENT 152: ['m', 'ɪ', 's']\n",
      "PRED 152: m ɪ s t\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:35,106 INFO] \n",
      "SENT 153: ['b', 'ɛ', 'l', 't']\n",
      "PRED 153: b ɛ l t ɪ d\n",
      "PRED SCORE: -0.0554\n",
      "\n",
      "[2021-01-30 02:53:35,107 INFO] \n",
      "SENT 154: ['ɹ', 'ɪ', 'n', 's']\n",
      "PRED 154: ɹ ɪ n s t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:35,107 INFO] \n",
      "SENT 155: ['p', 'ɹ', 'æ', 'k', 't', 'a', 'j', 'z']\n",
      "PRED 155: p ɹ æ k t a j z d\n",
      "PRED SCORE: -0.0020\n",
      "\n",
      "[2021-01-30 02:53:35,107 INFO] \n",
      "SENT 156: ['ɹ', 'ɪ', 'ŋ']\n",
      "PRED 156: ɹ æ ŋ\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-01-30 02:53:35,107 INFO] \n",
      "SENT 157: ['d', 'æ', 'ŋ', 'ɡ', 'ə', 'l']\n",
      "PRED 157: d æ ŋ ɡ ə l d\n",
      "PRED SCORE: -0.0210\n",
      "\n",
      "[2021-01-30 02:53:35,108 INFO] \n",
      "SENT 158: ['ɪ', 'k', 's', 't', 'ɛ', 'n', 'd']\n",
      "PRED 158: ɪ k s t ɛ n d ɪ d\n",
      "PRED SCORE: -0.0039\n",
      "\n",
      "[2021-01-30 02:53:35,108 INFO] \n",
      "SENT 159: ['l', 'ə', 'r', 'n']\n",
      "PRED 159: l ə r n d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:35,108 INFO] \n",
      "SENT 160: ['r', 'ɪ', 'k', 'ɔ', 'r', 'd']\n",
      "PRED 160: r ɪ k ɔ r d ɪ d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:35,108 INFO] \n",
      "SENT 161: ['θ', 'ɪ', 'ŋ', 'k']\n",
      "PRED 161: θ ɪ ŋ k t\n",
      "PRED SCORE: -0.0072\n",
      "\n",
      "[2021-01-30 02:53:35,108 INFO] \n",
      "SENT 162: ['k', 'ɹ', 'æ', 'm']\n",
      "PRED 162: k ɹ æ m d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:35,109 INFO] \n",
      "SENT 163: ['s', 'ɪ', 'ŋ']\n",
      "PRED 163: s æ ŋ\n",
      "PRED SCORE: -0.2811\n",
      "\n",
      "[2021-01-30 02:53:35,109 INFO] \n",
      "SENT 164: ['d', 'ɹ', 'ɛ', 's']\n",
      "PRED 164: d ɹ ɛ s t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:35,109 INFO] \n",
      "SENT 165: ['w', 'i', 'l']\n",
      "PRED 165: w i l d\n",
      "PRED SCORE: -0.0023\n",
      "\n",
      "[2021-01-30 02:53:35,109 INFO] \n",
      "SENT 166: ['s', 't', 'ɑ', 'ɹ', 'v']\n",
      "PRED 166: s t ɑ ɹ v d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:35,109 INFO] \n",
      "SENT 167: ['ɡ', 'æ', 's', 'p']\n",
      "PRED 167: ɡ æ s t\n",
      "PRED SCORE: -0.1075\n",
      "\n",
      "[2021-01-30 02:53:35,110 INFO] \n",
      "SENT 168: ['d', 'ɪ', 's', 'm', 'æ', 'n', 't', 'ə', 'l']\n",
      "PRED 168: d ɪ s m æ n t ə l d\n",
      "PRED SCORE: -0.1077\n",
      "\n",
      "[2021-01-30 02:53:35,110 INFO] \n",
      "SENT 169: ['n', 'ɪ', 't']\n",
      "PRED 169: n ɪ t ɪ d\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-01-30 02:53:35,110 INFO] \n",
      "SENT 170: ['g', 'ɑ', 'r', 'b', 'ə', 'l']\n",
      "PRED 170: g ɑ r b ə l d\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-01-30 02:53:35,110 INFO] \n",
      "SENT 171: ['ə', 'r', 'a', 'ɪ', 'v']\n",
      "PRED 171: ə r a ɪ v d\n",
      "PRED SCORE: -0.0367\n",
      "\n",
      "[2021-01-30 02:53:35,110 INFO] \n",
      "SENT 172: ['r', 'e', 'ɪ', 'z']\n",
      "PRED 172: r e ɪ z d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:35,111 INFO] \n",
      "SENT 173: ['f', 'l', 'ə', 's', 't', 'ə', 'r']\n",
      "PRED 173: f l ə s t ə r d\n",
      "PRED SCORE: -0.0345\n",
      "\n",
      "[2021-01-30 02:53:35,111 INFO] \n",
      "SENT 174: ['æ', 'd', 'v', 'ə', 'r', 't', 'a', 'ɪ', 'z']\n",
      "PRED 174: æ d ə r t a ɪ z d\n",
      "PRED SCORE: -0.2560\n",
      "\n",
      "[2021-01-30 02:53:35,111 INFO] \n",
      "SENT 175: ['ɹ', 'ɪ', 'f', 'j', 'u', 'ː', 'z']\n",
      "PRED 175: ɹ ɪ f j u z d ɪ d\n",
      "PRED SCORE: -0.3342\n",
      "\n",
      "[2021-01-30 02:53:35,111 INFO] \n",
      "SENT 176: ['m', 'ə', 's', 'ɑ', 'ʒ']\n",
      "PRED 176: m ə s ɑ ʒ d\n",
      "PRED SCORE: -0.1045\n",
      "\n",
      "[2021-01-30 02:53:35,111 INFO] \n",
      "SENT 177: ['p', 'ɪ', 'k', 'ə', 'l']\n",
      "PRED 177: p ɪ k ə l d\n",
      "PRED SCORE: -0.0037\n",
      "\n",
      "[2021-01-30 02:53:35,111 INFO] \n",
      "SENT 178: ['ʧ', 'e', 'ɪ', 'n']\n",
      "PRED 178: ʧ e ɪ n d\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:35,112 INFO] \n",
      "SENT 179: ['s', 'l', 'a', 'ɪ', 'd']\n",
      "PRED 179: s l a ɪ d ɪ d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:35,112 INFO] \n",
      "SENT 180: ['f', 'ə', 'ʤ']\n",
      "PRED 180: f ə ʤ d\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-01-30 02:53:35,180 INFO] \n",
      "SENT 181: ['b', 'ʌ', 'ɡ']\n",
      "PRED 181: b ʌ ɡ d\n",
      "PRED SCORE: -0.0023\n",
      "\n",
      "[2021-01-30 02:53:35,180 INFO] \n",
      "SENT 182: ['k', 'ɪ', 's']\n",
      "PRED 182: k ɪ s t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:35,180 INFO] \n",
      "SENT 183: ['s', 't', 'ʌ', 'f']\n",
      "PRED 183: s t ʌ f t\n",
      "PRED SCORE: -0.1630\n",
      "\n",
      "[2021-01-30 02:53:35,181 INFO] \n",
      "SENT 184: ['p', 'l', 'e', 'ɪ']\n",
      "PRED 184: p l e ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:35,181 INFO] \n",
      "SENT 185: ['s', 'o', 'ʊ', 'k']\n",
      "PRED 185: s o ʊ k t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:35,181 INFO] \n",
      "SENT 186: ['t', 'e', 'ɪ', 'k']\n",
      "PRED 186: t e ɪ k t\n",
      "PRED SCORE: -0.0203\n",
      "\n",
      "[2021-01-30 02:53:35,181 INFO] \n",
      "SENT 187: ['w', 'ɑ', 'd', 'ə', 'l']\n",
      "PRED 187: w ɑ d ə l d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:35,181 INFO] \n",
      "SENT 188: ['k', 'l', 'ʌ', 'k']\n",
      "PRED 188: k l ʌ k t\n",
      "PRED SCORE: -0.2919\n",
      "\n",
      "[2021-01-30 02:53:35,182 INFO] \n",
      "SENT 189: ['t', 'a', 'ɪ', 'p']\n",
      "PRED 189: t a ɪ p t\n",
      "PRED SCORE: -0.0386\n",
      "\n",
      "[2021-01-30 02:53:35,182 INFO] \n",
      "SENT 190: ['r', 'i', 's', 'a', 'ɪ', 'k', 'ə', 'l']\n",
      "PRED 190: r i s a ɪ k ə l d\n",
      "PRED SCORE: -0.1198\n",
      "\n",
      "[2021-01-30 02:53:35,182 INFO] \n",
      "SENT 191: ['p', 'æ', 'n', 't']\n",
      "PRED 191: p æ n t ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:35,182 INFO] \n",
      "SENT 192: ['s', 't', 'u', 'p']\n",
      "PRED 192: s t u p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:35,182 INFO] \n",
      "SENT 193: ['k', 'l', 'æ', 'ŋ']\n",
      "PRED 193: k l æ ŋ d\n",
      "PRED SCORE: -0.0013\n",
      "\n",
      "[2021-01-30 02:53:35,183 INFO] \n",
      "SENT 194: ['f', 'r', 'i']\n",
      "PRED 194: f r æ f t\n",
      "PRED SCORE: -0.2023\n",
      "\n",
      "[2021-01-30 02:53:35,183 INFO] \n",
      "SENT 195: ['w', 'ɪ', 'm', 'p', 'ɹ', '̩']\n",
      "PRED 195: w ɪ m p ɹ ̩ d\n",
      "PRED SCORE: -0.0277\n",
      "\n",
      "[2021-01-30 02:53:35,183 INFO] \n",
      "SENT 196: ['b', 'l', 'i', 'd']\n",
      "PRED 196: b l i d ɪ d\n",
      "PRED SCORE: -0.3355\n",
      "\n",
      "[2021-01-30 02:53:35,183 INFO] \n",
      "SENT 197: ['ʃ', 'o', 'ʊ', 'f', 'ə', 'r']\n",
      "PRED 197: ʃ o ʊ f ə r d\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-01-30 02:53:35,184 INFO] \n",
      "SENT 198: ['h', 'a', 'ɪ', 'ʤ', 'æ', 'k']\n",
      "PRED 198: h a ɪ ʤ æ k t\n",
      "PRED SCORE: -0.2294\n",
      "\n",
      "[2021-01-30 02:53:35,184 INFO] \n",
      "SENT 199: ['s', 'w', 'u', 'p']\n",
      "PRED 199: s w u p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:35,184 INFO] \n",
      "SENT 200: ['ɪ', 'm', 'æ', 'ʤ', 'ə', 'n']\n",
      "PRED 200: ɪ m æ ʤ ə n d\n",
      "PRED SCORE: -0.0806\n",
      "\n",
      "[2021-01-30 02:53:35,184 INFO] PRED AVG SCORE: -0.0103, PRED PPL: 1.0104\n",
      "[2021-01-30 02:53:36,613 INFO] Translating shard 0.\n",
      "[2021-01-30 02:53:36,708 INFO] \n",
      "SENT 1: ['t', 'ɑ', 'p']\n",
      "PRED 1: t ɑ p t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:36,709 INFO] \n",
      "SENT 2: ['b', 'ɹ', 'u', 'm']\n",
      "PRED 2: b ɹ u m d\n",
      "PRED SCORE: -0.0022\n",
      "\n",
      "[2021-01-30 02:53:36,709 INFO] \n",
      "SENT 3: ['t', 'ə', 'r', 'n']\n",
      "PRED 3: t ə r n d\n",
      "PRED SCORE: -0.0013\n",
      "\n",
      "[2021-01-30 02:53:36,709 INFO] \n",
      "SENT 4: ['t', 'ɹ', 'ɪ', 'p']\n",
      "PRED 4: t ɹ ɪ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:36,709 INFO] \n",
      "SENT 5: ['m', 'ɛ', 'n', 'd']\n",
      "PRED 5: m ɛ n t\n",
      "PRED SCORE: -0.0716\n",
      "\n",
      "[2021-01-30 02:53:36,710 INFO] \n",
      "SENT 6: ['b', 'u']\n",
      "PRED 6: b u d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:36,710 INFO] \n",
      "SENT 7: ['s', 't', 'r', 'a', 'ɪ', 'p']\n",
      "PRED 7: s t r a ɪ p t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:36,710 INFO] \n",
      "SENT 8: ['p', 'ɪ', 'n']\n",
      "PRED 8: p æ n\n",
      "PRED SCORE: -0.0195\n",
      "\n",
      "[2021-01-30 02:53:36,710 INFO] \n",
      "SENT 9: ['l', 'ɪ', 'k']\n",
      "PRED 9: l ɪ k t\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:36,710 INFO] \n",
      "SENT 10: ['s', 'k', 'ɹ', 'ɪ', 'b', 'ə', 'l']\n",
      "PRED 10: s k ɹ ɪ b ə l d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:36,711 INFO] \n",
      "SENT 11: ['k', 'r', 'a', 'ɪ']\n",
      "PRED 11: k r a ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:36,711 INFO] \n",
      "SENT 12: ['p', 'ʌ', 'n', 'ɪ', 'ʃ']\n",
      "PRED 12: p ʌ n ɪ ʃ t\n",
      "PRED SCORE: -0.0018\n",
      "\n",
      "[2021-01-30 02:53:36,711 INFO] \n",
      "SENT 13: ['k', 'r', 'o', 'ʊ']\n",
      "PRED 13: k r o ʊ d\n",
      "PRED SCORE: -0.1607\n",
      "\n",
      "[2021-01-30 02:53:36,711 INFO] \n",
      "SENT 14: ['m', 'ə', 's', 'ɑ', 'ʒ']\n",
      "PRED 14: m ə s ɑ ʒ d\n",
      "PRED SCORE: -0.0036\n",
      "\n",
      "[2021-01-30 02:53:36,711 INFO] \n",
      "SENT 15: ['s', 'u', 'p', 'ə', 'r', 'v', 'a', 'ɪ', 'z']\n",
      "PRED 15: s u p ə r v a ɪ z d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:36,712 INFO] \n",
      "SENT 16: ['r', 'ɪ', 'v', 'ə', 'r', 's']\n",
      "PRED 16: r ɪ v ə r s t\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:36,712 INFO] \n",
      "SENT 17: ['k', 'ə', 'l', 'æ', 'p', 's']\n",
      "PRED 17: k ə l æ p s t\n",
      "PRED SCORE: -0.4767\n",
      "\n",
      "[2021-01-30 02:53:36,712 INFO] \n",
      "SENT 18: ['θ', 'ə', 'n', 'd', 'ə', 'r']\n",
      "PRED 18: θ ə n d ə r d\n",
      "PRED SCORE: -0.0014\n",
      "\n",
      "[2021-01-30 02:53:36,712 INFO] \n",
      "SENT 19: ['k', 'o', 'ʊ', 'ɔ', 'r', 'd', 'ə', 'n', 'e', 'ɪ', 't']\n",
      "PRED 19: k o ʊ r d ə n e ɪ t ɪ d\n",
      "PRED SCORE: -0.0085\n",
      "\n",
      "[2021-01-30 02:53:36,712 INFO] \n",
      "SENT 20: ['s', 't', 'u']\n",
      "PRED 20: s t u d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:36,713 INFO] \n",
      "SENT 21: ['ə', 'p', 'l', 'a', 'ɪ']\n",
      "PRED 21: ə p l a ɪ d\n",
      "PRED SCORE: -0.0060\n",
      "\n",
      "[2021-01-30 02:53:36,713 INFO] \n",
      "SENT 22: ['f', 'e', 'ɪ', 'd']\n",
      "PRED 22: f ɛ d\n",
      "PRED SCORE: -0.4800\n",
      "\n",
      "[2021-01-30 02:53:36,713 INFO] \n",
      "SENT 23: ['ɪ', 'm', 'p', 'r', 'u', 'v']\n",
      "PRED 23: ɪ m p r u v d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:36,713 INFO] \n",
      "SENT 24: ['p', 'ɪ', 'ʧ']\n",
      "PRED 24: p ɪ ʧ t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:36,713 INFO] \n",
      "SENT 25: ['r', 'o', 'ʊ', 'm']\n",
      "PRED 25: r o ʊ m d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:36,714 INFO] \n",
      "SENT 26: ['k', 'a', 'ʊ', 'n', 't']\n",
      "PRED 26: k a ʊ n t ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:36,714 INFO] \n",
      "SENT 27: ['θ', 'r', 'o', 'ʊ']\n",
      "PRED 27: θ r u\n",
      "PRED SCORE: -0.2788\n",
      "\n",
      "[2021-01-30 02:53:36,714 INFO] \n",
      "SENT 28: ['ʃ', 'o', 'ʊ']\n",
      "PRED 28: ʃ o ʊ d\n",
      "PRED SCORE: -0.4124\n",
      "\n",
      "[2021-01-30 02:53:36,714 INFO] \n",
      "SENT 29: ['w', 'i', 'd']\n",
      "PRED 29: w i d ɪ d\n",
      "PRED SCORE: -0.0424\n",
      "\n",
      "[2021-01-30 02:53:36,714 INFO] \n",
      "SENT 30: ['h', 'i', 'p']\n",
      "PRED 30: h i p t\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:36,807 INFO] \n",
      "SENT 31: ['ʧ', 'ə', 'r', 'n']\n",
      "PRED 31: ʧ ə r n d\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-01-30 02:53:36,807 INFO] \n",
      "SENT 32: ['d', 'a', 'ɪ', 'v']\n",
      "PRED 32: d a ɪ v d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:36,808 INFO] \n",
      "SENT 33: ['t', 'ɔ', 'k']\n",
      "PRED 33: t ɔ k t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:36,808 INFO] \n",
      "SENT 34: ['k', 'r', 'ɪ', 'p', 'ə', 'l']\n",
      "PRED 34: k r ɪ p ə l d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:36,808 INFO] \n",
      "SENT 35: ['p', 'r', 'ɑ', 'm', 'ə', 's']\n",
      "PRED 35: p r ɑ m ə s t\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:36,808 INFO] \n",
      "SENT 36: ['s', 'i', 'm']\n",
      "PRED 36: s i m d\n",
      "PRED SCORE: -0.0267\n",
      "\n",
      "[2021-01-30 02:53:36,808 INFO] \n",
      "SENT 37: ['d', 'ɛ', 'k', 'ə', 'r', 'e', 'ɪ', 't']\n",
      "PRED 37: d ɛ k ə r e ɪ t ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:36,809 INFO] \n",
      "SENT 38: ['t', 'ɹ', 'ɑ', 't']\n",
      "PRED 38: t ɹ ɑ t ɪ d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:36,809 INFO] \n",
      "SENT 39: ['ɹ', 'ʌ', 'ʃ']\n",
      "PRED 39: ɹ ʌ ʃ t\n",
      "PRED SCORE: -0.5564\n",
      "\n",
      "[2021-01-30 02:53:36,809 INFO] \n",
      "SENT 40: ['g', 'e', 'ɪ', 'n']\n",
      "PRED 40: g e ɪ n d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:36,809 INFO] \n",
      "SENT 41: ['s', 't', 'ʌ', 'd', 'i']\n",
      "PRED 41: s t ʌ d i d\n",
      "PRED SCORE: -0.1528\n",
      "\n",
      "[2021-01-30 02:53:36,810 INFO] \n",
      "SENT 42: ['t', 'r', 'ə', 'm', 'p', 'ə', 't']\n",
      "PRED 42: t r ə m p ə t ɪ d\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-01-30 02:53:36,810 INFO] \n",
      "SENT 43: ['d', 'ɪ', 's', 'ə', 'p', 'o', 'j', 'n', 't']\n",
      "PRED 43: d ɪ s ə p o ʊ n t ɪ d\n",
      "PRED SCORE: -0.0376\n",
      "\n",
      "[2021-01-30 02:53:36,810 INFO] \n",
      "SENT 44: ['b', 'ɹ', '̩', 'p']\n",
      "PRED 44: b ɹ ̩ p t\n",
      "PRED SCORE: -0.3047\n",
      "\n",
      "[2021-01-30 02:53:36,810 INFO] \n",
      "SENT 45: ['ɑ', 'ɹ', 'm']\n",
      "PRED 45: ɑ ɹ m d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:36,810 INFO] \n",
      "SENT 46: ['d', 'ɪ', 's', 'a', 'ɪ', 'd']\n",
      "PRED 46: d ɪ s a ɪ d ɪ d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:36,811 INFO] \n",
      "SENT 47: ['s', 'p', 'ɑ', 'ɹ', 'k', 'ə', 'l']\n",
      "PRED 47: s p ɑ ɹ k ə l d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:36,811 INFO] \n",
      "SENT 48: ['v', 'ɪ', 'd', 'i', 'o', 'ʊ']\n",
      "PRED 48: v ɪ d ɛ o ʊ d\n",
      "PRED SCORE: -0.2679\n",
      "\n",
      "[2021-01-30 02:53:36,811 INFO] \n",
      "SENT 49: ['ɪ', 'n', 'k', 'l', 'o', 'ʊ', 'z']\n",
      "PRED 49: ɪ n k l o ʊ z d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:36,811 INFO] \n",
      "SENT 50: ['s', 't', 'r', 'a', 'ɪ', 'k']\n",
      "PRED 50: s t r a ɪ k t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:36,811 INFO] \n",
      "SENT 51: ['p', 'ɹ', 'ɛ', 'z', 'ə', 'n', 't']\n",
      "PRED 51: p ɹ ɛ z ə n t ɪ d\n",
      "PRED SCORE: -0.0011\n",
      "\n",
      "[2021-01-30 02:53:36,812 INFO] \n",
      "SENT 52: ['d', 'ɪ', 's', 't', 'r', 'ɔ', 'ɪ']\n",
      "PRED 52: d ɪ s t r ɔ ɪ d\n",
      "PRED SCORE: -0.0072\n",
      "\n",
      "[2021-01-30 02:53:36,812 INFO] \n",
      "SENT 53: ['ɪ', 'n', 'h', 'e', 'ɪ', 'l']\n",
      "PRED 53: ɪ n h n h l d\n",
      "PRED SCORE: -0.0734\n",
      "\n",
      "[2021-01-30 02:53:36,812 INFO] \n",
      "SENT 54: ['g', 'o', 'ʊ']\n",
      "PRED 54: g o ʊ d\n",
      "PRED SCORE: -0.0047\n",
      "\n",
      "[2021-01-30 02:53:36,812 INFO] \n",
      "SENT 55: ['w', 'æ', 'k']\n",
      "PRED 55: w æ k t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:36,812 INFO] \n",
      "SENT 56: ['k', 'ɪ', 'd', 'n', 'æ', 'p']\n",
      "PRED 56: k ɪ d æ p t\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:36,813 INFO] \n",
      "SENT 57: ['l', 'e', 'ɪ', 'ə', 'r']\n",
      "PRED 57: l e ɪ ə r d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:36,813 INFO] \n",
      "SENT 58: ['p', 'ə', 'n', 'ʧ']\n",
      "PRED 58: p ə n ʧ t\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:36,813 INFO] \n",
      "SENT 59: ['f', 'r', 'ə', 's', 't', 'r', 'e', 'ɪ', 't']\n",
      "PRED 59: f r ə s t r e ɪ t ɪ d\n",
      "PRED SCORE: -0.0393\n",
      "\n",
      "[2021-01-30 02:53:36,813 INFO] \n",
      "SENT 60: ['g', 'l', 'a', 'ɪ', 'd']\n",
      "PRED 60: g l a ɪ d ɪ d\n",
      "PRED SCORE: -0.0378\n",
      "\n",
      "[2021-01-30 02:53:36,908 INFO] \n",
      "SENT 61: ['d', 'ɪ', 'k', 'l', 'ɛ', 'r']\n",
      "PRED 61: d ɪ k l ɛ r d\n",
      "PRED SCORE: -0.0016\n",
      "\n",
      "[2021-01-30 02:53:36,908 INFO] \n",
      "SENT 62: ['s', 'ə', 'r', 'ʧ']\n",
      "PRED 62: s ə r ʧ t\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:36,908 INFO] \n",
      "SENT 63: ['f', 'ɛ', 's', 't', 'u', 'n']\n",
      "PRED 63: f ɛ s t u n d\n",
      "PRED SCORE: -0.0020\n",
      "\n",
      "[2021-01-30 02:53:36,909 INFO] \n",
      "SENT 64: ['r', 'a', 'ɪ', 'd']\n",
      "PRED 64: r a ɪ d ɪ d\n",
      "PRED SCORE: -0.0540\n",
      "\n",
      "[2021-01-30 02:53:36,909 INFO] \n",
      "SENT 65: ['ɛ', 'n', 'd', 'a', 'ʊ']\n",
      "PRED 65: ɛ n d a ʊ d\n",
      "PRED SCORE: -0.0510\n",
      "\n",
      "[2021-01-30 02:53:36,909 INFO] \n",
      "SENT 66: ['θ', 'ɹ', 'ɪ', 'l']\n",
      "PRED 66: θ ɹ ɪ l d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:36,910 INFO] \n",
      "SENT 67: ['t', 'æ', 'p']\n",
      "PRED 67: t æ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:36,910 INFO] \n",
      "SENT 68: ['l', 'o', 'ʊ', 'ə', 'r']\n",
      "PRED 68: l o ʊ ə r d\n",
      "PRED SCORE: -0.0025\n",
      "\n",
      "[2021-01-30 02:53:36,910 INFO] \n",
      "SENT 69: ['b', 'e', 'ɪ', 'k']\n",
      "PRED 69: b e ɪ k t\n",
      "PRED SCORE: -0.0280\n",
      "\n",
      "[2021-01-30 02:53:36,910 INFO] \n",
      "SENT 70: ['r', 'ə', 'm', 'ɪ', 'ʤ']\n",
      "PRED 70: r ə m ɪ ʤ d\n",
      "PRED SCORE: -0.0013\n",
      "\n",
      "[2021-01-30 02:53:36,911 INFO] \n",
      "SENT 71: ['s', 'p', 'ɛ', 'r']\n",
      "PRED 71: s p ɛ r d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:36,911 INFO] \n",
      "SENT 72: ['k', 'ə', 'm', 'æ', 'n', 'd']\n",
      "PRED 72: k ə m æ n d ɪ d\n",
      "PRED SCORE: -0.0163\n",
      "\n",
      "[2021-01-30 02:53:36,911 INFO] \n",
      "SENT 73: ['h', 'o', 'ʊ']\n",
      "PRED 73: h o ʊ d\n",
      "PRED SCORE: -0.0349\n",
      "\n",
      "[2021-01-30 02:53:36,911 INFO] \n",
      "SENT 74: ['t', 'a', 'ɪ', 'r']\n",
      "PRED 74: t a ɪ r d\n",
      "PRED SCORE: -0.0015\n",
      "\n",
      "[2021-01-30 02:53:36,912 INFO] \n",
      "SENT 75: ['n', 'e', 'ɪ', 'l']\n",
      "PRED 75: n e ɪ l d\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:36,912 INFO] \n",
      "SENT 76: ['ʧ', 'æ', 'l', 'ə', 'n', 'ʤ']\n",
      "PRED 76: ʧ æ l ə n ʤ d\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:36,912 INFO] \n",
      "SENT 77: ['g', 'æ', 'ð', 'ə', 'r']\n",
      "PRED 77: g æ ð ə r d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:36,912 INFO] \n",
      "SENT 78: ['ɹ', 'ɪ', 'f', 'j', 'u', 'ː', 'z']\n",
      "PRED 78: ɹ ɪ f j u ʌ ŋ d\n",
      "PRED SCORE: -1.5039\n",
      "\n",
      "[2021-01-30 02:53:36,913 INFO] \n",
      "SENT 79: ['p', 'ʌ', 'z', 'ə', 'l']\n",
      "PRED 79: p ʌ z ə l d\n",
      "PRED SCORE: -0.0238\n",
      "\n",
      "[2021-01-30 02:53:36,913 INFO] \n",
      "SENT 80: ['k', 'ə', 'r', 'ɛ', 'k', 't']\n",
      "PRED 80: k ə r ɛ k t ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:36,913 INFO] \n",
      "SENT 81: ['m', 'ə', 't', 'ə', 'r']\n",
      "PRED 81: m ə t ə r d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:36,914 INFO] \n",
      "SENT 82: ['t', 'ɹ', 'ɛ', 'd']\n",
      "PRED 82: t ɹ ɛ d ɪ d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:36,914 INFO] \n",
      "SENT 83: ['b', 'ɹ', 'ɪ', 'ŋ']\n",
      "PRED 83: b ɹ æ ŋ\n",
      "PRED SCORE: -0.0064\n",
      "\n",
      "[2021-01-30 02:53:36,914 INFO] \n",
      "SENT 84: ['w', 'e', 'ɪ', 'v']\n",
      "PRED 84: w e ɪ v d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:36,914 INFO] \n",
      "SENT 85: ['k', 'l', 'a', 'ɪ', 'm']\n",
      "PRED 85: k l a ɪ m d\n",
      "PRED SCORE: -0.0969\n",
      "\n",
      "[2021-01-30 02:53:36,914 INFO] \n",
      "SENT 86: ['s', 't', 'ʌ', 'f']\n",
      "PRED 86: s t ʌ f t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:36,915 INFO] \n",
      "SENT 87: ['d', 'ɪ', 'm', 'ɑ', 'l', 'ɪ', 'ʃ']\n",
      "PRED 87: d ɪ m ɑ l ɪ ʃ t\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:36,915 INFO] \n",
      "SENT 88: ['ʃ', 'æ', 't', 'ə', 'r']\n",
      "PRED 88: ʃ æ t ə r d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:36,915 INFO] \n",
      "SENT 89: ['ə', 'd', 'ɔ', 'ɹ']\n",
      "PRED 89: ə d ɔ ɹ d\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-01-30 02:53:36,915 INFO] \n",
      "SENT 90: ['f', 'l', 'a', 'ʊ', 'ə', 'r']\n",
      "PRED 90: f l a ʊ ə r d\n",
      "PRED SCORE: -0.0124\n",
      "\n",
      "[2021-01-30 02:53:37,009 INFO] \n",
      "SENT 91: ['k', 'l', 'ɑ', 'b', 'ə', 'r']\n",
      "PRED 91: k l ɑ b ə r d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:37,009 INFO] \n",
      "SENT 92: ['p', 'ə', 'v', 'ɪ', 'l', 'j', 'ə', 'n']\n",
      "PRED 92: p ə v ɪ l ə n j u d\n",
      "PRED SCORE: -0.7115\n",
      "\n",
      "[2021-01-30 02:53:37,009 INFO] \n",
      "SENT 93: ['s', 't', 'r', 'a', 'ɪ', 'd']\n",
      "PRED 93: s t r a ɪ d ɪ d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:37,010 INFO] \n",
      "SENT 94: ['ɹ', 'ʌ', 's', 'ə', 'l']\n",
      "PRED 94: ɹ ʌ s ə l d\n",
      "PRED SCORE: -0.6621\n",
      "\n",
      "[2021-01-30 02:53:37,010 INFO] \n",
      "SENT 95: ['ɡ', 'ɑ', 'ɹ', 'd']\n",
      "PRED 95: ɡ ɑ ɹ d ɪ d\n",
      "PRED SCORE: -0.1585\n",
      "\n",
      "[2021-01-30 02:53:37,010 INFO] \n",
      "SENT 96: ['s', 'ə', 'ʤ', 'ɛ', 's', 't']\n",
      "PRED 96: s ə ʤ ɛ s t ɪ d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:37,010 INFO] \n",
      "SENT 97: ['b', 'ɔ', 'g']\n",
      "PRED 97: b ɔ g d\n",
      "PRED SCORE: -0.2969\n",
      "\n",
      "[2021-01-30 02:53:37,010 INFO] \n",
      "SENT 98: ['r', 'ɪ', 's', 'i', 'v']\n",
      "PRED 98: r ɪ s i v d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:37,011 INFO] \n",
      "SENT 99: ['ɪ', 'n', 's', 'p', 'ɛ', 'k', 't']\n",
      "PRED 99: ɪ n s p ɛ k t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:37,011 INFO] \n",
      "SENT 100: ['b', 'ɪ', 'l']\n",
      "PRED 100: b ɪ l d\n",
      "PRED SCORE: -0.0047\n",
      "\n",
      "[2021-01-30 02:53:37,011 INFO] \n",
      "SENT 101: ['l', 'e', 'ɪ', 'b', 'ə', 'l']\n",
      "PRED 101: l e ɪ b ə l d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:37,011 INFO] \n",
      "SENT 102: ['h', 'ɑ', 'p']\n",
      "PRED 102: h ɑ p t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:37,011 INFO] \n",
      "SENT 103: ['p', 'o', 'ʊ', 'z']\n",
      "PRED 103: p o ʊ z d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:37,012 INFO] \n",
      "SENT 104: ['l', 'æ', 'f']\n",
      "PRED 104: l æ f t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:37,012 INFO] \n",
      "SENT 105: ['ə', 'r', 'ʤ']\n",
      "PRED 105: ə r ʤ d\n",
      "PRED SCORE: -0.0075\n",
      "\n",
      "[2021-01-30 02:53:37,012 INFO] \n",
      "SENT 106: ['f', 'a', 'ɪ', 'n', 'd']\n",
      "PRED 106: f a ɪ n d ɪ d\n",
      "PRED SCORE: -0.0035\n",
      "\n",
      "[2021-01-30 02:53:37,012 INFO] \n",
      "SENT 107: ['v', 'æ', 'k', 'j', 'u', 'm']\n",
      "PRED 107: v æ k j u m d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:37,013 INFO] \n",
      "SENT 108: ['s', 'ə', 'r', 'v']\n",
      "PRED 108: s ə r v d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:37,013 INFO] \n",
      "SENT 109: ['d', 'o', 'ʊ', 'z']\n",
      "PRED 109: d o ʊ z d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:37,013 INFO] \n",
      "SENT 110: ['f', 'r', 'i']\n",
      "PRED 110: f r i d\n",
      "PRED SCORE: -0.0396\n",
      "\n",
      "[2021-01-30 02:53:37,013 INFO] \n",
      "SENT 111: ['t', 'u', 'n']\n",
      "PRED 111: t u n d\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-01-30 02:53:37,013 INFO] \n",
      "SENT 112: ['m', 'u']\n",
      "PRED 112: m u d\n",
      "PRED SCORE: -0.0008\n",
      "\n",
      "[2021-01-30 02:53:37,014 INFO] \n",
      "SENT 113: ['k', 'w', 'o', 'ʊ', 't']\n",
      "PRED 113: k w o ʊ t ɪ d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:37,014 INFO] \n",
      "SENT 114: ['s', 'ə', 's', 'p', 'ɛ', 'n', 'd']\n",
      "PRED 114: s p ɛ s p ɛ n t\n",
      "PRED SCORE: -0.0033\n",
      "\n",
      "[2021-01-30 02:53:37,014 INFO] \n",
      "SENT 115: ['p', 'l', 'ə', 'n', 'ʤ']\n",
      "PRED 115: p l ə n ʤ d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:37,014 INFO] \n",
      "SENT 116: ['f', 'æ', 'n', 's', 'i']\n",
      "PRED 116: f æ n s u d\n",
      "PRED SCORE: -0.4454\n",
      "\n",
      "[2021-01-30 02:53:37,015 INFO] \n",
      "SENT 117: ['ʤ', 'ɛ', 'n', 'ə', 'r', 'e', 'ɪ', 't']\n",
      "PRED 117: ʤ ɛ n ə r e ɪ t ɪ d\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-01-30 02:53:37,015 INFO] \n",
      "SENT 118: ['a', 'ɪ', 'd', 'ɛ', 'n', 't', 'ə', 'f', 'a', 'ɪ']\n",
      "PRED 118: a ɪ d ɛ n t ə f a ɪ d\n",
      "PRED SCORE: -0.0768\n",
      "\n",
      "[2021-01-30 02:53:37,015 INFO] \n",
      "SENT 119: ['d', 'ɹ', 'ɛ', 's']\n",
      "PRED 119: d ɹ ɛ s t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:37,015 INFO] \n",
      "SENT 120: ['d', 'ɪ', 'g', 'n', 'ə', 'f', 'a', 'ɪ']\n",
      "PRED 120: d ɪ g n ə f a ɪ d\n",
      "PRED SCORE: -0.0030\n",
      "\n",
      "[2021-01-30 02:53:37,098 INFO] \n",
      "SENT 121: ['k', 'l', 'æ', 'm', 'b', 'ɹ', '̩']\n",
      "PRED 121: k l æ m b ɹ ̩ d\n",
      "PRED SCORE: -0.6242\n",
      "\n",
      "[2021-01-30 02:53:37,098 INFO] \n",
      "SENT 122: ['k', 'o', 'ʊ', 'm']\n",
      "PRED 122: k o ʊ m d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:37,098 INFO] \n",
      "SENT 123: ['l', 'ɑ', 'k']\n",
      "PRED 123: l ɑ k t\n",
      "PRED SCORE: -0.0012\n",
      "\n",
      "[2021-01-30 02:53:37,098 INFO] \n",
      "SENT 124: ['f', 'ɪ', 'g', 'j', 'ə', 'r']\n",
      "PRED 124: f ɪ g j ə r d\n",
      "PRED SCORE: -0.0036\n",
      "\n",
      "[2021-01-30 02:53:37,099 INFO] \n",
      "SENT 125: ['w', 'ɪ', 'ŋ']\n",
      "PRED 125: w æ ŋ\n",
      "PRED SCORE: -0.0014\n",
      "\n",
      "[2021-01-30 02:53:37,099 INFO] \n",
      "SENT 126: ['b', 'ɹ', 'ʌ', 'ʃ']\n",
      "PRED 126: b ɹ ʌ ʃ t\n",
      "PRED SCORE: -0.0087\n",
      "\n",
      "[2021-01-30 02:53:37,099 INFO] \n",
      "SENT 127: ['p', 'l', 'i', 'z']\n",
      "PRED 127: p l e ɪ z\n",
      "PRED SCORE: -1.1803\n",
      "\n",
      "[2021-01-30 02:53:37,099 INFO] \n",
      "SENT 128: ['ɹ', 'ɪ', 't', 'ɹ', 'i', 'v']\n",
      "PRED 128: ɹ ɪ t ɹ i v d\n",
      "PRED SCORE: -0.0030\n",
      "\n",
      "[2021-01-30 02:53:37,099 INFO] \n",
      "SENT 129: ['w', 'ɪ', 'ŋ', 'k']\n",
      "PRED 129: w æ ŋ k\n",
      "PRED SCORE: -0.0761\n",
      "\n",
      "[2021-01-30 02:53:37,099 INFO] \n",
      "SENT 130: ['p', 'ʌ', 'm', 'p']\n",
      "PRED 130: p ʌ m p t\n",
      "PRED SCORE: -0.0018\n",
      "\n",
      "[2021-01-30 02:53:37,100 INFO] \n",
      "SENT 131: ['f', 'l', 'ɪ', 'p']\n",
      "PRED 131: f l ɪ p t\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:37,100 INFO] \n",
      "SENT 132: ['s', 'm', 'æ', 'k']\n",
      "PRED 132: s m æ k t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:37,100 INFO] \n",
      "SENT 133: ['ɹ', 'ɪ', 's', 'p', 'ɛ', 'k', 't']\n",
      "PRED 133: ɹ ɪ s p ɛ k t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:37,100 INFO] \n",
      "SENT 134: ['d', 'ɹ', 'ɑ', 'p']\n",
      "PRED 134: d ɹ ɑ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:37,101 INFO] \n",
      "SENT 135: ['p', 'ɛ', 't', 'r', 'ə', 'f', 'a', 'ɪ']\n",
      "PRED 135: p ɛ t r ə f a ɪ d\n",
      "PRED SCORE: -0.0031\n",
      "\n",
      "[2021-01-30 02:53:37,101 INFO] \n",
      "SENT 136: ['s', 'k', 'ɹ', 'æ', 'p']\n",
      "PRED 136: s k ɹ æ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:37,101 INFO] \n",
      "SENT 137: ['o', 'ʊ', 'p', 'ə', 'n']\n",
      "PRED 137: o ʊ p ə n d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:37,101 INFO] \n",
      "SENT 138: ['d', 'r', 'a', 'ɪ']\n",
      "PRED 138: d r a ɪ d\n",
      "PRED SCORE: -0.0013\n",
      "\n",
      "[2021-01-30 02:53:37,101 INFO] \n",
      "SENT 139: ['θ', 'ɪ', 'ŋ', 'k']\n",
      "PRED 139: θ ɪ ŋ k t\n",
      "PRED SCORE: -0.6894\n",
      "\n",
      "[2021-01-30 02:53:37,101 INFO] \n",
      "SENT 140: ['s', 't', 'ɹ', 'æ', 'ŋ', 'ɡ', 'ə', 'l']\n",
      "PRED 140: s t ɹ æ ŋ ɡ ə l d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:37,102 INFO] \n",
      "SENT 141: ['g', 'l', 'ɛ', 'r']\n",
      "PRED 141: g l ɛ r d\n",
      "PRED SCORE: -0.0022\n",
      "\n",
      "[2021-01-30 02:53:37,102 INFO] \n",
      "SENT 142: ['b', 'ɪ', 'l', 'i', 'v']\n",
      "PRED 142: b ɪ l i v d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:37,102 INFO] \n",
      "SENT 143: ['d', 'ɪ', 'z', 'ɑ', 'l', 'v']\n",
      "PRED 143: d ɪ z ɑ l v d\n",
      "PRED SCORE: -0.0048\n",
      "\n",
      "[2021-01-30 02:53:37,102 INFO] \n",
      "SENT 144: ['s', 'ə', 'r', 'k', 'ə', 't']\n",
      "PRED 144: s ə r k ə t ɪ d\n",
      "PRED SCORE: -0.0027\n",
      "\n",
      "[2021-01-30 02:53:37,103 INFO] \n",
      "SENT 145: ['n', 'e', 'ɪ', 'm']\n",
      "PRED 145: n e ɪ m d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:37,103 INFO] \n",
      "SENT 146: ['ʧ', 'æ', 'n', 't']\n",
      "PRED 146: ʧ æ n t ɪ d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:37,103 INFO] \n",
      "SENT 147: ['ə', 'b', 's', 'ɛ', 's']\n",
      "PRED 147: ə b s ɛ s t\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:37,103 INFO] \n",
      "SENT 148: ['b', 'ɑ', 't', 'ə', 'l']\n",
      "PRED 148: b ɑ t ə l d\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:37,104 INFO] \n",
      "SENT 149: ['k', 'æ', 't', '͡', 'ʃ']\n",
      "PRED 149: k æ t ͡ ʃ t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:37,104 INFO] \n",
      "SENT 150: ['s', 'k', 'ɛ', 'r']\n",
      "PRED 150: s k ɛ r d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:37,195 INFO] \n",
      "SENT 151: ['p', 'a', 'ʊ', 'n', 's']\n",
      "PRED 151: p a ʊ n s t\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:37,196 INFO] \n",
      "SENT 152: ['m', 'ɛ', 'm', 'ə', 'r', 'a', 'ɪ', 'z']\n",
      "PRED 152: m ɛ m ə r a ɪ z d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:37,196 INFO] \n",
      "SENT 153: ['w', 'e', 'ɪ', 'k']\n",
      "PRED 153: w e ɪ k t\n",
      "PRED SCORE: -0.0016\n",
      "\n",
      "[2021-01-30 02:53:37,196 INFO] \n",
      "SENT 154: ['r', 'a', 'ʊ', 'z']\n",
      "PRED 154: r a ʊ z d\n",
      "PRED SCORE: -0.0069\n",
      "\n",
      "[2021-01-30 02:53:37,196 INFO] \n",
      "SENT 155: ['r', 'i', 'm', 'u', 'v']\n",
      "PRED 155: r i m u v d\n",
      "PRED SCORE: -0.0330\n",
      "\n",
      "[2021-01-30 02:53:37,196 INFO] \n",
      "SENT 156: ['f', 'l', 'ə', 's', 't', 'ə', 'r']\n",
      "PRED 156: f l ə s t ə r d\n",
      "PRED SCORE: -0.0158\n",
      "\n",
      "[2021-01-30 02:53:37,197 INFO] \n",
      "SENT 157: ['s', 'ə', 'r', 'v', 'a', 'ɪ', 'v']\n",
      "PRED 157: s ə r v a ɪ v d\n",
      "PRED SCORE: -0.0017\n",
      "\n",
      "[2021-01-30 02:53:37,197 INFO] \n",
      "SENT 158: ['n', 'o', 'ʊ', 'z']\n",
      "PRED 158: n o ʊ z d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:37,197 INFO] \n",
      "SENT 159: ['v', 'ɪ', 'z', 'ɪ', 't']\n",
      "PRED 159: v ɪ z ɪ t ɪ d\n",
      "PRED SCORE: -0.0063\n",
      "\n",
      "[2021-01-30 02:53:37,197 INFO] \n",
      "SENT 160: ['m', 'ɪ', 'k', 's']\n",
      "PRED 160: m ɪ k s t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:37,197 INFO] \n",
      "SENT 161: ['ɹ', 'æ', 'm']\n",
      "PRED 161: ɹ æ m d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:37,198 INFO] \n",
      "SENT 162: ['ʃ', 'ʌ', 'f', 'ə', 'l']\n",
      "PRED 162: ʃ ʌ f ə l d\n",
      "PRED SCORE: -0.0017\n",
      "\n",
      "[2021-01-30 02:53:37,198 INFO] \n",
      "SENT 163: ['s', 'm', 'ɛ', 'l']\n",
      "PRED 163: s m ɛ l d\n",
      "PRED SCORE: -0.0018\n",
      "\n",
      "[2021-01-30 02:53:37,198 INFO] \n",
      "SENT 164: ['w', 'ɪ', 'p']\n",
      "PRED 164: w ɪ p t\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:37,198 INFO] \n",
      "SENT 165: ['r', 'ɪ', 'p', 'l', 'a', 'ɪ']\n",
      "PRED 165: r ɪ p l a ɪ d\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:37,198 INFO] \n",
      "SENT 166: ['s', 'ɔ', 'l', 't']\n",
      "PRED 166: s ɔ l t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:37,199 INFO] \n",
      "SENT 167: ['ɡ', 'ɹ', 'ʌ', 'n', 't']\n",
      "PRED 167: ɡ ɹ æ n t ɪ d\n",
      "PRED SCORE: -0.4673\n",
      "\n",
      "[2021-01-30 02:53:37,199 INFO] \n",
      "SENT 168: ['f', 'a', 'ɪ', 'n']\n",
      "PRED 168: f ɔ n\n",
      "PRED SCORE: -0.4728\n",
      "\n",
      "[2021-01-30 02:53:37,199 INFO] \n",
      "SENT 169: ['k', 'l', 'ɔ']\n",
      "PRED 169: k l ɔ d\n",
      "PRED SCORE: -0.0069\n",
      "\n",
      "[2021-01-30 02:53:37,199 INFO] \n",
      "SENT 170: ['t', 'r', 'e', 'ɪ', 'n']\n",
      "PRED 170: t r e ɪ n d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:37,199 INFO] \n",
      "SENT 171: ['s', 'ə', 'r', 'a', 'ʊ', 'n', 'd']\n",
      "PRED 171: s ə r a ʊ n d ɪ d\n",
      "PRED SCORE: -0.0017\n",
      "\n",
      "[2021-01-30 02:53:37,200 INFO] \n",
      "SENT 172: ['s', 'k', 'w', 'ə', 'r', 't']\n",
      "PRED 172: s k w ə r t ɪ d\n",
      "PRED SCORE: -0.0317\n",
      "\n",
      "[2021-01-30 02:53:37,200 INFO] \n",
      "SENT 173: ['ɛ', 's', 'k', 'ɔ', 'ɹ', 't']\n",
      "PRED 173: ɛ s k ɔ ɹ t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:37,200 INFO] \n",
      "SENT 174: ['s', 't', 'r', 'ə', 'g', 'ə', 'l']\n",
      "PRED 174: s t r ə g ə l d\n",
      "PRED SCORE: -0.0010\n",
      "\n",
      "[2021-01-30 02:53:37,200 INFO] \n",
      "SENT 175: ['æ', 'd', 'ɪ', 'k', 't']\n",
      "PRED 175: æ d ɪ k t ɪ d\n",
      "PRED SCORE: -0.0009\n",
      "\n",
      "[2021-01-30 02:53:37,200 INFO] \n",
      "SENT 176: ['r', 'o', 'ʊ', 'l']\n",
      "PRED 176: r o ʊ l d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:37,201 INFO] \n",
      "SENT 177: ['d', 'r', 'a', 'ɪ', 'v']\n",
      "PRED 177: d r a ɪ v d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:37,201 INFO] \n",
      "SENT 178: ['s', 't', 'e', 'ɪ', 'n']\n",
      "PRED 178: s t e ɪ n d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:37,201 INFO] \n",
      "SENT 179: ['ɹ', 'ɪ', 'ɹ']\n",
      "PRED 179: ɹ ɪ ɹ d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:37,201 INFO] \n",
      "SENT 180: ['ɪ', 'k', 's', 't', 'ɛ', 'n', 'd']\n",
      "PRED 180: ɪ k s t ɛ n d ɪ d\n",
      "PRED SCORE: -0.0604\n",
      "\n",
      "[2021-01-30 02:53:37,260 INFO] \n",
      "SENT 181: ['ɪ', 'n', 'h', 'ɛ', 'r', 'ə', 't']\n",
      "PRED 181: ɪ n h ɛ r ə t ɪ d\n",
      "PRED SCORE: -0.0025\n",
      "\n",
      "[2021-01-30 02:53:37,260 INFO] \n",
      "SENT 182: ['p', 'i', 'r']\n",
      "PRED 182: p i r d\n",
      "PRED SCORE: -0.0003\n",
      "\n",
      "[2021-01-30 02:53:37,261 INFO] \n",
      "SENT 183: ['k', 'ə', 'n', 's', 'ɪ', 'd', 'ə', 'r']\n",
      "PRED 183: k ə n s ɪ d ə r d\n",
      "PRED SCORE: -0.0175\n",
      "\n",
      "[2021-01-30 02:53:37,261 INFO] \n",
      "SENT 184: ['b', 'a', 'ʊ', 'n', 's']\n",
      "PRED 184: b a ʊ n s t\n",
      "PRED SCORE: -0.0007\n",
      "\n",
      "[2021-01-30 02:53:37,261 INFO] \n",
      "SENT 185: ['b', 'ʊ', 'l', 'd', 'o', 'ʊ', 'z']\n",
      "PRED 185: b ʊ l d o ʊ z d\n",
      "PRED SCORE: -0.0360\n",
      "\n",
      "[2021-01-30 02:53:37,261 INFO] \n",
      "SENT 186: ['r', 'e', 'ɪ', 'k']\n",
      "PRED 186: r e ɪ k t\n",
      "PRED SCORE: -0.0159\n",
      "\n",
      "[2021-01-30 02:53:37,261 INFO] \n",
      "SENT 187: ['d', 'ɹ', 'ɔ']\n",
      "PRED 187: d ɹ ɔ ɹ d\n",
      "PRED SCORE: -0.0051\n",
      "\n",
      "[2021-01-30 02:53:37,262 INFO] \n",
      "SENT 188: ['b', 'l', 'i', 'ʧ']\n",
      "PRED 188: b l ɛ ʧ t\n",
      "PRED SCORE: -0.2915\n",
      "\n",
      "[2021-01-30 02:53:37,262 INFO] \n",
      "SENT 189: ['ɹ', 'ɛ', 'n', 't']\n",
      "PRED 189: ɹ ɛ n t ɪ d\n",
      "PRED SCORE: -0.0000\n",
      "\n",
      "[2021-01-30 02:53:37,262 INFO] \n",
      "SENT 190: ['ɛ', 'ʤ']\n",
      "PRED 190: ɛ ʤ d\n",
      "PRED SCORE: -0.0012\n",
      "\n",
      "[2021-01-30 02:53:37,262 INFO] \n",
      "SENT 191: ['b', 'ɑ', 'b']\n",
      "PRED 191: b ɑ b d\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:37,263 INFO] \n",
      "SENT 192: ['d', 'ɪ', 'v', 'ɛ', 'l', 'ə', 'p']\n",
      "PRED 192: d ɪ v ɛ l ə p t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:37,263 INFO] \n",
      "SENT 193: ['t', 'æ', 'ŋ', 'ɡ', 'ə', 'l']\n",
      "PRED 193: t æ ŋ ɡ ə l d\n",
      "PRED SCORE: -0.0005\n",
      "\n",
      "[2021-01-30 02:53:37,263 INFO] \n",
      "SENT 194: ['t', 'e', 'ɪ', 'k']\n",
      "PRED 194: t e ɪ k t\n",
      "PRED SCORE: -0.0033\n",
      "\n",
      "[2021-01-30 02:53:37,263 INFO] \n",
      "SENT 195: ['s', 'k', 'ɪ', 'm']\n",
      "PRED 195: s k æ m\n",
      "PRED SCORE: -0.1180\n",
      "\n",
      "[2021-01-30 02:53:37,263 INFO] \n",
      "SENT 196: ['f', 'l', 'ɪ', 'ŋ']\n",
      "PRED 196: f l æ ŋ\n",
      "PRED SCORE: -0.0194\n",
      "\n",
      "[2021-01-30 02:53:37,264 INFO] \n",
      "SENT 197: ['g', 'l', 'ɪ', 't', 'ə', 'r']\n",
      "PRED 197: g l ɪ t ə r d\n",
      "PRED SCORE: -0.0006\n",
      "\n",
      "[2021-01-30 02:53:37,264 INFO] \n",
      "SENT 198: ['p', 'a', 'ɪ', 'l']\n",
      "PRED 198: p a ɪ l d\n",
      "PRED SCORE: -0.0001\n",
      "\n",
      "[2021-01-30 02:53:37,264 INFO] \n",
      "SENT 199: ['p', 'ɹ', 'ɑ', 's', 'ɛ', 's']\n",
      "PRED 199: p ɹ ɑ s ɛ s t\n",
      "PRED SCORE: -0.0004\n",
      "\n",
      "[2021-01-30 02:53:37,264 INFO] \n",
      "SENT 200: ['s', 'ə', 's', 'p', 'ɛ', 'k', 't']\n",
      "PRED 200: s p ɛ s p ɛ k t\n",
      "PRED SCORE: -0.0002\n",
      "\n",
      "[2021-01-30 02:53:37,264 INFO] PRED AVG SCORE: -0.0099, PRED PPL: 1.0099\n"
     ]
    }
   ],
   "source": [
    "for datasize in datasizes:\n",
    "  epochs, n_examples, batchsize,  = 100, int(datasize.split('_')[0]), 20\n",
    "  steps = str(int(epochs * n_examples / batchsize))\n",
    "  datadir = f'drive/MyDrive/EnglishToleranceBaseline/processed_data/{datasize}'\n",
    "  rnn_modelpath = f'{outdir}/english_rnn_model_{datasize}'\n",
    "  rnn_trans_args = ' '.join([\n",
    "    '-model '+rnn_modelpath+'_step_'+steps+'.pt',\n",
    "    f'-src {datadir}/english-src-test.txt',\n",
    "    f'-output {outdir}/english-rnn-{datasize}-pred.txt',\n",
    "    '-replace_unk -verbose',\n",
    "    '-beam_size 12'\n",
    "    ])\n",
    "  !python OpenNMT-py/translate.py $rnn_trans_args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSt5iRyJ7lwc"
   },
   "source": [
    "## Evaluate English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OJGuob0_pyp",
    "outputId": "fed2b75a-b40b-4347-c45f-af902bc8b409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data type 100_0\n",
      "Test accuracy: 0.015\n",
      "-ed predicted: 0.745\n",
      "% of inflections (len 1) that match most popular in training 0.53\n",
      "\n",
      "Data type 100_1\n",
      "Test accuracy: 0.01\n",
      "-ed predicted: 0.87\n",
      "% of inflections (len 1) that match most popular in training 0.5\n",
      "\n",
      "Data type 100_2\n",
      "Test accuracy: 0.005\n",
      "-ed predicted: 0.86\n",
      "% of inflections (len 1) that match most popular in training 0.57\n",
      "\n",
      "Data type 100_3\n",
      "Test accuracy: 0.025\n",
      "-ed predicted: 0.945\n",
      "% of inflections (len 1) that match most popular in training 0.58\n",
      "\n",
      "Data type 100_4\n",
      "Test accuracy: 0.015\n",
      "-ed predicted: 0.81\n",
      "% of inflections (len 1) that match most popular in training 0.54\n",
      "\n",
      "Data type 100_5\n",
      "Test accuracy: 0.015\n",
      "-ed predicted: 0.865\n",
      "% of inflections (len 1) that match most popular in training 0.525\n",
      "\n",
      "Data type 100_6\n",
      "Test accuracy: 0.005\n",
      "-ed predicted: 0.905\n",
      "% of inflections (len 1) that match most popular in training 0.56\n",
      "\n",
      "Data type 100_7\n",
      "Test accuracy: 0.015\n",
      "-ed predicted: 0.88\n",
      "% of inflections (len 1) that match most popular in training 0.52\n",
      "\n",
      "Data type 100_8\n",
      "Test accuracy: 0.0\n",
      "-ed predicted: 0.945\n",
      "% of inflections (len 1) that match most popular in training 0.565\n",
      "\n",
      "Data type 100_9\n",
      "Test accuracy: 0.005\n",
      "-ed predicted: 0.91\n",
      "% of inflections (len 1) that match most popular in training 0.59\n",
      "\n",
      "Data type 200_0\n",
      "Test accuracy: 0.065\n",
      "-ed predicted: 0.945\n",
      "% of inflections (len 1) that match most popular in training 0.665\n",
      "\n",
      "Data type 200_1\n",
      "Test accuracy: 0.14\n",
      "-ed predicted: 0.885\n",
      "% of inflections (len 1) that match most popular in training 0.68\n",
      "\n",
      "Data type 200_2\n",
      "Test accuracy: 0.195\n",
      "-ed predicted: 0.93\n",
      "% of inflections (len 1) that match most popular in training 0.75\n",
      "\n",
      "Data type 200_3\n",
      "Test accuracy: 0.11\n",
      "-ed predicted: 0.855\n",
      "% of inflections (len 1) that match most popular in training 0.66\n",
      "\n",
      "Data type 200_4\n",
      "Test accuracy: 0.195\n",
      "-ed predicted: 0.93\n",
      "% of inflections (len 1) that match most popular in training 0.75\n",
      "\n",
      "Data type 200_5\n",
      "Test accuracy: 0.07\n",
      "-ed predicted: 0.9\n",
      "% of inflections (len 1) that match most popular in training 0.65\n",
      "\n",
      "Data type 200_6\n",
      "Test accuracy: 0.165\n",
      "-ed predicted: 0.9\n",
      "% of inflections (len 1) that match most popular in training 0.75\n",
      "\n",
      "Data type 200_7\n",
      "Test accuracy: 0.105\n",
      "-ed predicted: 0.875\n",
      "% of inflections (len 1) that match most popular in training 0.71\n",
      "\n",
      "Data type 200_8\n",
      "Test accuracy: 0.055\n",
      "-ed predicted: 0.885\n",
      "% of inflections (len 1) that match most popular in training 0.605\n",
      "\n",
      "Data type 200_9\n",
      "Test accuracy: 0.095\n",
      "-ed predicted: 0.905\n",
      "% of inflections (len 1) that match most popular in training 0.735\n",
      "\n",
      "Data type 400_0\n",
      "Test accuracy: 0.535\n",
      "-ed predicted: 0.93\n",
      "% of inflections (len 1) that match most popular in training 0.855\n",
      "\n",
      "Data type 400_1\n",
      "Test accuracy: 0.565\n",
      "-ed predicted: 0.925\n",
      "% of inflections (len 1) that match most popular in training 0.9\n",
      "\n",
      "Data type 400_2\n",
      "Test accuracy: 0.495\n",
      "-ed predicted: 0.955\n",
      "% of inflections (len 1) that match most popular in training 0.885\n",
      "\n",
      "Data type 400_3\n",
      "Test accuracy: 0.585\n",
      "-ed predicted: 0.95\n",
      "% of inflections (len 1) that match most popular in training 0.94\n",
      "\n",
      "Data type 400_4\n",
      "Test accuracy: 0.59\n",
      "-ed predicted: 0.96\n",
      "% of inflections (len 1) that match most popular in training 0.895\n",
      "\n",
      "Data type 400_5\n",
      "Test accuracy: 0.435\n",
      "-ed predicted: 0.86\n",
      "% of inflections (len 1) that match most popular in training 0.85\n",
      "\n",
      "Data type 400_6\n",
      "Test accuracy: 0.44\n",
      "-ed predicted: 0.96\n",
      "% of inflections (len 1) that match most popular in training 0.89\n",
      "\n",
      "Data type 400_7\n",
      "Test accuracy: 0.495\n",
      "-ed predicted: 0.95\n",
      "% of inflections (len 1) that match most popular in training 0.87\n",
      "\n",
      "Data type 400_8\n",
      "Test accuracy: 0.53\n",
      "-ed predicted: 0.975\n",
      "% of inflections (len 1) that match most popular in training 0.885\n",
      "\n",
      "Data type 400_9\n",
      "Test accuracy: 0.57\n",
      "-ed predicted: 0.925\n",
      "% of inflections (len 1) that match most popular in training 0.865\n",
      "\n",
      "Data type 600_0\n",
      "Test accuracy: 0.735\n",
      "-ed predicted: 0.975\n",
      "% of inflections (len 1) that match most popular in training 0.925\n",
      "\n",
      "Data type 600_1\n",
      "Test accuracy: 0.745\n",
      "-ed predicted: 0.96\n",
      "% of inflections (len 1) that match most popular in training 0.925\n",
      "\n",
      "Data type 600_2\n",
      "Test accuracy: 0.77\n",
      "-ed predicted: 0.98\n",
      "% of inflections (len 1) that match most popular in training 0.94\n",
      "\n",
      "Data type 600_3\n",
      "Test accuracy: 0.735\n",
      "-ed predicted: 0.97\n",
      "% of inflections (len 1) that match most popular in training 0.93\n",
      "\n",
      "Data type 600_4\n",
      "Test accuracy: 0.75\n",
      "-ed predicted: 0.98\n",
      "% of inflections (len 1) that match most popular in training 0.93\n",
      "\n",
      "Data type 600_5\n",
      "Test accuracy: 0.725\n",
      "-ed predicted: 0.95\n",
      "% of inflections (len 1) that match most popular in training 0.945\n",
      "\n",
      "Data type 600_6\n",
      "Test accuracy: 0.695\n",
      "-ed predicted: 0.97\n",
      "% of inflections (len 1) that match most popular in training 0.96\n",
      "\n",
      "Data type 600_7\n",
      "Test accuracy: 0.755\n",
      "-ed predicted: 0.995\n",
      "% of inflections (len 1) that match most popular in training 0.975\n",
      "\n",
      "Data type 600_8\n",
      "Test accuracy: 0.68\n",
      "-ed predicted: 0.985\n",
      "% of inflections (len 1) that match most popular in training 0.95\n",
      "\n",
      "Data type 600_9\n",
      "Test accuracy: 0.775\n",
      "-ed predicted: 0.955\n",
      "% of inflections (len 1) that match most popular in training 0.955\n",
      "\n",
      "Data type 800_0\n",
      "Test accuracy: 0.765\n",
      "-ed predicted: 0.95\n",
      "% of inflections (len 1) that match most popular in training 0.935\n",
      "\n",
      "Data type 800_1\n",
      "Test accuracy: 0.79\n",
      "-ed predicted: 0.965\n",
      "% of inflections (len 1) that match most popular in training 0.97\n",
      "\n",
      "Data type 800_2\n",
      "Test accuracy: 0.765\n",
      "-ed predicted: 0.955\n",
      "% of inflections (len 1) that match most popular in training 0.95\n",
      "\n",
      "Data type 800_3\n",
      "Test accuracy: 0.81\n",
      "-ed predicted: 0.975\n",
      "% of inflections (len 1) that match most popular in training 0.95\n",
      "\n",
      "Data type 800_4\n",
      "Test accuracy: 0.78\n",
      "-ed predicted: 0.965\n",
      "% of inflections (len 1) that match most popular in training 0.95\n",
      "\n",
      "Data type 800_5\n",
      "Test accuracy: 0.82\n",
      "-ed predicted: 0.96\n",
      "% of inflections (len 1) that match most popular in training 0.965\n",
      "\n",
      "Data type 800_6\n",
      "Test accuracy: 0.795\n",
      "-ed predicted: 0.965\n",
      "% of inflections (len 1) that match most popular in training 0.945\n",
      "\n",
      "Data type 800_7\n",
      "Test accuracy: 0.79\n",
      "-ed predicted: 0.965\n",
      "% of inflections (len 1) that match most popular in training 0.93\n",
      "\n",
      "Data type 800_8\n",
      "Test accuracy: 0.775\n",
      "-ed predicted: 0.98\n",
      "% of inflections (len 1) that match most popular in training 0.955\n",
      "\n",
      "Data type 800_9\n",
      "Test accuracy: 0.82\n",
      "-ed predicted: 0.965\n",
      "% of inflections (len 1) that match most popular in training 0.97\n",
      "\n",
      "Data type 1000_0\n",
      "Test accuracy: 0.845\n",
      "-ed predicted: 0.98\n",
      "% of inflections (len 1) that match most popular in training 0.955\n",
      "\n",
      "Data type 1000_1\n",
      "Test accuracy: 0.83\n",
      "-ed predicted: 0.95\n",
      "% of inflections (len 1) that match most popular in training 0.945\n",
      "\n",
      "Data type 1000_2\n",
      "Test accuracy: 0.795\n",
      "-ed predicted: 0.95\n",
      "% of inflections (len 1) that match most popular in training 0.94\n",
      "\n",
      "Data type 1000_3\n",
      "Test accuracy: 0.865\n",
      "-ed predicted: 0.99\n",
      "% of inflections (len 1) that match most popular in training 0.99\n",
      "\n",
      "Data type 1000_4\n",
      "Test accuracy: 0.835\n",
      "-ed predicted: 0.975\n",
      "% of inflections (len 1) that match most popular in training 0.955\n",
      "\n",
      "Data type 1000_5\n",
      "Test accuracy: 0.84\n",
      "-ed predicted: 0.975\n",
      "% of inflections (len 1) that match most popular in training 0.975\n",
      "\n",
      "Data type 1000_6\n",
      "Test accuracy: 0.81\n",
      "-ed predicted: 0.975\n",
      "% of inflections (len 1) that match most popular in training 0.96\n",
      "\n",
      "Data type 1000_7\n",
      "Test accuracy: 0.86\n",
      "-ed predicted: 0.97\n",
      "% of inflections (len 1) that match most popular in training 0.945\n",
      "\n",
      "Data type 1000_8\n",
      "Test accuracy: 0.815\n",
      "-ed predicted: 0.985\n",
      "% of inflections (len 1) that match most popular in training 0.965\n",
      "\n",
      "Data type 1000_9\n",
      "Test accuracy: 0.82\n",
      "-ed predicted: 0.955\n",
      "% of inflections (len 1) that match most popular in training 0.955\n",
      "\n",
      "datasize, split, test_accuracy, ed_predicted, train_match\n",
      "100,0,0.015,0.745,0.53\n",
      "100,1,0.01,0.87,0.5\n",
      "100,2,0.005,0.86,0.57\n",
      "100,3,0.025,0.945,0.58\n",
      "100,4,0.015,0.81,0.54\n",
      "100,5,0.015,0.865,0.525\n",
      "100,6,0.005,0.905,0.56\n",
      "100,7,0.015,0.88,0.52\n",
      "100,8,0.0,0.945,0.565\n",
      "100,9,0.005,0.91,0.59\n",
      "200,0,0.065,0.945,0.665\n",
      "200,1,0.14,0.885,0.68\n",
      "200,2,0.195,0.93,0.75\n",
      "200,3,0.11,0.855,0.66\n",
      "200,4,0.195,0.93,0.75\n",
      "200,5,0.07,0.9,0.65\n",
      "200,6,0.165,0.9,0.75\n",
      "200,7,0.105,0.875,0.71\n",
      "200,8,0.055,0.885,0.605\n",
      "200,9,0.095,0.905,0.735\n",
      "400,0,0.535,0.93,0.855\n",
      "400,1,0.565,0.925,0.9\n",
      "400,2,0.495,0.955,0.885\n",
      "400,3,0.585,0.95,0.94\n",
      "400,4,0.59,0.96,0.895\n",
      "400,5,0.435,0.86,0.85\n",
      "400,6,0.44,0.96,0.89\n",
      "400,7,0.495,0.95,0.87\n",
      "400,8,0.53,0.975,0.885\n",
      "400,9,0.57,0.925,0.865\n",
      "600,0,0.735,0.975,0.925\n",
      "600,1,0.745,0.96,0.925\n",
      "600,2,0.77,0.98,0.94\n",
      "600,3,0.735,0.97,0.93\n",
      "600,4,0.75,0.98,0.93\n",
      "600,5,0.725,0.95,0.945\n",
      "600,6,0.695,0.97,0.96\n",
      "600,7,0.755,0.995,0.975\n",
      "600,8,0.68,0.985,0.95\n",
      "600,9,0.775,0.955,0.955\n",
      "800,0,0.765,0.95,0.935\n",
      "800,1,0.79,0.965,0.97\n",
      "800,2,0.765,0.955,0.95\n",
      "800,3,0.81,0.975,0.95\n",
      "800,4,0.78,0.965,0.95\n",
      "800,5,0.82,0.96,0.965\n",
      "800,6,0.795,0.965,0.945\n",
      "800,7,0.79,0.965,0.93\n",
      "800,8,0.775,0.98,0.955\n",
      "800,9,0.82,0.965,0.97\n",
      "1000,0,0.845,0.98,0.955\n",
      "1000,1,0.83,0.95,0.945\n",
      "1000,2,0.795,0.95,0.94\n",
      "1000,3,0.865,0.99,0.99\n",
      "1000,4,0.835,0.975,0.955\n",
      "1000,5,0.84,0.975,0.975\n",
      "1000,6,0.81,0.975,0.96\n",
      "1000,7,0.86,0.97,0.945\n",
      "1000,8,0.815,0.985,0.965\n",
      "1000,9,0.82,0.955,0.955\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "results = []\n",
    "for datasize in datasizes:\n",
    "  print('\\nData type', datasize)\n",
    "  datadir = f'drive/MyDrive/EnglishToleranceBaseline/processed_data/{datasize}'\n",
    "\n",
    "  train_tgt_lines = open(f'{datadir}/english-tgt-train.txt','r').read().splitlines()\n",
    "  train_src_lines = open(f'{datadir}/english-src-train.txt','r').read().splitlines()\n",
    "    \n",
    "  train_pairs = list(zip([t.replace(' ','') for t in train_src_lines], \n",
    "                        [t.replace(' ','') for t in train_tgt_lines]))\n",
    "\n",
    "  predlines = open(f'{outdir}/english-rnn-{datasize}-pred.txt','r').read().splitlines()\n",
    "  test_src_lines = open(f'{datadir}/english-src-test.txt','r').read().splitlines()\n",
    "  test_tgt_lines = open(f'{datadir}/english-tgt-test.txt','r').read().splitlines()\n",
    "  tups = list(zip(test_src_lines,test_tgt_lines,predlines))\n",
    "\n",
    "  r = []\n",
    "  for tst,tgt,pred in tups:\n",
    "    # get the learned inflection\n",
    "    gold, learned = tgt.strip(),pred.strip()\n",
    "    r.append((gold,learned,tst.strip()))\n",
    "\n",
    "  test_accuracy = sum([t[0]==t[1] for t in r])/len(r)\n",
    "  print('Test accuracy:', test_accuracy)\n",
    "  ed_predicted = sum([t[1][-1] in ['d','t'] and t[1]!=t[2] for t in r if t[1]])/len(r)\n",
    "  print('-ed predicted:', ed_predicted)\n",
    "  \n",
    "  # Frequency test\n",
    "  # For each of the verbs in the test set, get the original ending. Then, get the predictions for number of times that ending appears \n",
    "  c = 1\n",
    "  verbs_with_inflections_matching_train = [] \n",
    "  for i,verb in enumerate(test_src_lines):\n",
    "    verb = verb.replace(' ','')\n",
    "    ending = verb[-c:]\n",
    "    predicted_inflection = predlines[i].replace(' ','')[-c:]\n",
    "    train_pairs_with_same_ending = [(s,t) for s,t in train_pairs if s[-c:] == ending]\n",
    "    # get most frequent inflection for the train data verbs that share the ending\n",
    "    inflections = [t[-c:] for s,t in train_pairs_with_same_ending]\n",
    "    if not inflections: continue\n",
    "    popular_inflection = Counter(inflections).most_common(1)[0][0]\n",
    "    if predicted_inflection == popular_inflection:\n",
    "      verbs_with_inflections_matching_train.append((verb, predicted_inflection, popular_inflection))\n",
    "\n",
    "  train_match = len(verbs_with_inflections_matching_train)/len(test_src_lines)\n",
    "  print(f'% of inflections (len {c}) that match most popular in training', train_match)\n",
    "  size, split = tuple(datasize.split('_'))\n",
    "  results.append((size,split, test_accuracy, ed_predicted, train_match))\n",
    "\n",
    "print()\n",
    "print('datasize, split, test_accuracy, ed_predicted, train_match')\n",
    "for r in results:\n",
    "  print(','.join(str(x) for x in r))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CogSci - English Tolerance Baseline",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
